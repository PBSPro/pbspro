2016-07-18 16:55:08,359 INFO     input command: pbs_benchpress -f cgroups_tests.py -t CgroupsTests -o CgroupsTests.out
2016-07-18 16:55:08,359 INFO     param: None
2016-07-18 16:55:08,359 INFO     ptl version: 1.0.0
2016-07-18 16:55:08,359 INFO     platform: Linux centos7.usa.org 3.10.0-327.4.4.el7.x86_64 #1 SMP Tue Jan 5 16:07:00 UTC 2016 x86_64 x86_64
2016-07-18 16:55:08,359 INFO     python version: 2.7.5
2016-07-18 16:55:08,359 INFO     user: root
2016-07-18 16:55:08,360 INFO     --------------------------------------------------------------------------------
2016-07-18 16:55:08,360 INFO     Cleaning up temporary files
2016-07-18 16:55:08,371 INFO     ======================================================================
2016-07-18 16:55:08,371 INFO     suite name: CgroupsTests
2016-07-18 16:55:08,371 INFO     suite docstring: 

    This tests Linux Cgroups functionality.
    Date: July 8, 2016

    

2016-07-18 16:55:08,371 INFO     ======================================================================
2016-07-18 16:55:08,372 INFO     ================================
2016-07-18 16:55:08,372 INFO      Entered CgroupsTests setUpClass
2016-07-18 16:55:08,372 INFO     ================================
2016-07-18 16:55:08,372 INFOCLI2 centos7: id pbsuser
2016-07-18 16:55:08,376 INFOCLI2 centos7: id pbsuser1
2016-07-18 16:55:08,381 INFOCLI2 centos7: id pbsuser2
2016-07-18 16:55:08,386 INFOCLI2 centos7: id pbsuser3
2016-07-18 16:55:08,393 INFOCLI2 centos7: which cat
2016-07-18 16:55:08,407 INFO     server centos7: server operating mode set to cli
2016-07-18 16:55:08,407 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:08,460 INFO     server centos7: version 14.0.1
2016-07-18 16:55:08,460 INFO     expect action: created new action kicksched
2016-07-18 16:55:08,460 INFO     expect action: added action kicksched to server centos7
2016-07-18 16:55:08,494 INFOCLI2 centos7: sudo -H /usr/local/sbin/pbsfs
2016-07-18 16:55:08,509 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/resource_group
2016-07-18 16:55:08,521 INFOCLI  centos7: /usr/local/bin/qmgr -c list sched
2016-07-18 16:55:08,547 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/holidays
2016-07-18 16:55:08,575 INFO     server centos7: server operating mode set to cli
2016-07-18 16:55:08,575 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:08,630 INFO     server centos7: version 14.0.1
2016-07-18 16:55:08,636 INFO     =================================
2016-07-18 16:55:08,636 INFO     Completed CgroupsTests setUpClass
2016-07-18 16:55:08,636 INFO     =================================
2016-07-18 16:55:08,637 INFO     test name: test_t1 (tests.cgroups_tests.CgroupsTests)...
2016-07-18 16:55:08,637 INFO     test start time: Mon Jul 18 16:55:08 2016
2016-07-18 16:55:08,637 INFO     test docstring: 
 Test to verify that the job cgroup is created correctly
            Check to see that cpuset.cpus=0, cpuset.mems=0 and that
            memory.limit_in_bytes = 104857600
         
2016-07-18 16:55:08,637 INFO     ===========================
2016-07-18 16:55:08,637 INFO      Entered CgroupsTests setUp
2016-07-18 16:55:08,637 INFO     ===========================
2016-07-18 16:55:08,637 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:08,704 INFO     status on centos7: server
2016-07-18 16:55:08,704 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:08,795 INFO     manager on centos7: set server {'managers': (3, 'root@*')}
2016-07-18 16:55:08,796 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers-=root@*
2016-07-18 16:55:08,824 INFO     manager on centos7: set server {'managers': (2, 'root@*')}
2016-07-18 16:55:08,824 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers+=root@*
2016-07-18 16:55:08,848 INFO     server centos7: reverting configuration to defaults
2016-07-18 16:55:08,848 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:08,917 INFO     select on centos7: __ALL__
2016-07-18 16:55:08,918 INFOCLI  centos7: /usr/local/bin/qselect
2016-07-18 16:55:08,929 INFOCLI  centos7: /usr/local/bin/qstat -f @centos7.usa.org
2016-07-18 16:55:08,990 INFO     expect on server centos7: job_state set 0 job ...  OK
2016-07-18 16:55:08,991 INFOCLI  centos7: /usr/local/bin/pbs_rstat -f
2016-07-18 16:55:09,009 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:09,050 INFO     manager on centos7: delete hook t1_l1
2016-07-18 16:55:09,050 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c delete hook t1_l1
2016-07-18 16:55:09,106 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:55:09,130 INFO     expect on server centos7:  unset  hook t1_l1 ...  OK
2016-07-18 16:55:09,131 INFOCLI  centos7: /usr/local/bin/qstat -Qf @centos7.usa.org
2016-07-18 16:55:09,146 INFO     manager on centos7: delete queue workq
2016-07-18 16:55:09,146 INFOCLI  centos7: /usr/local/bin/qmgr -c delete queue workq
2016-07-18 16:55:09,174 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:55:09,189 INFO     expect on server centos7:  unset  queue workq ...  OK
2016-07-18 16:55:09,189 INFO     manager on centos7: create queue workq {'started': 'True', 'queue_type': 'Execution', 'enabled': 'True'}
2016-07-18 16:55:09,189 INFOCLI  centos7: /usr/local/bin/qmgr -c create queue workq started=True,queue_type=Execution,enabled=True
2016-07-18 16:55:09,222 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:55:09,242 INFO     expect on server centos7: started set True || queue_type set Execution || enabled set True queue workq ...  OK
2016-07-18 16:55:09,243 INFO     manager on centos7: set server {'log_events': '511', 'default_queue': 'workq'}
2016-07-18 16:55:09,243 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=511,default_queue=workq
2016-07-18 16:55:09,259 INFO     status on centos7: resource
2016-07-18 16:55:09,260 INFO     manager on centos7: list resource
2016-07-18 16:55:09,260 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource
2016-07-18 16:55:09,315 INFO     manager on centos7: delete resource nmics,ngpus
2016-07-18 16:55:09,315 INFOCLI  centos7: /usr/local/bin/qmgr -c delete resource nmics,ngpus
2016-07-18 16:55:09,379 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource nmics
2016-07-18 16:55:09,390 INFO     expect on server centos7:  unset  resource nmics ...  OK
2016-07-18 16:55:09,391 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource ngpus
2016-07-18 16:55:09,404 INFO     expect on server centos7:  unset  resource ngpus ...  OK
2016-07-18 16:55:09,404 INFOCLI  status on centos7: server license_count
2016-07-18 16:55:09,405 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:09,458 INFO     server: centos7.usa.org licensed
2016-07-18 16:55:09,484 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/server_priv/comm.lock
2016-07-18 16:55:09,516 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:55:09,532 INFO     scheduler centos7: reverting configuration to defaults
2016-07-18 16:55:09,533 INFO     manager on centos7: list sched
2016-07-18 16:55:09,533 INFOCLI  centos7: /usr/local/bin/qmgr -c list sched
2016-07-18 16:55:09,546 INFO     manager on centos7: unset sched ['sched_cycle_length']
2016-07-18 16:55:09,547 INFOCLI  centos7: /usr/local/bin/qmgr -c unset sched sched_cycle_length
2016-07-18 16:55:09,566 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/dedicated_time
2016-07-18 16:55:09,583 INFOCLI2 centos7: cmp /usr/local/etc/pbs_resource_group /var/spool/pbs/sched_priv/resource_group
2016-07-18 16:55:09,587 INFO     scheduler centos7: reverting holidays file to default
2016-07-18 16:55:09,588 INFOCLI2 centos7: cmp /usr/local/etc/pbs_holidays /var/spool/pbs/sched_priv/holidays
2016-07-18 16:55:09,593 INFOCLI2 centos7: cmp /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:09,597 INFOCLI2 centos7: which cp
2016-07-18 16:55:09,601 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:09,620 INFOCLI2 centos7: which chmod
2016-07-18 16:55:09,622 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:09,644 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:55:09,644 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:55:09,691 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:55:09,701 INFOCLI2 centos7: sudo -H cmp /usr/local/sbin/pbs_mom /usr/local/sbin/pbs_mom.cpuset
2016-07-18 16:55:09,735 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:55:09,763 INFOCLI2 centos7: sudo -H /usr/local/sbin/pbs_mom --version
2016-07-18 16:55:09,776 INFO     mom centos7: reverting configuration to defaults
2016-07-18 16:55:09,777 INFOCLI2 centos7: which rm
2016-07-18 16:55:09,780 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/epilogue
2016-07-18 16:55:09,802 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/prologue
2016-07-18 16:55:09,811 INFOCLI2 centos7: sudo -H ls /var/spool/pbs/mom_priv/config.d
2016-07-18 16:55:09,830 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsu5jO1F /var/spool/pbs/mom_priv/config
2016-07-18 16:55:09,852 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/mom_priv/config
2016-07-18 16:55:09,865 INFOCLI2 centos7: which chown
2016-07-18 16:55:09,868 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/mom_priv/config
2016-07-18 16:55:09,877 INFOCLI2 centos7: which chgrp
2016-07-18 16:55:09,881 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/mom_priv/config
2016-07-18 16:55:09,892 INFO     mom centos7: sent signal -HUP
2016-07-18 16:55:09,893 INFOCLI2 centos7: sudo -H kill -HUP 42976
2016-07-18 16:55:09,937 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:55:09,950 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 16:55:09,973 INFO     server centos7: server operating mode set to cli
2016-07-18 16:55:09,974 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7
2016-07-18 16:55:10,031 INFO     server centos7: version 14.0.1
2016-07-18 16:55:10,032 INFO     status on centos7: node centos7
2016-07-18 16:55:10,032 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:55:10,047 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:55:10,057 INFO     expect on server centos7: state = free node centos7 ...  OK
2016-07-18 16:55:10,057 INFO     ============================
2016-07-18 16:55:10,057 INFO     Completed CgroupsTests setUp
2016-07-18 16:55:10,058 INFO     ============================
2016-07-18 16:55:10,058 INFO     server centos7: server operating mode set to cli
2016-07-18 16:55:10,058 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:10,111 INFO     expect on server centos7: pbs_version ~ .*14.* server centos7.usa.org ...  OK
2016-07-18 16:55:10,111 INFO     manager on centos7: set server {'log_events': '2047'}
2016-07-18 16:55:10,111 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=2047
2016-07-18 16:55:10,137 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:10,193 INFO     expect on server centos7: log_events set 2047 server centos7.usa.org ...  OK
2016-07-18 16:55:10,194 INFO     scheduler centos7: config {'resources': 'ncpus,mem,host,vnode,vmem'}
2016-07-18 16:55:10,194 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /var/spool/pbs/sched_priv/sched_config /var/spool/pbs/sched_priv/sched_config.bak
2016-07-18 16:55:10,206 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsk2eQHx /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:10,237 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:10,252 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:10,269 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:10,285 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:55:10,285 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:55:10,296 INFO     scheduler centos7 log match: searching for "Error reading line" - No match
2016-07-18 16:55:11,298 INFO     mom centos7: config {'$min_check_poll': 8, '$clienthost': 'centos7.usa.org', '$max_check_poll': 12, '$logevent': '0xffffffff'}
2016-07-18 16:55:11,299 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbs5lBRuO /var/spool/pbs/mom_priv/config
2016-07-18 16:55:11,311 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/mom_priv/config
2016-07-18 16:55:11,321 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/mom_priv/config
2016-07-18 16:55:11,344 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/mom_priv/config
2016-07-18 16:55:11,354 INFO     mom centos7: sent signal -HUP
2016-07-18 16:55:11,354 INFOCLI2 centos7: sudo -H kill -HUP 42976
2016-07-18 16:55:11,364 INFO     manager on centos7 as root: create resource nmics {'flag': 'nh', 'type': 'long'}
2016-07-18 16:55:11,364 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource nmics flag=nh,type=long
2016-07-18 16:55:11,400 INFO     manager on centos7 as root: create resource ngpus {'flag': 'nh', 'type': 'long'}
2016-07-18 16:55:11,401 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource ngpus flag=nh,type=long
2016-07-18 16:55:11,438 INFO     Test Summary: test_t1_l1 tests "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" cgroup hook
2016-07-18 16:55:11,438 INFO     status on centos7: hook
2016-07-18 16:55:11,439 INFO     manager on centos7: list hook
2016-07-18 16:55:11,439 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:11,460 INFO     status on centos7: hook
2016-07-18 16:55:11,460 INFO     manager on centos7: list hook
2016-07-18 16:55:11,460 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:11,481 INFO     manager on centos7: create hook t1_l1
2016-07-18 16:55:11,481 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c create hook t1_l1
2016-07-18 16:55:11,503 INFO     manager on centos7: set hook t1_l1 {'freq': 2, 'enabled': 'True', 'event': '"execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"'}
2016-07-18 16:55:11,503 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set hook t1_l1 freq=2,enabled=True,event="execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"
2016-07-18 16:55:11,532 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:55:11,561 INFO     expect on server centos7: freq set 2 || enabled set True || event set "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" hook t1_l1 ...  OK
2016-07-18 16:55:11,562 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-python', 'input-file': '/tmp/PtlPbs08nKK0', 'content-encoding': 'default'}
2016-07-18 16:55:11,562 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-python default /tmp/PtlPbs08nKK0
2016-07-18 16:55:11,627 INFO     server centos7: imported hook body
---
#  Copyright (C) 2003-2016 Altair Engineering, Inc. All rights reserved.
#
#  ALTAIR ENGINEERING INC. Proprietary and Confidential. Contains Trade Secret
#  Information. Not for use or disclosure outside ALTAIR and its licensed
#  clients. Information contained herein shall not be decompiled, disassembled,
#  duplicated or disclosed in whole or in part for any purpose. Usage of the
#  software is only as explicitly permitted in the end user software license
#  agreement.
#
#  Copyright notice does not imply publication.

# Last Updated
# Date: 07-06-2016
#
# When soft_limit is true for memory, memsw represents the hard limit.
#
# The resources value in sched_config must contain entries for mem and
# vmem if those subsystems are enabled in the hook configuration file. The
# amount of resource requested will not be avaiable to the hook if they
# are not present.

# This hook handles all of the operations necessary for PBS to support
# cgroups on linux hosts that support them (kernel 2.6.28 and higher)
#
# This hook services the following events:
# - exechost_periodic
# - exechost_startup
# - execjob_attach
# - execjob_begin
# - execjob_end
# - execjob_epilogue
# - execjob_launch

# Imports from __future__ must happen first
from __future__ import with_statement

# Additional imports
import sys
import os
import errno
import signal
import subprocess
import re
import glob
import time
import string
import platform
import traceback
import copy
from stat import S_ISBLK, S_ISCHR
import operator
import pwd
import pbs
import fnmatch
import socket

try:
    import json
except:
    import simplejson as json

CGROUP_KILL_ATTEMPTS = 12

# Flag to turn off features not in 12.2.40X branch
CRAY_12_BRANCH = False


# ============================================================================
# Derived error classes
# ============================================================================

# Base class for errors fixable only by administrative action.


class AdminError(Exception):
    pass

# Base class for errors in processing, unknown cause.


class ProcessingError(Exception):
    pass

# Base class for errors fixable by the user.


class UserError(Exception):
    pass

# Errors in PBS job resource values.


class JobValueError(UserError):
    pass

# Errors when the cgroup is busy.


class CgroupBusyError(ProcessingError):
    pass

# Errors in configuring cgroup.


class CgroupConfigError(AdminError):
    pass

# Errors in configuring cgroup.


class CgroupLimitError(AdminError):
    pass

# Errors processing cgroup.


class CgroupProcessingError(ProcessingError):
    pass

# ============================================================================
# Utility functions
# ============================================================================

#
# FUNCTION caller_name
#
# Return the name of the calling function or method.
#


def caller_name():
    return str(sys._getframe(1).f_code.co_name)

#
# FUNCTION convert_size
#
# Convert a string containing a size specification (e.g. "1m") to a
# string using different units (e.g. "1024k").
#
# This function only interprets a decimal number at the start of the string,
# stopping at any unrecognized character and ignoring the rest of the string.
#
# When down-converting (e.g. MB to KB), all calculations involve integers and
# the result returned is exact. When up-converting (e.g. KB to MB) floating
# point numbers are involved. The result is rounded up. For example:
#
# 1023MB -> GB yields 1g
# 1024MB -> GB yields 1g
# 1025MB -> GB yields 2g  <-- This value was rounded up
#
# Pattern matching or conversion may result in exceptions.
#


def convert_size(value, units='b'):
    logs = {'b': 0, 'k': 10, 'm': 20, 'g': 30,
            't': 40, 'p': 50, 'e': 60, 'z': 70, 'y': 80}
    try:
        new = units[0].lower()
        if new not in logs.keys():
            new = 'b'
        val, old = re.match('([-+]?\d+)([bkmgtpezy]?)',
                            str(value).lower()).groups()
        val = int(val)
        if val < 0:
            raise ValueError('Value may not be negative')
        if old not in logs.keys():
            old = 'b'
        factor = logs[old] - logs[new]
        val *= 2 ** factor
        slop = val - int(val)
        val = int(val)
        if slop > 0:
            val += 1
        # pbs.size() does not like units following zero
        if val <= 0:
            return '0'
        else:
            return str(val) + new
    except:
        return None

#
# FUNCTION size_as_int
#
# Convert a size string to an integer representation of size in bytes
#


def size_as_int(value):
    return int(convert_size(value).rstrip(string.ascii_lowercase))

#
# FUNCTION convert_time
#
# Converts a integer value for time into the value of the return unit
#
# A valid decimal number, with optional sign, may be followed by a character
# representing a scaling factor.  Scaling factors may be either upper or
# lower case. Examples include:
# 250ms
# 40s
# +15min
#
# Valid scaling factors are:
# ns  = 10**-9
# us  = 10**-6
# ms  = 10**-3
# s   =      1
# min =     60
# hr  =   3600
#
# Pattern matching or conversion may result in exceptions.
#


def convert_time(value, return_unit='s'):
    multipliers = {'':  1, 'ns': 10 ** -9, 'us': 10 ** -6,
                   'ms': 10 ** -3, 's': 1, 'min': 60, 'hr': 3600}
    n, factor = re.match('([-+]?\d+)(\w*[a-zA-Z])?',
                         str(value).lower()).groups(0)

    # Check to see if there was not unit of time specified
    if factor == 0:
        factor = ''

    # Check to see if the unit is valid
    if str.lower(factor) not in multipliers.keys():
        raise ValueError('Time unit not recognized.')

    # Convert the value to seconds
    value_in_sec = float(n) * float(multipliers[str.lower(factor)])
    if return_unit != 's':
        return value_in_sec / multipliers[str.lower(return_unit)]
    else:
        return float('%lf' % value_in_sec)

# simplejson hook to convert lists from unicode to utf-8


def decode_list(data):
    rv = []
    for item in data:
        if isinstance(item, unicode):
            item = item.encode('utf-8')
        elif isinstance(item, list):
            item = decode_list(item)
        elif isinstance(item, dict):
            item = decode_dict(item)
        rv.append(item)
    return rv

# simplejson hook to convert dictionaries from unicode to utf-8


def decode_dict(data):
    rv = {}
    for key, value in data.iteritems():
        if isinstance(key, unicode):
            key = key.encode('utf-8')
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        elif isinstance(value, list):
            value = decode_list(value)
        elif isinstance(value, dict):
            value = decode_dict(value)
        rv[key] = value
    return rv

# Convert CPU list format (with ranges) to a Python list


def cpus2list(s):
    # The input string is a comma separated list of digits and ranges.
    # Examples include:
    # 0-3,8-11
    # 0,2,4,6
    # 2,5-7,10
    cpus = []
    if s.strip() == '':
        return cpus
    for r in s.split(','):
        if '-' in r[1:]:
            start, end = r.split('-', 1)
            for i in range(int(start), int(end) + 1):
                cpus.append(int(i))
        else:
            cpus.append(int(r))
    return cpus


# Helper function to ignore suspended jobs when removing
# "already used" resources
def job_to_be_ignored(jobid):
    # Get job substate from small utility that calls printjob
    pbs_exec = ''
    if not CRAY_12_BRANCH:
        pbs_conf = pbs.get_pbs_conf()
        pbs_exec = pbs_conf['PBS_EXEC']
    else:
        if 'PBS_EXEC' in self.cfg:
            pbs_exec = self.cfg['PBS_EXEC']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "PBS_EXEC needs to be defined in the config file")
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Exiting the cgroups hook")
            pbs.accept()

    printjob_cmd = pbs_exec+os.sep+'bin'+os.sep+'printjob'

    cmd = [printjob_cmd, jobid]

    substate = None
    try:
        pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
        # Collect the job substate information
        process = subprocess.Popen(
                                   cmd,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        (out, err) = process.communicate()
        # Find the job substate
        substate_re = re.compile(r"substate:\s+(?P<jobstate>\S+)\s+")
        substate = substate_re.search(out)
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Unexpected error in job_to_be_ignored: %s" %
                   ' '.join([repr(sys.exc_info()[0]),
                            repr(sys.exc_info()[1])]))
        out = "Unknown: failed to run " + printjob_cmd

    if substate is not None:
        substate = substate.group().split(':')[1].strip()
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Job %s has substate %s" % (jobid, substate))

        suspended_substates = ['0x2b', '0x2d', 'unknown']
        return (substate in suspended_substates)
    else:
        return False

# ============================================================================
# Utility classes
# ============================================================================

#
# CLASS HookUtils
#


class HookUtils:

    def __init__(self, hook_events=None):
        if hook_events is not None:
            self.hook_events = hook_events
        else:
            self.hook_events = {
                # Defined in the order they appear in module_pbs_v1.c
                pbs.QUEUEJOB: {
                    'name': 'queuejob',
                    'handler': None
                },
                pbs.MODIFYJOB: {
                    'name': 'modifyjob',
                    'handler': None
                },
                pbs.RESVSUB: {
                    'name': 'resvsub',
                    'handler': None
                },
                pbs.MOVEJOB: {
                    'name': 'movejob',
                    'handler': None
                },
                pbs.RUNJOB: {
                    'name': 'runjob',
                    'handler': None
                },
                pbs.PROVISION: {
                    'name': 'provision',
                    'handler': None
                },
                pbs.EXECJOB_BEGIN: {
                    'name': 'execjob_begin',
                    'handler': self.__execjob_begin_handler
                },
                pbs.EXECJOB_PROLOGUE: {
                    'name': 'execjob_prologue',
                    'handler': None
                },
                pbs.EXECJOB_EPILOGUE: {
                    'name': 'execjob_epilogue',
                    'handler': self.__execjob_epilogue_handler
                },
                pbs.EXECJOB_PRETERM: {
                    'name': 'execjob_preterm',
                    'handler': None
                },
                pbs.EXECJOB_END: {
                    'name': 'execjob_end',
                    'handler': self.__execjob_end_handler
                },
                pbs.EXECJOB_LAUNCH: {
                    'name': 'execjob_launch',
                    'handler': self.__execjob_launch_handler
                },
                pbs.EXECHOST_PERIODIC: {
                    'name': 'exechost_periodic',
                    'handler': self.__exechost_periodic_handler
                },
                pbs.EXECHOST_STARTUP: {
                    'name': 'exechost_startup',
                    'handler': self.__exechost_startup_handler
                },
                pbs.MOM_EVENTS: {
                    'name': 'mom_events',
                    'handler': None
                },
            }

            if not CRAY_12_BRANCH:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "CRAY 12 Branch: %s" % (CRAY_12_BRANCH))
                self.hook_events[pbs.EXECJOB_ATTACH] = {
                    'name': 'execjob_attach',
                    'handler': self.__execjob_attach_handler
                }

    def __repr__(self):
        return "HookUtils(%s)" % (repr(self.hook_events))

    def event_name(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['name']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Type: %s not found" % (caller_name(), type))
            return None

    def hashandler(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['handler'] is not None
        else:
            return None

    def invoke_handler(self, event, cgroup, jobutil, *args):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: UID: real=%d, effective=%d" %
                   (caller_name(), os.getuid(), os.geteuid()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: GID: real=%d, effective=%d" %
                   (caller_name(), os.getgid(), os.getegid()))
        if self.hashandler(event.type):
            result = self.hook_events[event.type][
                'handler'](event, cgroup, jobutil, *args)
            return result
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: %s event not handled by this hook." %
                       (caller_name(), self.event_name(event.type)))

    def __execjob_begin_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Instantiate the NodeConfig class for get_memory_on_node and
        # get_vmem_on node
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Host assigned job resources: %s" %
                   (caller_name(), jobutil.host_resources))

        # Make sure the parent cgroup directories exist
        cgroup.create_paths()

        # Make sure that any old cgroups created in an earlier attempt
        # at creating or running the job are gone
        cgroup.delete(e.job.id)

        # Gather the assigned resources
        cgroup.gather_assigned_resources()
        # Create the cgroup(s) for the job
        cgroup.create_job(e.job.id, node)
        # Configure the new cgroup
        cgroup.configure_job(e.job.id, jobutil.host_resources, node)
        # Initialize resource usage for the job
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        # Write out the assigned resources
        cgroup.write_out_cgroup_host_assigned_resources(e.job.id)
        # Write out the environment variable for the host (pbs_attach)
        if 'devices_name' in cgroup.host_assigned_resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Devices: %s" %
                       cgroup.host_assigned_resources['devices_name'])
            env_list = list()
            if len(cgroup.host_assigned_resources['devices_name']) > 0:
                line = str()
                mics = list()
                gpus = list()
                for key in cgroup.host_assigned_resources['devices_name']:
                    if key.startswith('mic'):
                        mics.append(key[3:])
                    elif key.startswith('nvidia'):
                        gpus.append(key[6:])
                if len(mics) > 0:
                    env_list.append('OFFLOAD_DEVICES="%s"' %
                                    string.join(mics, ','))
                if len(gpus) > 0:
                    # don't put quotes around the values. ex "0" or "0,1".
                    # This will cause it to fail.
                    env_list.append('CUDA_VISIBLE_DEVICES=%s' %
                                    string.join(gpus, ','))

            pbs.logmsg(pbs.EVENT_DEBUG, "ENV_LIST: %s" % env_list)
            cgroup.write_out_cgroup_host_job_env_file(e.job.id, env_list)
        return True

    def __execjob_epilogue_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # The resources_used information has a base type of pbs_resource
        # Set the usage data
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        return True

    def __execjob_end_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Delete the cgroup(s) for the job
        cgroup.delete(e.job.id)
        # Remove the host_assigned_resources and job_env file
        for filename in [cgroup.hook_storage_dir+os.sep+e.job.id,
                         cgroup.host_job_env_filename % e.job.id]:
            try:
                os.remove(filename)
            except OSError:
                pbs.logmsg(pbs.EVENT_DEBUG3, "File: %s not found" % (filename))
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Error removing file: %s" % (filename))
                pass

        return True

    def __execjob_launch_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Add the parent process id to the appropriate cgroups.
        cgroup.add_pids(os.getppid(), jobutil.job.id)
        # FUTURE: Add environment variable to the job environment
        # if job requested mic or gpu
        cgroup.read_in_cgroup_host_assigned_resources(e.job.id)
        if cgroup.host_assigned_resources is not None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "host_assigned_resources: %s" %
                       (cgroup.host_assigned_resources))
            cgroup.setup_job_devices_env()
        return True

    def __exechost_periodic_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Cleanup cgroups for jobs not present on this node
        count = cgroup.cleanup_orphans(e.job_list)
        if ('periodic_resc_update' in cgroup.cfg and
                cgroup.cfg['periodic_resc_update']):
            for job in e.job_list.keys():
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: job is %s" %
                           (caller_name(), job))
                cgroup.update_job_usage(job, e.job_list[job].resources_used)
        # Online nodes that were offlined due to a cgroup not cleaning up
        if count == 0 and cgroup.cfg['online_offlined_nodes']:
            vnode = pbs.event().vnode_list[cgroup.hostname]
            if os.path.isfile(cgroup.offline_file):
                line = 'Orphan cgroup(s) have been cleaned up. '

                # Check with the pbs server to see if the node comment matches
                try:
                    tmp_comment = pbs.server().vnode(cgroup.hostname).comment
                except:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Unable to contact server for node comment")
                    tmp_comment = None
                if tmp_comment == cgroup.offline_msg:
                    line += 'Will bring the node back online'
                    vnode.state = pbs.ND_FREE
                    vnode.comment = None
                else:
                    line += 'However, the comment has changed since the node '
                    line += 'was offlined. Node will remain offline'

                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: %s" % (caller_name(), line))
                # Remove file
                os.remove(cgroup.offline_file)
        return True

    def __exechost_startup_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        cgroup.create_paths()
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))

        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        node.create_vnodes()
        if 'memory' in cgroup.subsystems:
            val = node.get_memory_on_node(cgroup.cfg)
            if val is not None:
                if 'vnode_per_numa_node' in cgroup.cfg:
                    if not cgroup.cfg['vnode_per_numa_node']:
                        hn = node.hostname
                        e.vnode_list[hn].resources_available['mem'] = \
                            pbs.size(val)
                        val_cpus = node.totalcpus
                        if val_cpus is not None:
                            e.vnode_list[hn].resources_available['ncpus'] = \
                                int(val_cpus)
                cgroup.set_node_limit('mem', val)
        if 'memsw' in cgroup.subsystems:
            val = node.get_vmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['vmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('vmem', val)
        if 'hugetlb' in cgroup.subsystems:
            val = node.get_hpmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['hpmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('hpmem', val)
        return True

    def __execjob_attach_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logjobmsg(jobutil.job.id, "%s: Attaching PID %s" %
                      (caller_name(), e.pid))
        # Add the job process id to the appropriate cgroups.
        cgroup.add_pids(e.pid, jobutil.job.id)
        return True

#
# CLASS ShallIRunUtils
#


class ShallIRunUtils:

    def __init__(self, hostname, cfg):
        self.cfg = cfg
        self.hostname = hostname

    def allow_users(self, allow_users):
        """ This still needs to be defined """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pass

    def exclude_hosts(self, exclude_hosts):
        """
        Gets the hostname for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        hostname = pbs.get_local_nodename()
        # check to see if hostname is in the exclude_hosts list
        if self.hostname in exclude_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: exclude host %s is in %s" %
                       (caller_name(), self.hostname, exclude_hosts))
            pbs.event().accept()

        # check to see if it regex syntax is used
        pass

    def exclude_vntypes(self, exclude_vntypes):
        """
        Gets the vntype for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        vntype = None

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'
        if os.path.isfile(vntype_file):
            fdata = open(vntype_file).readlines()
            vntype = fdata[0].strip()
            pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
        if vntype is None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype is set to %s" % (vntype))
        elif vntype in exclude_vntypes:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s is in %s" % (vntype, exclude_vntypes))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Exiting Hook")
            pbs.event().accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s not in global exclude: %s" %
                       (vntype, exclude_vntypes))
        pass

    def run_only_on_hosts(self, approved_hosts):
        """
        Gets the list of approved nodes to run on and checks to see if the hook
        should run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Approved hosts: %s, %s" %
                   (type(approved_hosts), approved_hosts))

        # check to see if hostname is in the approved_hosts list
        if approved_hosts == []:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "approved hosts list is empty: %s" % approved_hosts)
        elif self.hostname not in approved_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s is not in the approved list of hosts: %s" %
                       (self.hostname, approved_hosts))
            pbs.event().accept()

        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s in list of approved hosts: %s" %
                       (self.hostname, approved_hosts))

#
# CLASS JobUtils
#


class JobUtils:

    def __init__(self, job, hostname=None, resources=None):
        self.job = job
        self.host_vnodes = list()
        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if resources is not None:
            self.host_resources = resources
        else:
            self.host_resources = self.__host_assigned_resources()

    def __repr__(self):
        return "JobUtils(%s, %s, %s)" % (repr(self.job), repr(self.hostname),
                                         repr(self.host_resources))

    # Return a dictionary of resources assigned to the local node
    def __host_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Bail out if no hostname was provided
        if self.hostname is None:
            raise CgroupProcessingError('No hostname available')
        # Bail out if no job information is present
        if self.job is None:
            raise CgroupProcessingError('No job information available')
        # Determine which vnodes are on this host, if any
        if self.hostname is not None:
            vnhost_pattern = "%s\[[\d+]\]" % self.hostname
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnhost pattern: %s" %
                       (caller_name(), vnhost_pattern))
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job exec_vnode list: %s" %
                       (caller_name(), self.job.exec_vnode))
            for match in re.findall(vnhost_pattern, str(self.job.exec_vnode)):
                self.host_vnodes.append(match)
            if self.host_vnodes is not None:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Associate %s to vnodes on %s" %
                           (caller_name(), self.host_vnodes, self.hostname))

        # Collect host assigned resources
        resources = {}
        for chunk in self.job.exec_vnode.chunks:
            vnode = False
            if self.host_vnodes == [] and chunk.vnode_name != self.hostname:
                continue
            elif self.host_vnodes != [] and \
                    chunk.vnode_name not in self.host_vnodes:
                continue
            elif chunk.vnode_name in self.host_vnodes:
                vnode = True
                if 'vnodes' not in resources:
                    resources['vnodes'] = {}
                if chunk.vnode_name not in resources['vnodes']:
                    resources['vnodes'][chunk.vnode_name] = {}
                # This check is needed since not all resources are
                # required to be in each chunk of a job
                # i.e. exec_vnodes =
                # (node1[0]:ncpus=4:mem=4gb+node1[1]:mem=2gb) +
                # (node1[1]:ncpus=3+node[0]:ncpus=1:mem=4gb)
                if all(resc in resources['vnodes'][chunk.vnode_name]
                       for resc in chunk.chunk_resources.keys()):
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: resources already defined" %
                               (caller_name()))
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s,\t%s" %
                               (caller_name(),
                                chunk.chunk_resources.keys(),
                                resources['vnodes'][chunk.vnode_name].keys()))
                else:
                    # Initialize resources for each vnode
                    for resc in chunk.chunk_resources.keys():
                        # Initialize the new value
                        if isinstance(chunk.chunk_resources[resc],
                                      pbs.pbs_int):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_int(0)
                        elif isinstance(chunk.chunk_resources[resc],
                                        pbs.pbs_float):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_float(0)
                        elif isinstance(chunk.chunk_resources[resc], pbs.size):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.size('0')

            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Chunk %s" %
                       (caller_name(), chunk.vnode_name))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resources: %s" %
                       (caller_name(), resources))
            for resc in chunk.chunk_resources.keys():
                if resc not in resources.keys():
                    # Initialize the new value
                    if isinstance(chunk.chunk_resources[resc], pbs.pbs_int):
                        resources[resc] = pbs.pbs_int(0)
                    elif isinstance(chunk.chunk_resources[resc],
                                    pbs.pbs_float):
                        resources[resc] = pbs.pbs_float(0.0)
                    elif isinstance(chunk.chunk_resources[resc], pbs.size):
                        resources[resc] = pbs.size('0')
                # Add resource value to total
                if type(chunk.chunk_resources[resc]) in \
                        [pbs.pbs_int, pbs.pbs_float, pbs.size]:
                    resources[resc] += chunk.chunk_resources[resc]
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s: resources[%s][%s] is now %s" %
                               (caller_name(), self.hostname, resc,
                                resources[resc]))
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] += \
                                  chunk.chunk_resources[resc]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Setting resource %s to string %s" %
                               (caller_name(), resc,
                                str(chunk.chunk_resources[resc])))
                    resources[resc] = str(chunk.chunk_resources[resc])
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] = \
                                  str(chunk.chunk_resources[resc])
        if not resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: No resources assigned to host %s" %
                       (caller_name(), self.hostname))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources for %s: %s" %
                       (caller_name(), self.hostname, repr(resources)))

        # Return assigned resources for specified host
        pbs.logmsg(pbs.EVENT_DEBUG3, "Job Resources: %s" % (resources))
        return resources

    # Write a message to the job stdout file
    def write_to_stderr(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stderr_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

    # Write a message to the job stdout file
    def write_to_stdout(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stdout_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

#
# CLASS NodeConfig
#


class NodeConfig:

    def __init__(self, cfg, **kwargs):

        self.cfg = cfg
        hostname = None
        meminfo = None
        numa_nodes = None
        devices = None
        hyperthreading = None
        self.hyperthreads_per_core = 1
        self.totalcpus = self.__discover_cpus()

        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        self.host_mom_jobdir = pbs_home+'/mom_priv/jobs'

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'hostname':
                    hostname = val
                elif arg == 'meminfo':
                    meminfo = val
                elif arg == 'numa_nodes':
                    numa_nodes = val
                elif arg == 'devices':
                    devices = val
                elif arg == 'hyperthreading':
                    hyperthreading = val

        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if meminfo is not None:
            self.meminfo = meminfo
        else:
            self.meminfo = self.__discover_meminfo()
        if numa_nodes is not None:
            self.numa_nodes = numa_nodes
        else:
            self.numa_nodes = self.__discover_numa_nodes()
        if devices is not None:
            self.devices = devices
        else:
            self.devices = self.__discover_devices()
        if hyperthreading is not None:
            self.hyperthreading = hyperthreading
        else:
            self.hyperthreading = self.__hyperthreading_enabled()

        # Add the devices count i.e. nmics and ngpus to the numa nodes
        self.__add_devices_to_numa_node()

    def __repr__(self):
        return ("NodeConfig(%s, %s, %s, %s, %s, %s)" %
                (repr(self.cfg), repr(self.hostname), repr(self.meminfo),
                 repr(self.numa_nodes), repr(self.devices),
                 repr(self.hyperthreading)))

    def __add_devices_to_numa_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (self.devices.keys()))
        for device in self.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" % (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" %
                           (self.devices[device].keys()))
                for device_name in self.devices[device]:
                    device_socket = \
                        self.devices[device][device_name]['numa_node']
                    if device == 'mic':
                        if 'nmics' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['nmics'] = 1
                        else:
                            self.numa_nodes[device_socket]['nmics'] += 1
                    elif device == 'gpu':
                        if 'ngpus' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['ngpus'] = 1
                        else:
                            self.numa_nodes[device_socket]['ngpus'] += 1

        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (self.numa_nodes))

    # Discover what type of hardware is on this node and how is it partitioned
    def __discover_numa_nodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        numa_nodes = {}
        for dir in glob.glob(os.path.join(os.sep,
                             "sys", "devices", "system", "node", "node*")):
            id = int(dir.split(os.sep)[5][4:])
            if id not in numa_nodes.keys():
                numa_nodes[id] = {}
                numa_nodes[id]['devices'] = list()
            numa_nodes[id]['cpus'] = open(os.path.join(dir, "cpulist"),
                                          'r').readline().strip()
            with open(os.path.join(dir, "meminfo"), 'r') as fd:
                for line in fd:
                    # Each line will contain four or five fields. Examples:
                    # Node 0 MemTotal:       32995028 kB
                    # Node 0 HugePages_Total:     0
                    entries = line.split()
                    if len(entries) < 4:
                        continue
                    if entries[2] == "MemTotal:":
                        numa_nodes[id][entries[2].rstrip(':')] = \
                            convert_size(entries[3] + entries[4], 'kb')
                    elif entries[2] == "HugePages_Total:":
                        numa_nodes[id][entries[2].rstrip(':')] = entries[3]
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover Numa Nodes: %s" % numa_nodes)
        return numa_nodes

    # Identify devices and to which numa nodes they are attached
    def __discover_devices(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        devices = {}
        for file in glob.glob(os.path.join(os.sep, "sys", "class", "*", "*",
                                           "device", "numa_node")):
            # The file should contain a single integer
            numa_node = int(open(file, 'r').readline().strip())
            if numa_node < 0:
                numa_node = 0
            dirs = file.split(os.sep)
            name = dirs[3]
            instance = dirs[4]
            if name not in devices.keys():
                devices[name] = {}
            devices[name][instance] = {}
            devices[name][instance]['numa_node'] = numa_node
            if name == 'mic':
                s = os.stat('/dev/%s' % instance)
                devices[name][instance]['major'] = os.major(s.st_rdev)
                devices[name][instance]['minor'] = os.minor(s.st_rdev)
                devices[name][instance]['device_type'] = 'c'

        # Check to see if there are gpus on the node
        gpus = self.discover_gpus()
        if isinstance(gpus, dict) and len(gpus.keys()) > 0:
            devices['gpu'] = gpus['gpu']

        pbs.logmsg(pbs.EVENT_DEBUG3, "Discovered devices: %s" % (devices))
        return devices

    def discover_gpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Check to see if nvidia-smi exists and produces valid output
            cmd = ['/usr/bin/nvidia-smi', '-q', '-x']
            if 'nvidia-smi' in self.cfg:
                cmd[0] = self.cfg['nvidia-smi']

            pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
            # Collect the gpu information
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.PIPE)
            output, err = process.communicate()

            # pbs.logmsg(pbs.EVENT_DEBUG3,"output: %s" % output)
            # import the xml library and parse the output
            import xml.etree.ElementTree as ET

            # Parse the data
            name = 'gpu'
            gpu_data = dict()
            gpu_data[name] = {}
            root = ET.fromstring(output)
            pbs.logmsg(pbs.EVENT_DEBUG3, "root.tag: %s" % root.tag)
            for child in root:
                if child.tag == "attached_gpus":
                    # gpu_data['ngpus'] = int(child.text)
                    pass
                if child.tag == "gpu":
                    device_id = child.get('id')
                    number = 'nvidia%s' % child.find('minor_number').text
                    gpu_data[name][number] = dict()
                    gpu_data[name][number]['id'] = device_id

                    # Determine which socket the device is on
                    filename = '/sys/bus/pci/devices/%s/numa_node' % \
                        device_id.lower()
                    isfile = os.path.isfile(filename)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s, %s" % (filename, isfile))
                    if isfile:
                        numa_node = open(filename).read().strip()
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "numa_node: %s" % (numa_node))
                        if int(numa_node) == -1:
                            numa_node = 0
                        gpu_data[name][number]['numa_node'] = int(numa_node)
                        try:
                            dev_filename = '/dev/%s' % number
                            s = os.stat(dev_filename)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find %s" % dev_filename)
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                        gpu_data[name][number]['major'] = os.major(s.st_rdev)
                        gpu_data[name][number]['minor'] = os.minor(s.st_rdev)
                        gpu_data[name][number]['device_type'] = 'c'
            pbs.logmsg(pbs.EVENT_DEBUG, "%s" % gpu_data)
            return gpu_data
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unable to find %s" % string.join(cmd, " "))
            return {'gpu': {}}
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        return {'gpu': {}}

    # Get the memory info on this host
    def __discover_meminfo(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        meminfo = {}
        with open(os.path.join(os.sep, "proc", "meminfo"), 'r') as fd:
            for line in fd:
                entries = line.split()
                if entries[0] == "MemTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "SwapTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "Hugepagesize:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "HugePages_Total:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
                elif entries[0] == "HugePages_Rsvd:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover meminfo: %s" % meminfo)
        return meminfo

    # Determine if the cpus have hyperthreading enabled
    def __hyperthreading_enabled(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            siblings = 0
            cpu_cores = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "siblings":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    siblings = entries[1].strip()
                elif entries[0].strip() == "cpu cores":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_cores = entries[1].strip()
                elif entries[0].strip() == "flags":
                    flag_options = entries[1].split()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: flag_options: %s" %
                               (caller_name(), flag_options))
                    if "ht" in flag_options:
                        rc = True
                        break
        if rc is True:
            self.hyperthreads_per_core = int(siblings)/int(cpu_cores)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: hyperthreads/core: %d" %
                       (caller_name(), self.hyperthreads_per_core))
            if self.hyperthreads_per_core == 1:
                rc = False
        return rc

    def __discover_cpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            cpu_threads = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "processor":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_threads += 1
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: processors found: %d" %
                   (caller_name(), cpu_threads))

        return cpu_threads

    # Gather the jobs running on this node
    def gather_jobs_on_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        # Get the job list from the server
        jobs = None
        try:
            jobs = pbs.server().vnode(self.hostname).jobs
        except:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Unable to contact server to get jobs")
            return None

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs from server for %s: %s" % (self.hostname, jobs))

        if jobs is None:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "No jobs found on %s " % (self.hostname))
            return list()

        jobs_list = dict()
        for job in jobs.split(','):
            # The job list from the node has a space in front of the job id
            # This must be removed or it will not match the cgroup dir
            jobs_list[job.split('/')[0].strip()] = 0
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs on %s: %s" % (self.hostname, jobs_list.keys()))

        return jobs_list.keys()

    # Gather the jobs running on this node
    def gather_jobs_on_node_local(self):
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Method called" % (caller_name()))
        # Get the job list from the jobs directory
        jobs_list = list()
        try:
            for file in os.listdir(self.host_mom_jobdir):
                if fnmatch.fnmatch(file, "*.JB"):
                    jobs_list.append(file[:-3])
            if not jobs_list:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "No jobs found on %s " % (self.hostname))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Jobs from server for %s: %s" %
                           (self.hostname, repr(jobs_list)))

            return jobs_list
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Could not get job list from mom_priv/jobs for %s" %
                       self.hostname)

            return self.gather_jobs_on_node()

    # Get the memory resource on this mom
    def get_memory_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Calculate memory
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
            pbs.logmsg(pbs.EVENT_DEBUG3, "val: %s" % val)
        else:
            val = size_as_int(MemTotal)
        if 'percent_reserve' in config['cgroup']['memory']:
            percent_reserve = config['cgroup']['memory']['percent_reserve']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memory'].keys():
            reserve_memory = config['cgroup']['memory']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                reserve_mem = size_as_int(reserve_memory)
                if val > reserve_mem:
                    val -= reserve_mem
                else:
                    val -= size_as_int("100mb")
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Unable to reserve more memory than " +
                               "available on the node. Available %s, " +
                               "Tried to reserve %s. Reserving 100mb" %
                               (val, reserve_mem))

            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))

        pbs.logmsg(pbs.EVENT_DEBUG3, "Return val: %s" % val)
        return convert_size(str(val), 'kb')

    # Get the virtual memory resource on this mom
    def get_vmem_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Return the value for memory if no swap is configured
        if size_as_int(self.meminfo['SwapTotal']) <= 0:
            return self.get_memory_on_node(config)
        # Calculate vmem
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
        else:
            val = size_as_int(MemTotal)

        val += size_as_int(self.meminfo['SwapTotal'])
        # Determine if memory needs to be reserved
        if 'percent_reserve' in config['cgroup']['memsw']:
            percent_reserve = config['cgroup']['memsw']['reserve_memory']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memsw']:
            reserve_memory = config['cgroup']['memsw']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                val -= size_as_int(reserve_memory)
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))
        return convert_size(str(val), 'kb')

    # Get the huge page memory resource on this mom
    def get_hpmem_on_node(self, config):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        if 'percent_reserve' in config['cgroup']['hugetlb'].keys():
            percent_reserve = config['cgroup']['hugetlb']['percent_reserve']
        else:
            percent_reserve = 0
        # Calculate vmem
        val = size_as_int(self.meminfo['Hugepagesize'])
        val *= (self.meminfo['HugePages_Total'] -
                self.meminfo['HugePages_Rsvd'])
        val -= (val * percent_reserve) / 100
        return convert_size(str(val), 'kb')

    # Create individual vnodes per socket
    def create_vnodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        key = "vnode_per_numa_node"
        if key in self.cfg.keys():
            if not self.cfg[key]:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s is false" %
                           (caller_name(), key))
                return
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s not configured" %
                       (caller_name(), key))
            return

        # Create one vnode per numa node
        vnode_list = pbs.event().vnode_list
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: numa nodes: %s" %
                   (caller_name(), self.numa_nodes))
        # Define the vnodes
        vnode_name = self.hostname
        vnode_list[vnode_name] = pbs.vnode(vnode_name)
        for id in self.numa_nodes.keys():
            vnode_name = self.hostname + "[%d]" % id
            vnode_list[vnode_name] = pbs.vnode(vnode_name)
            for key, val in sorted(self.numa_nodes[id].iteritems()):
                if key == 'cpus':
                    vnode_list[vnode_name].resources_available['ncpus'] = \
                        len(cpus2list(val))/self.hyperthreads_per_core
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['ncpus'] = 0
                elif key == 'MemTotal':
                    mem = self.get_memory_on_node(self.cfg, val)
                    vnode_list[vnode_name].resources_available['mem'] = \
                        pbs.size(mem)
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['mem'] = \
                        pbs.size('0kb')
                elif key == 'HugePages_Total':
                    pass
                elif isinstance(val, list):
                    pass
                elif isinstance(val, dict):
                    pass
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s=%s" %
                               (caller_name(), key, val))
                    vnode_list[vnode_name].resources_available[key] = val

                    if isinstance(val, int):
                        vnode_list[self.hostname].resources_available[key] = 0
                    elif isinstance(val, float):
                        vnode_list[self.hostname].resources_available[key] = \
                            0.0
                    elif isinstance(val, pbs.size):
                        vnode_list[self.hostname].resources_available[key] = \
                            pbs.size('0kb')
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnode list: %s" %
                   (caller_name(), vnode_list))
        return True

#
# CLASS CgroupUtils
#


class CgroupUtils:
    def __init__(self, hostname, vnode, **kwargs):
        cfg = None
        subsystems = None
        paths = None
        vntype = None
        event = None
        self.assigned_resources = dict()
        self.existing_cgroups = dict()
        self.host_assigned_resources = None
        self.pid_lower_limit = 3

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'subsystems':
                    subsystems = val
                elif arg == 'paths':
                    paths = val
                elif arg == 'vntype':
                    vntype = val
                elif arg == 'event':
                    event = val

        try:
            self.hostname = hostname
            self.vnode = vnode
            # __check_os will raise an exception if cgroups are not present
            self.__check_os()
            # Read in the config file
            if cfg is not None:
                self.cfg = cfg
            else:
                self.cfg = self.__parse_config_file()

            # Determine if the hook should run or exit
            siru = ShallIRunUtils(self.hostname, self.cfg)
            siru.exclude_hosts(self.cfg['exclude_hosts'])
            siru.exclude_vntypes(self.cfg['exclude_vntypes'])
            siru.run_only_on_hosts(self.cfg['run_only_on_hosts'])

            # Collect the mount points
            if paths is not None:
                self.paths = paths
            else:
                self.paths = self.__set_paths()
            # Define the local vnode type
            if vntype is not None:
                self.vntype = vntype
            else:
                self.vntype = self.__get_vnode_type()
            # Determine which subsystems we care about
            if subsystems is not None:
                self.subsystems = subsystems
            else:
                self.subsystems = self.__target_subsystems()

            # location to store information for the different hook events
            pbs_home = ''
            if not CRAY_12_BRANCH:
                pbs_conf = pbs.get_pbs_conf()
                pbs_home = pbs_conf['PBS_HOME']
            else:
                if 'PBS_HOME' in self.cfg:
                    pbs_home = self.cfg['PBS_HOME']
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "PBS_HOME needs to be defined in the " +
                               "config file")
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Exiting the cgroups hook")
                    pbs.accept()

            self.hook_storage_dir = pbs_home+'/mom_priv/hooks/hook_data'
            self.host_job_env_dir = pbs_home+'/aux'
            self.host_job_env_filename = self.host_job_env_dir+os.sep+"%s.env"

            # information for offlining nodes
            self.offline_file = \
                pbs_home+os.sep+'mom_priv'+os.sep+'hooks'+os.sep
            self.offline_file += "%s.offline" % pbs.event().hook_name
            self.offline_msg = "Hook %s: " % pbs.event().hook_name
            self.offline_msg += "Unable to clean up one or more cgroups"

        except:
            raise

    def __repr__(self):
        return ("CgroupUtils(%s, %s, %s, %s, %s, %s)" %
                (repr(self.hostname), repr(self.vnode), repr(self.cfg),
                 repr(self.subsystems), repr(self.paths), repr(self.vntype)))

    # Validate the OS type and version
    def __check_os(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check to see if the platform is linux and the kernel is new enough
        if platform.system() != 'Linux':
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: OS does not support cgroups" %
                       (caller_name()))
            raise CgroupConfigError("OS type not supported")
        rel = map(int,
                  string.split(string.split(platform.release(), '-')[0], '.'))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Detected Linux kernel version %d.%d.%d" %
                   (caller_name(), rel[0], rel[1], rel[2]))
        supported = False
        if rel[0] > 2:
            supported = True
        elif rel[0] == 2:
            if rel[1] > 6:
                supported = True
            elif rel[1] == 6:
                if rel[2] >= 28:
                    supported = True
        if not supported:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Kernel needs to be >= 2.6.28: %s" %
                       (caller_name(), system_info[2]))
            raise CgroupConfigError("OS version not supported")
        return supported

    # Read the config file in json format
    def __parse_config_file(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        config = {}
        # Identify the config file and read in the data
        if 'PBS_HOOK_CONFIG_FILE' in os.environ:
            config_file = os.environ["PBS_HOOK_CONFIG_FILE"]
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Config file is %s" %
                       (caller_name(), config_file))
            try:
                config = json.load(open(config_file, 'r'),
                                   object_hook=decode_dict)
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
        else:
            raise CgroupConfigError("No configuration file present")
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Initial Cgroup Config: %s" %
                   (caller_name(), config))
        # Set some defaults if they are not present
        if 'cgroup_prefix' not in config.keys():
            config['cgroup_prefix'] = 'pbspro'
        if 'periodic_resc_update' not in config.keys():
            config['periodic_resc_update'] = False
        if 'vnode_per_numa_node' not in config.keys():
            config['vnode_per_numa_node'] = False
        if 'online_offlined_nodes' not in config.keys():
            config['online_offlined_nodes'] = False
        if 'exclude_hosts' not in config.keys():
            config['exclude_hosts'] = []
        if 'run_only_on_hosts' not in config.keys():
            config['run_only_on_hosts'] = []
        if 'exclude_vntypes' not in config.keys():
            config['exclude_vntypes'] = []
        if 'cgroup' not in config.keys():
            config['cgroup'] = {}
        else:
            for subsys in config['cgroup']:
                subsystem = config['cgroup'][subsys]
                if 'enabled' not in subsystem.keys():
                    config['cgroup'][subsys]['enabled'] = False
                if 'exclude_hosts' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_hosts'] = []
                if 'exclude_vntypes' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_vntypes'] = []

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Cgroup Config with defaults: %s" %
                   (caller_name(), config))
        return config

    # Determine which subsystems are being requested
    def __target_subsystems(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        subsystems = []
        for key in self.cfg['cgroup'].keys():
            if self.enabled(key):
                subsystems.append(key)
        if 'memory' not in subsystems:
            if 'memsw' in subsystems:
                # Remove memsw since it must be greater then
                # or equal to memory
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing memsw from enabled subsystems")
                subsystems.remove('memsw')
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Enabled subsystems: %s" %
                   (caller_name(), subsystems))
        # It is not an error for all subsystems to be disabled.
        # This host or vnode type may be in the excluded list.
        return subsystems

    # Copy a setting from the parent cgroup
    def __copy_from_parent(self, dest):
        try:
            file = os.path.basename(dest)
            dir = os.path.dirname(dest)
            parent = os.path.dirname(dir)
            source = os.path.join(parent, file)
            if not os.path.isfile(source):
                raise CgroupConfigError('Failed to read %s' % (source))
            self.write_value(dest, open(source, 'r').read().strip())
        except:
            raise

    # Create a dictionary of the cgroup subsystems and their corresponding
    # directories
    def __set_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        paths = {}
        try:
            # Loop through the mounts and collect the ones for cgroups
            with open(os.path.join(os.sep, "proc", "mounts"), 'r') as fd:
                for line in fd:
                    entries = line.split()
                    if len(entries) > 3 and entries[2] == "cgroup":
                        flags = entries[3].split(',')
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "subsysdir: %s (flags=%s)" %
                                   (entries[1], flags))
                        for subsys in flags:
                            paths[subsys] = os.path.join(
                                entries[1], self.cfg["cgroup_prefix"])
        except:
            raise
        if len(paths.keys()) < 1:
            raise CgroupConfigError("Cgroup paths not detected")
        # Both the memory and memsw cgroups share the same mount point
        if "memory" in paths.keys():
            paths["memsw"] = paths["memory"]
        return paths

    # Create the cgroup parent directories that will contain the jobs
    def create_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Create the directories that PBS will use to house the jobs
            old_umask = os.umask(0022)
            for subsys in self.subsystems:
                if subsys not in self.paths.keys():
                    raise CgroupConfigError('No path for subsystem: %s' %
                                            (subsys))
                if not os.path.exists(self.paths[subsys]):
                    os.makedirs(self.paths[subsys], 0755)
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Created directory %s" %
                               (caller_name(), self.paths[subsys]))
                    if subsys == 'memory' or subsys == 'memsw':
                        # Set file to
                        # <cgroup>/memory/pbspro/memory.use_hierarchy
                        file = os.path.join(self.paths[subsys],
                                            'memory.use_hierarchy')
                        if not os.path.isfile(file):
                            raise CgroupConfigError('Failed to configure %s' %
                                                    (file))
                        self.write_value(file, 1)
                    elif subsys == 'cpuset':
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.cpus'))
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.mems'))

        except:
            raise CgroupConfigError("Failed to create directory: %s" %
                                    (self.paths[subsys]))
        finally:
            os.umask(old_umask)

    # Return the vnode type of the local node
    def __get_vnode_type(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")

        # File to check for if it is not defined in the hook input file
        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'

        # self.vnode is None for pbs_attach events
        pbs.logmsg(pbs.EVENT_DEBUG3, "vnode: %s" % self.vnode)
        if self.vnode is not None:
            if 'vntype' in self.vnode.resources_available and \
                    self.vnode.resources_available['vntype'] is not None:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "vntype: %s" %
                           self.vnode.resources_available['vntype'])
                return self.vnode.resources_available['vntype']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3, "vntype file: %s" % vntype_file)
                if os.path.isfile(vntype_file):
                    fdata = open(vntype_file).readlines()
                    vntype = fdata[0].strip()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
                    return vntype
        # If vntype was not set then log a message. It is too expensive
        # to have all moms query the server for large jobs.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: resources_available.vntype is " % caller_name() +
                   "not set for this vnode")
        return None

    # Gather resources that are already assigned
    def gather_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        assigned_resources = {}

        # Gather the cpus and memory sockets assigned
        directories = [x[0] for x in os.walk(self.paths['cpuset'])]

        # Add the existing cgroups to a list to check if assigning
        # resources fail
        for dir in directories[1:]:
            if dir.find(pbs.event().job.id) == -1:
                self.existing_cgroups[dir] = []

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'cpuset' in self.subsystems and \
                self.cfg['cgroup']['cpuset']['enabled']:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather cpuset resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['cpuset'], tmp_dir)
                    with open(path+os.sep+'cpuset.cpus') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.cpus'] = \
                            cpus2list(tmp_val.strip())
                    with open(path+os.sep+'cpuset.mems') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.mems'] = \
                            cpus2list(tmp_val.strip())

        # Check to see if the subsystem is enabled before trying
        # to gather resources
        if 'memory' in self.subsystems and \
                self.cfg['cgroup']['memory']['enabled']:
            # Gather the memory assigned for each socket
            directories = [x[0] for x in os.walk(self.paths['memory'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather memory resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['memory'], tmp_dir)
                    with open(path+os.sep+'memory.limit_in_bytes') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memory.limit'] = \
                            tmp_val.strip()
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    if os.path.isfile(tmp_filename) is False:
                        continue
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    with open(tmp_filename) as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memsw.limit'] = \
                            tmp_val.strip()

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'devices' in self.subsystems and \
                self.cfg['cgroup']['devices']['enabled']:
            # Gather the devices assigned
            directories = [x[0] for x in os.walk(self.paths['devices'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather device resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    pbs.logmsg(pbs.EVENT_DEBUG3, "Dir: %s" % tmp_dir)
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['devices'], tmp_dir)
                    with open(path+os.sep+'devices.list') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['devices.list'] = \
                            tmp_val.strip().split('\n')
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Dir: %s\n\tValue: %s" %
                                   (path, tmp_val.replace('\n', ',')))

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Gathered assigned resources: %s" % assigned_resources)
        self.assigned_resources = assigned_resources

    # Return whether a subsystem is enabled
    def enabled(self, subsystem):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check whether the subsystem is enabled in the configuration file
        if subsystem not in self.cfg['cgroup'].keys():
            return False
        if 'enabled' not in self.cfg['cgroup'][subsystem].keys():
            return False
        if not self.cfg['cgroup'][subsystem]['enabled']:
            return False
        # Check whether the cgroup is mounted for this subsystem
        if subsystem not in self.paths.keys():
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup not mounted for %s" %
                       (caller_name(), subsystem))
            return False
        # Check whether this host is excluded
        # if self.hostname in self.cfg['exclude_hosts']:
        #    pbs.logmsg(pbs.EVENT_DEBUG, "%s: cgroup excluded on host %s" %
        #               (caller_name(), self.hostname))
        #    pbs.event().accept()
        if self.hostname in self.cfg['cgroup'][subsystem]['exclude_hosts']:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup excluded for subsystem %s on host %s" %
                       (caller_name(), subsystem, self.hostname))
            return False
        # Check whether the vnode type is excluded
        if self.vntype is not None:
            # if self.vntype in self.cfg['exclude_vntypes']:
            #    pbs.logmsg(pbs.EVENT_DEBUG,
            #               "%s: cgroup excluded on vnode type %s" %
            #               (caller_name(), self.vntype))
            #    pbs.event().accept()
            if self.vntype in self.cfg['cgroup'][subsystem]['exclude_vntypes']:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           ("%s: cgroup excluded for " +
                            "subsystem %s on vnode type %s") %
                           (caller_name(), subsystem, self.vntype))
                return False
        return True

    # Return the default value for a subsystem
    def default(self, subsystem):
        if subsystem in self.cfg['cgroup']:
            if 'default' in self.cfg['cgroup'][subsystem]:
                return self.cfg['cgroup'][subsystem]['default']
        return None

    # Check to see if the pid matches the job owners uid
    def is_pid_owned_by_job_owner(self, pid):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        try:
            proc_uid = os.stat('/proc/%d' % pid).st_uid
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       '/proc/%d uid:%d' % (pid, proc_uid))

            job_owner_uid = pwd.getpwnam(pbs.event().job.euser)[2]
            pbs.logmsg(pbs.EVENT_DEBUG3, "Job uid: %d" % job_owner_uid)

            if proc_uid == job_owner_uid:
                return True
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Proc uid: %d != Job owner:  %d" %
                           (proc_uid, job_owner_uid))
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG, "Unknown pid: %d" % pid)
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        # If we got to this point something did not match up
        return False

    # Add some number of PIDs to the cgroup tasks files for each subsystem
    def add_pids(self, pids, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # make pids a list
        if isinstance(pids, int):
            pids = [pids]

        if pbs.event().type == pbs.EXECJOB_LAUNCH:
            if 1 in pids:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "1 is not a valid pid to add")
                # Hold the job for further review
                e.reject("Hook tried to add pid 1 to the tasks file " +
                         "for job %s" % jobid)

        # check pids to make sure that they are owned by the job owner
        if not CRAY_12_BRANCH:
            pbs.logmsg(pbs.EVENT_DEBUG3, "Not in the Cray 12 Branch")
            pbs.logmsg(pbs.EVENT_DEBUG3, "check to see if execjob_attach")
            if pbs.event().type == pbs.EXECJOB_ATTACH:
                pbs.logmsg(pbs.EVENT_DEBUG3, "event type: attach or launch")
                tmp_pids = list()
                for p in pids:
                    if self.is_pid_owned_by_job_owner(p):
                        tmp_pids.append(p)
                if len(tmp_pids) == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%d is not a valid pids to add" % p)
                    return False
                else:
                    pids = tmp_pids

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: subsys = %s" %
                       (caller_name(), subsys))
            # memsw and memory use the same tasks file
            if subsys == "memsw":
                if "memory" in self.subsystems:
                    continue
            path = os.path.join(self.paths[subsys], jobid)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path = %s" %
                       (caller_name(), path))
            try:
                tasks_path = os.path.join(path, "tasks")
                for p in pids:
                    self.write_value(tasks_path, p, 'a')
            except IOError, exc:
                raise CgroupLimitError("Failed to add PIDs %s to %s (%s)" %
                                       (str(pids), tasks_path,
                                        errno.errorcode[exc.errno]))
            except:
                raise

    # Set a node limit
    def set_node_limit(self, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s = %s" %
                   (caller_name(), resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'],
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Node resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    def setup_job_devices_env(self):
        """ Setup the job environment for the devices assigned to the job for an
            execjob_launch hook
         """
        if 'devices_name' in self.host_assigned_resources:
            names = self.host_assigned_resources['devices_name']
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "devices: %s" % (names))
            offload_devices = list()
            cuda_visible_devices = list()
            for name in names:
                if name.startswith('mic'):
                    offload_devices.append(name[3:])
                elif name.startswith('nvidia'):
                    cuda_visible_devices.append(name[6:])
            if len(offload_devices) > 0:
                value = '"%s"' % string.join(offload_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['OFFLOAD_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "offload_devices: %s" % offload_devices)
            if len(cuda_visible_devices) > 0:
                value = '"%s"' % string.join(cuda_visible_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['CUDA_VISIBLE_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "cuda_visible_devices: %s" % cuda_visible_devices)
            return [offload_devices, cuda_visible_devices]
        else:
            return False

    def setup_subsys_devices(self, path, node):
        subsys = 'devices'
        # Add the devices that the user is allowed to use
        if subsys in self.cfg['cgroup']:
            devices_allowed = open(os.path.join(path,
                                   "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Initial devices.list: %s" %
                       devices_allowed)
            # Deny access to mic and gpu devices
            devices_list = list()
            devices = node.devices
            for device in devices:
                if device == 'mic' or device == 'gpu':
                    for name in devices[device]:
                        d = devices[device][name]
                        devices_list.append("%d:%d" % (d['major'], d['minor']))
            # For CentOS 7 we need to remove a *:* rwm from devices.list we
            # before can add anything to devices.allow. Otherwise our changes
            # are ignored. Check to see if a *:* rwm is in devices.list.
            # If so remove it
            if "a *:* rwm\n" in devices_allowed:
                value = "a *:* rwm"
                self.write_value(os.path.join(path, 'devices.deny'), value)
            else:
                # Verify that the following devices are not in devices.list
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing access to the following: %s" %
                           devices_list)
            for device_str in devices_list:
                value = "c %s rwm" % device_str
                self.write_value(os.path.join(path, 'devices.deny'), value)
            # Add devices back to the list
            devices_allow = list()
            if 'allow' in self.cfg['cgroup'][subsys]:
                devices_allow = self.cfg['cgroup'][subsys]['allow']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Allowing access to the following: %s" %
                           devices_allow)
                for item in devices_allow:
                    if isinstance(item, str):
                        pbs.logmsg(pbs.EVENT_DEBUG3, "string item: %s" % item)
                        value = "%s" % item
                        self.write_value(os.path.join(
                                         path, 'devices.allow'), value)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "write_value: %s" % value)
                    elif isinstance(item, list):
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "list item: %s" % item)
                        try:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Device allow: %s" % item)
                            stat_filename = '/dev/%s' % item[0]
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Stat file: %s" % stat_filename)
                            s = os.stat(stat_filename)
                            device_type = None
                            if S_ISBLK(s.st_mode):
                                device_type = "b"
                            elif S_ISCHR(s.st_mode):
                                device_type = "c"
                            if device_type is not None:
                                if len(item) == 3 and isinstance(item[2], str):
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % item[2]
                                    value += "%s" % item[1]
                                else:
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % os.minor(s.st_rdev)
                                    value += "%s" % item[1]
                                self.write_value(os.path.join(
                                                path, 'devices.allow'), value)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "write_value: %s" % value)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find /dev/%s. " % item[0] +
                                       "Not added to devices.allow!")
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                    else:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Not sure what to do with: %s" % item)
            devices_allowed = open(os.path.join(
                                   path, "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "After setup devices.list: %s" % devices_allowed)

    # Select devices to assign to the job
    def assign_devices(
                       self,
                       device_type,
                       device_list,
                       number_of_devices,
                       node):
        devices = device_list[:number_of_devices]
        device_ids = list()
        device_names = list()
        for device in devices:
            device_info = node.devices[device_type][device]
            if len(device_ids) == 0:
                if device.find("mic") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:0 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                    device_ids.append("%s %d:1 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                elif device.find("nvidia") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:255 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
            device_ids.append("%s %d:%d rwm" %
                              (device_info['device_type'],
                               device_info['major'],
                               device_info['minor']))
            device_names.append(device)
        return device_names, device_ids

    # Find the device name
    def get_device_name(self, node, available, socket, major, minor):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Get device name: major: %s, minor: %s" % (major, minor))
        avail_device = None
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Possible devices: %s" % (available[socket]['devices']))
        for avail_device in available[socket]['devices']:
            avail_major = None
            avail_minor = None
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Checking device: %s" % (avail_device))
            if avail_device.find('mic') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check mic device: %s" % (avail_device))
                avail_major = node.devices['mic'][avail_device]['major']
                avail_minor = node.devices['mic'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            elif avail_device.find('nvidia') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check gpu device: %s" % (avail_device))
                avail_major = node.devices['gpu'][avail_device]['major']
                avail_minor = node.devices['gpu'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            if int(avail_major) == int(major) and \
                    int(avail_minor) == int(minor):
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device match: name: %s, major: %s, minor: %s" %
                           (avail_device, major, minor))
                return avail_device
        pbs.logmsg(pbs.EVENT_DEBUG3, "No match found")
        return None

    # Assign resources to the job based off the vnodes assignments
    def assign_job_resources_by_vnode(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # Loop through the vnodes and assign resources
        assigned = dict()
        assigned['cpuset.cpus'] = list()
        assigned['cpuset.mems'] = list()
        room_on_socket = True

        for vnode in resources['vnodes']:
            regex = re.compile(".*\[(\d+)\].*")
            socket = int(regex.search(vnode).group(1))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current Vnode: %s" % vnode)
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current socket: %d" % socket)

            if 'ncpus' in resources['vnodes'][vnode]:
                tmp_ncpus = int(resources['vnodes'][vnode]['ncpus'])
                if int(resources['vnodes'][vnode]['ncpus']) <= \
                        len(available[socket]['cpus']):
                    assigned['cpuset.cpus'] += \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] += [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_ncpus: %s" % (tmp_ncpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "available[%d]: %s" %
                               (socket, available[socket]))
                    room_on_socket = False
            if ('nmics' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['nmics']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(mic).*")
                tmp_nmics = int(resources['vnodes'][vnode]['nmics'])
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['nmics']) > 0 and
                   int(resources['vnodes'][vnode]['nmics']) <= len(mics)):
                    tmp_names, tmp_devices = self.assign_devices(
                             'mic', mics[:tmp_nmics], tmp_nmics, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_nmics: %s" % (tmp_nmics))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "mics: %s" % (mics))
                    room_on_socket = False
            if ('ngpus' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['ngpus']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(nvidia).*")
                tmp_ngpus = int(resources['vnodes'][vnode]['ngpus'])
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['ngpus']) > 0 and
                   int(resources['vnodes'][vnode]['ngpus']) <= len(gpus)):
                    tmp_names, tmp_devices = self.assign_devices(
                        'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "tmp_ngpus: %s" % (tmp_ngpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "gpus: %s" % (gpus))
                    room_on_socket = False

        if room_on_socket:
            assigned['cpuset.cpus'].sort()
            assigned['cpuset.mems'].sort()
            if 'devices' in assigned:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids pre set and sort: %s" %
                           (assigned['devices']))
                assigned['devices'] = list(set(assigned['devices']))
                assigned['devices'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids post set and sort: %s" %
                           (assigned['devices']))
                assigned['devices_name'] = list(set(assigned['devices_name']))
                assigned['devices_name'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

    # Assign resources to the job
    def assign_job_resources(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # What would we need to do different for a large smp (UV) system?
        # See if requested resources can fit on a single socket
        assigned = dict()
        pbs.logmsg(pbs.EVENT_DEBUG3, "Requested: %s" % (resources))

        # Determine the order that we should evaluate the sockets.
        socket_list = list()
        if 'placement_type' in self.cfg:
            if self.cfg['placement_type'] == 'round_robin':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Requested round_robin placement")
                # Look at assigned_resources and determine which socket
                # to start with
                jobs_per_socket_cnt = dict()
                for socket in available.keys():
                    jobs_per_socket_cnt[socket] = 0
                for job in self.assigned_resources:
                    if 'cpuset.mems' in self.assigned_resources[job]:
                        for socket in \
                                self.assigned_resources[job]['cpuset.mems']:
                            jobs_per_socket_cnt[socket] += 1

                sorted_dict = sorted(jobs_per_socket_cnt.items(),
                                     key=operator.itemgetter(1))
                for x in sorted_dict:
                    socket_list.append(x[0])
        else:
            socket_list = available.keys()

        # Loop through the socket list and determine if the job
        # can fit on the socket
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Evaluate sockets in the following order: %s" %
                   (socket_list))
        for socket in socket_list:
            room_on_socket = True
            if 'ncpus' in resources:
                if int(resources['ncpus']) <= len(available[socket]['cpus']):
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Requested: %s, Available: %s" %
                               (resources['ncpus'], available[socket]['cpus']))
                    tmp_ncpus = int(resources['ncpus'])
                    assigned['cpuset.cpus'] = \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] = [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient ncpus on socket %d: R:%d,A:%s" %
                               (socket, resources['ncpus'],
                                available[socket]['cpus']))
                    room_on_socket = False
            # Check to see that nmics has been requested and not equal to 0
            if 'nmics' in resources and int(resources['nmics']) > 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'nmics' in resources and int(resources['nmics']) > 0 \
                        and int(resources['nmics']) <= len(mics):
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient nmics on socket %d: R:%s,A:%s" %
                               (socket, resources['nmics'], mics))
                    room_on_socket = False
            # Check to see that ngpus has been requested and not equal to 0
            if 'ngpus' in resources and int(resources['ngpus']) > 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'ngpus' in resources and int(resources['ngpus']) > 0 \
                        and int(resources['ngpus']) <= len(gpus):
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient gpus on socket %d: R:%s,A:%s" %
                               (socket, resources['ngpus'], gpus))
                    room_on_socket = False
            if 'vmem' in resources:
                if size_as_int(resources['vmem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient vmem on socket %d: R:%s,A:%s" %
                               (socket, resources['vmem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if 'mem' in resources:
                if size_as_int(resources['mem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient memory on socket %d: R:%s,A:%s" %
                               (socket, resources['mem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if room_on_socket:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
                self.host_assigned_resources = assigned
                return assigned
        # If we made it here then the requested resources did not fit
        # on a single socket
        # Future: take distance into account when selecting sockets?
        # Combine available resources into a single list
        # This should work for a dual socket machine.
        # Needs additional work for machines with more than 2 sockets
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Unable to find requested resources: %s" % resources +
                   " on a socket. Checking the entire node")
        available_copy = copy.deepcopy(available)
        available_on_node = dict()
        socket_keys = available.keys()
        socket_keys.sort()
        available_on_node['sockets'] = socket_keys
        for socket in socket_keys:
            for key in available_copy[socket].keys():
                if isinstance(available[socket][key], int):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]
                if isinstance(available_copy[socket][key], str):
                    try:
                        if key in available_on_node is False:
                            available_on_node[key] = \
                                size_as_int(available_copy[socket][key])
                        else:
                            available_on_node[key] += \
                                size_as_int(available_copy[socket][key])
                    except:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Unable to convert to int: %s" %
                                   (available_copy[socket][key]))

                if isinstance(available_copy[socket][key], list):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available on node: %s" % (available_on_node))
        assigned = dict()
        room_on_node = False
        if 'ncpus' in resources:
            if int(resources['ncpus']) <= len(available_on_node['cpus']):
                room_on_node = True
                tmp_ncpus = int(resources['ncpus'])
                assigned['cpuset.cpus'] = available_on_node['cpus'][:tmp_ncpus]
                assigned['cpuset.mems'] = available_on_node['sockets']
            if 'nmics' in resources and int(resources['nmics']) != 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['nmics']) <= len(mics) and \
                        int(resources['nmics']) > 0:
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
            if 'ngpus' in resources and int(resources['ngpus']) != 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['ngpus']) <= len(gpus) and \
                        int(resources['ngpus']) > 0:
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
        if room_on_node:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

        # Consolidate resources if needed to place the job

    # Determine which resources are available
    def available_node_resources(self, node):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))

        available = copy.deepcopy(node.numa_nodes)
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Keys: %s" % (available[0].keys()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        for socket in available.keys():
            if 'cpus' in available[socket]:
                # Find the physical cores
                if available[socket]['cpus'].find('-') != -1 and \
                        node.hyperthreading:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Splitting %s at ','" %
                               (available[socket]['cpus']))
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'].split(',')[0])
                else:
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'])
            if 'MemTotal' in available[socket]:
                # Find the memory on the socket in bites.
                # Remove the 'b' to simply math
                available[socket]['memory'] = size_as_int(
                    available[socket]['MemTotal'])

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Pre device add: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (node.devices.keys()))
        for device in node.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" %
                       (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Devices: %s" %
                           (node.devices[device].keys()))
                for device_name in node.devices[device].keys():
                    device_socket = \
                        node.devices[device][device_name]['numa_node']
                    if 'devices' not in available[device_socket]:
                        available[device_socket]['devices'] = list()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Device: %s, Socket: %s" %
                               (device, device_socket))
                    available[device_socket]['devices'].append(device_name)
            # pbs.logmsg(pbs.EVENT_DEBUG3,
            #            "Available on socket %s: %s" %
            #            (socket,available[socket]))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))

        # Remove all of the resources that are assigned to other jobs
        for job in self.assigned_resources.keys():
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            if (not job_to_be_ignored(job)):
                cpus = list()
                sockets = list()
                devices = list()
                memory = 0
                if 'cpuset.cpus' in self.assigned_resources[job]:
                    cpus = self.assigned_resources[job]['cpuset.cpus']
                if 'cpuset.mems' in self.assigned_resources[job]:
                    sockets = self.assigned_resources[job]['cpuset.mems']
                if 'devices.list' in self.assigned_resources[job]:
                    devices = self.assigned_resources[job]['devices.list']
                if 'memory.limit' in self.assigned_resources[job]:
                    memory = size_as_int(
                                self.assigned_resources[job]['memory.limit'])

                # Loop through the sockets and remove cpus that are
                # assigned to other cgroups
                for socket in sockets:
                    for cpu in cpus:
                        try:
                            available[socket]['cpus'].remove(cpu)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %d from %s" %
                                       (cpu, available[socket]['cpus']))

                if len(sockets) == 1:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Decrementing memory: %d by %d" %
                               (size_as_int(available[sockets[0]]['memory']),
                                memory))
                    available[sockets[0]]['memory'] -= memory

                # Loop throught the available sockets
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned device to %s: %s" % (job, devices))
                for socket in available.keys():
                    for device in devices:
                        try:
                            # loop through know devices and see if they match
                            if len(available[socket]['devices']) != 0:
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Check device: %s" % (device))
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Available device: %s" %
                                           (available[socket]['devices']))
                                major, minor = device.split()[1].split(':')
                                avail_device = self.get_device_name(
                                                   node, available, socket,
                                                   major, minor)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Returned device: %s" %
                                           (avail_device))
                                if avail_device is not None:
                                    pbs.logmsg(pbs.EVENT_DEBUG3,
                                               "socket: %d,\t" % socket +
                                               "devices: %s,\t" %
                                               available[socket]['devices'] +
                                               "device to remove: %s" %
                                               (avail_device))
                                    available[socket]['devices'].remove(
                                        avail_device)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %s from %s" %
                                       (device, available[socket]['devices']))
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Job %s res not removed from host " % job +
                           "available res: suspended job")
        pbs.logmsg(pbs.EVENT_DEBUG, "Available resources: %s" % (available))
        return available

    # Set a job limit
    def set_job_limit(self, jobid, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s: %s = %s" %
                   (caller_name(), jobid, resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'softmem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.soft_limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     jobid, 'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'], jobid,
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            elif resource == 'ncpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = self.select_cpus(path, value)
                    if cpus is None:
                        raise CgroupLimitError("Failed to configure cpuset.")
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
                    self.__copy_from_parent(os.path.join(self.paths['cpuset'],
                                            jobid, 'cpuset.mems'))
            elif resource == 'cpuset.cpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = value
                    if cpus is None:
                        msg = "Failed to configure cpus in cpuset."
                        raise CgroupLimitError(msg)
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
            elif resource == 'cpuset.mems':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.mems')
                    mems = value
                    if mems is None:
                        msg = "Failed to configure mems in cpuset."
                        raise CgroupLimitError(msg)
                    mems = ','.join(map(str, mems))
                    self.write_value(path, mems)
            elif resource == 'devices':
                if 'devices' in self.subsystems and \
                        self.cfg['cgroup']['devices']['enabled']:

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.allow')
                    devices = value
                    if devices is None:
                        msg = "Failed to configure device(s)."
                        raise CgroupLimitError(msg)
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Setting devices: %s for %s" % (devices, jobid))
                    for device in devices:
                        self.write_value(path, device)

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.list')
                    output = open(path, 'r').read()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "devices.list: %s" % output.replace("\n", ","))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    # Update resource usage for a job
    def update_job_usage(self, jobid, resc_used):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resc_used = %s" %
                   (caller_name(), str(resc_used)))
        # Sort the subsystems so that we consistently look at the subsystems
        # in the same order every time
        self.subsystems.sort()
        for subsys in self.subsystems:
            path = os.path.join(self.paths[subsys], jobid)
            if subsys == "memory":
                max_mem = self.__get_max_mem_usage(path)
                if max_mem is None:
                    pbs.logjobmsg(jobid, "%s: No max mem data" %
                                  (caller_name()))
                else:
                    resc_used['mem'] = pbs.size(convert_size(max_mem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: mem=%s" %
                                  (caller_name(), resc_used['mem']))
                mem_failcnt = self.__get_mem_failcnt(path)
                if mem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No mem fail count data" %
                                  (caller_name()))
                else:
                    # Check to see if the job exceeded its resource limits
                    if mem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memory limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "memsw":
                max_vmem = self.__get_max_memsw_usage(path)
                if max_vmem is None:
                    pbs.logjobmsg(jobid, "%s: No max vmem data" %
                                  (caller_name()))
                else:
                    resc_used['vmem'] = pbs.size(convert_size(max_vmem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: vmem=%s" %
                                  (caller_name(), resc_used['vmem']))

                vmem_failcnt = self.__get_memsw_failcnt(path)
                if vmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No vmem fail count data" %
                                  (caller_name()))
                else:
                    pbs.logjobmsg(jobid, "%s: vmem fail count: %d " %
                                  (caller_name(), vmem_failcnt))
                    if vmem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memsw limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "hugetlb":
                max_hpmem = self.__get_max_hugetlb_usage(path)
                if max_hpmem is None:
                    pbs.logjobmsg(jobid, "%s: No max hpmem data" %
                                  (caller_name()))
                    return
                hpmem_failcnt = self.__get_hugetlb_failcnt(path)
                if hpmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No hpmem fail count data" %
                                  (caller_name()))
                    return
                if hpmem_failcnt > 0:
                    err_msg = self.__get_error_msg(jobid)
                    pbs.logjobmsg(jobid, "Cgroup hugetlb limit exceeded: %s" %
                                  (err_msg))
                resc_used['hpmem'] = pbs.size(convert_size(max_hpmem, 'kb'))
                pbs.logjobmsg(jobid, "%s: Hugepage usage: %s" %
                              (caller_name(), resc_used['hpmem']))
            elif subsys == "cpuacct":
                cpu_usage = self.__get_cpu_usage(path)
                if cpu_usage is None:
                    pbs.logjobmsg(jobid, "%s: No CPU usage data" %
                                  (caller_name()))
                    return
                cpu_usage = convert_time(str(cpu_usage) + "ns")
                pbs.logjobmsg(jobid, "%s: CPU usage: %.3lf secs" %
                              (caller_name(), cpu_usage))
                resc_used['cput'] = pbs.duration(cpu_usage)

    # Creates the cgroup if it doesn't exists
    def create_job(self, jobid, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Iterate over the subsystems required
        for subsys in self.subsystems:
            # Create a directory for the job
            try:
                old_umask = os.umask(0022)
                path = self.paths[subsys]
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                path = os.path.join(self.paths[subsys], jobid)
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                if subsys == 'devices':
                    self.setup_subsys_devices(path, node)

            except OSError, exc:
                raise CgroupConfigError("Failed to create directory: %s (%s)" %
                                        (path, errno.errorcode[exc.errno]))
            except:
                raise
            finally:
                os.umask(old_umask)

    # Determine the cgroup limits and configure the cgroups
    def configure_job(self, jobid, hostresc, node):
        mem_enabled = 'memory' in self.subsystems
        vmem_enabled = 'memsw' in self.subsystems
        if mem_enabled or vmem_enabled:
            # Initialize mem variables
            mem_avail = node.get_memory_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "mem_avail %s" % mem_avail)
            mem_requested = None
            if 'mem' in hostresc.keys():
                mem_requested = convert_size(hostresc['mem'], 'kb')
            mem_default = None
            if mem_enabled:
                mem_default = self.default('memory')
            # Initialize vmem variables
            vmem_avail = node.get_vmem_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "vmem_avail %s" % vmem_avail)
            vmem_requested = None
            if 'vmem' in hostresc.keys():
                vmem_requested = convert_size(hostresc['vmem'], 'kb')
            vmem_default = None
            if vmem_enabled:
                vmem_default = self.default('memsw')
            # Initialize softmem variables
            if 'soft_limit' in self.cfg['cgroup']['memory'].keys():
                softmem_enabled = self.cfg['cgroup']['memory']['soft_limit']
            else:
                softmem_enabled = False
            # Sanity check
            if size_as_int(mem_avail) > size_as_int(vmem_avail):
                if size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                    raise CgroupLimitError(
                        'mem available (%s) exceeds vmem available (%s)' %
                        (mem_avail, vmem_avail))
            # Determine the mem limit
            if mem_requested is not None:
                # mem requested may not exceed available
                if size_as_int(mem_requested) > size_as_int(mem_avail):
                    raise JobValueError(
                        'mem requested (%s) exceeds mem available (%s)' %
                        (mem_requested, mem_avail))
                mem_limit = mem_requested
            else:
                # mem was not requested
                if mem_default is None:
                    mem_limit = mem_avail
                else:
                    mem_limit = mem_default
            # Determine the vmem limit
            if vmem_requested is not None:
                # vmem requested may not exceed available
                if size_as_int(vmem_requested) > size_as_int(vmem_avail):
                    raise JobValueError(
                        'vmem requested (%s) exceeds vmem available (%s)' %
                        (vmem_requested, vmem_avail))
                vmem_limit = vmem_requested
            else:
                # vmem was not requested
                if vmem_default is None:
                    vmem_limit = vmem_avail
                else:
                    vmem_limit = vmem_default
            # Ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Adjust for soft limits if enabled
            if mem_enabled and softmem_enabled:
                softmem_limit = mem_limit
                # The hard memory limit is assigned the lesser of the vmem
                # limit and available memory
                if size_as_int(vmem_limit) < size_as_int(mem_avail):
                    mem_limit = vmem_limit
                else:
                    mem_limit = mem_avail
            # Again, ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Sanity checks when both memory and memsw are enabled
            if mem_enabled and vmem_enabled:
                if vmem_requested is not None:
                    if size_as_int(vmem_limit) > size_as_int(vmem_requested):
                        # The user requested an invalid limit
                        raise JobValueError(
                            'vmem limit (%s) exceeds vmem requested (%s)' %
                            (vmem_limit, vmem_requested))
                if size_as_int(vmem_limit) > size_as_int(mem_limit):
                    # This job may utilize swap
                    if size_as_int(vmem_avail) <= size_as_int(mem_avail) and \
                      size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                        # No swap available
                        raise CgroupLimitError(
                            ('Job might utilize swap ' +
                             'and no swap space available'))
            # Assign mem and vmem
            if mem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: mem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), mem_limit))
                hostresc['mem'] = pbs.size(mem_limit)
                if softmem_enabled:
                    hostresc['softmem'] = pbs.size(softmem_limit)
            if vmem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: vmem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), vmem_limit))
                hostresc['vmem'] = pbs.size(vmem_limit)
        # Initialize hpmem variables
        hpmem_enabled = 'hugetlb' in self.subsystems
        if hpmem_enabled:
            hpmem_avail = node.get_hpmem_on_node(self.cfg)
            hpmem_limit = None
            hpmem_default = self.default('hugetlb')
            if hpmem_default is None:
                hpmem_default = hpmem_avail
            if 'hpmem' in hostresc.keys():
                hpmem_limit = convert_size(hostresc['hpmem'], 'kb')
            else:
                hpmem_limit = hpmem_default
            # Assign hpmem
            if size_as_int(hpmem_limit) > size_as_int(hpmem_avail):
                raise JobValueError('hpmem limit (%s) exceeds available (%s)' %
                                    (hpmem_limit, hpmem_avail))
            hostresc['hpmem'] = pbs.size(hpmem_limit)
        # Initialize cpuset variables
        cpuset_enabled = 'cpuset' in self.subsystems
        if cpuset_enabled:
            cpu_limit = 1
            if 'ncpus' in hostresc.keys():
                cpu_limit = hostresc['ncpus']
            if cpu_limit < 1:
                cpu_limit = 1
            hostresc['ncpus'] = pbs.pbs_int(cpu_limit)

        # Find the available resources and assign the right ones to the job

        attempt = 0
        assign_resources = False

        # Two attempts since self.cleanup_orphans may actually fix the
        # problem we see in a first attempt
        while attempt < 2:
            attempt += 1
            available_resources = self.available_node_resources(node)
            if 'vnodes' in hostresc:
                assign_resources = self.assign_job_resources_by_vnode(
                    hostresc, available_resources, node)
            else:
                assign_resources = self.assign_job_resources(
                    hostresc, available_resources, node)

            if (assign_resources is False) and (attempt < 2):
                # No resources were assigned to the job.
                # Most likely cause was that a cgroup has
                # not been cleaned up yet
                pbs.logmsg(pbs.EVENT_DEBUG, "Failed to assign resources. " +
                           "Checking the following cgroups on the node " +
                           "with the server: %s" % self.existing_cgroups)

                # Collect the jobs on the node (try reading mom_priv/jobs,
                # if not successful ask server)
                jobs_list = node.gather_jobs_on_node_local()

                if jobs_list is not None:
                    # Don't clean up the cgroup we just made for the
                    # job just yet
                    if jobid not in jobs_list:
                        jobs_list.append(jobid)

                    self.cleanup_orphans(jobs_list)

                    # Recheck what is now assigned after things were cleaned up
                    self.gather_assigned_resources()

        if assign_resources is False:
            # Cleanup cgroups for jobs not present on this node
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Assign resources failed. Attempting to cleanup all " +
                       "leftover cgroups (including event job %s)" % jobid)

            # Remove the current job from the jobs list so it will be
            # cleaned up as well.
            jobs_list.remove(jobid)

            self.cleanup_orphans(jobs_list)

            # Rerun the job and log the message
            line = 'Unable to assign resources to job. '
            line += 'Will requeue the job and try again.'
            line += 'job run_count: %d' % pbs.event().job.run_count
            pbs.event().job.rerun()
            pbs.event().reject(line)

        # Print out the assigned resources
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Assigned resources: %s" % (assign_resources))

        if cpuset_enabled:
            hostresc.pop('ncpus', None)
            for key in ['cpuset.cpus', 'cpuset.mems']:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Initialize devices variables
        devices_enabled = 'devices' in self.subsystems
        if devices_enabled:
            key = 'devices'
            if key in assign_resources:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Apply the resource limits to the cgroups
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Setting cgroup limits for: %s" %
                   (caller_name(), hostresc))

        # The vmem limit must be set after the mem limit, so sort the keys
        for resc in sorted(hostresc.keys()):
            self.set_job_limit(jobid, resc, hostresc[resc])

    # Kill any processes contained within a tasks file
    def __kill_tasks(self, tasks_file):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        with open(tasks_file, 'r') as fd:
            for line in fd:
                os.kill(int(line.strip()), signal.SIGKILL)
        fd.close()
        count = 0
        with open(tasks_file, 'r') as fd:
            for line in fd:
                count += 1
                pid = line.strip()
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Pid: %s did not clean up" %
                           (caller_name(), pid))
                if os.path.isfile('/proc/%s/status' % pid):
                    tmp = open('/proc/%s/status' % pid).readlines()
                    tmp_line = ''
                    for entry in tmp:
                        if entry.find('Name:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('State:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('Uid:') != -1:
                            tmp_line += entry.strip()
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: %s" % (caller_name(), tmp_line))

        return count

    # Perform the actual removal of the cgroup directory
    # by default, only one attempt at killing tasks in cgroup, without sleeping
    # since this method could be called many times
    # (for N directories times M jobs)
    def __remove_cgroup(self, path, tasks_kill_attempts=1):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            if os.path.exists(path):
                tasks_file = os.path.join(path, 'tasks')
                task_cnt = self.__kill_tasks(tasks_file)
                kill_attempts = 0
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Task Kill Attempts: %d" % tasks_kill_attempts)
                while (task_cnt > 0) and (kill_attempts < tasks_kill_attempts):
                    kill_attempts += 1
                    count = 0

                    with open(tasks_file, 'r') as fd:
                        for line in fd:
                            count += 1
                        task_cnt = count

                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "Attempt: %d - cgroup still has %d tasks: %s" %
                               (kill_attempts, task_cnt, path))
                    if kill_attempts < tasks_kill_attempts:
                        time.sleep(1)
                        # Added since there are race conditions where tasks are
                        # being added at the same time we get the initial
                        # task count
                        tasks_file = os.path.join(path, 'tasks')
                        task_cnt = self.__kill_tasks(tasks_file)

                if task_cnt == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Removing directory %s" %
                               (caller_name(), path))
                    os.rmdir(path)
                    if os.path.exists(path):
                        time.sleep(.25)
                        if os.path.exists(path):
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "%s: 2nd Try Removing directory %s" %
                                       (caller_name(), path))
                            os.rmdir(path)
                            time.sleep(.25)
                        if os.path.exists(path):
                            return False
                else:
                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "cgroup still has %d tasks: %s" %
                               (task_cnt, path))

                    # Check to see if the offline file is already present
                    # on the mom
                    offline_file_exists = False
                    if os.path.isfile(self.offline_file):
                        msg = "Cgroup(s) not cleaning up but the node already "
                        msg += "has the offline file."

                        pbs.logmsg(pbs.EVENT_DEBUG, msg)
                    else:
                        # Check to see that the node is not already offline.
                        try:
                            tmp_state = pbs.server().vnode(self.hostname).state
                        except:
                            msg = "Unable to contact server for node state"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            tmp_state = None
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Current Node State: %d" % tmp_state)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Offline Node State: %d" % pbs.ND_OFFLINE)

                        if tmp_state == pbs.ND_OFFLINE:
                            msg = "Cgroup(s) not cleaning up but the node is "
                            msg += "already offline."
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                        elif tmp_state is not None:
                            # Offline the node(s)
                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                            msg = "Offlining node since cgroup(s) are not "
                            msg += "cleaning up"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            vnode = pbs.event().vnode_list[self.hostname]

                            vnode.state = pbs.ND_OFFLINE

                            # Write a file locally to reduce server traffic
                            # when it comes time to online the node
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Offline file: %s" % self.offline_file)
                            open(self.offline_file, 'a').close()
                            vnode.comment = self.offline_msg

                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                        else:
                            pass

                    return False

        except OSError, exc:
            pbs.logmsg(pbs.EVENT_SYSTEM,
                       "Failed to remove cgroup path: %s (%s)" %
                       (path, errno.errorcode[exc.errno]))
        except:
            pbs.logmsg(pbs.EVENT_SYSTEM, "Failed to remove cgroup path: %s" %
                       (path))

        return True

    # Removes cgroup directories that are not associated with a local job
    def cleanup_orphans(self, local_jobs):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        orphan_cnt = 0
        for key in self.paths.keys():
            for dir in glob.glob(os.path.join(self.paths[key], '[0-9]*')):
                jobid = os.path.basename(dir)
                if jobid not in local_jobs:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Removing orphaned cgroup: %s" %
                               (caller_name(), dir))
                    # default number of attempts at killing tasks
                    status = self.__remove_cgroup(dir)
                    if not status:
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "%s: Removing orphaned cgroup %s failed " %
                                   (caller_name(), dir))
                        orphan_cnt += 1
        return orphan_cnt

    # Removes the cgroup directories for a job
    def delete(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        tasks_kill_attempted = False

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            if not tasks_kill_attempted:
                # Make multiple attempts at killing tasks in cgroups on
                # first subsystem encountered since it is "normal" for
                # a cgroup.delete to encounter processes hard to kill
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                                           jobid),
                                              tasks_kill_attempts=(
                                              CGROUP_KILL_ATTEMPTS))
                tasks_kill_attempted = True
            else:
                # We tried getting rid of job processes earlier,
                # so don't try N times with sleeps
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                              jobid), tasks_kill_attempts=1)

            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Status: %s" %
                       (caller_name(), status))
            if status is False:
                # Rerun the job and log the message
                line = 'Unable to cleanup a cgroup. '
                line += 'Will offline the node and requeue the job '
                line += 'and try again. job run_count: %d' % \
                        pbs.event().job.run_count
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Failed to remove cgroup: %s" %
                           (caller_name(), line))
                pbs.event().accept()

    # Write a value to a limit file
    def write_value(self, file, value, mode='w'):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: writing %s to %s" %
                   (caller_name(), value, file))
        try:
            with open(file, mode) as fd:
                fd.write(str(value) + '\n')
        except IOError, exc:
            if exc.errno == errno.ENOENT:
                pbs.logmsg(pbs.EVENT_SYSTEM,
                           "Trying to set limit to unknown file %s" % file)
            elif exc.errno in [errno.EACCES, errno.EPERM]:
                pbs.logmsg(pbs.EVENT_SYSTEM, "Permission denied on file %s" %
                           file)
            elif exc.errno == errno.EBUSY:
                raise CgroupBusyError("Limit rejected")
            elif exc.errno == errno.EINVAL:
                raise CgroupLimitError("Invalid limit value: %s" % (value))
            else:
                raise
        except:
            raise

    # Return memory failcount
    def __get_mem_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.failcnt"), 'r').read().strip())
        except:
            return None

    # Return vmem failcount
    def __get_memsw_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.failcnt"), 'r').read().strip())
        except:
            return None

    # Return hpmem failcount
    def __get_hugetlb_failcnt(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.failcnt"))[0], 'r').read().strip())
        except:
            return None

    # Return the max usage of memory in bytes
    def __get_max_mem_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of memsw in bytes
    def __get_max_memsw_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of hugetlb in bytes
    def __get_max_hugetlb_usage(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.max_usage_in_bytes"))[0],
                            'r').read().strip())
        except:
            return None

    # Return the cpuacct.usage in cpu seconds
    def __get_cpu_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "cpuacct.usage"), 'r').read().strip())
        except:
            return None

    # Assign CPUs to the cpuset
    def select_cpus(self, path, ncpus):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path is %s" % (caller_name(), path))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: ncpus is %s" %
                   (caller_name(), ncpus))
        if ncpus < 1:
            ncpus = 1
        try:
            # Must select from those currently available
            cpufile = os.path.basename(path)
            base = os.path.dirname(path)
            parent = os.path.dirname(base)
            avail = cpus2list(open(os.path.join(parent, cpufile),
                              'r').read().strip())
            if len(avail) < 1:
                raise CgroupProcessingError("No CPUs avaialble in cgroup.")
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Available CPUs: %s" %
                       (caller_name(), avail))
            assigned = []
            for file in glob.glob(os.path.join(parent, '[0-9]*', cpufile)):
                cpus = cpus2list(open(file, 'r').read().strip())
                for id in cpus:
                    if id in avail:
                        avail.remove(id)
            if len(avail) < ncpus:
                raise CgroupProcessingError(
                    "Insufficient CPUs avaialble in cgroup.")
            if len(avail) == ncpus:
                return avail
            # FUTURE: Try to minimize NUMA nodes based on memory requirement
            return avail[0:ncpus]
        except:
            raise

    # Return the error message in system message file
    def __get_error_msg(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        proc = subprocess.Popen(['dmesg'],
                                shell=False, stdout=subprocess.PIPE)
        stdout = proc.communicate()[0].splitlines()
        stdout.reverse()
        # Check to see if the job id is found in dmesg otherwise
        # you could get the information for another job that was killed
        # the line will look like Memory cgroup stats for /pbspro/279.centos7
        kill_line = ""

        for line in stdout:
            start = line.find('Killed process ')
            if start >= 0:
                kill_line = line[start:]
            job_start = line.find(jobid)
            if job_start >= 0:
                # pbs.logmsg(pbs.EVENT_DEBUG3,
                #           "%s: Jobid: %s, Line: %s" %
                #           (caller_name(), jobid, line))
                return kill_line
        return ""

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_job_env_file(self, jobid, env_list):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        # if not os.path.exists(self.host_job_env_dir):
        #    os.makedirs(self.host_job_env_dir, 0755)

        # Write out assigned_resources
        try:
            lines = string.join(env_list, '\n')
            filename = self.host_job_env_filename % jobid
            outfile = open(filename, "w")
            outfile.write(lines)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" % (filename))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (lines))
            return True
        except:
            return False

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        if not os.path.exists(self.hook_storage_dir):
            os.makedirs(self.hook_storage_dir, 0755)

        # Write out assigned_resources
        try:
            json_str = json.dumps(self.host_assigned_resources)
            outfile = open(self.hook_storage_dir+os.sep+jobid, "w")
            outfile.write(json_str)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" %
                       (self.hook_storage_dir+os.sep+jobid))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (json_str))
            return True
        except:
            return False

    def read_in_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        pbs.logmsg(pbs.EVENT_DEBUG3, "Host assigned resources: %s" %
                   (self.host_assigned_resources))
        if os.path.isfile(self.hook_storage_dir+os.sep+jobid):
            # Write out assigned_resources
            try:
                infile = open(self.hook_storage_dir+os.sep+jobid, 'r')
                json_data = json.load(infile, object_hook=decode_dict)
                self.host_assigned_resources = json_data
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Host assigned resources: %s" %
                           (self.host_assigned_resources))
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
            finally:
                infile.close()

        if self.host_assigned_resources is not None:
            return True
        else:
            return False

#
#
# FUNCTION main
#


def main():
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Function called" % (caller_name()))
    hostname = pbs.get_local_nodename()
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Host is %s" % (caller_name(), hostname))

    # Log the hook event type
    e = pbs.event()
    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook name is %s" %
               (caller_name(), e.hook_name))

    try:
        # Instantiate the hook utility class
        hooks = HookUtils()
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Event type is %s" %
                   (caller_name(), hooks.event_name(e.type)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(hooks)))
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: Failed to instantiate hook utility class" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        e.accept()

    if not hooks.hashandler(e.type):
        # Bail out if there is no handler for this event
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s event not handled by this hook" %
                   (caller_name(), hooks.event_name(e.type)))
        e.accept()

    try:
        # If an exception occurs, jobutil must be set
        jobutil = None
        # Instantiate the job utility class first so jobutil can be accessed
        # by the exception handlers.
        if hasattr(e, 'job'):
            jobutil = JobUtils(e.job)
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Job information class instantiated" %
                       (caller_name()))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" %
                       (caller_name(), repr(jobutil)))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Event does not include a job" %
                       (caller_name()))
        # Instantiate the cgroup utility class
        vnode = None
        if hasattr(e, 'vnode_list'):
            if hostname in e.vnode_list.keys():
                vnode = e.vnode_list[hostname]
        cgroup = CgroupUtils(hostname, vnode)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Cgroup utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(cgroup)))
        # Bail out if there is nothing to do
        if len(cgroup.subsystems) < 1:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: All cgroups disabled" %
                       (caller_name()))
            e.accept()

        # Call the appropriate handler
        if hooks.invoke_handler(e, cgroup, jobutil) is True:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned success" %
                       (caller_name()))
            e.accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned failure" %
                       (caller_name()))
            e.reject()

    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass

    except AdminError, exc:
        # Something on the system is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Admin error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except UserError, exc:
        # User must correct problem and resubmit job
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("User error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.delete()
                msg += " (deleted)"
            except:
                msg += " (delete failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except JobValueError, exc:
        # Something in PBS is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Job value error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except Exception, exc:
        # Catch all other exceptions and report them.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Unexpected error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

# line below required for unittesting
if __name__ == "__builtin__":
    start_time = time.time()
    try:
        main()
    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
    finally:
        pbs.logmsg(pbs.EVENT_DEBUG, "Elapsed time: %0.4lf" %
                   (time.time() - start_time))
---
2016-07-18 16:55:11,650 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups.json', 'content-encoding': 'default'}
2016-07-18 16:55:11,650 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups.json
2016-07-18 16:55:11,684 INFO     job: executable set to /bin/sleep with arguments: 100
2016-07-18 16:55:11,685 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0755 /tmp/PtlPbsJobScriptca_Nqg
2016-07-18 16:55:11,699 INFOCLI  centos7: sudo -H -u pbsuser /usr/local/bin/qsub -N test_t1 -l select=1:ncpus=1:mem=300mb /tmp/PtlPbsJobScriptca_Nqg
2016-07-18 16:55:11,770 INFO     submit to centos7 as pbsuser: job 373.centos7 OrderedDict([('Job_Name', 'test_t1'), ('Resource_List.select', '1:ncpus=1:mem=300mb')])
2016-07-18 16:55:11,771 INFOCLI  job script /tmp/PtlPbsJobScriptca_Nqg
---
#!/bin/bash
if [ -d /cgroup/cpuset/pbspro/$PBS_JOBID ]; then
    ls -1 /cgroup/cpuset/pbspro
    cpus=`cat /cgroup/cpuset/pbspro/$PBS_JOBID/cpuset.cpus`
    echo "CpuSocket=${cpus}"
    mems=`cat /cgroup/cpuset/pbspro/$PBS_JOBID/cpuset.mems`
    echo "MemorySocket=${mems}"
    if [ -d /cgroup/memory/pbspro/$PBS_JOBID ]; then
       mem_limit=`cat /cgroup/memory/pbspro/$PBS_JOBID/memory.limit_in_bytes`
       memsw_limit=`cat /cgroup/memory/pbspro/$PBS_JOBID/memory.memsw.limit_in_bytes`
       echo "MemoryLimit=${mem_limit}"
       echo "MemswLimit=${memsw_limit}"
    fi
elif [ -d /sys/fs/cgroup/cpuset/pbspro/$PBS_JOBID ]; then
    ls -1 /sys/fs/cgroup/cpuset/pbspro
    cpus=`cat /sys/fs/cgroup/cpuset/pbspro/$PBS_JOBID/cpuset.cpus`
    echo "CpuSocket=${cpus}"
    mems=`cat /sys/fs/cgroup/cpuset/pbspro/$PBS_JOBID/cpuset.mems`
    echo "MemorySocket=${mems}"
    if [ -d /sys/fs/cgroup/memory/pbspro/$PBS_JOBID ]; then
       mem_limit=`cat /sys/fs/cgroup/memory/pbspro/$PBS_JOBID/memory.limit_in_bytes`
       memsw_limit=`cat /sys/fs/cgroup/memory/pbspro/$PBS_JOBID/memory.memsw.limit_in_bytes`
       echo "MemoryLimit=${mem_limit}"
       echo "MemswLimit=${memsw_limit}"
    fi
else
    echo "Unable to find the pbspro directory in the cgroup directory"
fi
sleep 1

---
2016-07-18 16:55:11,771 INFOCLI  centos7: /usr/local/bin/qstat -f 373.centos7
2016-07-18 16:55:11,870 INFO     expect on server centos7: job_state = R job 373.centos7  got: job_state = Q
2016-07-18 16:55:12,372 INFOCLI  centos7: /usr/local/bin/qmgr -c set server scheduling=True
2016-07-18 16:55:12,391 INFOCLI  centos7: /usr/local/bin/qstat -f 373.centos7
2016-07-18 16:55:12,494 INFO     expect on server centos7: job_state = R job 373.centos7 attempt: 2 ...  OK
2016-07-18 16:55:12,494 INFO     status on centos7: job 373.centos7 ['Output_Path', 'Error_Path', 'Keep_Files']
2016-07-18 16:55:12,494 INFOCLI  centos7: /usr/local/bin/qstat -f 373.centos7
2016-07-18 16:55:12,591 INFO     Job .o file: centos7.usa.org:/tmp/test_t1.o373
2016-07-18 16:55:14,592 INFO     test_t1: ['373.centos7', 'cgroup.clone_children', 'cgroup.event_control', 'cgroup.procs', 'cpuset.cpu_exclusive', 'cpuset.cpus', 'cpuset.mem_exclusive', 'cpuset.mem_hardwall', 'cpuset.memory_migrate', 'cpuset.memory_pressure', 'cpuset.memory_spread_page', 'cpuset.memory_spread_slab', 'cpuset.mems', 'cpuset.sched_load_balance', 'cpuset.sched_relax_domain_level', 'notify_on_release', 'tasks', 'CpuSocket=0', 'MemorySocket=0', 'MemoryLimit=314572800', 'MemswLimit=314572800', '']
2016-07-18 16:55:14,592 INFO     job dir check passed
2016-07-18 16:55:14,592 INFO     CpuSocket check passed
2016-07-18 16:55:14,592 INFO     MemorySocket check passed
2016-07-18 16:55:14,593 INFO     MemoryLimit check passed
2016-07-18 16:55:14,593 INFO     ==============================
2016-07-18 16:55:14,593 INFO      Entered CgroupsTests tearDown
2016-07-18 16:55:14,593 INFO     ==============================
2016-07-18 16:55:14,593 INFO     ===============================
2016-07-18 16:55:14,593 INFO     Completed CgroupsTests tearDown
2016-07-18 16:55:14,593 INFO     ===============================
2016-07-18 16:55:14,594 INFO     ok

2016-07-18 16:55:14,594 INFO     test name: test_t1b (tests.cgroups_tests.CgroupsTests)...
2016-07-18 16:55:14,594 INFO     test start time: Mon Jul 18 16:55:14 2016
2016-07-18 16:55:14,594 INFO     test docstring: 
 Test to verify that the job cgroup is created correctly
            using the default memory and swap
            Check to see that cpuset.cpus=0, cpuset.mems=0 and that
            memory.limit_in_bytes = 104857600
            memory.memsw.limit_in_bytes = 104857600
         
2016-07-18 16:55:14,595 INFO     ===========================
2016-07-18 16:55:14,595 INFO      Entered CgroupsTests setUp
2016-07-18 16:55:14,595 INFO     ===========================
2016-07-18 16:55:14,595 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:14,648 INFO     status on centos7: server
2016-07-18 16:55:14,648 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:14,703 INFO     manager on centos7: set server {'managers': (3, 'root@*')}
2016-07-18 16:55:14,703 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers-=root@*
2016-07-18 16:55:14,748 INFO     manager on centos7: set server {'managers': (2, 'root@*')}
2016-07-18 16:55:14,748 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers+=root@*
2016-07-18 16:55:14,774 INFO     server centos7: reverting configuration to defaults
2016-07-18 16:55:14,775 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:14,847 INFO     select on centos7: __ALL__
2016-07-18 16:55:14,847 INFOCLI  centos7: /usr/local/bin/qselect
2016-07-18 16:55:14,864 INFOCLI  centos7: /usr/local/bin/qstat -f @centos7.usa.org
2016-07-18 16:55:14,920 INFO     expect on server centos7: job_state set 0 job ...  OK
2016-07-18 16:55:14,921 INFOCLI  centos7: /usr/local/bin/pbs_rstat -f
2016-07-18 16:55:14,948 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:14,977 INFO     manager on centos7: delete hook t1_l1
2016-07-18 16:55:14,978 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c delete hook t1_l1
2016-07-18 16:55:15,003 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:55:15,030 INFO     expect on server centos7:  unset  hook t1_l1 ...  OK
2016-07-18 16:55:15,031 INFOCLI  centos7: /usr/local/bin/qstat -Qf @centos7.usa.org
2016-07-18 16:55:15,065 INFO     manager on centos7: delete queue workq
2016-07-18 16:55:15,065 INFOCLI  centos7: /usr/local/bin/qmgr -c delete queue workq
2016-07-18 16:55:15,092 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:55:15,112 INFO     expect on server centos7:  unset  queue workq ...  OK
2016-07-18 16:55:15,112 INFO     manager on centos7: create queue workq {'started': 'True', 'queue_type': 'Execution', 'enabled': 'True'}
2016-07-18 16:55:15,112 INFOCLI  centos7: /usr/local/bin/qmgr -c create queue workq started=True,queue_type=Execution,enabled=True
2016-07-18 16:55:15,131 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:55:15,154 INFO     expect on server centos7: started set True || queue_type set Execution || enabled set True queue workq ...  OK
2016-07-18 16:55:15,154 INFO     manager on centos7: set server {'log_events': '511', 'default_queue': 'workq'}
2016-07-18 16:55:15,155 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=511,default_queue=workq
2016-07-18 16:55:15,193 INFO     status on centos7: resource
2016-07-18 16:55:15,194 INFO     manager on centos7: list resource
2016-07-18 16:55:15,194 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource
2016-07-18 16:55:15,247 INFO     manager on centos7: delete resource nmics,ngpus
2016-07-18 16:55:15,247 INFOCLI  centos7: /usr/local/bin/qmgr -c delete resource nmics,ngpus
2016-07-18 16:55:15,330 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource nmics
2016-07-18 16:55:15,350 INFO     expect on server centos7:  unset  resource nmics ...  OK
2016-07-18 16:55:15,351 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource ngpus
2016-07-18 16:55:15,372 INFO     expect on server centos7:  unset  resource ngpus ...  OK
2016-07-18 16:55:15,372 INFOCLI  status on centos7: server license_count
2016-07-18 16:55:15,373 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:15,430 INFO     server: centos7.usa.org licensed
2016-07-18 16:55:15,457 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/server_priv/comm.lock
2016-07-18 16:55:15,512 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:55:15,528 INFO     scheduler centos7: reverting configuration to defaults
2016-07-18 16:55:15,528 INFO     manager on centos7: list sched
2016-07-18 16:55:15,528 INFOCLI  centos7: /usr/local/bin/qmgr -c list sched
2016-07-18 16:55:15,548 INFO     manager on centos7: unset sched ['sched_cycle_length']
2016-07-18 16:55:15,549 INFOCLI  centos7: /usr/local/bin/qmgr -c unset sched sched_cycle_length
2016-07-18 16:55:15,565 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/dedicated_time
2016-07-18 16:55:15,578 INFOCLI2 centos7: cmp /usr/local/etc/pbs_resource_group /var/spool/pbs/sched_priv/resource_group
2016-07-18 16:55:15,582 INFO     scheduler centos7: reverting holidays file to default
2016-07-18 16:55:15,582 INFOCLI2 centos7: cmp /usr/local/etc/pbs_holidays /var/spool/pbs/sched_priv/holidays
2016-07-18 16:55:15,585 INFOCLI2 centos7: cmp /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:15,589 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:15,615 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:15,636 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:55:15,636 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:55:15,724 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:55:15,748 INFOCLI2 centos7: sudo -H cmp /usr/local/sbin/pbs_mom /usr/local/sbin/pbs_mom.cpuset
2016-07-18 16:55:15,783 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:55:15,797 INFO     mom centos7: reverting configuration to defaults
2016-07-18 16:55:15,797 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/epilogue
2016-07-18 16:55:15,810 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/prologue
2016-07-18 16:55:15,826 INFOCLI2 centos7: sudo -H ls /var/spool/pbs/mom_priv/config.d
2016-07-18 16:55:15,839 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbswkkGX0 /var/spool/pbs/mom_priv/config
2016-07-18 16:55:15,850 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/mom_priv/config
2016-07-18 16:55:15,864 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/mom_priv/config
2016-07-18 16:55:15,881 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/mom_priv/config
2016-07-18 16:55:15,891 INFO     mom centos7: sent signal -HUP
2016-07-18 16:55:15,891 INFOCLI2 centos7: sudo -H kill -HUP 42976
2016-07-18 16:55:15,934 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:55:15,946 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 16:55:15,958 INFO     status on centos7: node centos7
2016-07-18 16:55:15,959 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:55:15,977 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:55:15,995 INFO     expect on server centos7: state = free node centos7 ...  OK
2016-07-18 16:55:15,995 INFO     ============================
2016-07-18 16:55:15,996 INFO     Completed CgroupsTests setUp
2016-07-18 16:55:15,996 INFO     ============================
2016-07-18 16:55:15,996 INFO     server centos7: server operating mode set to cli
2016-07-18 16:55:15,996 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:16,072 INFO     expect on server centos7: pbs_version ~ .*14.* server centos7.usa.org ...  OK
2016-07-18 16:55:16,073 INFO     manager on centos7: set server {'log_events': '2047'}
2016-07-18 16:55:16,073 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=2047
2016-07-18 16:55:16,096 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:16,162 INFO     expect on server centos7: log_events set 2047 server centos7.usa.org ...  OK
2016-07-18 16:55:16,162 INFO     scheduler centos7: config {'resources': 'ncpus,mem,host,vnode,vmem'}
2016-07-18 16:55:16,164 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /var/spool/pbs/sched_priv/sched_config /var/spool/pbs/sched_priv/sched_config.bak
2016-07-18 16:55:16,177 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsGqRGVi /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:16,198 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:16,210 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:16,224 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:16,248 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:55:16,249 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:55:16,261 INFO     scheduler centos7 log match: searching for "Error reading line" - No match
2016-07-18 16:55:17,262 INFO     manager on centos7 as root: create resource nmics {'flag': 'nh', 'type': 'long'}
2016-07-18 16:55:17,263 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource nmics flag=nh,type=long
2016-07-18 16:55:17,302 INFO     manager on centos7 as root: create resource ngpus {'flag': 'nh', 'type': 'long'}
2016-07-18 16:55:17,302 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource ngpus flag=nh,type=long
2016-07-18 16:55:17,338 INFO     Test Summary: test_t1_l1 tests "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" cgroup hook
2016-07-18 16:55:17,338 INFO     status on centos7: hook
2016-07-18 16:55:17,338 INFO     manager on centos7: list hook
2016-07-18 16:55:17,338 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:17,360 INFO     status on centos7: hook
2016-07-18 16:55:17,360 INFO     manager on centos7: list hook
2016-07-18 16:55:17,360 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:17,399 INFO     manager on centos7: create hook t1_l1
2016-07-18 16:55:17,399 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c create hook t1_l1
2016-07-18 16:55:17,425 INFO     manager on centos7: set hook t1_l1 {'freq': 2, 'enabled': 'True', 'event': '"execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"'}
2016-07-18 16:55:17,425 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set hook t1_l1 freq=2,enabled=True,event="execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"
2016-07-18 16:55:17,454 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:55:17,493 INFO     expect on server centos7: freq set 2 || enabled set True || event set "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" hook t1_l1 ...  OK
2016-07-18 16:55:17,494 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-python', 'input-file': '/tmp/PtlPbseG9ukk', 'content-encoding': 'default'}
2016-07-18 16:55:17,494 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-python default /tmp/PtlPbseG9ukk
2016-07-18 16:55:17,569 INFO     server centos7: imported hook body
---
#  Copyright (C) 2003-2016 Altair Engineering, Inc. All rights reserved.
#
#  ALTAIR ENGINEERING INC. Proprietary and Confidential. Contains Trade Secret
#  Information. Not for use or disclosure outside ALTAIR and its licensed
#  clients. Information contained herein shall not be decompiled, disassembled,
#  duplicated or disclosed in whole or in part for any purpose. Usage of the
#  software is only as explicitly permitted in the end user software license
#  agreement.
#
#  Copyright notice does not imply publication.

# Last Updated
# Date: 07-06-2016
#
# When soft_limit is true for memory, memsw represents the hard limit.
#
# The resources value in sched_config must contain entries for mem and
# vmem if those subsystems are enabled in the hook configuration file. The
# amount of resource requested will not be avaiable to the hook if they
# are not present.

# This hook handles all of the operations necessary for PBS to support
# cgroups on linux hosts that support them (kernel 2.6.28 and higher)
#
# This hook services the following events:
# - exechost_periodic
# - exechost_startup
# - execjob_attach
# - execjob_begin
# - execjob_end
# - execjob_epilogue
# - execjob_launch

# Imports from __future__ must happen first
from __future__ import with_statement

# Additional imports
import sys
import os
import errno
import signal
import subprocess
import re
import glob
import time
import string
import platform
import traceback
import copy
from stat import S_ISBLK, S_ISCHR
import operator
import pwd
import pbs
import fnmatch
import socket

try:
    import json
except:
    import simplejson as json

CGROUP_KILL_ATTEMPTS = 12

# Flag to turn off features not in 12.2.40X branch
CRAY_12_BRANCH = False


# ============================================================================
# Derived error classes
# ============================================================================

# Base class for errors fixable only by administrative action.


class AdminError(Exception):
    pass

# Base class for errors in processing, unknown cause.


class ProcessingError(Exception):
    pass

# Base class for errors fixable by the user.


class UserError(Exception):
    pass

# Errors in PBS job resource values.


class JobValueError(UserError):
    pass

# Errors when the cgroup is busy.


class CgroupBusyError(ProcessingError):
    pass

# Errors in configuring cgroup.


class CgroupConfigError(AdminError):
    pass

# Errors in configuring cgroup.


class CgroupLimitError(AdminError):
    pass

# Errors processing cgroup.


class CgroupProcessingError(ProcessingError):
    pass

# ============================================================================
# Utility functions
# ============================================================================

#
# FUNCTION caller_name
#
# Return the name of the calling function or method.
#


def caller_name():
    return str(sys._getframe(1).f_code.co_name)

#
# FUNCTION convert_size
#
# Convert a string containing a size specification (e.g. "1m") to a
# string using different units (e.g. "1024k").
#
# This function only interprets a decimal number at the start of the string,
# stopping at any unrecognized character and ignoring the rest of the string.
#
# When down-converting (e.g. MB to KB), all calculations involve integers and
# the result returned is exact. When up-converting (e.g. KB to MB) floating
# point numbers are involved. The result is rounded up. For example:
#
# 1023MB -> GB yields 1g
# 1024MB -> GB yields 1g
# 1025MB -> GB yields 2g  <-- This value was rounded up
#
# Pattern matching or conversion may result in exceptions.
#


def convert_size(value, units='b'):
    logs = {'b': 0, 'k': 10, 'm': 20, 'g': 30,
            't': 40, 'p': 50, 'e': 60, 'z': 70, 'y': 80}
    try:
        new = units[0].lower()
        if new not in logs.keys():
            new = 'b'
        val, old = re.match('([-+]?\d+)([bkmgtpezy]?)',
                            str(value).lower()).groups()
        val = int(val)
        if val < 0:
            raise ValueError('Value may not be negative')
        if old not in logs.keys():
            old = 'b'
        factor = logs[old] - logs[new]
        val *= 2 ** factor
        slop = val - int(val)
        val = int(val)
        if slop > 0:
            val += 1
        # pbs.size() does not like units following zero
        if val <= 0:
            return '0'
        else:
            return str(val) + new
    except:
        return None

#
# FUNCTION size_as_int
#
# Convert a size string to an integer representation of size in bytes
#


def size_as_int(value):
    return int(convert_size(value).rstrip(string.ascii_lowercase))

#
# FUNCTION convert_time
#
# Converts a integer value for time into the value of the return unit
#
# A valid decimal number, with optional sign, may be followed by a character
# representing a scaling factor.  Scaling factors may be either upper or
# lower case. Examples include:
# 250ms
# 40s
# +15min
#
# Valid scaling factors are:
# ns  = 10**-9
# us  = 10**-6
# ms  = 10**-3
# s   =      1
# min =     60
# hr  =   3600
#
# Pattern matching or conversion may result in exceptions.
#


def convert_time(value, return_unit='s'):
    multipliers = {'':  1, 'ns': 10 ** -9, 'us': 10 ** -6,
                   'ms': 10 ** -3, 's': 1, 'min': 60, 'hr': 3600}
    n, factor = re.match('([-+]?\d+)(\w*[a-zA-Z])?',
                         str(value).lower()).groups(0)

    # Check to see if there was not unit of time specified
    if factor == 0:
        factor = ''

    # Check to see if the unit is valid
    if str.lower(factor) not in multipliers.keys():
        raise ValueError('Time unit not recognized.')

    # Convert the value to seconds
    value_in_sec = float(n) * float(multipliers[str.lower(factor)])
    if return_unit != 's':
        return value_in_sec / multipliers[str.lower(return_unit)]
    else:
        return float('%lf' % value_in_sec)

# simplejson hook to convert lists from unicode to utf-8


def decode_list(data):
    rv = []
    for item in data:
        if isinstance(item, unicode):
            item = item.encode('utf-8')
        elif isinstance(item, list):
            item = decode_list(item)
        elif isinstance(item, dict):
            item = decode_dict(item)
        rv.append(item)
    return rv

# simplejson hook to convert dictionaries from unicode to utf-8


def decode_dict(data):
    rv = {}
    for key, value in data.iteritems():
        if isinstance(key, unicode):
            key = key.encode('utf-8')
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        elif isinstance(value, list):
            value = decode_list(value)
        elif isinstance(value, dict):
            value = decode_dict(value)
        rv[key] = value
    return rv

# Convert CPU list format (with ranges) to a Python list


def cpus2list(s):
    # The input string is a comma separated list of digits and ranges.
    # Examples include:
    # 0-3,8-11
    # 0,2,4,6
    # 2,5-7,10
    cpus = []
    if s.strip() == '':
        return cpus
    for r in s.split(','):
        if '-' in r[1:]:
            start, end = r.split('-', 1)
            for i in range(int(start), int(end) + 1):
                cpus.append(int(i))
        else:
            cpus.append(int(r))
    return cpus


# Helper function to ignore suspended jobs when removing
# "already used" resources
def job_to_be_ignored(jobid):
    # Get job substate from small utility that calls printjob
    pbs_exec = ''
    if not CRAY_12_BRANCH:
        pbs_conf = pbs.get_pbs_conf()
        pbs_exec = pbs_conf['PBS_EXEC']
    else:
        if 'PBS_EXEC' in self.cfg:
            pbs_exec = self.cfg['PBS_EXEC']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "PBS_EXEC needs to be defined in the config file")
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Exiting the cgroups hook")
            pbs.accept()

    printjob_cmd = pbs_exec+os.sep+'bin'+os.sep+'printjob'

    cmd = [printjob_cmd, jobid]

    substate = None
    try:
        pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
        # Collect the job substate information
        process = subprocess.Popen(
                                   cmd,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        (out, err) = process.communicate()
        # Find the job substate
        substate_re = re.compile(r"substate:\s+(?P<jobstate>\S+)\s+")
        substate = substate_re.search(out)
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Unexpected error in job_to_be_ignored: %s" %
                   ' '.join([repr(sys.exc_info()[0]),
                            repr(sys.exc_info()[1])]))
        out = "Unknown: failed to run " + printjob_cmd

    if substate is not None:
        substate = substate.group().split(':')[1].strip()
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Job %s has substate %s" % (jobid, substate))

        suspended_substates = ['0x2b', '0x2d', 'unknown']
        return (substate in suspended_substates)
    else:
        return False

# ============================================================================
# Utility classes
# ============================================================================

#
# CLASS HookUtils
#


class HookUtils:

    def __init__(self, hook_events=None):
        if hook_events is not None:
            self.hook_events = hook_events
        else:
            self.hook_events = {
                # Defined in the order they appear in module_pbs_v1.c
                pbs.QUEUEJOB: {
                    'name': 'queuejob',
                    'handler': None
                },
                pbs.MODIFYJOB: {
                    'name': 'modifyjob',
                    'handler': None
                },
                pbs.RESVSUB: {
                    'name': 'resvsub',
                    'handler': None
                },
                pbs.MOVEJOB: {
                    'name': 'movejob',
                    'handler': None
                },
                pbs.RUNJOB: {
                    'name': 'runjob',
                    'handler': None
                },
                pbs.PROVISION: {
                    'name': 'provision',
                    'handler': None
                },
                pbs.EXECJOB_BEGIN: {
                    'name': 'execjob_begin',
                    'handler': self.__execjob_begin_handler
                },
                pbs.EXECJOB_PROLOGUE: {
                    'name': 'execjob_prologue',
                    'handler': None
                },
                pbs.EXECJOB_EPILOGUE: {
                    'name': 'execjob_epilogue',
                    'handler': self.__execjob_epilogue_handler
                },
                pbs.EXECJOB_PRETERM: {
                    'name': 'execjob_preterm',
                    'handler': None
                },
                pbs.EXECJOB_END: {
                    'name': 'execjob_end',
                    'handler': self.__execjob_end_handler
                },
                pbs.EXECJOB_LAUNCH: {
                    'name': 'execjob_launch',
                    'handler': self.__execjob_launch_handler
                },
                pbs.EXECHOST_PERIODIC: {
                    'name': 'exechost_periodic',
                    'handler': self.__exechost_periodic_handler
                },
                pbs.EXECHOST_STARTUP: {
                    'name': 'exechost_startup',
                    'handler': self.__exechost_startup_handler
                },
                pbs.MOM_EVENTS: {
                    'name': 'mom_events',
                    'handler': None
                },
            }

            if not CRAY_12_BRANCH:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "CRAY 12 Branch: %s" % (CRAY_12_BRANCH))
                self.hook_events[pbs.EXECJOB_ATTACH] = {
                    'name': 'execjob_attach',
                    'handler': self.__execjob_attach_handler
                }

    def __repr__(self):
        return "HookUtils(%s)" % (repr(self.hook_events))

    def event_name(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['name']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Type: %s not found" % (caller_name(), type))
            return None

    def hashandler(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['handler'] is not None
        else:
            return None

    def invoke_handler(self, event, cgroup, jobutil, *args):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: UID: real=%d, effective=%d" %
                   (caller_name(), os.getuid(), os.geteuid()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: GID: real=%d, effective=%d" %
                   (caller_name(), os.getgid(), os.getegid()))
        if self.hashandler(event.type):
            result = self.hook_events[event.type][
                'handler'](event, cgroup, jobutil, *args)
            return result
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: %s event not handled by this hook." %
                       (caller_name(), self.event_name(event.type)))

    def __execjob_begin_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Instantiate the NodeConfig class for get_memory_on_node and
        # get_vmem_on node
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Host assigned job resources: %s" %
                   (caller_name(), jobutil.host_resources))

        # Make sure the parent cgroup directories exist
        cgroup.create_paths()

        # Make sure that any old cgroups created in an earlier attempt
        # at creating or running the job are gone
        cgroup.delete(e.job.id)

        # Gather the assigned resources
        cgroup.gather_assigned_resources()
        # Create the cgroup(s) for the job
        cgroup.create_job(e.job.id, node)
        # Configure the new cgroup
        cgroup.configure_job(e.job.id, jobutil.host_resources, node)
        # Initialize resource usage for the job
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        # Write out the assigned resources
        cgroup.write_out_cgroup_host_assigned_resources(e.job.id)
        # Write out the environment variable for the host (pbs_attach)
        if 'devices_name' in cgroup.host_assigned_resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Devices: %s" %
                       cgroup.host_assigned_resources['devices_name'])
            env_list = list()
            if len(cgroup.host_assigned_resources['devices_name']) > 0:
                line = str()
                mics = list()
                gpus = list()
                for key in cgroup.host_assigned_resources['devices_name']:
                    if key.startswith('mic'):
                        mics.append(key[3:])
                    elif key.startswith('nvidia'):
                        gpus.append(key[6:])
                if len(mics) > 0:
                    env_list.append('OFFLOAD_DEVICES="%s"' %
                                    string.join(mics, ','))
                if len(gpus) > 0:
                    # don't put quotes around the values. ex "0" or "0,1".
                    # This will cause it to fail.
                    env_list.append('CUDA_VISIBLE_DEVICES=%s' %
                                    string.join(gpus, ','))

            pbs.logmsg(pbs.EVENT_DEBUG, "ENV_LIST: %s" % env_list)
            cgroup.write_out_cgroup_host_job_env_file(e.job.id, env_list)
        return True

    def __execjob_epilogue_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # The resources_used information has a base type of pbs_resource
        # Set the usage data
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        return True

    def __execjob_end_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Delete the cgroup(s) for the job
        cgroup.delete(e.job.id)
        # Remove the host_assigned_resources and job_env file
        for filename in [cgroup.hook_storage_dir+os.sep+e.job.id,
                         cgroup.host_job_env_filename % e.job.id]:
            try:
                os.remove(filename)
            except OSError:
                pbs.logmsg(pbs.EVENT_DEBUG3, "File: %s not found" % (filename))
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Error removing file: %s" % (filename))
                pass

        return True

    def __execjob_launch_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Add the parent process id to the appropriate cgroups.
        cgroup.add_pids(os.getppid(), jobutil.job.id)
        # FUTURE: Add environment variable to the job environment
        # if job requested mic or gpu
        cgroup.read_in_cgroup_host_assigned_resources(e.job.id)
        if cgroup.host_assigned_resources is not None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "host_assigned_resources: %s" %
                       (cgroup.host_assigned_resources))
            cgroup.setup_job_devices_env()
        return True

    def __exechost_periodic_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Cleanup cgroups for jobs not present on this node
        count = cgroup.cleanup_orphans(e.job_list)
        if ('periodic_resc_update' in cgroup.cfg and
                cgroup.cfg['periodic_resc_update']):
            for job in e.job_list.keys():
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: job is %s" %
                           (caller_name(), job))
                cgroup.update_job_usage(job, e.job_list[job].resources_used)
        # Online nodes that were offlined due to a cgroup not cleaning up
        if count == 0 and cgroup.cfg['online_offlined_nodes']:
            vnode = pbs.event().vnode_list[cgroup.hostname]
            if os.path.isfile(cgroup.offline_file):
                line = 'Orphan cgroup(s) have been cleaned up. '

                # Check with the pbs server to see if the node comment matches
                try:
                    tmp_comment = pbs.server().vnode(cgroup.hostname).comment
                except:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Unable to contact server for node comment")
                    tmp_comment = None
                if tmp_comment == cgroup.offline_msg:
                    line += 'Will bring the node back online'
                    vnode.state = pbs.ND_FREE
                    vnode.comment = None
                else:
                    line += 'However, the comment has changed since the node '
                    line += 'was offlined. Node will remain offline'

                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: %s" % (caller_name(), line))
                # Remove file
                os.remove(cgroup.offline_file)
        return True

    def __exechost_startup_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        cgroup.create_paths()
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))

        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        node.create_vnodes()
        if 'memory' in cgroup.subsystems:
            val = node.get_memory_on_node(cgroup.cfg)
            if val is not None:
                if 'vnode_per_numa_node' in cgroup.cfg:
                    if not cgroup.cfg['vnode_per_numa_node']:
                        hn = node.hostname
                        e.vnode_list[hn].resources_available['mem'] = \
                            pbs.size(val)
                        val_cpus = node.totalcpus
                        if val_cpus is not None:
                            e.vnode_list[hn].resources_available['ncpus'] = \
                                int(val_cpus)
                cgroup.set_node_limit('mem', val)
        if 'memsw' in cgroup.subsystems:
            val = node.get_vmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['vmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('vmem', val)
        if 'hugetlb' in cgroup.subsystems:
            val = node.get_hpmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['hpmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('hpmem', val)
        return True

    def __execjob_attach_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logjobmsg(jobutil.job.id, "%s: Attaching PID %s" %
                      (caller_name(), e.pid))
        # Add the job process id to the appropriate cgroups.
        cgroup.add_pids(e.pid, jobutil.job.id)
        return True

#
# CLASS ShallIRunUtils
#


class ShallIRunUtils:

    def __init__(self, hostname, cfg):
        self.cfg = cfg
        self.hostname = hostname

    def allow_users(self, allow_users):
        """ This still needs to be defined """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pass

    def exclude_hosts(self, exclude_hosts):
        """
        Gets the hostname for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        hostname = pbs.get_local_nodename()
        # check to see if hostname is in the exclude_hosts list
        if self.hostname in exclude_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: exclude host %s is in %s" %
                       (caller_name(), self.hostname, exclude_hosts))
            pbs.event().accept()

        # check to see if it regex syntax is used
        pass

    def exclude_vntypes(self, exclude_vntypes):
        """
        Gets the vntype for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        vntype = None

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'
        if os.path.isfile(vntype_file):
            fdata = open(vntype_file).readlines()
            vntype = fdata[0].strip()
            pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
        if vntype is None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype is set to %s" % (vntype))
        elif vntype in exclude_vntypes:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s is in %s" % (vntype, exclude_vntypes))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Exiting Hook")
            pbs.event().accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s not in global exclude: %s" %
                       (vntype, exclude_vntypes))
        pass

    def run_only_on_hosts(self, approved_hosts):
        """
        Gets the list of approved nodes to run on and checks to see if the hook
        should run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Approved hosts: %s, %s" %
                   (type(approved_hosts), approved_hosts))

        # check to see if hostname is in the approved_hosts list
        if approved_hosts == []:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "approved hosts list is empty: %s" % approved_hosts)
        elif self.hostname not in approved_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s is not in the approved list of hosts: %s" %
                       (self.hostname, approved_hosts))
            pbs.event().accept()

        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s in list of approved hosts: %s" %
                       (self.hostname, approved_hosts))

#
# CLASS JobUtils
#


class JobUtils:

    def __init__(self, job, hostname=None, resources=None):
        self.job = job
        self.host_vnodes = list()
        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if resources is not None:
            self.host_resources = resources
        else:
            self.host_resources = self.__host_assigned_resources()

    def __repr__(self):
        return "JobUtils(%s, %s, %s)" % (repr(self.job), repr(self.hostname),
                                         repr(self.host_resources))

    # Return a dictionary of resources assigned to the local node
    def __host_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Bail out if no hostname was provided
        if self.hostname is None:
            raise CgroupProcessingError('No hostname available')
        # Bail out if no job information is present
        if self.job is None:
            raise CgroupProcessingError('No job information available')
        # Determine which vnodes are on this host, if any
        if self.hostname is not None:
            vnhost_pattern = "%s\[[\d+]\]" % self.hostname
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnhost pattern: %s" %
                       (caller_name(), vnhost_pattern))
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job exec_vnode list: %s" %
                       (caller_name(), self.job.exec_vnode))
            for match in re.findall(vnhost_pattern, str(self.job.exec_vnode)):
                self.host_vnodes.append(match)
            if self.host_vnodes is not None:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Associate %s to vnodes on %s" %
                           (caller_name(), self.host_vnodes, self.hostname))

        # Collect host assigned resources
        resources = {}
        for chunk in self.job.exec_vnode.chunks:
            vnode = False
            if self.host_vnodes == [] and chunk.vnode_name != self.hostname:
                continue
            elif self.host_vnodes != [] and \
                    chunk.vnode_name not in self.host_vnodes:
                continue
            elif chunk.vnode_name in self.host_vnodes:
                vnode = True
                if 'vnodes' not in resources:
                    resources['vnodes'] = {}
                if chunk.vnode_name not in resources['vnodes']:
                    resources['vnodes'][chunk.vnode_name] = {}
                # This check is needed since not all resources are
                # required to be in each chunk of a job
                # i.e. exec_vnodes =
                # (node1[0]:ncpus=4:mem=4gb+node1[1]:mem=2gb) +
                # (node1[1]:ncpus=3+node[0]:ncpus=1:mem=4gb)
                if all(resc in resources['vnodes'][chunk.vnode_name]
                       for resc in chunk.chunk_resources.keys()):
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: resources already defined" %
                               (caller_name()))
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s,\t%s" %
                               (caller_name(),
                                chunk.chunk_resources.keys(),
                                resources['vnodes'][chunk.vnode_name].keys()))
                else:
                    # Initialize resources for each vnode
                    for resc in chunk.chunk_resources.keys():
                        # Initialize the new value
                        if isinstance(chunk.chunk_resources[resc],
                                      pbs.pbs_int):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_int(0)
                        elif isinstance(chunk.chunk_resources[resc],
                                        pbs.pbs_float):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_float(0)
                        elif isinstance(chunk.chunk_resources[resc], pbs.size):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.size('0')

            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Chunk %s" %
                       (caller_name(), chunk.vnode_name))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resources: %s" %
                       (caller_name(), resources))
            for resc in chunk.chunk_resources.keys():
                if resc not in resources.keys():
                    # Initialize the new value
                    if isinstance(chunk.chunk_resources[resc], pbs.pbs_int):
                        resources[resc] = pbs.pbs_int(0)
                    elif isinstance(chunk.chunk_resources[resc],
                                    pbs.pbs_float):
                        resources[resc] = pbs.pbs_float(0.0)
                    elif isinstance(chunk.chunk_resources[resc], pbs.size):
                        resources[resc] = pbs.size('0')
                # Add resource value to total
                if type(chunk.chunk_resources[resc]) in \
                        [pbs.pbs_int, pbs.pbs_float, pbs.size]:
                    resources[resc] += chunk.chunk_resources[resc]
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s: resources[%s][%s] is now %s" %
                               (caller_name(), self.hostname, resc,
                                resources[resc]))
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] += \
                                  chunk.chunk_resources[resc]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Setting resource %s to string %s" %
                               (caller_name(), resc,
                                str(chunk.chunk_resources[resc])))
                    resources[resc] = str(chunk.chunk_resources[resc])
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] = \
                                  str(chunk.chunk_resources[resc])
        if not resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: No resources assigned to host %s" %
                       (caller_name(), self.hostname))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources for %s: %s" %
                       (caller_name(), self.hostname, repr(resources)))

        # Return assigned resources for specified host
        pbs.logmsg(pbs.EVENT_DEBUG3, "Job Resources: %s" % (resources))
        return resources

    # Write a message to the job stdout file
    def write_to_stderr(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stderr_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

    # Write a message to the job stdout file
    def write_to_stdout(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stdout_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

#
# CLASS NodeConfig
#


class NodeConfig:

    def __init__(self, cfg, **kwargs):

        self.cfg = cfg
        hostname = None
        meminfo = None
        numa_nodes = None
        devices = None
        hyperthreading = None
        self.hyperthreads_per_core = 1
        self.totalcpus = self.__discover_cpus()

        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        self.host_mom_jobdir = pbs_home+'/mom_priv/jobs'

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'hostname':
                    hostname = val
                elif arg == 'meminfo':
                    meminfo = val
                elif arg == 'numa_nodes':
                    numa_nodes = val
                elif arg == 'devices':
                    devices = val
                elif arg == 'hyperthreading':
                    hyperthreading = val

        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if meminfo is not None:
            self.meminfo = meminfo
        else:
            self.meminfo = self.__discover_meminfo()
        if numa_nodes is not None:
            self.numa_nodes = numa_nodes
        else:
            self.numa_nodes = self.__discover_numa_nodes()
        if devices is not None:
            self.devices = devices
        else:
            self.devices = self.__discover_devices()
        if hyperthreading is not None:
            self.hyperthreading = hyperthreading
        else:
            self.hyperthreading = self.__hyperthreading_enabled()

        # Add the devices count i.e. nmics and ngpus to the numa nodes
        self.__add_devices_to_numa_node()

    def __repr__(self):
        return ("NodeConfig(%s, %s, %s, %s, %s, %s)" %
                (repr(self.cfg), repr(self.hostname), repr(self.meminfo),
                 repr(self.numa_nodes), repr(self.devices),
                 repr(self.hyperthreading)))

    def __add_devices_to_numa_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (self.devices.keys()))
        for device in self.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" % (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" %
                           (self.devices[device].keys()))
                for device_name in self.devices[device]:
                    device_socket = \
                        self.devices[device][device_name]['numa_node']
                    if device == 'mic':
                        if 'nmics' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['nmics'] = 1
                        else:
                            self.numa_nodes[device_socket]['nmics'] += 1
                    elif device == 'gpu':
                        if 'ngpus' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['ngpus'] = 1
                        else:
                            self.numa_nodes[device_socket]['ngpus'] += 1

        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (self.numa_nodes))

    # Discover what type of hardware is on this node and how is it partitioned
    def __discover_numa_nodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        numa_nodes = {}
        for dir in glob.glob(os.path.join(os.sep,
                             "sys", "devices", "system", "node", "node*")):
            id = int(dir.split(os.sep)[5][4:])
            if id not in numa_nodes.keys():
                numa_nodes[id] = {}
                numa_nodes[id]['devices'] = list()
            numa_nodes[id]['cpus'] = open(os.path.join(dir, "cpulist"),
                                          'r').readline().strip()
            with open(os.path.join(dir, "meminfo"), 'r') as fd:
                for line in fd:
                    # Each line will contain four or five fields. Examples:
                    # Node 0 MemTotal:       32995028 kB
                    # Node 0 HugePages_Total:     0
                    entries = line.split()
                    if len(entries) < 4:
                        continue
                    if entries[2] == "MemTotal:":
                        numa_nodes[id][entries[2].rstrip(':')] = \
                            convert_size(entries[3] + entries[4], 'kb')
                    elif entries[2] == "HugePages_Total:":
                        numa_nodes[id][entries[2].rstrip(':')] = entries[3]
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover Numa Nodes: %s" % numa_nodes)
        return numa_nodes

    # Identify devices and to which numa nodes they are attached
    def __discover_devices(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        devices = {}
        for file in glob.glob(os.path.join(os.sep, "sys", "class", "*", "*",
                                           "device", "numa_node")):
            # The file should contain a single integer
            numa_node = int(open(file, 'r').readline().strip())
            if numa_node < 0:
                numa_node = 0
            dirs = file.split(os.sep)
            name = dirs[3]
            instance = dirs[4]
            if name not in devices.keys():
                devices[name] = {}
            devices[name][instance] = {}
            devices[name][instance]['numa_node'] = numa_node
            if name == 'mic':
                s = os.stat('/dev/%s' % instance)
                devices[name][instance]['major'] = os.major(s.st_rdev)
                devices[name][instance]['minor'] = os.minor(s.st_rdev)
                devices[name][instance]['device_type'] = 'c'

        # Check to see if there are gpus on the node
        gpus = self.discover_gpus()
        if isinstance(gpus, dict) and len(gpus.keys()) > 0:
            devices['gpu'] = gpus['gpu']

        pbs.logmsg(pbs.EVENT_DEBUG3, "Discovered devices: %s" % (devices))
        return devices

    def discover_gpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Check to see if nvidia-smi exists and produces valid output
            cmd = ['/usr/bin/nvidia-smi', '-q', '-x']
            if 'nvidia-smi' in self.cfg:
                cmd[0] = self.cfg['nvidia-smi']

            pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
            # Collect the gpu information
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.PIPE)
            output, err = process.communicate()

            # pbs.logmsg(pbs.EVENT_DEBUG3,"output: %s" % output)
            # import the xml library and parse the output
            import xml.etree.ElementTree as ET

            # Parse the data
            name = 'gpu'
            gpu_data = dict()
            gpu_data[name] = {}
            root = ET.fromstring(output)
            pbs.logmsg(pbs.EVENT_DEBUG3, "root.tag: %s" % root.tag)
            for child in root:
                if child.tag == "attached_gpus":
                    # gpu_data['ngpus'] = int(child.text)
                    pass
                if child.tag == "gpu":
                    device_id = child.get('id')
                    number = 'nvidia%s' % child.find('minor_number').text
                    gpu_data[name][number] = dict()
                    gpu_data[name][number]['id'] = device_id

                    # Determine which socket the device is on
                    filename = '/sys/bus/pci/devices/%s/numa_node' % \
                        device_id.lower()
                    isfile = os.path.isfile(filename)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s, %s" % (filename, isfile))
                    if isfile:
                        numa_node = open(filename).read().strip()
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "numa_node: %s" % (numa_node))
                        if int(numa_node) == -1:
                            numa_node = 0
                        gpu_data[name][number]['numa_node'] = int(numa_node)
                        try:
                            dev_filename = '/dev/%s' % number
                            s = os.stat(dev_filename)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find %s" % dev_filename)
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                        gpu_data[name][number]['major'] = os.major(s.st_rdev)
                        gpu_data[name][number]['minor'] = os.minor(s.st_rdev)
                        gpu_data[name][number]['device_type'] = 'c'
            pbs.logmsg(pbs.EVENT_DEBUG, "%s" % gpu_data)
            return gpu_data
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unable to find %s" % string.join(cmd, " "))
            return {'gpu': {}}
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        return {'gpu': {}}

    # Get the memory info on this host
    def __discover_meminfo(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        meminfo = {}
        with open(os.path.join(os.sep, "proc", "meminfo"), 'r') as fd:
            for line in fd:
                entries = line.split()
                if entries[0] == "MemTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "SwapTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "Hugepagesize:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "HugePages_Total:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
                elif entries[0] == "HugePages_Rsvd:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover meminfo: %s" % meminfo)
        return meminfo

    # Determine if the cpus have hyperthreading enabled
    def __hyperthreading_enabled(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            siblings = 0
            cpu_cores = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "siblings":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    siblings = entries[1].strip()
                elif entries[0].strip() == "cpu cores":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_cores = entries[1].strip()
                elif entries[0].strip() == "flags":
                    flag_options = entries[1].split()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: flag_options: %s" %
                               (caller_name(), flag_options))
                    if "ht" in flag_options:
                        rc = True
                        break
        if rc is True:
            self.hyperthreads_per_core = int(siblings)/int(cpu_cores)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: hyperthreads/core: %d" %
                       (caller_name(), self.hyperthreads_per_core))
            if self.hyperthreads_per_core == 1:
                rc = False
        return rc

    def __discover_cpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            cpu_threads = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "processor":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_threads += 1
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: processors found: %d" %
                   (caller_name(), cpu_threads))

        return cpu_threads

    # Gather the jobs running on this node
    def gather_jobs_on_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        # Get the job list from the server
        jobs = None
        try:
            jobs = pbs.server().vnode(self.hostname).jobs
        except:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Unable to contact server to get jobs")
            return None

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs from server for %s: %s" % (self.hostname, jobs))

        if jobs is None:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "No jobs found on %s " % (self.hostname))
            return list()

        jobs_list = dict()
        for job in jobs.split(','):
            # The job list from the node has a space in front of the job id
            # This must be removed or it will not match the cgroup dir
            jobs_list[job.split('/')[0].strip()] = 0
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs on %s: %s" % (self.hostname, jobs_list.keys()))

        return jobs_list.keys()

    # Gather the jobs running on this node
    def gather_jobs_on_node_local(self):
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Method called" % (caller_name()))
        # Get the job list from the jobs directory
        jobs_list = list()
        try:
            for file in os.listdir(self.host_mom_jobdir):
                if fnmatch.fnmatch(file, "*.JB"):
                    jobs_list.append(file[:-3])
            if not jobs_list:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "No jobs found on %s " % (self.hostname))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Jobs from server for %s: %s" %
                           (self.hostname, repr(jobs_list)))

            return jobs_list
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Could not get job list from mom_priv/jobs for %s" %
                       self.hostname)

            return self.gather_jobs_on_node()

    # Get the memory resource on this mom
    def get_memory_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Calculate memory
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
            pbs.logmsg(pbs.EVENT_DEBUG3, "val: %s" % val)
        else:
            val = size_as_int(MemTotal)
        if 'percent_reserve' in config['cgroup']['memory']:
            percent_reserve = config['cgroup']['memory']['percent_reserve']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memory'].keys():
            reserve_memory = config['cgroup']['memory']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                reserve_mem = size_as_int(reserve_memory)
                if val > reserve_mem:
                    val -= reserve_mem
                else:
                    val -= size_as_int("100mb")
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Unable to reserve more memory than " +
                               "available on the node. Available %s, " +
                               "Tried to reserve %s. Reserving 100mb" %
                               (val, reserve_mem))

            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))

        pbs.logmsg(pbs.EVENT_DEBUG3, "Return val: %s" % val)
        return convert_size(str(val), 'kb')

    # Get the virtual memory resource on this mom
    def get_vmem_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Return the value for memory if no swap is configured
        if size_as_int(self.meminfo['SwapTotal']) <= 0:
            return self.get_memory_on_node(config)
        # Calculate vmem
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
        else:
            val = size_as_int(MemTotal)

        val += size_as_int(self.meminfo['SwapTotal'])
        # Determine if memory needs to be reserved
        if 'percent_reserve' in config['cgroup']['memsw']:
            percent_reserve = config['cgroup']['memsw']['reserve_memory']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memsw']:
            reserve_memory = config['cgroup']['memsw']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                val -= size_as_int(reserve_memory)
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))
        return convert_size(str(val), 'kb')

    # Get the huge page memory resource on this mom
    def get_hpmem_on_node(self, config):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        if 'percent_reserve' in config['cgroup']['hugetlb'].keys():
            percent_reserve = config['cgroup']['hugetlb']['percent_reserve']
        else:
            percent_reserve = 0
        # Calculate vmem
        val = size_as_int(self.meminfo['Hugepagesize'])
        val *= (self.meminfo['HugePages_Total'] -
                self.meminfo['HugePages_Rsvd'])
        val -= (val * percent_reserve) / 100
        return convert_size(str(val), 'kb')

    # Create individual vnodes per socket
    def create_vnodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        key = "vnode_per_numa_node"
        if key in self.cfg.keys():
            if not self.cfg[key]:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s is false" %
                           (caller_name(), key))
                return
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s not configured" %
                       (caller_name(), key))
            return

        # Create one vnode per numa node
        vnode_list = pbs.event().vnode_list
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: numa nodes: %s" %
                   (caller_name(), self.numa_nodes))
        # Define the vnodes
        vnode_name = self.hostname
        vnode_list[vnode_name] = pbs.vnode(vnode_name)
        for id in self.numa_nodes.keys():
            vnode_name = self.hostname + "[%d]" % id
            vnode_list[vnode_name] = pbs.vnode(vnode_name)
            for key, val in sorted(self.numa_nodes[id].iteritems()):
                if key == 'cpus':
                    vnode_list[vnode_name].resources_available['ncpus'] = \
                        len(cpus2list(val))/self.hyperthreads_per_core
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['ncpus'] = 0
                elif key == 'MemTotal':
                    mem = self.get_memory_on_node(self.cfg, val)
                    vnode_list[vnode_name].resources_available['mem'] = \
                        pbs.size(mem)
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['mem'] = \
                        pbs.size('0kb')
                elif key == 'HugePages_Total':
                    pass
                elif isinstance(val, list):
                    pass
                elif isinstance(val, dict):
                    pass
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s=%s" %
                               (caller_name(), key, val))
                    vnode_list[vnode_name].resources_available[key] = val

                    if isinstance(val, int):
                        vnode_list[self.hostname].resources_available[key] = 0
                    elif isinstance(val, float):
                        vnode_list[self.hostname].resources_available[key] = \
                            0.0
                    elif isinstance(val, pbs.size):
                        vnode_list[self.hostname].resources_available[key] = \
                            pbs.size('0kb')
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnode list: %s" %
                   (caller_name(), vnode_list))
        return True

#
# CLASS CgroupUtils
#


class CgroupUtils:
    def __init__(self, hostname, vnode, **kwargs):
        cfg = None
        subsystems = None
        paths = None
        vntype = None
        event = None
        self.assigned_resources = dict()
        self.existing_cgroups = dict()
        self.host_assigned_resources = None
        self.pid_lower_limit = 3

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'subsystems':
                    subsystems = val
                elif arg == 'paths':
                    paths = val
                elif arg == 'vntype':
                    vntype = val
                elif arg == 'event':
                    event = val

        try:
            self.hostname = hostname
            self.vnode = vnode
            # __check_os will raise an exception if cgroups are not present
            self.__check_os()
            # Read in the config file
            if cfg is not None:
                self.cfg = cfg
            else:
                self.cfg = self.__parse_config_file()

            # Determine if the hook should run or exit
            siru = ShallIRunUtils(self.hostname, self.cfg)
            siru.exclude_hosts(self.cfg['exclude_hosts'])
            siru.exclude_vntypes(self.cfg['exclude_vntypes'])
            siru.run_only_on_hosts(self.cfg['run_only_on_hosts'])

            # Collect the mount points
            if paths is not None:
                self.paths = paths
            else:
                self.paths = self.__set_paths()
            # Define the local vnode type
            if vntype is not None:
                self.vntype = vntype
            else:
                self.vntype = self.__get_vnode_type()
            # Determine which subsystems we care about
            if subsystems is not None:
                self.subsystems = subsystems
            else:
                self.subsystems = self.__target_subsystems()

            # location to store information for the different hook events
            pbs_home = ''
            if not CRAY_12_BRANCH:
                pbs_conf = pbs.get_pbs_conf()
                pbs_home = pbs_conf['PBS_HOME']
            else:
                if 'PBS_HOME' in self.cfg:
                    pbs_home = self.cfg['PBS_HOME']
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "PBS_HOME needs to be defined in the " +
                               "config file")
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Exiting the cgroups hook")
                    pbs.accept()

            self.hook_storage_dir = pbs_home+'/mom_priv/hooks/hook_data'
            self.host_job_env_dir = pbs_home+'/aux'
            self.host_job_env_filename = self.host_job_env_dir+os.sep+"%s.env"

            # information for offlining nodes
            self.offline_file = \
                pbs_home+os.sep+'mom_priv'+os.sep+'hooks'+os.sep
            self.offline_file += "%s.offline" % pbs.event().hook_name
            self.offline_msg = "Hook %s: " % pbs.event().hook_name
            self.offline_msg += "Unable to clean up one or more cgroups"

        except:
            raise

    def __repr__(self):
        return ("CgroupUtils(%s, %s, %s, %s, %s, %s)" %
                (repr(self.hostname), repr(self.vnode), repr(self.cfg),
                 repr(self.subsystems), repr(self.paths), repr(self.vntype)))

    # Validate the OS type and version
    def __check_os(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check to see if the platform is linux and the kernel is new enough
        if platform.system() != 'Linux':
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: OS does not support cgroups" %
                       (caller_name()))
            raise CgroupConfigError("OS type not supported")
        rel = map(int,
                  string.split(string.split(platform.release(), '-')[0], '.'))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Detected Linux kernel version %d.%d.%d" %
                   (caller_name(), rel[0], rel[1], rel[2]))
        supported = False
        if rel[0] > 2:
            supported = True
        elif rel[0] == 2:
            if rel[1] > 6:
                supported = True
            elif rel[1] == 6:
                if rel[2] >= 28:
                    supported = True
        if not supported:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Kernel needs to be >= 2.6.28: %s" %
                       (caller_name(), system_info[2]))
            raise CgroupConfigError("OS version not supported")
        return supported

    # Read the config file in json format
    def __parse_config_file(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        config = {}
        # Identify the config file and read in the data
        if 'PBS_HOOK_CONFIG_FILE' in os.environ:
            config_file = os.environ["PBS_HOOK_CONFIG_FILE"]
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Config file is %s" %
                       (caller_name(), config_file))
            try:
                config = json.load(open(config_file, 'r'),
                                   object_hook=decode_dict)
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
        else:
            raise CgroupConfigError("No configuration file present")
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Initial Cgroup Config: %s" %
                   (caller_name(), config))
        # Set some defaults if they are not present
        if 'cgroup_prefix' not in config.keys():
            config['cgroup_prefix'] = 'pbspro'
        if 'periodic_resc_update' not in config.keys():
            config['periodic_resc_update'] = False
        if 'vnode_per_numa_node' not in config.keys():
            config['vnode_per_numa_node'] = False
        if 'online_offlined_nodes' not in config.keys():
            config['online_offlined_nodes'] = False
        if 'exclude_hosts' not in config.keys():
            config['exclude_hosts'] = []
        if 'run_only_on_hosts' not in config.keys():
            config['run_only_on_hosts'] = []
        if 'exclude_vntypes' not in config.keys():
            config['exclude_vntypes'] = []
        if 'cgroup' not in config.keys():
            config['cgroup'] = {}
        else:
            for subsys in config['cgroup']:
                subsystem = config['cgroup'][subsys]
                if 'enabled' not in subsystem.keys():
                    config['cgroup'][subsys]['enabled'] = False
                if 'exclude_hosts' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_hosts'] = []
                if 'exclude_vntypes' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_vntypes'] = []

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Cgroup Config with defaults: %s" %
                   (caller_name(), config))
        return config

    # Determine which subsystems are being requested
    def __target_subsystems(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        subsystems = []
        for key in self.cfg['cgroup'].keys():
            if self.enabled(key):
                subsystems.append(key)
        if 'memory' not in subsystems:
            if 'memsw' in subsystems:
                # Remove memsw since it must be greater then
                # or equal to memory
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing memsw from enabled subsystems")
                subsystems.remove('memsw')
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Enabled subsystems: %s" %
                   (caller_name(), subsystems))
        # It is not an error for all subsystems to be disabled.
        # This host or vnode type may be in the excluded list.
        return subsystems

    # Copy a setting from the parent cgroup
    def __copy_from_parent(self, dest):
        try:
            file = os.path.basename(dest)
            dir = os.path.dirname(dest)
            parent = os.path.dirname(dir)
            source = os.path.join(parent, file)
            if not os.path.isfile(source):
                raise CgroupConfigError('Failed to read %s' % (source))
            self.write_value(dest, open(source, 'r').read().strip())
        except:
            raise

    # Create a dictionary of the cgroup subsystems and their corresponding
    # directories
    def __set_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        paths = {}
        try:
            # Loop through the mounts and collect the ones for cgroups
            with open(os.path.join(os.sep, "proc", "mounts"), 'r') as fd:
                for line in fd:
                    entries = line.split()
                    if len(entries) > 3 and entries[2] == "cgroup":
                        flags = entries[3].split(',')
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "subsysdir: %s (flags=%s)" %
                                   (entries[1], flags))
                        for subsys in flags:
                            paths[subsys] = os.path.join(
                                entries[1], self.cfg["cgroup_prefix"])
        except:
            raise
        if len(paths.keys()) < 1:
            raise CgroupConfigError("Cgroup paths not detected")
        # Both the memory and memsw cgroups share the same mount point
        if "memory" in paths.keys():
            paths["memsw"] = paths["memory"]
        return paths

    # Create the cgroup parent directories that will contain the jobs
    def create_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Create the directories that PBS will use to house the jobs
            old_umask = os.umask(0022)
            for subsys in self.subsystems:
                if subsys not in self.paths.keys():
                    raise CgroupConfigError('No path for subsystem: %s' %
                                            (subsys))
                if not os.path.exists(self.paths[subsys]):
                    os.makedirs(self.paths[subsys], 0755)
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Created directory %s" %
                               (caller_name(), self.paths[subsys]))
                    if subsys == 'memory' or subsys == 'memsw':
                        # Set file to
                        # <cgroup>/memory/pbspro/memory.use_hierarchy
                        file = os.path.join(self.paths[subsys],
                                            'memory.use_hierarchy')
                        if not os.path.isfile(file):
                            raise CgroupConfigError('Failed to configure %s' %
                                                    (file))
                        self.write_value(file, 1)
                    elif subsys == 'cpuset':
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.cpus'))
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.mems'))

        except:
            raise CgroupConfigError("Failed to create directory: %s" %
                                    (self.paths[subsys]))
        finally:
            os.umask(old_umask)

    # Return the vnode type of the local node
    def __get_vnode_type(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")

        # File to check for if it is not defined in the hook input file
        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'

        # self.vnode is None for pbs_attach events
        pbs.logmsg(pbs.EVENT_DEBUG3, "vnode: %s" % self.vnode)
        if self.vnode is not None:
            if 'vntype' in self.vnode.resources_available and \
                    self.vnode.resources_available['vntype'] is not None:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "vntype: %s" %
                           self.vnode.resources_available['vntype'])
                return self.vnode.resources_available['vntype']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3, "vntype file: %s" % vntype_file)
                if os.path.isfile(vntype_file):
                    fdata = open(vntype_file).readlines()
                    vntype = fdata[0].strip()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
                    return vntype
        # If vntype was not set then log a message. It is too expensive
        # to have all moms query the server for large jobs.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: resources_available.vntype is " % caller_name() +
                   "not set for this vnode")
        return None

    # Gather resources that are already assigned
    def gather_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        assigned_resources = {}

        # Gather the cpus and memory sockets assigned
        directories = [x[0] for x in os.walk(self.paths['cpuset'])]

        # Add the existing cgroups to a list to check if assigning
        # resources fail
        for dir in directories[1:]:
            if dir.find(pbs.event().job.id) == -1:
                self.existing_cgroups[dir] = []

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'cpuset' in self.subsystems and \
                self.cfg['cgroup']['cpuset']['enabled']:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather cpuset resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['cpuset'], tmp_dir)
                    with open(path+os.sep+'cpuset.cpus') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.cpus'] = \
                            cpus2list(tmp_val.strip())
                    with open(path+os.sep+'cpuset.mems') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.mems'] = \
                            cpus2list(tmp_val.strip())

        # Check to see if the subsystem is enabled before trying
        # to gather resources
        if 'memory' in self.subsystems and \
                self.cfg['cgroup']['memory']['enabled']:
            # Gather the memory assigned for each socket
            directories = [x[0] for x in os.walk(self.paths['memory'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather memory resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['memory'], tmp_dir)
                    with open(path+os.sep+'memory.limit_in_bytes') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memory.limit'] = \
                            tmp_val.strip()
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    if os.path.isfile(tmp_filename) is False:
                        continue
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    with open(tmp_filename) as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memsw.limit'] = \
                            tmp_val.strip()

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'devices' in self.subsystems and \
                self.cfg['cgroup']['devices']['enabled']:
            # Gather the devices assigned
            directories = [x[0] for x in os.walk(self.paths['devices'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather device resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    pbs.logmsg(pbs.EVENT_DEBUG3, "Dir: %s" % tmp_dir)
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['devices'], tmp_dir)
                    with open(path+os.sep+'devices.list') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['devices.list'] = \
                            tmp_val.strip().split('\n')
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Dir: %s\n\tValue: %s" %
                                   (path, tmp_val.replace('\n', ',')))

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Gathered assigned resources: %s" % assigned_resources)
        self.assigned_resources = assigned_resources

    # Return whether a subsystem is enabled
    def enabled(self, subsystem):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check whether the subsystem is enabled in the configuration file
        if subsystem not in self.cfg['cgroup'].keys():
            return False
        if 'enabled' not in self.cfg['cgroup'][subsystem].keys():
            return False
        if not self.cfg['cgroup'][subsystem]['enabled']:
            return False
        # Check whether the cgroup is mounted for this subsystem
        if subsystem not in self.paths.keys():
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup not mounted for %s" %
                       (caller_name(), subsystem))
            return False
        # Check whether this host is excluded
        # if self.hostname in self.cfg['exclude_hosts']:
        #    pbs.logmsg(pbs.EVENT_DEBUG, "%s: cgroup excluded on host %s" %
        #               (caller_name(), self.hostname))
        #    pbs.event().accept()
        if self.hostname in self.cfg['cgroup'][subsystem]['exclude_hosts']:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup excluded for subsystem %s on host %s" %
                       (caller_name(), subsystem, self.hostname))
            return False
        # Check whether the vnode type is excluded
        if self.vntype is not None:
            # if self.vntype in self.cfg['exclude_vntypes']:
            #    pbs.logmsg(pbs.EVENT_DEBUG,
            #               "%s: cgroup excluded on vnode type %s" %
            #               (caller_name(), self.vntype))
            #    pbs.event().accept()
            if self.vntype in self.cfg['cgroup'][subsystem]['exclude_vntypes']:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           ("%s: cgroup excluded for " +
                            "subsystem %s on vnode type %s") %
                           (caller_name(), subsystem, self.vntype))
                return False
        return True

    # Return the default value for a subsystem
    def default(self, subsystem):
        if subsystem in self.cfg['cgroup']:
            if 'default' in self.cfg['cgroup'][subsystem]:
                return self.cfg['cgroup'][subsystem]['default']
        return None

    # Check to see if the pid matches the job owners uid
    def is_pid_owned_by_job_owner(self, pid):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        try:
            proc_uid = os.stat('/proc/%d' % pid).st_uid
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       '/proc/%d uid:%d' % (pid, proc_uid))

            job_owner_uid = pwd.getpwnam(pbs.event().job.euser)[2]
            pbs.logmsg(pbs.EVENT_DEBUG3, "Job uid: %d" % job_owner_uid)

            if proc_uid == job_owner_uid:
                return True
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Proc uid: %d != Job owner:  %d" %
                           (proc_uid, job_owner_uid))
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG, "Unknown pid: %d" % pid)
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        # If we got to this point something did not match up
        return False

    # Add some number of PIDs to the cgroup tasks files for each subsystem
    def add_pids(self, pids, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # make pids a list
        if isinstance(pids, int):
            pids = [pids]

        if pbs.event().type == pbs.EXECJOB_LAUNCH:
            if 1 in pids:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "1 is not a valid pid to add")
                # Hold the job for further review
                e.reject("Hook tried to add pid 1 to the tasks file " +
                         "for job %s" % jobid)

        # check pids to make sure that they are owned by the job owner
        if not CRAY_12_BRANCH:
            pbs.logmsg(pbs.EVENT_DEBUG3, "Not in the Cray 12 Branch")
            pbs.logmsg(pbs.EVENT_DEBUG3, "check to see if execjob_attach")
            if pbs.event().type == pbs.EXECJOB_ATTACH:
                pbs.logmsg(pbs.EVENT_DEBUG3, "event type: attach or launch")
                tmp_pids = list()
                for p in pids:
                    if self.is_pid_owned_by_job_owner(p):
                        tmp_pids.append(p)
                if len(tmp_pids) == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%d is not a valid pids to add" % p)
                    return False
                else:
                    pids = tmp_pids

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: subsys = %s" %
                       (caller_name(), subsys))
            # memsw and memory use the same tasks file
            if subsys == "memsw":
                if "memory" in self.subsystems:
                    continue
            path = os.path.join(self.paths[subsys], jobid)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path = %s" %
                       (caller_name(), path))
            try:
                tasks_path = os.path.join(path, "tasks")
                for p in pids:
                    self.write_value(tasks_path, p, 'a')
            except IOError, exc:
                raise CgroupLimitError("Failed to add PIDs %s to %s (%s)" %
                                       (str(pids), tasks_path,
                                        errno.errorcode[exc.errno]))
            except:
                raise

    # Set a node limit
    def set_node_limit(self, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s = %s" %
                   (caller_name(), resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'],
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Node resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    def setup_job_devices_env(self):
        """ Setup the job environment for the devices assigned to the job for an
            execjob_launch hook
         """
        if 'devices_name' in self.host_assigned_resources:
            names = self.host_assigned_resources['devices_name']
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "devices: %s" % (names))
            offload_devices = list()
            cuda_visible_devices = list()
            for name in names:
                if name.startswith('mic'):
                    offload_devices.append(name[3:])
                elif name.startswith('nvidia'):
                    cuda_visible_devices.append(name[6:])
            if len(offload_devices) > 0:
                value = '"%s"' % string.join(offload_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['OFFLOAD_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "offload_devices: %s" % offload_devices)
            if len(cuda_visible_devices) > 0:
                value = '"%s"' % string.join(cuda_visible_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['CUDA_VISIBLE_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "cuda_visible_devices: %s" % cuda_visible_devices)
            return [offload_devices, cuda_visible_devices]
        else:
            return False

    def setup_subsys_devices(self, path, node):
        subsys = 'devices'
        # Add the devices that the user is allowed to use
        if subsys in self.cfg['cgroup']:
            devices_allowed = open(os.path.join(path,
                                   "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Initial devices.list: %s" %
                       devices_allowed)
            # Deny access to mic and gpu devices
            devices_list = list()
            devices = node.devices
            for device in devices:
                if device == 'mic' or device == 'gpu':
                    for name in devices[device]:
                        d = devices[device][name]
                        devices_list.append("%d:%d" % (d['major'], d['minor']))
            # For CentOS 7 we need to remove a *:* rwm from devices.list we
            # before can add anything to devices.allow. Otherwise our changes
            # are ignored. Check to see if a *:* rwm is in devices.list.
            # If so remove it
            if "a *:* rwm\n" in devices_allowed:
                value = "a *:* rwm"
                self.write_value(os.path.join(path, 'devices.deny'), value)
            else:
                # Verify that the following devices are not in devices.list
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing access to the following: %s" %
                           devices_list)
            for device_str in devices_list:
                value = "c %s rwm" % device_str
                self.write_value(os.path.join(path, 'devices.deny'), value)
            # Add devices back to the list
            devices_allow = list()
            if 'allow' in self.cfg['cgroup'][subsys]:
                devices_allow = self.cfg['cgroup'][subsys]['allow']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Allowing access to the following: %s" %
                           devices_allow)
                for item in devices_allow:
                    if isinstance(item, str):
                        pbs.logmsg(pbs.EVENT_DEBUG3, "string item: %s" % item)
                        value = "%s" % item
                        self.write_value(os.path.join(
                                         path, 'devices.allow'), value)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "write_value: %s" % value)
                    elif isinstance(item, list):
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "list item: %s" % item)
                        try:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Device allow: %s" % item)
                            stat_filename = '/dev/%s' % item[0]
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Stat file: %s" % stat_filename)
                            s = os.stat(stat_filename)
                            device_type = None
                            if S_ISBLK(s.st_mode):
                                device_type = "b"
                            elif S_ISCHR(s.st_mode):
                                device_type = "c"
                            if device_type is not None:
                                if len(item) == 3 and isinstance(item[2], str):
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % item[2]
                                    value += "%s" % item[1]
                                else:
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % os.minor(s.st_rdev)
                                    value += "%s" % item[1]
                                self.write_value(os.path.join(
                                                path, 'devices.allow'), value)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "write_value: %s" % value)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find /dev/%s. " % item[0] +
                                       "Not added to devices.allow!")
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                    else:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Not sure what to do with: %s" % item)
            devices_allowed = open(os.path.join(
                                   path, "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "After setup devices.list: %s" % devices_allowed)

    # Select devices to assign to the job
    def assign_devices(
                       self,
                       device_type,
                       device_list,
                       number_of_devices,
                       node):
        devices = device_list[:number_of_devices]
        device_ids = list()
        device_names = list()
        for device in devices:
            device_info = node.devices[device_type][device]
            if len(device_ids) == 0:
                if device.find("mic") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:0 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                    device_ids.append("%s %d:1 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                elif device.find("nvidia") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:255 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
            device_ids.append("%s %d:%d rwm" %
                              (device_info['device_type'],
                               device_info['major'],
                               device_info['minor']))
            device_names.append(device)
        return device_names, device_ids

    # Find the device name
    def get_device_name(self, node, available, socket, major, minor):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Get device name: major: %s, minor: %s" % (major, minor))
        avail_device = None
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Possible devices: %s" % (available[socket]['devices']))
        for avail_device in available[socket]['devices']:
            avail_major = None
            avail_minor = None
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Checking device: %s" % (avail_device))
            if avail_device.find('mic') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check mic device: %s" % (avail_device))
                avail_major = node.devices['mic'][avail_device]['major']
                avail_minor = node.devices['mic'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            elif avail_device.find('nvidia') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check gpu device: %s" % (avail_device))
                avail_major = node.devices['gpu'][avail_device]['major']
                avail_minor = node.devices['gpu'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            if int(avail_major) == int(major) and \
                    int(avail_minor) == int(minor):
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device match: name: %s, major: %s, minor: %s" %
                           (avail_device, major, minor))
                return avail_device
        pbs.logmsg(pbs.EVENT_DEBUG3, "No match found")
        return None

    # Assign resources to the job based off the vnodes assignments
    def assign_job_resources_by_vnode(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # Loop through the vnodes and assign resources
        assigned = dict()
        assigned['cpuset.cpus'] = list()
        assigned['cpuset.mems'] = list()
        room_on_socket = True

        for vnode in resources['vnodes']:
            regex = re.compile(".*\[(\d+)\].*")
            socket = int(regex.search(vnode).group(1))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current Vnode: %s" % vnode)
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current socket: %d" % socket)

            if 'ncpus' in resources['vnodes'][vnode]:
                tmp_ncpus = int(resources['vnodes'][vnode]['ncpus'])
                if int(resources['vnodes'][vnode]['ncpus']) <= \
                        len(available[socket]['cpus']):
                    assigned['cpuset.cpus'] += \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] += [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_ncpus: %s" % (tmp_ncpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "available[%d]: %s" %
                               (socket, available[socket]))
                    room_on_socket = False
            if ('nmics' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['nmics']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(mic).*")
                tmp_nmics = int(resources['vnodes'][vnode]['nmics'])
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['nmics']) > 0 and
                   int(resources['vnodes'][vnode]['nmics']) <= len(mics)):
                    tmp_names, tmp_devices = self.assign_devices(
                             'mic', mics[:tmp_nmics], tmp_nmics, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_nmics: %s" % (tmp_nmics))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "mics: %s" % (mics))
                    room_on_socket = False
            if ('ngpus' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['ngpus']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(nvidia).*")
                tmp_ngpus = int(resources['vnodes'][vnode]['ngpus'])
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['ngpus']) > 0 and
                   int(resources['vnodes'][vnode]['ngpus']) <= len(gpus)):
                    tmp_names, tmp_devices = self.assign_devices(
                        'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "tmp_ngpus: %s" % (tmp_ngpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "gpus: %s" % (gpus))
                    room_on_socket = False

        if room_on_socket:
            assigned['cpuset.cpus'].sort()
            assigned['cpuset.mems'].sort()
            if 'devices' in assigned:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids pre set and sort: %s" %
                           (assigned['devices']))
                assigned['devices'] = list(set(assigned['devices']))
                assigned['devices'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids post set and sort: %s" %
                           (assigned['devices']))
                assigned['devices_name'] = list(set(assigned['devices_name']))
                assigned['devices_name'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

    # Assign resources to the job
    def assign_job_resources(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # What would we need to do different for a large smp (UV) system?
        # See if requested resources can fit on a single socket
        assigned = dict()
        pbs.logmsg(pbs.EVENT_DEBUG3, "Requested: %s" % (resources))

        # Determine the order that we should evaluate the sockets.
        socket_list = list()
        if 'placement_type' in self.cfg:
            if self.cfg['placement_type'] == 'round_robin':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Requested round_robin placement")
                # Look at assigned_resources and determine which socket
                # to start with
                jobs_per_socket_cnt = dict()
                for socket in available.keys():
                    jobs_per_socket_cnt[socket] = 0
                for job in self.assigned_resources:
                    if 'cpuset.mems' in self.assigned_resources[job]:
                        for socket in \
                                self.assigned_resources[job]['cpuset.mems']:
                            jobs_per_socket_cnt[socket] += 1

                sorted_dict = sorted(jobs_per_socket_cnt.items(),
                                     key=operator.itemgetter(1))
                for x in sorted_dict:
                    socket_list.append(x[0])
        else:
            socket_list = available.keys()

        # Loop through the socket list and determine if the job
        # can fit on the socket
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Evaluate sockets in the following order: %s" %
                   (socket_list))
        for socket in socket_list:
            room_on_socket = True
            if 'ncpus' in resources:
                if int(resources['ncpus']) <= len(available[socket]['cpus']):
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Requested: %s, Available: %s" %
                               (resources['ncpus'], available[socket]['cpus']))
                    tmp_ncpus = int(resources['ncpus'])
                    assigned['cpuset.cpus'] = \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] = [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient ncpus on socket %d: R:%d,A:%s" %
                               (socket, resources['ncpus'],
                                available[socket]['cpus']))
                    room_on_socket = False
            # Check to see that nmics has been requested and not equal to 0
            if 'nmics' in resources and int(resources['nmics']) > 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'nmics' in resources and int(resources['nmics']) > 0 \
                        and int(resources['nmics']) <= len(mics):
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient nmics on socket %d: R:%s,A:%s" %
                               (socket, resources['nmics'], mics))
                    room_on_socket = False
            # Check to see that ngpus has been requested and not equal to 0
            if 'ngpus' in resources and int(resources['ngpus']) > 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'ngpus' in resources and int(resources['ngpus']) > 0 \
                        and int(resources['ngpus']) <= len(gpus):
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient gpus on socket %d: R:%s,A:%s" %
                               (socket, resources['ngpus'], gpus))
                    room_on_socket = False
            if 'vmem' in resources:
                if size_as_int(resources['vmem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient vmem on socket %d: R:%s,A:%s" %
                               (socket, resources['vmem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if 'mem' in resources:
                if size_as_int(resources['mem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient memory on socket %d: R:%s,A:%s" %
                               (socket, resources['mem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if room_on_socket:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
                self.host_assigned_resources = assigned
                return assigned
        # If we made it here then the requested resources did not fit
        # on a single socket
        # Future: take distance into account when selecting sockets?
        # Combine available resources into a single list
        # This should work for a dual socket machine.
        # Needs additional work for machines with more than 2 sockets
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Unable to find requested resources: %s" % resources +
                   " on a socket. Checking the entire node")
        available_copy = copy.deepcopy(available)
        available_on_node = dict()
        socket_keys = available.keys()
        socket_keys.sort()
        available_on_node['sockets'] = socket_keys
        for socket in socket_keys:
            for key in available_copy[socket].keys():
                if isinstance(available[socket][key], int):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]
                if isinstance(available_copy[socket][key], str):
                    try:
                        if key in available_on_node is False:
                            available_on_node[key] = \
                                size_as_int(available_copy[socket][key])
                        else:
                            available_on_node[key] += \
                                size_as_int(available_copy[socket][key])
                    except:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Unable to convert to int: %s" %
                                   (available_copy[socket][key]))

                if isinstance(available_copy[socket][key], list):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available on node: %s" % (available_on_node))
        assigned = dict()
        room_on_node = False
        if 'ncpus' in resources:
            if int(resources['ncpus']) <= len(available_on_node['cpus']):
                room_on_node = True
                tmp_ncpus = int(resources['ncpus'])
                assigned['cpuset.cpus'] = available_on_node['cpus'][:tmp_ncpus]
                assigned['cpuset.mems'] = available_on_node['sockets']
            if 'nmics' in resources and int(resources['nmics']) != 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['nmics']) <= len(mics) and \
                        int(resources['nmics']) > 0:
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
            if 'ngpus' in resources and int(resources['ngpus']) != 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['ngpus']) <= len(gpus) and \
                        int(resources['ngpus']) > 0:
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
        if room_on_node:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

        # Consolidate resources if needed to place the job

    # Determine which resources are available
    def available_node_resources(self, node):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))

        available = copy.deepcopy(node.numa_nodes)
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Keys: %s" % (available[0].keys()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        for socket in available.keys():
            if 'cpus' in available[socket]:
                # Find the physical cores
                if available[socket]['cpus'].find('-') != -1 and \
                        node.hyperthreading:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Splitting %s at ','" %
                               (available[socket]['cpus']))
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'].split(',')[0])
                else:
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'])
            if 'MemTotal' in available[socket]:
                # Find the memory on the socket in bites.
                # Remove the 'b' to simply math
                available[socket]['memory'] = size_as_int(
                    available[socket]['MemTotal'])

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Pre device add: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (node.devices.keys()))
        for device in node.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" %
                       (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Devices: %s" %
                           (node.devices[device].keys()))
                for device_name in node.devices[device].keys():
                    device_socket = \
                        node.devices[device][device_name]['numa_node']
                    if 'devices' not in available[device_socket]:
                        available[device_socket]['devices'] = list()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Device: %s, Socket: %s" %
                               (device, device_socket))
                    available[device_socket]['devices'].append(device_name)
            # pbs.logmsg(pbs.EVENT_DEBUG3,
            #            "Available on socket %s: %s" %
            #            (socket,available[socket]))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))

        # Remove all of the resources that are assigned to other jobs
        for job in self.assigned_resources.keys():
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            if (not job_to_be_ignored(job)):
                cpus = list()
                sockets = list()
                devices = list()
                memory = 0
                if 'cpuset.cpus' in self.assigned_resources[job]:
                    cpus = self.assigned_resources[job]['cpuset.cpus']
                if 'cpuset.mems' in self.assigned_resources[job]:
                    sockets = self.assigned_resources[job]['cpuset.mems']
                if 'devices.list' in self.assigned_resources[job]:
                    devices = self.assigned_resources[job]['devices.list']
                if 'memory.limit' in self.assigned_resources[job]:
                    memory = size_as_int(
                                self.assigned_resources[job]['memory.limit'])

                # Loop through the sockets and remove cpus that are
                # assigned to other cgroups
                for socket in sockets:
                    for cpu in cpus:
                        try:
                            available[socket]['cpus'].remove(cpu)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %d from %s" %
                                       (cpu, available[socket]['cpus']))

                if len(sockets) == 1:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Decrementing memory: %d by %d" %
                               (size_as_int(available[sockets[0]]['memory']),
                                memory))
                    available[sockets[0]]['memory'] -= memory

                # Loop throught the available sockets
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned device to %s: %s" % (job, devices))
                for socket in available.keys():
                    for device in devices:
                        try:
                            # loop through know devices and see if they match
                            if len(available[socket]['devices']) != 0:
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Check device: %s" % (device))
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Available device: %s" %
                                           (available[socket]['devices']))
                                major, minor = device.split()[1].split(':')
                                avail_device = self.get_device_name(
                                                   node, available, socket,
                                                   major, minor)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Returned device: %s" %
                                           (avail_device))
                                if avail_device is not None:
                                    pbs.logmsg(pbs.EVENT_DEBUG3,
                                               "socket: %d,\t" % socket +
                                               "devices: %s,\t" %
                                               available[socket]['devices'] +
                                               "device to remove: %s" %
                                               (avail_device))
                                    available[socket]['devices'].remove(
                                        avail_device)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %s from %s" %
                                       (device, available[socket]['devices']))
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Job %s res not removed from host " % job +
                           "available res: suspended job")
        pbs.logmsg(pbs.EVENT_DEBUG, "Available resources: %s" % (available))
        return available

    # Set a job limit
    def set_job_limit(self, jobid, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s: %s = %s" %
                   (caller_name(), jobid, resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'softmem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.soft_limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     jobid, 'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'], jobid,
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            elif resource == 'ncpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = self.select_cpus(path, value)
                    if cpus is None:
                        raise CgroupLimitError("Failed to configure cpuset.")
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
                    self.__copy_from_parent(os.path.join(self.paths['cpuset'],
                                            jobid, 'cpuset.mems'))
            elif resource == 'cpuset.cpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = value
                    if cpus is None:
                        msg = "Failed to configure cpus in cpuset."
                        raise CgroupLimitError(msg)
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
            elif resource == 'cpuset.mems':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.mems')
                    mems = value
                    if mems is None:
                        msg = "Failed to configure mems in cpuset."
                        raise CgroupLimitError(msg)
                    mems = ','.join(map(str, mems))
                    self.write_value(path, mems)
            elif resource == 'devices':
                if 'devices' in self.subsystems and \
                        self.cfg['cgroup']['devices']['enabled']:

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.allow')
                    devices = value
                    if devices is None:
                        msg = "Failed to configure device(s)."
                        raise CgroupLimitError(msg)
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Setting devices: %s for %s" % (devices, jobid))
                    for device in devices:
                        self.write_value(path, device)

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.list')
                    output = open(path, 'r').read()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "devices.list: %s" % output.replace("\n", ","))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    # Update resource usage for a job
    def update_job_usage(self, jobid, resc_used):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resc_used = %s" %
                   (caller_name(), str(resc_used)))
        # Sort the subsystems so that we consistently look at the subsystems
        # in the same order every time
        self.subsystems.sort()
        for subsys in self.subsystems:
            path = os.path.join(self.paths[subsys], jobid)
            if subsys == "memory":
                max_mem = self.__get_max_mem_usage(path)
                if max_mem is None:
                    pbs.logjobmsg(jobid, "%s: No max mem data" %
                                  (caller_name()))
                else:
                    resc_used['mem'] = pbs.size(convert_size(max_mem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: mem=%s" %
                                  (caller_name(), resc_used['mem']))
                mem_failcnt = self.__get_mem_failcnt(path)
                if mem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No mem fail count data" %
                                  (caller_name()))
                else:
                    # Check to see if the job exceeded its resource limits
                    if mem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memory limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "memsw":
                max_vmem = self.__get_max_memsw_usage(path)
                if max_vmem is None:
                    pbs.logjobmsg(jobid, "%s: No max vmem data" %
                                  (caller_name()))
                else:
                    resc_used['vmem'] = pbs.size(convert_size(max_vmem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: vmem=%s" %
                                  (caller_name(), resc_used['vmem']))

                vmem_failcnt = self.__get_memsw_failcnt(path)
                if vmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No vmem fail count data" %
                                  (caller_name()))
                else:
                    pbs.logjobmsg(jobid, "%s: vmem fail count: %d " %
                                  (caller_name(), vmem_failcnt))
                    if vmem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memsw limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "hugetlb":
                max_hpmem = self.__get_max_hugetlb_usage(path)
                if max_hpmem is None:
                    pbs.logjobmsg(jobid, "%s: No max hpmem data" %
                                  (caller_name()))
                    return
                hpmem_failcnt = self.__get_hugetlb_failcnt(path)
                if hpmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No hpmem fail count data" %
                                  (caller_name()))
                    return
                if hpmem_failcnt > 0:
                    err_msg = self.__get_error_msg(jobid)
                    pbs.logjobmsg(jobid, "Cgroup hugetlb limit exceeded: %s" %
                                  (err_msg))
                resc_used['hpmem'] = pbs.size(convert_size(max_hpmem, 'kb'))
                pbs.logjobmsg(jobid, "%s: Hugepage usage: %s" %
                              (caller_name(), resc_used['hpmem']))
            elif subsys == "cpuacct":
                cpu_usage = self.__get_cpu_usage(path)
                if cpu_usage is None:
                    pbs.logjobmsg(jobid, "%s: No CPU usage data" %
                                  (caller_name()))
                    return
                cpu_usage = convert_time(str(cpu_usage) + "ns")
                pbs.logjobmsg(jobid, "%s: CPU usage: %.3lf secs" %
                              (caller_name(), cpu_usage))
                resc_used['cput'] = pbs.duration(cpu_usage)

    # Creates the cgroup if it doesn't exists
    def create_job(self, jobid, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Iterate over the subsystems required
        for subsys in self.subsystems:
            # Create a directory for the job
            try:
                old_umask = os.umask(0022)
                path = self.paths[subsys]
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                path = os.path.join(self.paths[subsys], jobid)
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                if subsys == 'devices':
                    self.setup_subsys_devices(path, node)

            except OSError, exc:
                raise CgroupConfigError("Failed to create directory: %s (%s)" %
                                        (path, errno.errorcode[exc.errno]))
            except:
                raise
            finally:
                os.umask(old_umask)

    # Determine the cgroup limits and configure the cgroups
    def configure_job(self, jobid, hostresc, node):
        mem_enabled = 'memory' in self.subsystems
        vmem_enabled = 'memsw' in self.subsystems
        if mem_enabled or vmem_enabled:
            # Initialize mem variables
            mem_avail = node.get_memory_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "mem_avail %s" % mem_avail)
            mem_requested = None
            if 'mem' in hostresc.keys():
                mem_requested = convert_size(hostresc['mem'], 'kb')
            mem_default = None
            if mem_enabled:
                mem_default = self.default('memory')
            # Initialize vmem variables
            vmem_avail = node.get_vmem_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "vmem_avail %s" % vmem_avail)
            vmem_requested = None
            if 'vmem' in hostresc.keys():
                vmem_requested = convert_size(hostresc['vmem'], 'kb')
            vmem_default = None
            if vmem_enabled:
                vmem_default = self.default('memsw')
            # Initialize softmem variables
            if 'soft_limit' in self.cfg['cgroup']['memory'].keys():
                softmem_enabled = self.cfg['cgroup']['memory']['soft_limit']
            else:
                softmem_enabled = False
            # Sanity check
            if size_as_int(mem_avail) > size_as_int(vmem_avail):
                if size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                    raise CgroupLimitError(
                        'mem available (%s) exceeds vmem available (%s)' %
                        (mem_avail, vmem_avail))
            # Determine the mem limit
            if mem_requested is not None:
                # mem requested may not exceed available
                if size_as_int(mem_requested) > size_as_int(mem_avail):
                    raise JobValueError(
                        'mem requested (%s) exceeds mem available (%s)' %
                        (mem_requested, mem_avail))
                mem_limit = mem_requested
            else:
                # mem was not requested
                if mem_default is None:
                    mem_limit = mem_avail
                else:
                    mem_limit = mem_default
            # Determine the vmem limit
            if vmem_requested is not None:
                # vmem requested may not exceed available
                if size_as_int(vmem_requested) > size_as_int(vmem_avail):
                    raise JobValueError(
                        'vmem requested (%s) exceeds vmem available (%s)' %
                        (vmem_requested, vmem_avail))
                vmem_limit = vmem_requested
            else:
                # vmem was not requested
                if vmem_default is None:
                    vmem_limit = vmem_avail
                else:
                    vmem_limit = vmem_default
            # Ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Adjust for soft limits if enabled
            if mem_enabled and softmem_enabled:
                softmem_limit = mem_limit
                # The hard memory limit is assigned the lesser of the vmem
                # limit and available memory
                if size_as_int(vmem_limit) < size_as_int(mem_avail):
                    mem_limit = vmem_limit
                else:
                    mem_limit = mem_avail
            # Again, ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Sanity checks when both memory and memsw are enabled
            if mem_enabled and vmem_enabled:
                if vmem_requested is not None:
                    if size_as_int(vmem_limit) > size_as_int(vmem_requested):
                        # The user requested an invalid limit
                        raise JobValueError(
                            'vmem limit (%s) exceeds vmem requested (%s)' %
                            (vmem_limit, vmem_requested))
                if size_as_int(vmem_limit) > size_as_int(mem_limit):
                    # This job may utilize swap
                    if size_as_int(vmem_avail) <= size_as_int(mem_avail) and \
                      size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                        # No swap available
                        raise CgroupLimitError(
                            ('Job might utilize swap ' +
                             'and no swap space available'))
            # Assign mem and vmem
            if mem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: mem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), mem_limit))
                hostresc['mem'] = pbs.size(mem_limit)
                if softmem_enabled:
                    hostresc['softmem'] = pbs.size(softmem_limit)
            if vmem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: vmem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), vmem_limit))
                hostresc['vmem'] = pbs.size(vmem_limit)
        # Initialize hpmem variables
        hpmem_enabled = 'hugetlb' in self.subsystems
        if hpmem_enabled:
            hpmem_avail = node.get_hpmem_on_node(self.cfg)
            hpmem_limit = None
            hpmem_default = self.default('hugetlb')
            if hpmem_default is None:
                hpmem_default = hpmem_avail
            if 'hpmem' in hostresc.keys():
                hpmem_limit = convert_size(hostresc['hpmem'], 'kb')
            else:
                hpmem_limit = hpmem_default
            # Assign hpmem
            if size_as_int(hpmem_limit) > size_as_int(hpmem_avail):
                raise JobValueError('hpmem limit (%s) exceeds available (%s)' %
                                    (hpmem_limit, hpmem_avail))
            hostresc['hpmem'] = pbs.size(hpmem_limit)
        # Initialize cpuset variables
        cpuset_enabled = 'cpuset' in self.subsystems
        if cpuset_enabled:
            cpu_limit = 1
            if 'ncpus' in hostresc.keys():
                cpu_limit = hostresc['ncpus']
            if cpu_limit < 1:
                cpu_limit = 1
            hostresc['ncpus'] = pbs.pbs_int(cpu_limit)

        # Find the available resources and assign the right ones to the job

        attempt = 0
        assign_resources = False

        # Two attempts since self.cleanup_orphans may actually fix the
        # problem we see in a first attempt
        while attempt < 2:
            attempt += 1
            available_resources = self.available_node_resources(node)
            if 'vnodes' in hostresc:
                assign_resources = self.assign_job_resources_by_vnode(
                    hostresc, available_resources, node)
            else:
                assign_resources = self.assign_job_resources(
                    hostresc, available_resources, node)

            if (assign_resources is False) and (attempt < 2):
                # No resources were assigned to the job.
                # Most likely cause was that a cgroup has
                # not been cleaned up yet
                pbs.logmsg(pbs.EVENT_DEBUG, "Failed to assign resources. " +
                           "Checking the following cgroups on the node " +
                           "with the server: %s" % self.existing_cgroups)

                # Collect the jobs on the node (try reading mom_priv/jobs,
                # if not successful ask server)
                jobs_list = node.gather_jobs_on_node_local()

                if jobs_list is not None:
                    # Don't clean up the cgroup we just made for the
                    # job just yet
                    if jobid not in jobs_list:
                        jobs_list.append(jobid)

                    self.cleanup_orphans(jobs_list)

                    # Recheck what is now assigned after things were cleaned up
                    self.gather_assigned_resources()

        if assign_resources is False:
            # Cleanup cgroups for jobs not present on this node
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Assign resources failed. Attempting to cleanup all " +
                       "leftover cgroups (including event job %s)" % jobid)

            # Remove the current job from the jobs list so it will be
            # cleaned up as well.
            jobs_list.remove(jobid)

            self.cleanup_orphans(jobs_list)

            # Rerun the job and log the message
            line = 'Unable to assign resources to job. '
            line += 'Will requeue the job and try again.'
            line += 'job run_count: %d' % pbs.event().job.run_count
            pbs.event().job.rerun()
            pbs.event().reject(line)

        # Print out the assigned resources
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Assigned resources: %s" % (assign_resources))

        if cpuset_enabled:
            hostresc.pop('ncpus', None)
            for key in ['cpuset.cpus', 'cpuset.mems']:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Initialize devices variables
        devices_enabled = 'devices' in self.subsystems
        if devices_enabled:
            key = 'devices'
            if key in assign_resources:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Apply the resource limits to the cgroups
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Setting cgroup limits for: %s" %
                   (caller_name(), hostresc))

        # The vmem limit must be set after the mem limit, so sort the keys
        for resc in sorted(hostresc.keys()):
            self.set_job_limit(jobid, resc, hostresc[resc])

    # Kill any processes contained within a tasks file
    def __kill_tasks(self, tasks_file):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        with open(tasks_file, 'r') as fd:
            for line in fd:
                os.kill(int(line.strip()), signal.SIGKILL)
        fd.close()
        count = 0
        with open(tasks_file, 'r') as fd:
            for line in fd:
                count += 1
                pid = line.strip()
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Pid: %s did not clean up" %
                           (caller_name(), pid))
                if os.path.isfile('/proc/%s/status' % pid):
                    tmp = open('/proc/%s/status' % pid).readlines()
                    tmp_line = ''
                    for entry in tmp:
                        if entry.find('Name:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('State:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('Uid:') != -1:
                            tmp_line += entry.strip()
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: %s" % (caller_name(), tmp_line))

        return count

    # Perform the actual removal of the cgroup directory
    # by default, only one attempt at killing tasks in cgroup, without sleeping
    # since this method could be called many times
    # (for N directories times M jobs)
    def __remove_cgroup(self, path, tasks_kill_attempts=1):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            if os.path.exists(path):
                tasks_file = os.path.join(path, 'tasks')
                task_cnt = self.__kill_tasks(tasks_file)
                kill_attempts = 0
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Task Kill Attempts: %d" % tasks_kill_attempts)
                while (task_cnt > 0) and (kill_attempts < tasks_kill_attempts):
                    kill_attempts += 1
                    count = 0

                    with open(tasks_file, 'r') as fd:
                        for line in fd:
                            count += 1
                        task_cnt = count

                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "Attempt: %d - cgroup still has %d tasks: %s" %
                               (kill_attempts, task_cnt, path))
                    if kill_attempts < tasks_kill_attempts:
                        time.sleep(1)
                        # Added since there are race conditions where tasks are
                        # being added at the same time we get the initial
                        # task count
                        tasks_file = os.path.join(path, 'tasks')
                        task_cnt = self.__kill_tasks(tasks_file)

                if task_cnt == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Removing directory %s" %
                               (caller_name(), path))
                    os.rmdir(path)
                    if os.path.exists(path):
                        time.sleep(.25)
                        if os.path.exists(path):
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "%s: 2nd Try Removing directory %s" %
                                       (caller_name(), path))
                            os.rmdir(path)
                            time.sleep(.25)
                        if os.path.exists(path):
                            return False
                else:
                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "cgroup still has %d tasks: %s" %
                               (task_cnt, path))

                    # Check to see if the offline file is already present
                    # on the mom
                    offline_file_exists = False
                    if os.path.isfile(self.offline_file):
                        msg = "Cgroup(s) not cleaning up but the node already "
                        msg += "has the offline file."

                        pbs.logmsg(pbs.EVENT_DEBUG, msg)
                    else:
                        # Check to see that the node is not already offline.
                        try:
                            tmp_state = pbs.server().vnode(self.hostname).state
                        except:
                            msg = "Unable to contact server for node state"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            tmp_state = None
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Current Node State: %d" % tmp_state)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Offline Node State: %d" % pbs.ND_OFFLINE)

                        if tmp_state == pbs.ND_OFFLINE:
                            msg = "Cgroup(s) not cleaning up but the node is "
                            msg += "already offline."
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                        elif tmp_state is not None:
                            # Offline the node(s)
                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                            msg = "Offlining node since cgroup(s) are not "
                            msg += "cleaning up"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            vnode = pbs.event().vnode_list[self.hostname]

                            vnode.state = pbs.ND_OFFLINE

                            # Write a file locally to reduce server traffic
                            # when it comes time to online the node
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Offline file: %s" % self.offline_file)
                            open(self.offline_file, 'a').close()
                            vnode.comment = self.offline_msg

                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                        else:
                            pass

                    return False

        except OSError, exc:
            pbs.logmsg(pbs.EVENT_SYSTEM,
                       "Failed to remove cgroup path: %s (%s)" %
                       (path, errno.errorcode[exc.errno]))
        except:
            pbs.logmsg(pbs.EVENT_SYSTEM, "Failed to remove cgroup path: %s" %
                       (path))

        return True

    # Removes cgroup directories that are not associated with a local job
    def cleanup_orphans(self, local_jobs):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        orphan_cnt = 0
        for key in self.paths.keys():
            for dir in glob.glob(os.path.join(self.paths[key], '[0-9]*')):
                jobid = os.path.basename(dir)
                if jobid not in local_jobs:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Removing orphaned cgroup: %s" %
                               (caller_name(), dir))
                    # default number of attempts at killing tasks
                    status = self.__remove_cgroup(dir)
                    if not status:
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "%s: Removing orphaned cgroup %s failed " %
                                   (caller_name(), dir))
                        orphan_cnt += 1
        return orphan_cnt

    # Removes the cgroup directories for a job
    def delete(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        tasks_kill_attempted = False

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            if not tasks_kill_attempted:
                # Make multiple attempts at killing tasks in cgroups on
                # first subsystem encountered since it is "normal" for
                # a cgroup.delete to encounter processes hard to kill
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                                           jobid),
                                              tasks_kill_attempts=(
                                              CGROUP_KILL_ATTEMPTS))
                tasks_kill_attempted = True
            else:
                # We tried getting rid of job processes earlier,
                # so don't try N times with sleeps
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                              jobid), tasks_kill_attempts=1)

            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Status: %s" %
                       (caller_name(), status))
            if status is False:
                # Rerun the job and log the message
                line = 'Unable to cleanup a cgroup. '
                line += 'Will offline the node and requeue the job '
                line += 'and try again. job run_count: %d' % \
                        pbs.event().job.run_count
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Failed to remove cgroup: %s" %
                           (caller_name(), line))
                pbs.event().accept()

    # Write a value to a limit file
    def write_value(self, file, value, mode='w'):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: writing %s to %s" %
                   (caller_name(), value, file))
        try:
            with open(file, mode) as fd:
                fd.write(str(value) + '\n')
        except IOError, exc:
            if exc.errno == errno.ENOENT:
                pbs.logmsg(pbs.EVENT_SYSTEM,
                           "Trying to set limit to unknown file %s" % file)
            elif exc.errno in [errno.EACCES, errno.EPERM]:
                pbs.logmsg(pbs.EVENT_SYSTEM, "Permission denied on file %s" %
                           file)
            elif exc.errno == errno.EBUSY:
                raise CgroupBusyError("Limit rejected")
            elif exc.errno == errno.EINVAL:
                raise CgroupLimitError("Invalid limit value: %s" % (value))
            else:
                raise
        except:
            raise

    # Return memory failcount
    def __get_mem_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.failcnt"), 'r').read().strip())
        except:
            return None

    # Return vmem failcount
    def __get_memsw_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.failcnt"), 'r').read().strip())
        except:
            return None

    # Return hpmem failcount
    def __get_hugetlb_failcnt(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.failcnt"))[0], 'r').read().strip())
        except:
            return None

    # Return the max usage of memory in bytes
    def __get_max_mem_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of memsw in bytes
    def __get_max_memsw_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of hugetlb in bytes
    def __get_max_hugetlb_usage(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.max_usage_in_bytes"))[0],
                            'r').read().strip())
        except:
            return None

    # Return the cpuacct.usage in cpu seconds
    def __get_cpu_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "cpuacct.usage"), 'r').read().strip())
        except:
            return None

    # Assign CPUs to the cpuset
    def select_cpus(self, path, ncpus):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path is %s" % (caller_name(), path))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: ncpus is %s" %
                   (caller_name(), ncpus))
        if ncpus < 1:
            ncpus = 1
        try:
            # Must select from those currently available
            cpufile = os.path.basename(path)
            base = os.path.dirname(path)
            parent = os.path.dirname(base)
            avail = cpus2list(open(os.path.join(parent, cpufile),
                              'r').read().strip())
            if len(avail) < 1:
                raise CgroupProcessingError("No CPUs avaialble in cgroup.")
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Available CPUs: %s" %
                       (caller_name(), avail))
            assigned = []
            for file in glob.glob(os.path.join(parent, '[0-9]*', cpufile)):
                cpus = cpus2list(open(file, 'r').read().strip())
                for id in cpus:
                    if id in avail:
                        avail.remove(id)
            if len(avail) < ncpus:
                raise CgroupProcessingError(
                    "Insufficient CPUs avaialble in cgroup.")
            if len(avail) == ncpus:
                return avail
            # FUTURE: Try to minimize NUMA nodes based on memory requirement
            return avail[0:ncpus]
        except:
            raise

    # Return the error message in system message file
    def __get_error_msg(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        proc = subprocess.Popen(['dmesg'],
                                shell=False, stdout=subprocess.PIPE)
        stdout = proc.communicate()[0].splitlines()
        stdout.reverse()
        # Check to see if the job id is found in dmesg otherwise
        # you could get the information for another job that was killed
        # the line will look like Memory cgroup stats for /pbspro/279.centos7
        kill_line = ""

        for line in stdout:
            start = line.find('Killed process ')
            if start >= 0:
                kill_line = line[start:]
            job_start = line.find(jobid)
            if job_start >= 0:
                # pbs.logmsg(pbs.EVENT_DEBUG3,
                #           "%s: Jobid: %s, Line: %s" %
                #           (caller_name(), jobid, line))
                return kill_line
        return ""

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_job_env_file(self, jobid, env_list):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        # if not os.path.exists(self.host_job_env_dir):
        #    os.makedirs(self.host_job_env_dir, 0755)

        # Write out assigned_resources
        try:
            lines = string.join(env_list, '\n')
            filename = self.host_job_env_filename % jobid
            outfile = open(filename, "w")
            outfile.write(lines)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" % (filename))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (lines))
            return True
        except:
            return False

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        if not os.path.exists(self.hook_storage_dir):
            os.makedirs(self.hook_storage_dir, 0755)

        # Write out assigned_resources
        try:
            json_str = json.dumps(self.host_assigned_resources)
            outfile = open(self.hook_storage_dir+os.sep+jobid, "w")
            outfile.write(json_str)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" %
                       (self.hook_storage_dir+os.sep+jobid))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (json_str))
            return True
        except:
            return False

    def read_in_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        pbs.logmsg(pbs.EVENT_DEBUG3, "Host assigned resources: %s" %
                   (self.host_assigned_resources))
        if os.path.isfile(self.hook_storage_dir+os.sep+jobid):
            # Write out assigned_resources
            try:
                infile = open(self.hook_storage_dir+os.sep+jobid, 'r')
                json_data = json.load(infile, object_hook=decode_dict)
                self.host_assigned_resources = json_data
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Host assigned resources: %s" %
                           (self.host_assigned_resources))
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
            finally:
                infile.close()

        if self.host_assigned_resources is not None:
            return True
        else:
            return False

#
#
# FUNCTION main
#


def main():
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Function called" % (caller_name()))
    hostname = pbs.get_local_nodename()
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Host is %s" % (caller_name(), hostname))

    # Log the hook event type
    e = pbs.event()
    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook name is %s" %
               (caller_name(), e.hook_name))

    try:
        # Instantiate the hook utility class
        hooks = HookUtils()
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Event type is %s" %
                   (caller_name(), hooks.event_name(e.type)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(hooks)))
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: Failed to instantiate hook utility class" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        e.accept()

    if not hooks.hashandler(e.type):
        # Bail out if there is no handler for this event
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s event not handled by this hook" %
                   (caller_name(), hooks.event_name(e.type)))
        e.accept()

    try:
        # If an exception occurs, jobutil must be set
        jobutil = None
        # Instantiate the job utility class first so jobutil can be accessed
        # by the exception handlers.
        if hasattr(e, 'job'):
            jobutil = JobUtils(e.job)
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Job information class instantiated" %
                       (caller_name()))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" %
                       (caller_name(), repr(jobutil)))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Event does not include a job" %
                       (caller_name()))
        # Instantiate the cgroup utility class
        vnode = None
        if hasattr(e, 'vnode_list'):
            if hostname in e.vnode_list.keys():
                vnode = e.vnode_list[hostname]
        cgroup = CgroupUtils(hostname, vnode)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Cgroup utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(cgroup)))
        # Bail out if there is nothing to do
        if len(cgroup.subsystems) < 1:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: All cgroups disabled" %
                       (caller_name()))
            e.accept()

        # Call the appropriate handler
        if hooks.invoke_handler(e, cgroup, jobutil) is True:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned success" %
                       (caller_name()))
            e.accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned failure" %
                       (caller_name()))
            e.reject()

    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass

    except AdminError, exc:
        # Something on the system is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Admin error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except UserError, exc:
        # User must correct problem and resubmit job
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("User error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.delete()
                msg += " (deleted)"
            except:
                msg += " (delete failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except JobValueError, exc:
        # Something in PBS is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Job value error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except Exception, exc:
        # Catch all other exceptions and report them.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Unexpected error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

# line below required for unittesting
if __name__ == "__builtin__":
    start_time = time.time()
    try:
        main()
    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
    finally:
        pbs.logmsg(pbs.EVENT_DEBUG, "Elapsed time: %0.4lf" %
                   (time.time() - start_time))
---
2016-07-18 16:55:17,606 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups.json', 'content-encoding': 'default'}
2016-07-18 16:55:17,606 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups.json
2016-07-18 16:55:17,632 INFO     job: executable set to /bin/sleep with arguments: 100
2016-07-18 16:55:17,633 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0755 /tmp/PtlPbsJobScriptYMcRJs
2016-07-18 16:55:17,672 INFOCLI  centos7: sudo -H -u pbsuser /usr/local/bin/qsub -N test_t1b -l select=1:ncpus=1 /tmp/PtlPbsJobScriptYMcRJs
2016-07-18 16:55:17,696 INFO     submit to centos7 as pbsuser: job 374.centos7 OrderedDict([('Job_Name', 'test_t1b'), ('Resource_List.select', '1:ncpus=1')])
2016-07-18 16:55:17,697 INFOCLI  job script /tmp/PtlPbsJobScriptYMcRJs
---
#!/bin/bash
if [ -d /cgroup/cpuset/pbspro/$PBS_JOBID ]; then
    ls -1 /cgroup/cpuset/pbspro
    cpus=`cat /cgroup/cpuset/pbspro/$PBS_JOBID/cpuset.cpus`
    echo "CpuSocket=${cpus}"
    mems=`cat /cgroup/cpuset/pbspro/$PBS_JOBID/cpuset.mems`
    echo "MemorySocket=${mems}"
    if [ -d /cgroup/memory/pbspro/$PBS_JOBID ]; then
       mem_limit=`cat /cgroup/memory/pbspro/$PBS_JOBID/memory.limit_in_bytes`
       memsw_limit=`cat /cgroup/memory/pbspro/$PBS_JOBID/memory.memsw.limit_in_bytes`
       echo "MemoryLimit=${mem_limit}"
       echo "MemswLimit=${memsw_limit}"
    fi
elif [ -d /sys/fs/cgroup/cpuset/pbspro/$PBS_JOBID ]; then
    ls -1 /sys/fs/cgroup/cpuset/pbspro
    cpus=`cat /sys/fs/cgroup/cpuset/pbspro/$PBS_JOBID/cpuset.cpus`
    echo "CpuSocket=${cpus}"
    mems=`cat /sys/fs/cgroup/cpuset/pbspro/$PBS_JOBID/cpuset.mems`
    echo "MemorySocket=${mems}"
    if [ -d /sys/fs/cgroup/memory/pbspro/$PBS_JOBID ]; then
       mem_limit=`cat /sys/fs/cgroup/memory/pbspro/$PBS_JOBID/memory.limit_in_bytes`
       memsw_limit=`cat /sys/fs/cgroup/memory/pbspro/$PBS_JOBID/memory.memsw.limit_in_bytes`
       echo "MemoryLimit=${mem_limit}"
       echo "MemswLimit=${memsw_limit}"
    fi
else
    echo "Unable to find the pbspro directory in the cgroup directory"
fi
sleep 1

---
2016-07-18 16:55:17,697 INFOCLI  centos7: /usr/local/bin/qstat -f 374.centos7
2016-07-18 16:55:17,791 INFO     expect on server centos7: job_state = R job 374.centos7  got: job_state = Q
2016-07-18 16:55:18,292 INFOCLI  centos7: /usr/local/bin/qmgr -c set server scheduling=True
2016-07-18 16:55:18,306 INFOCLI  centos7: /usr/local/bin/qstat -f 374.centos7
2016-07-18 16:55:18,399 INFO     expect on server centos7: job_state = R job 374.centos7 attempt: 2 ...  OK
2016-07-18 16:55:18,399 INFO     status on centos7: job 374.centos7 ['Output_Path', 'Error_Path', 'Keep_Files']
2016-07-18 16:55:18,399 INFOCLI  centos7: /usr/local/bin/qstat -f 374.centos7
2016-07-18 16:55:18,490 INFO     Job .o file: centos7.usa.org:/tmp/test_t1b.o374
2016-07-18 16:55:20,494 INFO     test_t1: ['374.centos7', 'cgroup.clone_children', 'cgroup.event_control', 'cgroup.procs', 'cpuset.cpu_exclusive', 'cpuset.cpus', 'cpuset.mem_exclusive', 'cpuset.mem_hardwall', 'cpuset.memory_migrate', 'cpuset.memory_pressure', 'cpuset.memory_spread_page', 'cpuset.memory_spread_slab', 'cpuset.mems', 'cpuset.sched_load_balance', 'cpuset.sched_relax_domain_level', 'notify_on_release', 'tasks', 'CpuSocket=0', 'MemorySocket=0', 'MemoryLimit=262144000', 'MemswLimit=268435456', '']
2016-07-18 16:55:20,494 INFO     job dir check passed
2016-07-18 16:55:20,494 INFO     CpuSocket check passed
2016-07-18 16:55:20,494 INFO     MemorySocket check passed
2016-07-18 16:55:20,494 INFO     MemoryLimit check passed
2016-07-18 16:55:20,495 INFO     ==============================
2016-07-18 16:55:20,495 INFO      Entered CgroupsTests tearDown
2016-07-18 16:55:20,495 INFO     ==============================
2016-07-18 16:55:20,495 INFO     ===============================
2016-07-18 16:55:20,495 INFO     Completed CgroupsTests tearDown
2016-07-18 16:55:20,495 INFO     ===============================
2016-07-18 16:55:20,496 INFO     ok

2016-07-18 16:55:20,496 INFO     test name: test_t1c (tests.cgroups_tests.CgroupsTests)...
2016-07-18 16:55:20,496 INFO     test start time: Mon Jul 18 16:55:20 2016
2016-07-18 16:55:20,496 INFO     test docstring: 
 Test to verify that the cgroup prefix is set to pbs and that
            only the devices subsystem is enabled with the correct devices
            allowed
         
2016-07-18 16:55:20,496 INFO     ===========================
2016-07-18 16:55:20,496 INFO      Entered CgroupsTests setUp
2016-07-18 16:55:20,497 INFO     ===========================
2016-07-18 16:55:20,497 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:20,549 INFO     status on centos7: server
2016-07-18 16:55:20,550 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:20,609 INFO     manager on centos7: set server {'managers': (3, 'root@*')}
2016-07-18 16:55:20,610 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers-=root@*
2016-07-18 16:55:20,631 INFO     manager on centos7: set server {'managers': (2, 'root@*')}
2016-07-18 16:55:20,632 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers+=root@*
2016-07-18 16:55:20,656 INFO     server centos7: reverting configuration to defaults
2016-07-18 16:55:20,656 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:20,709 INFO     select on centos7: __ALL__
2016-07-18 16:55:20,709 INFOCLI  centos7: /usr/local/bin/qselect
2016-07-18 16:55:20,719 INFOCLI  centos7: /usr/local/bin/qstat -f @centos7.usa.org
2016-07-18 16:55:20,783 INFO     expect on server centos7: job_state set 0 job ...  OK
2016-07-18 16:55:20,783 INFOCLI  centos7: /usr/local/bin/pbs_rstat -f
2016-07-18 16:55:20,793 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:20,819 INFO     manager on centos7: delete hook t1_l1
2016-07-18 16:55:20,819 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c delete hook t1_l1
2016-07-18 16:55:20,848 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:55:20,874 INFO     expect on server centos7:  unset  hook t1_l1 ...  OK
2016-07-18 16:55:20,875 INFOCLI  centos7: /usr/local/bin/qstat -Qf @centos7.usa.org
2016-07-18 16:55:20,891 INFO     manager on centos7: delete queue workq
2016-07-18 16:55:20,892 INFOCLI  centos7: /usr/local/bin/qmgr -c delete queue workq
2016-07-18 16:55:20,928 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:55:20,938 INFO     expect on server centos7:  unset  queue workq ...  OK
2016-07-18 16:55:20,938 INFO     manager on centos7: create queue workq {'started': 'True', 'queue_type': 'Execution', 'enabled': 'True'}
2016-07-18 16:55:20,939 INFOCLI  centos7: /usr/local/bin/qmgr -c create queue workq started=True,queue_type=Execution,enabled=True
2016-07-18 16:55:20,957 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:55:20,986 INFO     expect on server centos7: started set True || queue_type set Execution || enabled set True queue workq ...  OK
2016-07-18 16:55:20,986 INFO     manager on centos7: set server {'log_events': '511', 'default_queue': 'workq'}
2016-07-18 16:55:20,986 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=511,default_queue=workq
2016-07-18 16:55:21,006 INFO     status on centos7: resource
2016-07-18 16:55:21,006 INFO     manager on centos7: list resource
2016-07-18 16:55:21,006 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource
2016-07-18 16:55:21,063 INFO     manager on centos7: delete resource nmics,ngpus
2016-07-18 16:55:21,064 INFOCLI  centos7: /usr/local/bin/qmgr -c delete resource nmics,ngpus
2016-07-18 16:55:21,114 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource nmics
2016-07-18 16:55:21,127 INFO     expect on server centos7:  unset  resource nmics ...  OK
2016-07-18 16:55:21,128 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource ngpus
2016-07-18 16:55:21,149 INFO     expect on server centos7:  unset  resource ngpus ...  OK
2016-07-18 16:55:21,150 INFOCLI  status on centos7: server license_count
2016-07-18 16:55:21,150 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:21,204 INFO     server: centos7.usa.org licensed
2016-07-18 16:55:21,229 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/server_priv/comm.lock
2016-07-18 16:55:21,273 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:55:21,285 INFO     scheduler centos7: reverting configuration to defaults
2016-07-18 16:55:21,286 INFO     manager on centos7: list sched
2016-07-18 16:55:21,286 INFOCLI  centos7: /usr/local/bin/qmgr -c list sched
2016-07-18 16:55:21,303 INFO     manager on centos7: unset sched ['sched_cycle_length']
2016-07-18 16:55:21,303 INFOCLI  centos7: /usr/local/bin/qmgr -c unset sched sched_cycle_length
2016-07-18 16:55:21,337 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/dedicated_time
2016-07-18 16:55:21,353 INFOCLI2 centos7: cmp /usr/local/etc/pbs_resource_group /var/spool/pbs/sched_priv/resource_group
2016-07-18 16:55:21,358 INFO     scheduler centos7: reverting holidays file to default
2016-07-18 16:55:21,359 INFOCLI2 centos7: cmp /usr/local/etc/pbs_holidays /var/spool/pbs/sched_priv/holidays
2016-07-18 16:55:21,362 INFOCLI2 centos7: cmp /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:21,370 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:21,382 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:21,392 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:55:21,392 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:55:21,443 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:55:21,470 INFOCLI2 centos7: sudo -H cmp /usr/local/sbin/pbs_mom /usr/local/sbin/pbs_mom.cpuset
2016-07-18 16:55:21,518 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:55:21,532 INFO     mom centos7: reverting configuration to defaults
2016-07-18 16:55:21,532 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/epilogue
2016-07-18 16:55:21,552 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/prologue
2016-07-18 16:55:21,564 INFOCLI2 centos7: sudo -H ls /var/spool/pbs/mom_priv/config.d
2016-07-18 16:55:21,576 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsdZNsC9 /var/spool/pbs/mom_priv/config
2016-07-18 16:55:21,590 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/mom_priv/config
2016-07-18 16:55:21,601 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/mom_priv/config
2016-07-18 16:55:21,625 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/mom_priv/config
2016-07-18 16:55:21,639 INFO     mom centos7: sent signal -HUP
2016-07-18 16:55:21,639 INFOCLI2 centos7: sudo -H kill -HUP 42976
2016-07-18 16:55:21,693 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:55:21,706 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 16:55:21,718 INFO     status on centos7: node centos7
2016-07-18 16:55:21,719 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:55:21,730 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:55:21,741 INFO     expect on server centos7: state = free node centos7 ...  OK
2016-07-18 16:55:21,741 INFO     ============================
2016-07-18 16:55:21,742 INFO     Completed CgroupsTests setUp
2016-07-18 16:55:21,742 INFO     ============================
2016-07-18 16:55:21,742 INFO     server centos7: server operating mode set to cli
2016-07-18 16:55:21,742 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:21,797 INFO     expect on server centos7: pbs_version ~ .*14.* server centos7.usa.org ...  OK
2016-07-18 16:55:21,797 INFO     manager on centos7: set server {'log_events': '2047'}
2016-07-18 16:55:21,798 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=2047
2016-07-18 16:55:21,823 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:21,874 INFO     expect on server centos7: log_events set 2047 server centos7.usa.org ...  OK
2016-07-18 16:55:21,874 INFO     scheduler centos7: config {'resources': 'ncpus,mem,host,vnode,vmem'}
2016-07-18 16:55:21,875 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /var/spool/pbs/sched_priv/sched_config /var/spool/pbs/sched_priv/sched_config.bak
2016-07-18 16:55:21,886 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsUnD5eC /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:21,909 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:21,923 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:21,935 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:21,963 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:55:21,963 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:55:21,974 INFO     scheduler centos7 log match: searching for "Error reading line" - No match
2016-07-18 16:55:22,976 INFO     manager on centos7 as root: create resource nmics {'flag': 'nh', 'type': 'long'}
2016-07-18 16:55:22,977 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource nmics flag=nh,type=long
2016-07-18 16:55:23,001 INFO     manager on centos7 as root: create resource ngpus {'flag': 'nh', 'type': 'long'}
2016-07-18 16:55:23,001 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource ngpus flag=nh,type=long
2016-07-18 16:55:23,038 INFO     Test Summary: test_t1_l1 tests "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" cgroup hook
2016-07-18 16:55:23,038 INFO     status on centos7: hook
2016-07-18 16:55:23,039 INFO     manager on centos7: list hook
2016-07-18 16:55:23,039 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:23,058 INFO     status on centos7: hook
2016-07-18 16:55:23,058 INFO     manager on centos7: list hook
2016-07-18 16:55:23,059 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:23,080 INFO     manager on centos7: create hook t1_l1
2016-07-18 16:55:23,080 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c create hook t1_l1
2016-07-18 16:55:23,117 INFO     manager on centos7: set hook t1_l1 {'freq': 2, 'enabled': 'True', 'event': '"execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"'}
2016-07-18 16:55:23,117 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set hook t1_l1 freq=2,enabled=True,event="execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"
2016-07-18 16:55:23,145 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:55:23,165 INFO     expect on server centos7: freq set 2 || enabled set True || event set "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" hook t1_l1 ...  OK
2016-07-18 16:55:23,166 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-python', 'input-file': '/tmp/PtlPbs6aWO80', 'content-encoding': 'default'}
2016-07-18 16:55:23,166 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-python default /tmp/PtlPbs6aWO80
2016-07-18 16:55:23,211 INFO     server centos7: imported hook body
---
#  Copyright (C) 2003-2016 Altair Engineering, Inc. All rights reserved.
#
#  ALTAIR ENGINEERING INC. Proprietary and Confidential. Contains Trade Secret
#  Information. Not for use or disclosure outside ALTAIR and its licensed
#  clients. Information contained herein shall not be decompiled, disassembled,
#  duplicated or disclosed in whole or in part for any purpose. Usage of the
#  software is only as explicitly permitted in the end user software license
#  agreement.
#
#  Copyright notice does not imply publication.

# Last Updated
# Date: 07-06-2016
#
# When soft_limit is true for memory, memsw represents the hard limit.
#
# The resources value in sched_config must contain entries for mem and
# vmem if those subsystems are enabled in the hook configuration file. The
# amount of resource requested will not be avaiable to the hook if they
# are not present.

# This hook handles all of the operations necessary for PBS to support
# cgroups on linux hosts that support them (kernel 2.6.28 and higher)
#
# This hook services the following events:
# - exechost_periodic
# - exechost_startup
# - execjob_attach
# - execjob_begin
# - execjob_end
# - execjob_epilogue
# - execjob_launch

# Imports from __future__ must happen first
from __future__ import with_statement

# Additional imports
import sys
import os
import errno
import signal
import subprocess
import re
import glob
import time
import string
import platform
import traceback
import copy
from stat import S_ISBLK, S_ISCHR
import operator
import pwd
import pbs
import fnmatch
import socket

try:
    import json
except:
    import simplejson as json

CGROUP_KILL_ATTEMPTS = 12

# Flag to turn off features not in 12.2.40X branch
CRAY_12_BRANCH = False


# ============================================================================
# Derived error classes
# ============================================================================

# Base class for errors fixable only by administrative action.


class AdminError(Exception):
    pass

# Base class for errors in processing, unknown cause.


class ProcessingError(Exception):
    pass

# Base class for errors fixable by the user.


class UserError(Exception):
    pass

# Errors in PBS job resource values.


class JobValueError(UserError):
    pass

# Errors when the cgroup is busy.


class CgroupBusyError(ProcessingError):
    pass

# Errors in configuring cgroup.


class CgroupConfigError(AdminError):
    pass

# Errors in configuring cgroup.


class CgroupLimitError(AdminError):
    pass

# Errors processing cgroup.


class CgroupProcessingError(ProcessingError):
    pass

# ============================================================================
# Utility functions
# ============================================================================

#
# FUNCTION caller_name
#
# Return the name of the calling function or method.
#


def caller_name():
    return str(sys._getframe(1).f_code.co_name)

#
# FUNCTION convert_size
#
# Convert a string containing a size specification (e.g. "1m") to a
# string using different units (e.g. "1024k").
#
# This function only interprets a decimal number at the start of the string,
# stopping at any unrecognized character and ignoring the rest of the string.
#
# When down-converting (e.g. MB to KB), all calculations involve integers and
# the result returned is exact. When up-converting (e.g. KB to MB) floating
# point numbers are involved. The result is rounded up. For example:
#
# 1023MB -> GB yields 1g
# 1024MB -> GB yields 1g
# 1025MB -> GB yields 2g  <-- This value was rounded up
#
# Pattern matching or conversion may result in exceptions.
#


def convert_size(value, units='b'):
    logs = {'b': 0, 'k': 10, 'm': 20, 'g': 30,
            't': 40, 'p': 50, 'e': 60, 'z': 70, 'y': 80}
    try:
        new = units[0].lower()
        if new not in logs.keys():
            new = 'b'
        val, old = re.match('([-+]?\d+)([bkmgtpezy]?)',
                            str(value).lower()).groups()
        val = int(val)
        if val < 0:
            raise ValueError('Value may not be negative')
        if old not in logs.keys():
            old = 'b'
        factor = logs[old] - logs[new]
        val *= 2 ** factor
        slop = val - int(val)
        val = int(val)
        if slop > 0:
            val += 1
        # pbs.size() does not like units following zero
        if val <= 0:
            return '0'
        else:
            return str(val) + new
    except:
        return None

#
# FUNCTION size_as_int
#
# Convert a size string to an integer representation of size in bytes
#


def size_as_int(value):
    return int(convert_size(value).rstrip(string.ascii_lowercase))

#
# FUNCTION convert_time
#
# Converts a integer value for time into the value of the return unit
#
# A valid decimal number, with optional sign, may be followed by a character
# representing a scaling factor.  Scaling factors may be either upper or
# lower case. Examples include:
# 250ms
# 40s
# +15min
#
# Valid scaling factors are:
# ns  = 10**-9
# us  = 10**-6
# ms  = 10**-3
# s   =      1
# min =     60
# hr  =   3600
#
# Pattern matching or conversion may result in exceptions.
#


def convert_time(value, return_unit='s'):
    multipliers = {'':  1, 'ns': 10 ** -9, 'us': 10 ** -6,
                   'ms': 10 ** -3, 's': 1, 'min': 60, 'hr': 3600}
    n, factor = re.match('([-+]?\d+)(\w*[a-zA-Z])?',
                         str(value).lower()).groups(0)

    # Check to see if there was not unit of time specified
    if factor == 0:
        factor = ''

    # Check to see if the unit is valid
    if str.lower(factor) not in multipliers.keys():
        raise ValueError('Time unit not recognized.')

    # Convert the value to seconds
    value_in_sec = float(n) * float(multipliers[str.lower(factor)])
    if return_unit != 's':
        return value_in_sec / multipliers[str.lower(return_unit)]
    else:
        return float('%lf' % value_in_sec)

# simplejson hook to convert lists from unicode to utf-8


def decode_list(data):
    rv = []
    for item in data:
        if isinstance(item, unicode):
            item = item.encode('utf-8')
        elif isinstance(item, list):
            item = decode_list(item)
        elif isinstance(item, dict):
            item = decode_dict(item)
        rv.append(item)
    return rv

# simplejson hook to convert dictionaries from unicode to utf-8


def decode_dict(data):
    rv = {}
    for key, value in data.iteritems():
        if isinstance(key, unicode):
            key = key.encode('utf-8')
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        elif isinstance(value, list):
            value = decode_list(value)
        elif isinstance(value, dict):
            value = decode_dict(value)
        rv[key] = value
    return rv

# Convert CPU list format (with ranges) to a Python list


def cpus2list(s):
    # The input string is a comma separated list of digits and ranges.
    # Examples include:
    # 0-3,8-11
    # 0,2,4,6
    # 2,5-7,10
    cpus = []
    if s.strip() == '':
        return cpus
    for r in s.split(','):
        if '-' in r[1:]:
            start, end = r.split('-', 1)
            for i in range(int(start), int(end) + 1):
                cpus.append(int(i))
        else:
            cpus.append(int(r))
    return cpus


# Helper function to ignore suspended jobs when removing
# "already used" resources
def job_to_be_ignored(jobid):
    # Get job substate from small utility that calls printjob
    pbs_exec = ''
    if not CRAY_12_BRANCH:
        pbs_conf = pbs.get_pbs_conf()
        pbs_exec = pbs_conf['PBS_EXEC']
    else:
        if 'PBS_EXEC' in self.cfg:
            pbs_exec = self.cfg['PBS_EXEC']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "PBS_EXEC needs to be defined in the config file")
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Exiting the cgroups hook")
            pbs.accept()

    printjob_cmd = pbs_exec+os.sep+'bin'+os.sep+'printjob'

    cmd = [printjob_cmd, jobid]

    substate = None
    try:
        pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
        # Collect the job substate information
        process = subprocess.Popen(
                                   cmd,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        (out, err) = process.communicate()
        # Find the job substate
        substate_re = re.compile(r"substate:\s+(?P<jobstate>\S+)\s+")
        substate = substate_re.search(out)
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Unexpected error in job_to_be_ignored: %s" %
                   ' '.join([repr(sys.exc_info()[0]),
                            repr(sys.exc_info()[1])]))
        out = "Unknown: failed to run " + printjob_cmd

    if substate is not None:
        substate = substate.group().split(':')[1].strip()
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Job %s has substate %s" % (jobid, substate))

        suspended_substates = ['0x2b', '0x2d', 'unknown']
        return (substate in suspended_substates)
    else:
        return False

# ============================================================================
# Utility classes
# ============================================================================

#
# CLASS HookUtils
#


class HookUtils:

    def __init__(self, hook_events=None):
        if hook_events is not None:
            self.hook_events = hook_events
        else:
            self.hook_events = {
                # Defined in the order they appear in module_pbs_v1.c
                pbs.QUEUEJOB: {
                    'name': 'queuejob',
                    'handler': None
                },
                pbs.MODIFYJOB: {
                    'name': 'modifyjob',
                    'handler': None
                },
                pbs.RESVSUB: {
                    'name': 'resvsub',
                    'handler': None
                },
                pbs.MOVEJOB: {
                    'name': 'movejob',
                    'handler': None
                },
                pbs.RUNJOB: {
                    'name': 'runjob',
                    'handler': None
                },
                pbs.PROVISION: {
                    'name': 'provision',
                    'handler': None
                },
                pbs.EXECJOB_BEGIN: {
                    'name': 'execjob_begin',
                    'handler': self.__execjob_begin_handler
                },
                pbs.EXECJOB_PROLOGUE: {
                    'name': 'execjob_prologue',
                    'handler': None
                },
                pbs.EXECJOB_EPILOGUE: {
                    'name': 'execjob_epilogue',
                    'handler': self.__execjob_epilogue_handler
                },
                pbs.EXECJOB_PRETERM: {
                    'name': 'execjob_preterm',
                    'handler': None
                },
                pbs.EXECJOB_END: {
                    'name': 'execjob_end',
                    'handler': self.__execjob_end_handler
                },
                pbs.EXECJOB_LAUNCH: {
                    'name': 'execjob_launch',
                    'handler': self.__execjob_launch_handler
                },
                pbs.EXECHOST_PERIODIC: {
                    'name': 'exechost_periodic',
                    'handler': self.__exechost_periodic_handler
                },
                pbs.EXECHOST_STARTUP: {
                    'name': 'exechost_startup',
                    'handler': self.__exechost_startup_handler
                },
                pbs.MOM_EVENTS: {
                    'name': 'mom_events',
                    'handler': None
                },
            }

            if not CRAY_12_BRANCH:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "CRAY 12 Branch: %s" % (CRAY_12_BRANCH))
                self.hook_events[pbs.EXECJOB_ATTACH] = {
                    'name': 'execjob_attach',
                    'handler': self.__execjob_attach_handler
                }

    def __repr__(self):
        return "HookUtils(%s)" % (repr(self.hook_events))

    def event_name(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['name']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Type: %s not found" % (caller_name(), type))
            return None

    def hashandler(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['handler'] is not None
        else:
            return None

    def invoke_handler(self, event, cgroup, jobutil, *args):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: UID: real=%d, effective=%d" %
                   (caller_name(), os.getuid(), os.geteuid()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: GID: real=%d, effective=%d" %
                   (caller_name(), os.getgid(), os.getegid()))
        if self.hashandler(event.type):
            result = self.hook_events[event.type][
                'handler'](event, cgroup, jobutil, *args)
            return result
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: %s event not handled by this hook." %
                       (caller_name(), self.event_name(event.type)))

    def __execjob_begin_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Instantiate the NodeConfig class for get_memory_on_node and
        # get_vmem_on node
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Host assigned job resources: %s" %
                   (caller_name(), jobutil.host_resources))

        # Make sure the parent cgroup directories exist
        cgroup.create_paths()

        # Make sure that any old cgroups created in an earlier attempt
        # at creating or running the job are gone
        cgroup.delete(e.job.id)

        # Gather the assigned resources
        cgroup.gather_assigned_resources()
        # Create the cgroup(s) for the job
        cgroup.create_job(e.job.id, node)
        # Configure the new cgroup
        cgroup.configure_job(e.job.id, jobutil.host_resources, node)
        # Initialize resource usage for the job
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        # Write out the assigned resources
        cgroup.write_out_cgroup_host_assigned_resources(e.job.id)
        # Write out the environment variable for the host (pbs_attach)
        if 'devices_name' in cgroup.host_assigned_resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Devices: %s" %
                       cgroup.host_assigned_resources['devices_name'])
            env_list = list()
            if len(cgroup.host_assigned_resources['devices_name']) > 0:
                line = str()
                mics = list()
                gpus = list()
                for key in cgroup.host_assigned_resources['devices_name']:
                    if key.startswith('mic'):
                        mics.append(key[3:])
                    elif key.startswith('nvidia'):
                        gpus.append(key[6:])
                if len(mics) > 0:
                    env_list.append('OFFLOAD_DEVICES="%s"' %
                                    string.join(mics, ','))
                if len(gpus) > 0:
                    # don't put quotes around the values. ex "0" or "0,1".
                    # This will cause it to fail.
                    env_list.append('CUDA_VISIBLE_DEVICES=%s' %
                                    string.join(gpus, ','))

            pbs.logmsg(pbs.EVENT_DEBUG, "ENV_LIST: %s" % env_list)
            cgroup.write_out_cgroup_host_job_env_file(e.job.id, env_list)
        return True

    def __execjob_epilogue_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # The resources_used information has a base type of pbs_resource
        # Set the usage data
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        return True

    def __execjob_end_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Delete the cgroup(s) for the job
        cgroup.delete(e.job.id)
        # Remove the host_assigned_resources and job_env file
        for filename in [cgroup.hook_storage_dir+os.sep+e.job.id,
                         cgroup.host_job_env_filename % e.job.id]:
            try:
                os.remove(filename)
            except OSError:
                pbs.logmsg(pbs.EVENT_DEBUG3, "File: %s not found" % (filename))
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Error removing file: %s" % (filename))
                pass

        return True

    def __execjob_launch_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Add the parent process id to the appropriate cgroups.
        cgroup.add_pids(os.getppid(), jobutil.job.id)
        # FUTURE: Add environment variable to the job environment
        # if job requested mic or gpu
        cgroup.read_in_cgroup_host_assigned_resources(e.job.id)
        if cgroup.host_assigned_resources is not None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "host_assigned_resources: %s" %
                       (cgroup.host_assigned_resources))
            cgroup.setup_job_devices_env()
        return True

    def __exechost_periodic_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Cleanup cgroups for jobs not present on this node
        count = cgroup.cleanup_orphans(e.job_list)
        if ('periodic_resc_update' in cgroup.cfg and
                cgroup.cfg['periodic_resc_update']):
            for job in e.job_list.keys():
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: job is %s" %
                           (caller_name(), job))
                cgroup.update_job_usage(job, e.job_list[job].resources_used)
        # Online nodes that were offlined due to a cgroup not cleaning up
        if count == 0 and cgroup.cfg['online_offlined_nodes']:
            vnode = pbs.event().vnode_list[cgroup.hostname]
            if os.path.isfile(cgroup.offline_file):
                line = 'Orphan cgroup(s) have been cleaned up. '

                # Check with the pbs server to see if the node comment matches
                try:
                    tmp_comment = pbs.server().vnode(cgroup.hostname).comment
                except:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Unable to contact server for node comment")
                    tmp_comment = None
                if tmp_comment == cgroup.offline_msg:
                    line += 'Will bring the node back online'
                    vnode.state = pbs.ND_FREE
                    vnode.comment = None
                else:
                    line += 'However, the comment has changed since the node '
                    line += 'was offlined. Node will remain offline'

                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: %s" % (caller_name(), line))
                # Remove file
                os.remove(cgroup.offline_file)
        return True

    def __exechost_startup_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        cgroup.create_paths()
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))

        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        node.create_vnodes()
        if 'memory' in cgroup.subsystems:
            val = node.get_memory_on_node(cgroup.cfg)
            if val is not None:
                if 'vnode_per_numa_node' in cgroup.cfg:
                    if not cgroup.cfg['vnode_per_numa_node']:
                        hn = node.hostname
                        e.vnode_list[hn].resources_available['mem'] = \
                            pbs.size(val)
                        val_cpus = node.totalcpus
                        if val_cpus is not None:
                            e.vnode_list[hn].resources_available['ncpus'] = \
                                int(val_cpus)
                cgroup.set_node_limit('mem', val)
        if 'memsw' in cgroup.subsystems:
            val = node.get_vmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['vmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('vmem', val)
        if 'hugetlb' in cgroup.subsystems:
            val = node.get_hpmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['hpmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('hpmem', val)
        return True

    def __execjob_attach_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logjobmsg(jobutil.job.id, "%s: Attaching PID %s" %
                      (caller_name(), e.pid))
        # Add the job process id to the appropriate cgroups.
        cgroup.add_pids(e.pid, jobutil.job.id)
        return True

#
# CLASS ShallIRunUtils
#


class ShallIRunUtils:

    def __init__(self, hostname, cfg):
        self.cfg = cfg
        self.hostname = hostname

    def allow_users(self, allow_users):
        """ This still needs to be defined """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pass

    def exclude_hosts(self, exclude_hosts):
        """
        Gets the hostname for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        hostname = pbs.get_local_nodename()
        # check to see if hostname is in the exclude_hosts list
        if self.hostname in exclude_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: exclude host %s is in %s" %
                       (caller_name(), self.hostname, exclude_hosts))
            pbs.event().accept()

        # check to see if it regex syntax is used
        pass

    def exclude_vntypes(self, exclude_vntypes):
        """
        Gets the vntype for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        vntype = None

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'
        if os.path.isfile(vntype_file):
            fdata = open(vntype_file).readlines()
            vntype = fdata[0].strip()
            pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
        if vntype is None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype is set to %s" % (vntype))
        elif vntype in exclude_vntypes:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s is in %s" % (vntype, exclude_vntypes))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Exiting Hook")
            pbs.event().accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s not in global exclude: %s" %
                       (vntype, exclude_vntypes))
        pass

    def run_only_on_hosts(self, approved_hosts):
        """
        Gets the list of approved nodes to run on and checks to see if the hook
        should run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Approved hosts: %s, %s" %
                   (type(approved_hosts), approved_hosts))

        # check to see if hostname is in the approved_hosts list
        if approved_hosts == []:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "approved hosts list is empty: %s" % approved_hosts)
        elif self.hostname not in approved_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s is not in the approved list of hosts: %s" %
                       (self.hostname, approved_hosts))
            pbs.event().accept()

        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s in list of approved hosts: %s" %
                       (self.hostname, approved_hosts))

#
# CLASS JobUtils
#


class JobUtils:

    def __init__(self, job, hostname=None, resources=None):
        self.job = job
        self.host_vnodes = list()
        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if resources is not None:
            self.host_resources = resources
        else:
            self.host_resources = self.__host_assigned_resources()

    def __repr__(self):
        return "JobUtils(%s, %s, %s)" % (repr(self.job), repr(self.hostname),
                                         repr(self.host_resources))

    # Return a dictionary of resources assigned to the local node
    def __host_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Bail out if no hostname was provided
        if self.hostname is None:
            raise CgroupProcessingError('No hostname available')
        # Bail out if no job information is present
        if self.job is None:
            raise CgroupProcessingError('No job information available')
        # Determine which vnodes are on this host, if any
        if self.hostname is not None:
            vnhost_pattern = "%s\[[\d+]\]" % self.hostname
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnhost pattern: %s" %
                       (caller_name(), vnhost_pattern))
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job exec_vnode list: %s" %
                       (caller_name(), self.job.exec_vnode))
            for match in re.findall(vnhost_pattern, str(self.job.exec_vnode)):
                self.host_vnodes.append(match)
            if self.host_vnodes is not None:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Associate %s to vnodes on %s" %
                           (caller_name(), self.host_vnodes, self.hostname))

        # Collect host assigned resources
        resources = {}
        for chunk in self.job.exec_vnode.chunks:
            vnode = False
            if self.host_vnodes == [] and chunk.vnode_name != self.hostname:
                continue
            elif self.host_vnodes != [] and \
                    chunk.vnode_name not in self.host_vnodes:
                continue
            elif chunk.vnode_name in self.host_vnodes:
                vnode = True
                if 'vnodes' not in resources:
                    resources['vnodes'] = {}
                if chunk.vnode_name not in resources['vnodes']:
                    resources['vnodes'][chunk.vnode_name] = {}
                # This check is needed since not all resources are
                # required to be in each chunk of a job
                # i.e. exec_vnodes =
                # (node1[0]:ncpus=4:mem=4gb+node1[1]:mem=2gb) +
                # (node1[1]:ncpus=3+node[0]:ncpus=1:mem=4gb)
                if all(resc in resources['vnodes'][chunk.vnode_name]
                       for resc in chunk.chunk_resources.keys()):
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: resources already defined" %
                               (caller_name()))
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s,\t%s" %
                               (caller_name(),
                                chunk.chunk_resources.keys(),
                                resources['vnodes'][chunk.vnode_name].keys()))
                else:
                    # Initialize resources for each vnode
                    for resc in chunk.chunk_resources.keys():
                        # Initialize the new value
                        if isinstance(chunk.chunk_resources[resc],
                                      pbs.pbs_int):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_int(0)
                        elif isinstance(chunk.chunk_resources[resc],
                                        pbs.pbs_float):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_float(0)
                        elif isinstance(chunk.chunk_resources[resc], pbs.size):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.size('0')

            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Chunk %s" %
                       (caller_name(), chunk.vnode_name))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resources: %s" %
                       (caller_name(), resources))
            for resc in chunk.chunk_resources.keys():
                if resc not in resources.keys():
                    # Initialize the new value
                    if isinstance(chunk.chunk_resources[resc], pbs.pbs_int):
                        resources[resc] = pbs.pbs_int(0)
                    elif isinstance(chunk.chunk_resources[resc],
                                    pbs.pbs_float):
                        resources[resc] = pbs.pbs_float(0.0)
                    elif isinstance(chunk.chunk_resources[resc], pbs.size):
                        resources[resc] = pbs.size('0')
                # Add resource value to total
                if type(chunk.chunk_resources[resc]) in \
                        [pbs.pbs_int, pbs.pbs_float, pbs.size]:
                    resources[resc] += chunk.chunk_resources[resc]
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s: resources[%s][%s] is now %s" %
                               (caller_name(), self.hostname, resc,
                                resources[resc]))
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] += \
                                  chunk.chunk_resources[resc]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Setting resource %s to string %s" %
                               (caller_name(), resc,
                                str(chunk.chunk_resources[resc])))
                    resources[resc] = str(chunk.chunk_resources[resc])
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] = \
                                  str(chunk.chunk_resources[resc])
        if not resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: No resources assigned to host %s" %
                       (caller_name(), self.hostname))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources for %s: %s" %
                       (caller_name(), self.hostname, repr(resources)))

        # Return assigned resources for specified host
        pbs.logmsg(pbs.EVENT_DEBUG3, "Job Resources: %s" % (resources))
        return resources

    # Write a message to the job stdout file
    def write_to_stderr(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stderr_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

    # Write a message to the job stdout file
    def write_to_stdout(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stdout_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

#
# CLASS NodeConfig
#


class NodeConfig:

    def __init__(self, cfg, **kwargs):

        self.cfg = cfg
        hostname = None
        meminfo = None
        numa_nodes = None
        devices = None
        hyperthreading = None
        self.hyperthreads_per_core = 1
        self.totalcpus = self.__discover_cpus()

        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        self.host_mom_jobdir = pbs_home+'/mom_priv/jobs'

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'hostname':
                    hostname = val
                elif arg == 'meminfo':
                    meminfo = val
                elif arg == 'numa_nodes':
                    numa_nodes = val
                elif arg == 'devices':
                    devices = val
                elif arg == 'hyperthreading':
                    hyperthreading = val

        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if meminfo is not None:
            self.meminfo = meminfo
        else:
            self.meminfo = self.__discover_meminfo()
        if numa_nodes is not None:
            self.numa_nodes = numa_nodes
        else:
            self.numa_nodes = self.__discover_numa_nodes()
        if devices is not None:
            self.devices = devices
        else:
            self.devices = self.__discover_devices()
        if hyperthreading is not None:
            self.hyperthreading = hyperthreading
        else:
            self.hyperthreading = self.__hyperthreading_enabled()

        # Add the devices count i.e. nmics and ngpus to the numa nodes
        self.__add_devices_to_numa_node()

    def __repr__(self):
        return ("NodeConfig(%s, %s, %s, %s, %s, %s)" %
                (repr(self.cfg), repr(self.hostname), repr(self.meminfo),
                 repr(self.numa_nodes), repr(self.devices),
                 repr(self.hyperthreading)))

    def __add_devices_to_numa_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (self.devices.keys()))
        for device in self.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" % (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" %
                           (self.devices[device].keys()))
                for device_name in self.devices[device]:
                    device_socket = \
                        self.devices[device][device_name]['numa_node']
                    if device == 'mic':
                        if 'nmics' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['nmics'] = 1
                        else:
                            self.numa_nodes[device_socket]['nmics'] += 1
                    elif device == 'gpu':
                        if 'ngpus' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['ngpus'] = 1
                        else:
                            self.numa_nodes[device_socket]['ngpus'] += 1

        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (self.numa_nodes))

    # Discover what type of hardware is on this node and how is it partitioned
    def __discover_numa_nodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        numa_nodes = {}
        for dir in glob.glob(os.path.join(os.sep,
                             "sys", "devices", "system", "node", "node*")):
            id = int(dir.split(os.sep)[5][4:])
            if id not in numa_nodes.keys():
                numa_nodes[id] = {}
                numa_nodes[id]['devices'] = list()
            numa_nodes[id]['cpus'] = open(os.path.join(dir, "cpulist"),
                                          'r').readline().strip()
            with open(os.path.join(dir, "meminfo"), 'r') as fd:
                for line in fd:
                    # Each line will contain four or five fields. Examples:
                    # Node 0 MemTotal:       32995028 kB
                    # Node 0 HugePages_Total:     0
                    entries = line.split()
                    if len(entries) < 4:
                        continue
                    if entries[2] == "MemTotal:":
                        numa_nodes[id][entries[2].rstrip(':')] = \
                            convert_size(entries[3] + entries[4], 'kb')
                    elif entries[2] == "HugePages_Total:":
                        numa_nodes[id][entries[2].rstrip(':')] = entries[3]
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover Numa Nodes: %s" % numa_nodes)
        return numa_nodes

    # Identify devices and to which numa nodes they are attached
    def __discover_devices(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        devices = {}
        for file in glob.glob(os.path.join(os.sep, "sys", "class", "*", "*",
                                           "device", "numa_node")):
            # The file should contain a single integer
            numa_node = int(open(file, 'r').readline().strip())
            if numa_node < 0:
                numa_node = 0
            dirs = file.split(os.sep)
            name = dirs[3]
            instance = dirs[4]
            if name not in devices.keys():
                devices[name] = {}
            devices[name][instance] = {}
            devices[name][instance]['numa_node'] = numa_node
            if name == 'mic':
                s = os.stat('/dev/%s' % instance)
                devices[name][instance]['major'] = os.major(s.st_rdev)
                devices[name][instance]['minor'] = os.minor(s.st_rdev)
                devices[name][instance]['device_type'] = 'c'

        # Check to see if there are gpus on the node
        gpus = self.discover_gpus()
        if isinstance(gpus, dict) and len(gpus.keys()) > 0:
            devices['gpu'] = gpus['gpu']

        pbs.logmsg(pbs.EVENT_DEBUG3, "Discovered devices: %s" % (devices))
        return devices

    def discover_gpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Check to see if nvidia-smi exists and produces valid output
            cmd = ['/usr/bin/nvidia-smi', '-q', '-x']
            if 'nvidia-smi' in self.cfg:
                cmd[0] = self.cfg['nvidia-smi']

            pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
            # Collect the gpu information
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.PIPE)
            output, err = process.communicate()

            # pbs.logmsg(pbs.EVENT_DEBUG3,"output: %s" % output)
            # import the xml library and parse the output
            import xml.etree.ElementTree as ET

            # Parse the data
            name = 'gpu'
            gpu_data = dict()
            gpu_data[name] = {}
            root = ET.fromstring(output)
            pbs.logmsg(pbs.EVENT_DEBUG3, "root.tag: %s" % root.tag)
            for child in root:
                if child.tag == "attached_gpus":
                    # gpu_data['ngpus'] = int(child.text)
                    pass
                if child.tag == "gpu":
                    device_id = child.get('id')
                    number = 'nvidia%s' % child.find('minor_number').text
                    gpu_data[name][number] = dict()
                    gpu_data[name][number]['id'] = device_id

                    # Determine which socket the device is on
                    filename = '/sys/bus/pci/devices/%s/numa_node' % \
                        device_id.lower()
                    isfile = os.path.isfile(filename)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s, %s" % (filename, isfile))
                    if isfile:
                        numa_node = open(filename).read().strip()
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "numa_node: %s" % (numa_node))
                        if int(numa_node) == -1:
                            numa_node = 0
                        gpu_data[name][number]['numa_node'] = int(numa_node)
                        try:
                            dev_filename = '/dev/%s' % number
                            s = os.stat(dev_filename)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find %s" % dev_filename)
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                        gpu_data[name][number]['major'] = os.major(s.st_rdev)
                        gpu_data[name][number]['minor'] = os.minor(s.st_rdev)
                        gpu_data[name][number]['device_type'] = 'c'
            pbs.logmsg(pbs.EVENT_DEBUG, "%s" % gpu_data)
            return gpu_data
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unable to find %s" % string.join(cmd, " "))
            return {'gpu': {}}
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        return {'gpu': {}}

    # Get the memory info on this host
    def __discover_meminfo(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        meminfo = {}
        with open(os.path.join(os.sep, "proc", "meminfo"), 'r') as fd:
            for line in fd:
                entries = line.split()
                if entries[0] == "MemTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "SwapTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "Hugepagesize:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "HugePages_Total:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
                elif entries[0] == "HugePages_Rsvd:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover meminfo: %s" % meminfo)
        return meminfo

    # Determine if the cpus have hyperthreading enabled
    def __hyperthreading_enabled(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            siblings = 0
            cpu_cores = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "siblings":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    siblings = entries[1].strip()
                elif entries[0].strip() == "cpu cores":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_cores = entries[1].strip()
                elif entries[0].strip() == "flags":
                    flag_options = entries[1].split()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: flag_options: %s" %
                               (caller_name(), flag_options))
                    if "ht" in flag_options:
                        rc = True
                        break
        if rc is True:
            self.hyperthreads_per_core = int(siblings)/int(cpu_cores)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: hyperthreads/core: %d" %
                       (caller_name(), self.hyperthreads_per_core))
            if self.hyperthreads_per_core == 1:
                rc = False
        return rc

    def __discover_cpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            cpu_threads = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "processor":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_threads += 1
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: processors found: %d" %
                   (caller_name(), cpu_threads))

        return cpu_threads

    # Gather the jobs running on this node
    def gather_jobs_on_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        # Get the job list from the server
        jobs = None
        try:
            jobs = pbs.server().vnode(self.hostname).jobs
        except:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Unable to contact server to get jobs")
            return None

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs from server for %s: %s" % (self.hostname, jobs))

        if jobs is None:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "No jobs found on %s " % (self.hostname))
            return list()

        jobs_list = dict()
        for job in jobs.split(','):
            # The job list from the node has a space in front of the job id
            # This must be removed or it will not match the cgroup dir
            jobs_list[job.split('/')[0].strip()] = 0
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs on %s: %s" % (self.hostname, jobs_list.keys()))

        return jobs_list.keys()

    # Gather the jobs running on this node
    def gather_jobs_on_node_local(self):
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Method called" % (caller_name()))
        # Get the job list from the jobs directory
        jobs_list = list()
        try:
            for file in os.listdir(self.host_mom_jobdir):
                if fnmatch.fnmatch(file, "*.JB"):
                    jobs_list.append(file[:-3])
            if not jobs_list:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "No jobs found on %s " % (self.hostname))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Jobs from server for %s: %s" %
                           (self.hostname, repr(jobs_list)))

            return jobs_list
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Could not get job list from mom_priv/jobs for %s" %
                       self.hostname)

            return self.gather_jobs_on_node()

    # Get the memory resource on this mom
    def get_memory_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Calculate memory
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
            pbs.logmsg(pbs.EVENT_DEBUG3, "val: %s" % val)
        else:
            val = size_as_int(MemTotal)
        if 'percent_reserve' in config['cgroup']['memory']:
            percent_reserve = config['cgroup']['memory']['percent_reserve']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memory'].keys():
            reserve_memory = config['cgroup']['memory']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                reserve_mem = size_as_int(reserve_memory)
                if val > reserve_mem:
                    val -= reserve_mem
                else:
                    val -= size_as_int("100mb")
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Unable to reserve more memory than " +
                               "available on the node. Available %s, " +
                               "Tried to reserve %s. Reserving 100mb" %
                               (val, reserve_mem))

            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))

        pbs.logmsg(pbs.EVENT_DEBUG3, "Return val: %s" % val)
        return convert_size(str(val), 'kb')

    # Get the virtual memory resource on this mom
    def get_vmem_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Return the value for memory if no swap is configured
        if size_as_int(self.meminfo['SwapTotal']) <= 0:
            return self.get_memory_on_node(config)
        # Calculate vmem
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
        else:
            val = size_as_int(MemTotal)

        val += size_as_int(self.meminfo['SwapTotal'])
        # Determine if memory needs to be reserved
        if 'percent_reserve' in config['cgroup']['memsw']:
            percent_reserve = config['cgroup']['memsw']['reserve_memory']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memsw']:
            reserve_memory = config['cgroup']['memsw']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                val -= size_as_int(reserve_memory)
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))
        return convert_size(str(val), 'kb')

    # Get the huge page memory resource on this mom
    def get_hpmem_on_node(self, config):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        if 'percent_reserve' in config['cgroup']['hugetlb'].keys():
            percent_reserve = config['cgroup']['hugetlb']['percent_reserve']
        else:
            percent_reserve = 0
        # Calculate vmem
        val = size_as_int(self.meminfo['Hugepagesize'])
        val *= (self.meminfo['HugePages_Total'] -
                self.meminfo['HugePages_Rsvd'])
        val -= (val * percent_reserve) / 100
        return convert_size(str(val), 'kb')

    # Create individual vnodes per socket
    def create_vnodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        key = "vnode_per_numa_node"
        if key in self.cfg.keys():
            if not self.cfg[key]:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s is false" %
                           (caller_name(), key))
                return
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s not configured" %
                       (caller_name(), key))
            return

        # Create one vnode per numa node
        vnode_list = pbs.event().vnode_list
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: numa nodes: %s" %
                   (caller_name(), self.numa_nodes))
        # Define the vnodes
        vnode_name = self.hostname
        vnode_list[vnode_name] = pbs.vnode(vnode_name)
        for id in self.numa_nodes.keys():
            vnode_name = self.hostname + "[%d]" % id
            vnode_list[vnode_name] = pbs.vnode(vnode_name)
            for key, val in sorted(self.numa_nodes[id].iteritems()):
                if key == 'cpus':
                    vnode_list[vnode_name].resources_available['ncpus'] = \
                        len(cpus2list(val))/self.hyperthreads_per_core
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['ncpus'] = 0
                elif key == 'MemTotal':
                    mem = self.get_memory_on_node(self.cfg, val)
                    vnode_list[vnode_name].resources_available['mem'] = \
                        pbs.size(mem)
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['mem'] = \
                        pbs.size('0kb')
                elif key == 'HugePages_Total':
                    pass
                elif isinstance(val, list):
                    pass
                elif isinstance(val, dict):
                    pass
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s=%s" %
                               (caller_name(), key, val))
                    vnode_list[vnode_name].resources_available[key] = val

                    if isinstance(val, int):
                        vnode_list[self.hostname].resources_available[key] = 0
                    elif isinstance(val, float):
                        vnode_list[self.hostname].resources_available[key] = \
                            0.0
                    elif isinstance(val, pbs.size):
                        vnode_list[self.hostname].resources_available[key] = \
                            pbs.size('0kb')
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnode list: %s" %
                   (caller_name(), vnode_list))
        return True

#
# CLASS CgroupUtils
#


class CgroupUtils:
    def __init__(self, hostname, vnode, **kwargs):
        cfg = None
        subsystems = None
        paths = None
        vntype = None
        event = None
        self.assigned_resources = dict()
        self.existing_cgroups = dict()
        self.host_assigned_resources = None
        self.pid_lower_limit = 3

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'subsystems':
                    subsystems = val
                elif arg == 'paths':
                    paths = val
                elif arg == 'vntype':
                    vntype = val
                elif arg == 'event':
                    event = val

        try:
            self.hostname = hostname
            self.vnode = vnode
            # __check_os will raise an exception if cgroups are not present
            self.__check_os()
            # Read in the config file
            if cfg is not None:
                self.cfg = cfg
            else:
                self.cfg = self.__parse_config_file()

            # Determine if the hook should run or exit
            siru = ShallIRunUtils(self.hostname, self.cfg)
            siru.exclude_hosts(self.cfg['exclude_hosts'])
            siru.exclude_vntypes(self.cfg['exclude_vntypes'])
            siru.run_only_on_hosts(self.cfg['run_only_on_hosts'])

            # Collect the mount points
            if paths is not None:
                self.paths = paths
            else:
                self.paths = self.__set_paths()
            # Define the local vnode type
            if vntype is not None:
                self.vntype = vntype
            else:
                self.vntype = self.__get_vnode_type()
            # Determine which subsystems we care about
            if subsystems is not None:
                self.subsystems = subsystems
            else:
                self.subsystems = self.__target_subsystems()

            # location to store information for the different hook events
            pbs_home = ''
            if not CRAY_12_BRANCH:
                pbs_conf = pbs.get_pbs_conf()
                pbs_home = pbs_conf['PBS_HOME']
            else:
                if 'PBS_HOME' in self.cfg:
                    pbs_home = self.cfg['PBS_HOME']
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "PBS_HOME needs to be defined in the " +
                               "config file")
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Exiting the cgroups hook")
                    pbs.accept()

            self.hook_storage_dir = pbs_home+'/mom_priv/hooks/hook_data'
            self.host_job_env_dir = pbs_home+'/aux'
            self.host_job_env_filename = self.host_job_env_dir+os.sep+"%s.env"

            # information for offlining nodes
            self.offline_file = \
                pbs_home+os.sep+'mom_priv'+os.sep+'hooks'+os.sep
            self.offline_file += "%s.offline" % pbs.event().hook_name
            self.offline_msg = "Hook %s: " % pbs.event().hook_name
            self.offline_msg += "Unable to clean up one or more cgroups"

        except:
            raise

    def __repr__(self):
        return ("CgroupUtils(%s, %s, %s, %s, %s, %s)" %
                (repr(self.hostname), repr(self.vnode), repr(self.cfg),
                 repr(self.subsystems), repr(self.paths), repr(self.vntype)))

    # Validate the OS type and version
    def __check_os(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check to see if the platform is linux and the kernel is new enough
        if platform.system() != 'Linux':
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: OS does not support cgroups" %
                       (caller_name()))
            raise CgroupConfigError("OS type not supported")
        rel = map(int,
                  string.split(string.split(platform.release(), '-')[0], '.'))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Detected Linux kernel version %d.%d.%d" %
                   (caller_name(), rel[0], rel[1], rel[2]))
        supported = False
        if rel[0] > 2:
            supported = True
        elif rel[0] == 2:
            if rel[1] > 6:
                supported = True
            elif rel[1] == 6:
                if rel[2] >= 28:
                    supported = True
        if not supported:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Kernel needs to be >= 2.6.28: %s" %
                       (caller_name(), system_info[2]))
            raise CgroupConfigError("OS version not supported")
        return supported

    # Read the config file in json format
    def __parse_config_file(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        config = {}
        # Identify the config file and read in the data
        if 'PBS_HOOK_CONFIG_FILE' in os.environ:
            config_file = os.environ["PBS_HOOK_CONFIG_FILE"]
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Config file is %s" %
                       (caller_name(), config_file))
            try:
                config = json.load(open(config_file, 'r'),
                                   object_hook=decode_dict)
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
        else:
            raise CgroupConfigError("No configuration file present")
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Initial Cgroup Config: %s" %
                   (caller_name(), config))
        # Set some defaults if they are not present
        if 'cgroup_prefix' not in config.keys():
            config['cgroup_prefix'] = 'pbspro'
        if 'periodic_resc_update' not in config.keys():
            config['periodic_resc_update'] = False
        if 'vnode_per_numa_node' not in config.keys():
            config['vnode_per_numa_node'] = False
        if 'online_offlined_nodes' not in config.keys():
            config['online_offlined_nodes'] = False
        if 'exclude_hosts' not in config.keys():
            config['exclude_hosts'] = []
        if 'run_only_on_hosts' not in config.keys():
            config['run_only_on_hosts'] = []
        if 'exclude_vntypes' not in config.keys():
            config['exclude_vntypes'] = []
        if 'cgroup' not in config.keys():
            config['cgroup'] = {}
        else:
            for subsys in config['cgroup']:
                subsystem = config['cgroup'][subsys]
                if 'enabled' not in subsystem.keys():
                    config['cgroup'][subsys]['enabled'] = False
                if 'exclude_hosts' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_hosts'] = []
                if 'exclude_vntypes' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_vntypes'] = []

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Cgroup Config with defaults: %s" %
                   (caller_name(), config))
        return config

    # Determine which subsystems are being requested
    def __target_subsystems(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        subsystems = []
        for key in self.cfg['cgroup'].keys():
            if self.enabled(key):
                subsystems.append(key)
        if 'memory' not in subsystems:
            if 'memsw' in subsystems:
                # Remove memsw since it must be greater then
                # or equal to memory
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing memsw from enabled subsystems")
                subsystems.remove('memsw')
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Enabled subsystems: %s" %
                   (caller_name(), subsystems))
        # It is not an error for all subsystems to be disabled.
        # This host or vnode type may be in the excluded list.
        return subsystems

    # Copy a setting from the parent cgroup
    def __copy_from_parent(self, dest):
        try:
            file = os.path.basename(dest)
            dir = os.path.dirname(dest)
            parent = os.path.dirname(dir)
            source = os.path.join(parent, file)
            if not os.path.isfile(source):
                raise CgroupConfigError('Failed to read %s' % (source))
            self.write_value(dest, open(source, 'r').read().strip())
        except:
            raise

    # Create a dictionary of the cgroup subsystems and their corresponding
    # directories
    def __set_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        paths = {}
        try:
            # Loop through the mounts and collect the ones for cgroups
            with open(os.path.join(os.sep, "proc", "mounts"), 'r') as fd:
                for line in fd:
                    entries = line.split()
                    if len(entries) > 3 and entries[2] == "cgroup":
                        flags = entries[3].split(',')
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "subsysdir: %s (flags=%s)" %
                                   (entries[1], flags))
                        for subsys in flags:
                            paths[subsys] = os.path.join(
                                entries[1], self.cfg["cgroup_prefix"])
        except:
            raise
        if len(paths.keys()) < 1:
            raise CgroupConfigError("Cgroup paths not detected")
        # Both the memory and memsw cgroups share the same mount point
        if "memory" in paths.keys():
            paths["memsw"] = paths["memory"]
        return paths

    # Create the cgroup parent directories that will contain the jobs
    def create_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Create the directories that PBS will use to house the jobs
            old_umask = os.umask(0022)
            for subsys in self.subsystems:
                if subsys not in self.paths.keys():
                    raise CgroupConfigError('No path for subsystem: %s' %
                                            (subsys))
                if not os.path.exists(self.paths[subsys]):
                    os.makedirs(self.paths[subsys], 0755)
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Created directory %s" %
                               (caller_name(), self.paths[subsys]))
                    if subsys == 'memory' or subsys == 'memsw':
                        # Set file to
                        # <cgroup>/memory/pbspro/memory.use_hierarchy
                        file = os.path.join(self.paths[subsys],
                                            'memory.use_hierarchy')
                        if not os.path.isfile(file):
                            raise CgroupConfigError('Failed to configure %s' %
                                                    (file))
                        self.write_value(file, 1)
                    elif subsys == 'cpuset':
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.cpus'))
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.mems'))

        except:
            raise CgroupConfigError("Failed to create directory: %s" %
                                    (self.paths[subsys]))
        finally:
            os.umask(old_umask)

    # Return the vnode type of the local node
    def __get_vnode_type(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")

        # File to check for if it is not defined in the hook input file
        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'

        # self.vnode is None for pbs_attach events
        pbs.logmsg(pbs.EVENT_DEBUG3, "vnode: %s" % self.vnode)
        if self.vnode is not None:
            if 'vntype' in self.vnode.resources_available and \
                    self.vnode.resources_available['vntype'] is not None:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "vntype: %s" %
                           self.vnode.resources_available['vntype'])
                return self.vnode.resources_available['vntype']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3, "vntype file: %s" % vntype_file)
                if os.path.isfile(vntype_file):
                    fdata = open(vntype_file).readlines()
                    vntype = fdata[0].strip()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
                    return vntype
        # If vntype was not set then log a message. It is too expensive
        # to have all moms query the server for large jobs.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: resources_available.vntype is " % caller_name() +
                   "not set for this vnode")
        return None

    # Gather resources that are already assigned
    def gather_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        assigned_resources = {}

        # Gather the cpus and memory sockets assigned
        directories = [x[0] for x in os.walk(self.paths['cpuset'])]

        # Add the existing cgroups to a list to check if assigning
        # resources fail
        for dir in directories[1:]:
            if dir.find(pbs.event().job.id) == -1:
                self.existing_cgroups[dir] = []

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'cpuset' in self.subsystems and \
                self.cfg['cgroup']['cpuset']['enabled']:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather cpuset resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['cpuset'], tmp_dir)
                    with open(path+os.sep+'cpuset.cpus') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.cpus'] = \
                            cpus2list(tmp_val.strip())
                    with open(path+os.sep+'cpuset.mems') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.mems'] = \
                            cpus2list(tmp_val.strip())

        # Check to see if the subsystem is enabled before trying
        # to gather resources
        if 'memory' in self.subsystems and \
                self.cfg['cgroup']['memory']['enabled']:
            # Gather the memory assigned for each socket
            directories = [x[0] for x in os.walk(self.paths['memory'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather memory resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['memory'], tmp_dir)
                    with open(path+os.sep+'memory.limit_in_bytes') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memory.limit'] = \
                            tmp_val.strip()
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    if os.path.isfile(tmp_filename) is False:
                        continue
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    with open(tmp_filename) as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memsw.limit'] = \
                            tmp_val.strip()

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'devices' in self.subsystems and \
                self.cfg['cgroup']['devices']['enabled']:
            # Gather the devices assigned
            directories = [x[0] for x in os.walk(self.paths['devices'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather device resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    pbs.logmsg(pbs.EVENT_DEBUG3, "Dir: %s" % tmp_dir)
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['devices'], tmp_dir)
                    with open(path+os.sep+'devices.list') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['devices.list'] = \
                            tmp_val.strip().split('\n')
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Dir: %s\n\tValue: %s" %
                                   (path, tmp_val.replace('\n', ',')))

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Gathered assigned resources: %s" % assigned_resources)
        self.assigned_resources = assigned_resources

    # Return whether a subsystem is enabled
    def enabled(self, subsystem):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check whether the subsystem is enabled in the configuration file
        if subsystem not in self.cfg['cgroup'].keys():
            return False
        if 'enabled' not in self.cfg['cgroup'][subsystem].keys():
            return False
        if not self.cfg['cgroup'][subsystem]['enabled']:
            return False
        # Check whether the cgroup is mounted for this subsystem
        if subsystem not in self.paths.keys():
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup not mounted for %s" %
                       (caller_name(), subsystem))
            return False
        # Check whether this host is excluded
        # if self.hostname in self.cfg['exclude_hosts']:
        #    pbs.logmsg(pbs.EVENT_DEBUG, "%s: cgroup excluded on host %s" %
        #               (caller_name(), self.hostname))
        #    pbs.event().accept()
        if self.hostname in self.cfg['cgroup'][subsystem]['exclude_hosts']:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup excluded for subsystem %s on host %s" %
                       (caller_name(), subsystem, self.hostname))
            return False
        # Check whether the vnode type is excluded
        if self.vntype is not None:
            # if self.vntype in self.cfg['exclude_vntypes']:
            #    pbs.logmsg(pbs.EVENT_DEBUG,
            #               "%s: cgroup excluded on vnode type %s" %
            #               (caller_name(), self.vntype))
            #    pbs.event().accept()
            if self.vntype in self.cfg['cgroup'][subsystem]['exclude_vntypes']:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           ("%s: cgroup excluded for " +
                            "subsystem %s on vnode type %s") %
                           (caller_name(), subsystem, self.vntype))
                return False
        return True

    # Return the default value for a subsystem
    def default(self, subsystem):
        if subsystem in self.cfg['cgroup']:
            if 'default' in self.cfg['cgroup'][subsystem]:
                return self.cfg['cgroup'][subsystem]['default']
        return None

    # Check to see if the pid matches the job owners uid
    def is_pid_owned_by_job_owner(self, pid):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        try:
            proc_uid = os.stat('/proc/%d' % pid).st_uid
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       '/proc/%d uid:%d' % (pid, proc_uid))

            job_owner_uid = pwd.getpwnam(pbs.event().job.euser)[2]
            pbs.logmsg(pbs.EVENT_DEBUG3, "Job uid: %d" % job_owner_uid)

            if proc_uid == job_owner_uid:
                return True
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Proc uid: %d != Job owner:  %d" %
                           (proc_uid, job_owner_uid))
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG, "Unknown pid: %d" % pid)
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        # If we got to this point something did not match up
        return False

    # Add some number of PIDs to the cgroup tasks files for each subsystem
    def add_pids(self, pids, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # make pids a list
        if isinstance(pids, int):
            pids = [pids]

        if pbs.event().type == pbs.EXECJOB_LAUNCH:
            if 1 in pids:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "1 is not a valid pid to add")
                # Hold the job for further review
                e.reject("Hook tried to add pid 1 to the tasks file " +
                         "for job %s" % jobid)

        # check pids to make sure that they are owned by the job owner
        if not CRAY_12_BRANCH:
            pbs.logmsg(pbs.EVENT_DEBUG3, "Not in the Cray 12 Branch")
            pbs.logmsg(pbs.EVENT_DEBUG3, "check to see if execjob_attach")
            if pbs.event().type == pbs.EXECJOB_ATTACH:
                pbs.logmsg(pbs.EVENT_DEBUG3, "event type: attach or launch")
                tmp_pids = list()
                for p in pids:
                    if self.is_pid_owned_by_job_owner(p):
                        tmp_pids.append(p)
                if len(tmp_pids) == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%d is not a valid pids to add" % p)
                    return False
                else:
                    pids = tmp_pids

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: subsys = %s" %
                       (caller_name(), subsys))
            # memsw and memory use the same tasks file
            if subsys == "memsw":
                if "memory" in self.subsystems:
                    continue
            path = os.path.join(self.paths[subsys], jobid)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path = %s" %
                       (caller_name(), path))
            try:
                tasks_path = os.path.join(path, "tasks")
                for p in pids:
                    self.write_value(tasks_path, p, 'a')
            except IOError, exc:
                raise CgroupLimitError("Failed to add PIDs %s to %s (%s)" %
                                       (str(pids), tasks_path,
                                        errno.errorcode[exc.errno]))
            except:
                raise

    # Set a node limit
    def set_node_limit(self, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s = %s" %
                   (caller_name(), resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'],
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Node resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    def setup_job_devices_env(self):
        """ Setup the job environment for the devices assigned to the job for an
            execjob_launch hook
         """
        if 'devices_name' in self.host_assigned_resources:
            names = self.host_assigned_resources['devices_name']
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "devices: %s" % (names))
            offload_devices = list()
            cuda_visible_devices = list()
            for name in names:
                if name.startswith('mic'):
                    offload_devices.append(name[3:])
                elif name.startswith('nvidia'):
                    cuda_visible_devices.append(name[6:])
            if len(offload_devices) > 0:
                value = '"%s"' % string.join(offload_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['OFFLOAD_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "offload_devices: %s" % offload_devices)
            if len(cuda_visible_devices) > 0:
                value = '"%s"' % string.join(cuda_visible_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['CUDA_VISIBLE_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "cuda_visible_devices: %s" % cuda_visible_devices)
            return [offload_devices, cuda_visible_devices]
        else:
            return False

    def setup_subsys_devices(self, path, node):
        subsys = 'devices'
        # Add the devices that the user is allowed to use
        if subsys in self.cfg['cgroup']:
            devices_allowed = open(os.path.join(path,
                                   "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Initial devices.list: %s" %
                       devices_allowed)
            # Deny access to mic and gpu devices
            devices_list = list()
            devices = node.devices
            for device in devices:
                if device == 'mic' or device == 'gpu':
                    for name in devices[device]:
                        d = devices[device][name]
                        devices_list.append("%d:%d" % (d['major'], d['minor']))
            # For CentOS 7 we need to remove a *:* rwm from devices.list we
            # before can add anything to devices.allow. Otherwise our changes
            # are ignored. Check to see if a *:* rwm is in devices.list.
            # If so remove it
            if "a *:* rwm\n" in devices_allowed:
                value = "a *:* rwm"
                self.write_value(os.path.join(path, 'devices.deny'), value)
            else:
                # Verify that the following devices are not in devices.list
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing access to the following: %s" %
                           devices_list)
            for device_str in devices_list:
                value = "c %s rwm" % device_str
                self.write_value(os.path.join(path, 'devices.deny'), value)
            # Add devices back to the list
            devices_allow = list()
            if 'allow' in self.cfg['cgroup'][subsys]:
                devices_allow = self.cfg['cgroup'][subsys]['allow']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Allowing access to the following: %s" %
                           devices_allow)
                for item in devices_allow:
                    if isinstance(item, str):
                        pbs.logmsg(pbs.EVENT_DEBUG3, "string item: %s" % item)
                        value = "%s" % item
                        self.write_value(os.path.join(
                                         path, 'devices.allow'), value)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "write_value: %s" % value)
                    elif isinstance(item, list):
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "list item: %s" % item)
                        try:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Device allow: %s" % item)
                            stat_filename = '/dev/%s' % item[0]
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Stat file: %s" % stat_filename)
                            s = os.stat(stat_filename)
                            device_type = None
                            if S_ISBLK(s.st_mode):
                                device_type = "b"
                            elif S_ISCHR(s.st_mode):
                                device_type = "c"
                            if device_type is not None:
                                if len(item) == 3 and isinstance(item[2], str):
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % item[2]
                                    value += "%s" % item[1]
                                else:
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % os.minor(s.st_rdev)
                                    value += "%s" % item[1]
                                self.write_value(os.path.join(
                                                path, 'devices.allow'), value)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "write_value: %s" % value)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find /dev/%s. " % item[0] +
                                       "Not added to devices.allow!")
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                    else:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Not sure what to do with: %s" % item)
            devices_allowed = open(os.path.join(
                                   path, "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "After setup devices.list: %s" % devices_allowed)

    # Select devices to assign to the job
    def assign_devices(
                       self,
                       device_type,
                       device_list,
                       number_of_devices,
                       node):
        devices = device_list[:number_of_devices]
        device_ids = list()
        device_names = list()
        for device in devices:
            device_info = node.devices[device_type][device]
            if len(device_ids) == 0:
                if device.find("mic") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:0 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                    device_ids.append("%s %d:1 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                elif device.find("nvidia") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:255 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
            device_ids.append("%s %d:%d rwm" %
                              (device_info['device_type'],
                               device_info['major'],
                               device_info['minor']))
            device_names.append(device)
        return device_names, device_ids

    # Find the device name
    def get_device_name(self, node, available, socket, major, minor):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Get device name: major: %s, minor: %s" % (major, minor))
        avail_device = None
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Possible devices: %s" % (available[socket]['devices']))
        for avail_device in available[socket]['devices']:
            avail_major = None
            avail_minor = None
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Checking device: %s" % (avail_device))
            if avail_device.find('mic') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check mic device: %s" % (avail_device))
                avail_major = node.devices['mic'][avail_device]['major']
                avail_minor = node.devices['mic'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            elif avail_device.find('nvidia') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check gpu device: %s" % (avail_device))
                avail_major = node.devices['gpu'][avail_device]['major']
                avail_minor = node.devices['gpu'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            if int(avail_major) == int(major) and \
                    int(avail_minor) == int(minor):
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device match: name: %s, major: %s, minor: %s" %
                           (avail_device, major, minor))
                return avail_device
        pbs.logmsg(pbs.EVENT_DEBUG3, "No match found")
        return None

    # Assign resources to the job based off the vnodes assignments
    def assign_job_resources_by_vnode(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # Loop through the vnodes and assign resources
        assigned = dict()
        assigned['cpuset.cpus'] = list()
        assigned['cpuset.mems'] = list()
        room_on_socket = True

        for vnode in resources['vnodes']:
            regex = re.compile(".*\[(\d+)\].*")
            socket = int(regex.search(vnode).group(1))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current Vnode: %s" % vnode)
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current socket: %d" % socket)

            if 'ncpus' in resources['vnodes'][vnode]:
                tmp_ncpus = int(resources['vnodes'][vnode]['ncpus'])
                if int(resources['vnodes'][vnode]['ncpus']) <= \
                        len(available[socket]['cpus']):
                    assigned['cpuset.cpus'] += \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] += [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_ncpus: %s" % (tmp_ncpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "available[%d]: %s" %
                               (socket, available[socket]))
                    room_on_socket = False
            if ('nmics' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['nmics']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(mic).*")
                tmp_nmics = int(resources['vnodes'][vnode]['nmics'])
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['nmics']) > 0 and
                   int(resources['vnodes'][vnode]['nmics']) <= len(mics)):
                    tmp_names, tmp_devices = self.assign_devices(
                             'mic', mics[:tmp_nmics], tmp_nmics, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_nmics: %s" % (tmp_nmics))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "mics: %s" % (mics))
                    room_on_socket = False
            if ('ngpus' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['ngpus']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(nvidia).*")
                tmp_ngpus = int(resources['vnodes'][vnode]['ngpus'])
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['ngpus']) > 0 and
                   int(resources['vnodes'][vnode]['ngpus']) <= len(gpus)):
                    tmp_names, tmp_devices = self.assign_devices(
                        'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "tmp_ngpus: %s" % (tmp_ngpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "gpus: %s" % (gpus))
                    room_on_socket = False

        if room_on_socket:
            assigned['cpuset.cpus'].sort()
            assigned['cpuset.mems'].sort()
            if 'devices' in assigned:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids pre set and sort: %s" %
                           (assigned['devices']))
                assigned['devices'] = list(set(assigned['devices']))
                assigned['devices'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids post set and sort: %s" %
                           (assigned['devices']))
                assigned['devices_name'] = list(set(assigned['devices_name']))
                assigned['devices_name'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

    # Assign resources to the job
    def assign_job_resources(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # What would we need to do different for a large smp (UV) system?
        # See if requested resources can fit on a single socket
        assigned = dict()
        pbs.logmsg(pbs.EVENT_DEBUG3, "Requested: %s" % (resources))

        # Determine the order that we should evaluate the sockets.
        socket_list = list()
        if 'placement_type' in self.cfg:
            if self.cfg['placement_type'] == 'round_robin':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Requested round_robin placement")
                # Look at assigned_resources and determine which socket
                # to start with
                jobs_per_socket_cnt = dict()
                for socket in available.keys():
                    jobs_per_socket_cnt[socket] = 0
                for job in self.assigned_resources:
                    if 'cpuset.mems' in self.assigned_resources[job]:
                        for socket in \
                                self.assigned_resources[job]['cpuset.mems']:
                            jobs_per_socket_cnt[socket] += 1

                sorted_dict = sorted(jobs_per_socket_cnt.items(),
                                     key=operator.itemgetter(1))
                for x in sorted_dict:
                    socket_list.append(x[0])
        else:
            socket_list = available.keys()

        # Loop through the socket list and determine if the job
        # can fit on the socket
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Evaluate sockets in the following order: %s" %
                   (socket_list))
        for socket in socket_list:
            room_on_socket = True
            if 'ncpus' in resources:
                if int(resources['ncpus']) <= len(available[socket]['cpus']):
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Requested: %s, Available: %s" %
                               (resources['ncpus'], available[socket]['cpus']))
                    tmp_ncpus = int(resources['ncpus'])
                    assigned['cpuset.cpus'] = \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] = [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient ncpus on socket %d: R:%d,A:%s" %
                               (socket, resources['ncpus'],
                                available[socket]['cpus']))
                    room_on_socket = False
            # Check to see that nmics has been requested and not equal to 0
            if 'nmics' in resources and int(resources['nmics']) > 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'nmics' in resources and int(resources['nmics']) > 0 \
                        and int(resources['nmics']) <= len(mics):
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient nmics on socket %d: R:%s,A:%s" %
                               (socket, resources['nmics'], mics))
                    room_on_socket = False
            # Check to see that ngpus has been requested and not equal to 0
            if 'ngpus' in resources and int(resources['ngpus']) > 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'ngpus' in resources and int(resources['ngpus']) > 0 \
                        and int(resources['ngpus']) <= len(gpus):
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient gpus on socket %d: R:%s,A:%s" %
                               (socket, resources['ngpus'], gpus))
                    room_on_socket = False
            if 'vmem' in resources:
                if size_as_int(resources['vmem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient vmem on socket %d: R:%s,A:%s" %
                               (socket, resources['vmem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if 'mem' in resources:
                if size_as_int(resources['mem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient memory on socket %d: R:%s,A:%s" %
                               (socket, resources['mem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if room_on_socket:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
                self.host_assigned_resources = assigned
                return assigned
        # If we made it here then the requested resources did not fit
        # on a single socket
        # Future: take distance into account when selecting sockets?
        # Combine available resources into a single list
        # This should work for a dual socket machine.
        # Needs additional work for machines with more than 2 sockets
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Unable to find requested resources: %s" % resources +
                   " on a socket. Checking the entire node")
        available_copy = copy.deepcopy(available)
        available_on_node = dict()
        socket_keys = available.keys()
        socket_keys.sort()
        available_on_node['sockets'] = socket_keys
        for socket in socket_keys:
            for key in available_copy[socket].keys():
                if isinstance(available[socket][key], int):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]
                if isinstance(available_copy[socket][key], str):
                    try:
                        if key in available_on_node is False:
                            available_on_node[key] = \
                                size_as_int(available_copy[socket][key])
                        else:
                            available_on_node[key] += \
                                size_as_int(available_copy[socket][key])
                    except:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Unable to convert to int: %s" %
                                   (available_copy[socket][key]))

                if isinstance(available_copy[socket][key], list):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available on node: %s" % (available_on_node))
        assigned = dict()
        room_on_node = False
        if 'ncpus' in resources:
            if int(resources['ncpus']) <= len(available_on_node['cpus']):
                room_on_node = True
                tmp_ncpus = int(resources['ncpus'])
                assigned['cpuset.cpus'] = available_on_node['cpus'][:tmp_ncpus]
                assigned['cpuset.mems'] = available_on_node['sockets']
            if 'nmics' in resources and int(resources['nmics']) != 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['nmics']) <= len(mics) and \
                        int(resources['nmics']) > 0:
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
            if 'ngpus' in resources and int(resources['ngpus']) != 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['ngpus']) <= len(gpus) and \
                        int(resources['ngpus']) > 0:
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
        if room_on_node:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

        # Consolidate resources if needed to place the job

    # Determine which resources are available
    def available_node_resources(self, node):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))

        available = copy.deepcopy(node.numa_nodes)
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Keys: %s" % (available[0].keys()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        for socket in available.keys():
            if 'cpus' in available[socket]:
                # Find the physical cores
                if available[socket]['cpus'].find('-') != -1 and \
                        node.hyperthreading:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Splitting %s at ','" %
                               (available[socket]['cpus']))
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'].split(',')[0])
                else:
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'])
            if 'MemTotal' in available[socket]:
                # Find the memory on the socket in bites.
                # Remove the 'b' to simply math
                available[socket]['memory'] = size_as_int(
                    available[socket]['MemTotal'])

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Pre device add: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (node.devices.keys()))
        for device in node.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" %
                       (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Devices: %s" %
                           (node.devices[device].keys()))
                for device_name in node.devices[device].keys():
                    device_socket = \
                        node.devices[device][device_name]['numa_node']
                    if 'devices' not in available[device_socket]:
                        available[device_socket]['devices'] = list()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Device: %s, Socket: %s" %
                               (device, device_socket))
                    available[device_socket]['devices'].append(device_name)
            # pbs.logmsg(pbs.EVENT_DEBUG3,
            #            "Available on socket %s: %s" %
            #            (socket,available[socket]))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))

        # Remove all of the resources that are assigned to other jobs
        for job in self.assigned_resources.keys():
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            if (not job_to_be_ignored(job)):
                cpus = list()
                sockets = list()
                devices = list()
                memory = 0
                if 'cpuset.cpus' in self.assigned_resources[job]:
                    cpus = self.assigned_resources[job]['cpuset.cpus']
                if 'cpuset.mems' in self.assigned_resources[job]:
                    sockets = self.assigned_resources[job]['cpuset.mems']
                if 'devices.list' in self.assigned_resources[job]:
                    devices = self.assigned_resources[job]['devices.list']
                if 'memory.limit' in self.assigned_resources[job]:
                    memory = size_as_int(
                                self.assigned_resources[job]['memory.limit'])

                # Loop through the sockets and remove cpus that are
                # assigned to other cgroups
                for socket in sockets:
                    for cpu in cpus:
                        try:
                            available[socket]['cpus'].remove(cpu)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %d from %s" %
                                       (cpu, available[socket]['cpus']))

                if len(sockets) == 1:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Decrementing memory: %d by %d" %
                               (size_as_int(available[sockets[0]]['memory']),
                                memory))
                    available[sockets[0]]['memory'] -= memory

                # Loop throught the available sockets
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned device to %s: %s" % (job, devices))
                for socket in available.keys():
                    for device in devices:
                        try:
                            # loop through know devices and see if they match
                            if len(available[socket]['devices']) != 0:
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Check device: %s" % (device))
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Available device: %s" %
                                           (available[socket]['devices']))
                                major, minor = device.split()[1].split(':')
                                avail_device = self.get_device_name(
                                                   node, available, socket,
                                                   major, minor)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Returned device: %s" %
                                           (avail_device))
                                if avail_device is not None:
                                    pbs.logmsg(pbs.EVENT_DEBUG3,
                                               "socket: %d,\t" % socket +
                                               "devices: %s,\t" %
                                               available[socket]['devices'] +
                                               "device to remove: %s" %
                                               (avail_device))
                                    available[socket]['devices'].remove(
                                        avail_device)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %s from %s" %
                                       (device, available[socket]['devices']))
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Job %s res not removed from host " % job +
                           "available res: suspended job")
        pbs.logmsg(pbs.EVENT_DEBUG, "Available resources: %s" % (available))
        return available

    # Set a job limit
    def set_job_limit(self, jobid, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s: %s = %s" %
                   (caller_name(), jobid, resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'softmem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.soft_limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     jobid, 'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'], jobid,
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            elif resource == 'ncpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = self.select_cpus(path, value)
                    if cpus is None:
                        raise CgroupLimitError("Failed to configure cpuset.")
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
                    self.__copy_from_parent(os.path.join(self.paths['cpuset'],
                                            jobid, 'cpuset.mems'))
            elif resource == 'cpuset.cpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = value
                    if cpus is None:
                        msg = "Failed to configure cpus in cpuset."
                        raise CgroupLimitError(msg)
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
            elif resource == 'cpuset.mems':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.mems')
                    mems = value
                    if mems is None:
                        msg = "Failed to configure mems in cpuset."
                        raise CgroupLimitError(msg)
                    mems = ','.join(map(str, mems))
                    self.write_value(path, mems)
            elif resource == 'devices':
                if 'devices' in self.subsystems and \
                        self.cfg['cgroup']['devices']['enabled']:

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.allow')
                    devices = value
                    if devices is None:
                        msg = "Failed to configure device(s)."
                        raise CgroupLimitError(msg)
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Setting devices: %s for %s" % (devices, jobid))
                    for device in devices:
                        self.write_value(path, device)

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.list')
                    output = open(path, 'r').read()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "devices.list: %s" % output.replace("\n", ","))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    # Update resource usage for a job
    def update_job_usage(self, jobid, resc_used):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resc_used = %s" %
                   (caller_name(), str(resc_used)))
        # Sort the subsystems so that we consistently look at the subsystems
        # in the same order every time
        self.subsystems.sort()
        for subsys in self.subsystems:
            path = os.path.join(self.paths[subsys], jobid)
            if subsys == "memory":
                max_mem = self.__get_max_mem_usage(path)
                if max_mem is None:
                    pbs.logjobmsg(jobid, "%s: No max mem data" %
                                  (caller_name()))
                else:
                    resc_used['mem'] = pbs.size(convert_size(max_mem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: mem=%s" %
                                  (caller_name(), resc_used['mem']))
                mem_failcnt = self.__get_mem_failcnt(path)
                if mem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No mem fail count data" %
                                  (caller_name()))
                else:
                    # Check to see if the job exceeded its resource limits
                    if mem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memory limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "memsw":
                max_vmem = self.__get_max_memsw_usage(path)
                if max_vmem is None:
                    pbs.logjobmsg(jobid, "%s: No max vmem data" %
                                  (caller_name()))
                else:
                    resc_used['vmem'] = pbs.size(convert_size(max_vmem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: vmem=%s" %
                                  (caller_name(), resc_used['vmem']))

                vmem_failcnt = self.__get_memsw_failcnt(path)
                if vmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No vmem fail count data" %
                                  (caller_name()))
                else:
                    pbs.logjobmsg(jobid, "%s: vmem fail count: %d " %
                                  (caller_name(), vmem_failcnt))
                    if vmem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memsw limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "hugetlb":
                max_hpmem = self.__get_max_hugetlb_usage(path)
                if max_hpmem is None:
                    pbs.logjobmsg(jobid, "%s: No max hpmem data" %
                                  (caller_name()))
                    return
                hpmem_failcnt = self.__get_hugetlb_failcnt(path)
                if hpmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No hpmem fail count data" %
                                  (caller_name()))
                    return
                if hpmem_failcnt > 0:
                    err_msg = self.__get_error_msg(jobid)
                    pbs.logjobmsg(jobid, "Cgroup hugetlb limit exceeded: %s" %
                                  (err_msg))
                resc_used['hpmem'] = pbs.size(convert_size(max_hpmem, 'kb'))
                pbs.logjobmsg(jobid, "%s: Hugepage usage: %s" %
                              (caller_name(), resc_used['hpmem']))
            elif subsys == "cpuacct":
                cpu_usage = self.__get_cpu_usage(path)
                if cpu_usage is None:
                    pbs.logjobmsg(jobid, "%s: No CPU usage data" %
                                  (caller_name()))
                    return
                cpu_usage = convert_time(str(cpu_usage) + "ns")
                pbs.logjobmsg(jobid, "%s: CPU usage: %.3lf secs" %
                              (caller_name(), cpu_usage))
                resc_used['cput'] = pbs.duration(cpu_usage)

    # Creates the cgroup if it doesn't exists
    def create_job(self, jobid, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Iterate over the subsystems required
        for subsys in self.subsystems:
            # Create a directory for the job
            try:
                old_umask = os.umask(0022)
                path = self.paths[subsys]
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                path = os.path.join(self.paths[subsys], jobid)
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                if subsys == 'devices':
                    self.setup_subsys_devices(path, node)

            except OSError, exc:
                raise CgroupConfigError("Failed to create directory: %s (%s)" %
                                        (path, errno.errorcode[exc.errno]))
            except:
                raise
            finally:
                os.umask(old_umask)

    # Determine the cgroup limits and configure the cgroups
    def configure_job(self, jobid, hostresc, node):
        mem_enabled = 'memory' in self.subsystems
        vmem_enabled = 'memsw' in self.subsystems
        if mem_enabled or vmem_enabled:
            # Initialize mem variables
            mem_avail = node.get_memory_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "mem_avail %s" % mem_avail)
            mem_requested = None
            if 'mem' in hostresc.keys():
                mem_requested = convert_size(hostresc['mem'], 'kb')
            mem_default = None
            if mem_enabled:
                mem_default = self.default('memory')
            # Initialize vmem variables
            vmem_avail = node.get_vmem_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "vmem_avail %s" % vmem_avail)
            vmem_requested = None
            if 'vmem' in hostresc.keys():
                vmem_requested = convert_size(hostresc['vmem'], 'kb')
            vmem_default = None
            if vmem_enabled:
                vmem_default = self.default('memsw')
            # Initialize softmem variables
            if 'soft_limit' in self.cfg['cgroup']['memory'].keys():
                softmem_enabled = self.cfg['cgroup']['memory']['soft_limit']
            else:
                softmem_enabled = False
            # Sanity check
            if size_as_int(mem_avail) > size_as_int(vmem_avail):
                if size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                    raise CgroupLimitError(
                        'mem available (%s) exceeds vmem available (%s)' %
                        (mem_avail, vmem_avail))
            # Determine the mem limit
            if mem_requested is not None:
                # mem requested may not exceed available
                if size_as_int(mem_requested) > size_as_int(mem_avail):
                    raise JobValueError(
                        'mem requested (%s) exceeds mem available (%s)' %
                        (mem_requested, mem_avail))
                mem_limit = mem_requested
            else:
                # mem was not requested
                if mem_default is None:
                    mem_limit = mem_avail
                else:
                    mem_limit = mem_default
            # Determine the vmem limit
            if vmem_requested is not None:
                # vmem requested may not exceed available
                if size_as_int(vmem_requested) > size_as_int(vmem_avail):
                    raise JobValueError(
                        'vmem requested (%s) exceeds vmem available (%s)' %
                        (vmem_requested, vmem_avail))
                vmem_limit = vmem_requested
            else:
                # vmem was not requested
                if vmem_default is None:
                    vmem_limit = vmem_avail
                else:
                    vmem_limit = vmem_default
            # Ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Adjust for soft limits if enabled
            if mem_enabled and softmem_enabled:
                softmem_limit = mem_limit
                # The hard memory limit is assigned the lesser of the vmem
                # limit and available memory
                if size_as_int(vmem_limit) < size_as_int(mem_avail):
                    mem_limit = vmem_limit
                else:
                    mem_limit = mem_avail
            # Again, ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Sanity checks when both memory and memsw are enabled
            if mem_enabled and vmem_enabled:
                if vmem_requested is not None:
                    if size_as_int(vmem_limit) > size_as_int(vmem_requested):
                        # The user requested an invalid limit
                        raise JobValueError(
                            'vmem limit (%s) exceeds vmem requested (%s)' %
                            (vmem_limit, vmem_requested))
                if size_as_int(vmem_limit) > size_as_int(mem_limit):
                    # This job may utilize swap
                    if size_as_int(vmem_avail) <= size_as_int(mem_avail) and \
                      size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                        # No swap available
                        raise CgroupLimitError(
                            ('Job might utilize swap ' +
                             'and no swap space available'))
            # Assign mem and vmem
            if mem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: mem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), mem_limit))
                hostresc['mem'] = pbs.size(mem_limit)
                if softmem_enabled:
                    hostresc['softmem'] = pbs.size(softmem_limit)
            if vmem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: vmem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), vmem_limit))
                hostresc['vmem'] = pbs.size(vmem_limit)
        # Initialize hpmem variables
        hpmem_enabled = 'hugetlb' in self.subsystems
        if hpmem_enabled:
            hpmem_avail = node.get_hpmem_on_node(self.cfg)
            hpmem_limit = None
            hpmem_default = self.default('hugetlb')
            if hpmem_default is None:
                hpmem_default = hpmem_avail
            if 'hpmem' in hostresc.keys():
                hpmem_limit = convert_size(hostresc['hpmem'], 'kb')
            else:
                hpmem_limit = hpmem_default
            # Assign hpmem
            if size_as_int(hpmem_limit) > size_as_int(hpmem_avail):
                raise JobValueError('hpmem limit (%s) exceeds available (%s)' %
                                    (hpmem_limit, hpmem_avail))
            hostresc['hpmem'] = pbs.size(hpmem_limit)
        # Initialize cpuset variables
        cpuset_enabled = 'cpuset' in self.subsystems
        if cpuset_enabled:
            cpu_limit = 1
            if 'ncpus' in hostresc.keys():
                cpu_limit = hostresc['ncpus']
            if cpu_limit < 1:
                cpu_limit = 1
            hostresc['ncpus'] = pbs.pbs_int(cpu_limit)

        # Find the available resources and assign the right ones to the job

        attempt = 0
        assign_resources = False

        # Two attempts since self.cleanup_orphans may actually fix the
        # problem we see in a first attempt
        while attempt < 2:
            attempt += 1
            available_resources = self.available_node_resources(node)
            if 'vnodes' in hostresc:
                assign_resources = self.assign_job_resources_by_vnode(
                    hostresc, available_resources, node)
            else:
                assign_resources = self.assign_job_resources(
                    hostresc, available_resources, node)

            if (assign_resources is False) and (attempt < 2):
                # No resources were assigned to the job.
                # Most likely cause was that a cgroup has
                # not been cleaned up yet
                pbs.logmsg(pbs.EVENT_DEBUG, "Failed to assign resources. " +
                           "Checking the following cgroups on the node " +
                           "with the server: %s" % self.existing_cgroups)

                # Collect the jobs on the node (try reading mom_priv/jobs,
                # if not successful ask server)
                jobs_list = node.gather_jobs_on_node_local()

                if jobs_list is not None:
                    # Don't clean up the cgroup we just made for the
                    # job just yet
                    if jobid not in jobs_list:
                        jobs_list.append(jobid)

                    self.cleanup_orphans(jobs_list)

                    # Recheck what is now assigned after things were cleaned up
                    self.gather_assigned_resources()

        if assign_resources is False:
            # Cleanup cgroups for jobs not present on this node
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Assign resources failed. Attempting to cleanup all " +
                       "leftover cgroups (including event job %s)" % jobid)

            # Remove the current job from the jobs list so it will be
            # cleaned up as well.
            jobs_list.remove(jobid)

            self.cleanup_orphans(jobs_list)

            # Rerun the job and log the message
            line = 'Unable to assign resources to job. '
            line += 'Will requeue the job and try again.'
            line += 'job run_count: %d' % pbs.event().job.run_count
            pbs.event().job.rerun()
            pbs.event().reject(line)

        # Print out the assigned resources
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Assigned resources: %s" % (assign_resources))

        if cpuset_enabled:
            hostresc.pop('ncpus', None)
            for key in ['cpuset.cpus', 'cpuset.mems']:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Initialize devices variables
        devices_enabled = 'devices' in self.subsystems
        if devices_enabled:
            key = 'devices'
            if key in assign_resources:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Apply the resource limits to the cgroups
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Setting cgroup limits for: %s" %
                   (caller_name(), hostresc))

        # The vmem limit must be set after the mem limit, so sort the keys
        for resc in sorted(hostresc.keys()):
            self.set_job_limit(jobid, resc, hostresc[resc])

    # Kill any processes contained within a tasks file
    def __kill_tasks(self, tasks_file):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        with open(tasks_file, 'r') as fd:
            for line in fd:
                os.kill(int(line.strip()), signal.SIGKILL)
        fd.close()
        count = 0
        with open(tasks_file, 'r') as fd:
            for line in fd:
                count += 1
                pid = line.strip()
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Pid: %s did not clean up" %
                           (caller_name(), pid))
                if os.path.isfile('/proc/%s/status' % pid):
                    tmp = open('/proc/%s/status' % pid).readlines()
                    tmp_line = ''
                    for entry in tmp:
                        if entry.find('Name:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('State:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('Uid:') != -1:
                            tmp_line += entry.strip()
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: %s" % (caller_name(), tmp_line))

        return count

    # Perform the actual removal of the cgroup directory
    # by default, only one attempt at killing tasks in cgroup, without sleeping
    # since this method could be called many times
    # (for N directories times M jobs)
    def __remove_cgroup(self, path, tasks_kill_attempts=1):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            if os.path.exists(path):
                tasks_file = os.path.join(path, 'tasks')
                task_cnt = self.__kill_tasks(tasks_file)
                kill_attempts = 0
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Task Kill Attempts: %d" % tasks_kill_attempts)
                while (task_cnt > 0) and (kill_attempts < tasks_kill_attempts):
                    kill_attempts += 1
                    count = 0

                    with open(tasks_file, 'r') as fd:
                        for line in fd:
                            count += 1
                        task_cnt = count

                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "Attempt: %d - cgroup still has %d tasks: %s" %
                               (kill_attempts, task_cnt, path))
                    if kill_attempts < tasks_kill_attempts:
                        time.sleep(1)
                        # Added since there are race conditions where tasks are
                        # being added at the same time we get the initial
                        # task count
                        tasks_file = os.path.join(path, 'tasks')
                        task_cnt = self.__kill_tasks(tasks_file)

                if task_cnt == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Removing directory %s" %
                               (caller_name(), path))
                    os.rmdir(path)
                    if os.path.exists(path):
                        time.sleep(.25)
                        if os.path.exists(path):
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "%s: 2nd Try Removing directory %s" %
                                       (caller_name(), path))
                            os.rmdir(path)
                            time.sleep(.25)
                        if os.path.exists(path):
                            return False
                else:
                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "cgroup still has %d tasks: %s" %
                               (task_cnt, path))

                    # Check to see if the offline file is already present
                    # on the mom
                    offline_file_exists = False
                    if os.path.isfile(self.offline_file):
                        msg = "Cgroup(s) not cleaning up but the node already "
                        msg += "has the offline file."

                        pbs.logmsg(pbs.EVENT_DEBUG, msg)
                    else:
                        # Check to see that the node is not already offline.
                        try:
                            tmp_state = pbs.server().vnode(self.hostname).state
                        except:
                            msg = "Unable to contact server for node state"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            tmp_state = None
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Current Node State: %d" % tmp_state)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Offline Node State: %d" % pbs.ND_OFFLINE)

                        if tmp_state == pbs.ND_OFFLINE:
                            msg = "Cgroup(s) not cleaning up but the node is "
                            msg += "already offline."
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                        elif tmp_state is not None:
                            # Offline the node(s)
                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                            msg = "Offlining node since cgroup(s) are not "
                            msg += "cleaning up"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            vnode = pbs.event().vnode_list[self.hostname]

                            vnode.state = pbs.ND_OFFLINE

                            # Write a file locally to reduce server traffic
                            # when it comes time to online the node
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Offline file: %s" % self.offline_file)
                            open(self.offline_file, 'a').close()
                            vnode.comment = self.offline_msg

                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                        else:
                            pass

                    return False

        except OSError, exc:
            pbs.logmsg(pbs.EVENT_SYSTEM,
                       "Failed to remove cgroup path: %s (%s)" %
                       (path, errno.errorcode[exc.errno]))
        except:
            pbs.logmsg(pbs.EVENT_SYSTEM, "Failed to remove cgroup path: %s" %
                       (path))

        return True

    # Removes cgroup directories that are not associated with a local job
    def cleanup_orphans(self, local_jobs):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        orphan_cnt = 0
        for key in self.paths.keys():
            for dir in glob.glob(os.path.join(self.paths[key], '[0-9]*')):
                jobid = os.path.basename(dir)
                if jobid not in local_jobs:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Removing orphaned cgroup: %s" %
                               (caller_name(), dir))
                    # default number of attempts at killing tasks
                    status = self.__remove_cgroup(dir)
                    if not status:
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "%s: Removing orphaned cgroup %s failed " %
                                   (caller_name(), dir))
                        orphan_cnt += 1
        return orphan_cnt

    # Removes the cgroup directories for a job
    def delete(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        tasks_kill_attempted = False

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            if not tasks_kill_attempted:
                # Make multiple attempts at killing tasks in cgroups on
                # first subsystem encountered since it is "normal" for
                # a cgroup.delete to encounter processes hard to kill
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                                           jobid),
                                              tasks_kill_attempts=(
                                              CGROUP_KILL_ATTEMPTS))
                tasks_kill_attempted = True
            else:
                # We tried getting rid of job processes earlier,
                # so don't try N times with sleeps
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                              jobid), tasks_kill_attempts=1)

            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Status: %s" %
                       (caller_name(), status))
            if status is False:
                # Rerun the job and log the message
                line = 'Unable to cleanup a cgroup. '
                line += 'Will offline the node and requeue the job '
                line += 'and try again. job run_count: %d' % \
                        pbs.event().job.run_count
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Failed to remove cgroup: %s" %
                           (caller_name(), line))
                pbs.event().accept()

    # Write a value to a limit file
    def write_value(self, file, value, mode='w'):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: writing %s to %s" %
                   (caller_name(), value, file))
        try:
            with open(file, mode) as fd:
                fd.write(str(value) + '\n')
        except IOError, exc:
            if exc.errno == errno.ENOENT:
                pbs.logmsg(pbs.EVENT_SYSTEM,
                           "Trying to set limit to unknown file %s" % file)
            elif exc.errno in [errno.EACCES, errno.EPERM]:
                pbs.logmsg(pbs.EVENT_SYSTEM, "Permission denied on file %s" %
                           file)
            elif exc.errno == errno.EBUSY:
                raise CgroupBusyError("Limit rejected")
            elif exc.errno == errno.EINVAL:
                raise CgroupLimitError("Invalid limit value: %s" % (value))
            else:
                raise
        except:
            raise

    # Return memory failcount
    def __get_mem_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.failcnt"), 'r').read().strip())
        except:
            return None

    # Return vmem failcount
    def __get_memsw_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.failcnt"), 'r').read().strip())
        except:
            return None

    # Return hpmem failcount
    def __get_hugetlb_failcnt(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.failcnt"))[0], 'r').read().strip())
        except:
            return None

    # Return the max usage of memory in bytes
    def __get_max_mem_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of memsw in bytes
    def __get_max_memsw_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of hugetlb in bytes
    def __get_max_hugetlb_usage(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.max_usage_in_bytes"))[0],
                            'r').read().strip())
        except:
            return None

    # Return the cpuacct.usage in cpu seconds
    def __get_cpu_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "cpuacct.usage"), 'r').read().strip())
        except:
            return None

    # Assign CPUs to the cpuset
    def select_cpus(self, path, ncpus):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path is %s" % (caller_name(), path))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: ncpus is %s" %
                   (caller_name(), ncpus))
        if ncpus < 1:
            ncpus = 1
        try:
            # Must select from those currently available
            cpufile = os.path.basename(path)
            base = os.path.dirname(path)
            parent = os.path.dirname(base)
            avail = cpus2list(open(os.path.join(parent, cpufile),
                              'r').read().strip())
            if len(avail) < 1:
                raise CgroupProcessingError("No CPUs avaialble in cgroup.")
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Available CPUs: %s" %
                       (caller_name(), avail))
            assigned = []
            for file in glob.glob(os.path.join(parent, '[0-9]*', cpufile)):
                cpus = cpus2list(open(file, 'r').read().strip())
                for id in cpus:
                    if id in avail:
                        avail.remove(id)
            if len(avail) < ncpus:
                raise CgroupProcessingError(
                    "Insufficient CPUs avaialble in cgroup.")
            if len(avail) == ncpus:
                return avail
            # FUTURE: Try to minimize NUMA nodes based on memory requirement
            return avail[0:ncpus]
        except:
            raise

    # Return the error message in system message file
    def __get_error_msg(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        proc = subprocess.Popen(['dmesg'],
                                shell=False, stdout=subprocess.PIPE)
        stdout = proc.communicate()[0].splitlines()
        stdout.reverse()
        # Check to see if the job id is found in dmesg otherwise
        # you could get the information for another job that was killed
        # the line will look like Memory cgroup stats for /pbspro/279.centos7
        kill_line = ""

        for line in stdout:
            start = line.find('Killed process ')
            if start >= 0:
                kill_line = line[start:]
            job_start = line.find(jobid)
            if job_start >= 0:
                # pbs.logmsg(pbs.EVENT_DEBUG3,
                #           "%s: Jobid: %s, Line: %s" %
                #           (caller_name(), jobid, line))
                return kill_line
        return ""

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_job_env_file(self, jobid, env_list):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        # if not os.path.exists(self.host_job_env_dir):
        #    os.makedirs(self.host_job_env_dir, 0755)

        # Write out assigned_resources
        try:
            lines = string.join(env_list, '\n')
            filename = self.host_job_env_filename % jobid
            outfile = open(filename, "w")
            outfile.write(lines)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" % (filename))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (lines))
            return True
        except:
            return False

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        if not os.path.exists(self.hook_storage_dir):
            os.makedirs(self.hook_storage_dir, 0755)

        # Write out assigned_resources
        try:
            json_str = json.dumps(self.host_assigned_resources)
            outfile = open(self.hook_storage_dir+os.sep+jobid, "w")
            outfile.write(json_str)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" %
                       (self.hook_storage_dir+os.sep+jobid))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (json_str))
            return True
        except:
            return False

    def read_in_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        pbs.logmsg(pbs.EVENT_DEBUG3, "Host assigned resources: %s" %
                   (self.host_assigned_resources))
        if os.path.isfile(self.hook_storage_dir+os.sep+jobid):
            # Write out assigned_resources
            try:
                infile = open(self.hook_storage_dir+os.sep+jobid, 'r')
                json_data = json.load(infile, object_hook=decode_dict)
                self.host_assigned_resources = json_data
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Host assigned resources: %s" %
                           (self.host_assigned_resources))
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
            finally:
                infile.close()

        if self.host_assigned_resources is not None:
            return True
        else:
            return False

#
#
# FUNCTION main
#


def main():
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Function called" % (caller_name()))
    hostname = pbs.get_local_nodename()
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Host is %s" % (caller_name(), hostname))

    # Log the hook event type
    e = pbs.event()
    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook name is %s" %
               (caller_name(), e.hook_name))

    try:
        # Instantiate the hook utility class
        hooks = HookUtils()
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Event type is %s" %
                   (caller_name(), hooks.event_name(e.type)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(hooks)))
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: Failed to instantiate hook utility class" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        e.accept()

    if not hooks.hashandler(e.type):
        # Bail out if there is no handler for this event
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s event not handled by this hook" %
                   (caller_name(), hooks.event_name(e.type)))
        e.accept()

    try:
        # If an exception occurs, jobutil must be set
        jobutil = None
        # Instantiate the job utility class first so jobutil can be accessed
        # by the exception handlers.
        if hasattr(e, 'job'):
            jobutil = JobUtils(e.job)
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Job information class instantiated" %
                       (caller_name()))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" %
                       (caller_name(), repr(jobutil)))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Event does not include a job" %
                       (caller_name()))
        # Instantiate the cgroup utility class
        vnode = None
        if hasattr(e, 'vnode_list'):
            if hostname in e.vnode_list.keys():
                vnode = e.vnode_list[hostname]
        cgroup = CgroupUtils(hostname, vnode)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Cgroup utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(cgroup)))
        # Bail out if there is nothing to do
        if len(cgroup.subsystems) < 1:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: All cgroups disabled" %
                       (caller_name()))
            e.accept()

        # Call the appropriate handler
        if hooks.invoke_handler(e, cgroup, jobutil) is True:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned success" %
                       (caller_name()))
            e.accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned failure" %
                       (caller_name()))
            e.reject()

    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass

    except AdminError, exc:
        # Something on the system is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Admin error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except UserError, exc:
        # User must correct problem and resubmit job
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("User error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.delete()
                msg += " (deleted)"
            except:
                msg += " (delete failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except JobValueError, exc:
        # Something in PBS is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Job value error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except Exception, exc:
        # Catch all other exceptions and report them.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Unexpected error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

# line below required for unittesting
if __name__ == "__builtin__":
    start_time = time.time()
    try:
        main()
    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
    finally:
        pbs.logmsg(pbs.EVENT_DEBUG, "Elapsed time: %0.4lf" %
                   (time.time() - start_time))
---
2016-07-18 16:55:23,270 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups.json', 'content-encoding': 'default'}
2016-07-18 16:55:23,270 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups.json
2016-07-18 16:55:23,312 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups3.json', 'content-encoding': 'default'}
2016-07-18 16:55:23,312 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups3.json
2016-07-18 16:55:23,340 ERROR    err: ["hook 't1_l1' contents overwritten"]
2016-07-18 16:55:23,341 INFO     job: executable set to /bin/sleep with arguments: 100
2016-07-18 16:55:23,342 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0755 /tmp/PtlPbsJobScriptx_KNNP
2016-07-18 16:55:23,357 INFOCLI  centos7: sudo -H -u pbsuser /usr/local/bin/qsub -N test_t2 -l select=1:ncpus=1:mem=300mb /tmp/PtlPbsJobScriptx_KNNP
2016-07-18 16:55:23,371 INFO     submit to centos7 as pbsuser: job 375.centos7 OrderedDict([('Job_Name', 'test_t2'), ('Resource_List.select', '1:ncpus=1:mem=300mb')])
2016-07-18 16:55:23,371 INFOCLI  job script /tmp/PtlPbsJobScriptx_KNNP
---
#!/bin/bash
if [ -d /cgroup/devices/pbs/$PBS_JOBID ]; then
    device_list=`cat /cgroup/devices/pbs/$PBS_JOBID/devices.list`
    echo "${device_list}"
    if [ -d /cgroup/cpuacct/pbs/$PBS_JOBID ] ||
       [ -d /cgroup/cpuset/pbs/$PBS_JOBID ] ||
       [ -d /cgroup/memory/pbs/$PBS_JOBID ]; then
        echo "Disabled cgroup subsystems are populated with the job id"
    fi
elif [ -d /sys/fs/cgroup/devices/pbs/$PBS_JOBID ]; then
    cat /sys/fs/cgroup/devices/pbs/$PBS_JOBID/devices.list
    device_list=`cat /sys/fs/cgroup/devices/pbs/$PBS_JOBID/devices.list`
    echo "${device_list}"
    if [ -d /sys/fs/cgroup/cpuacct/pbs/$PBS_JOBID ] ||
       [ -d /sys/fs/cgroup/cpuset/pbs/$PBS_JOBID ] ||
       [ -d /sys/fs/cgroup/memory/pbs/$PBS_JOBID ]; then
        echo "Disabled cgroup subsystems are populated with the job id"
    fi

else
    echo "Unable to find the pbs directory in the cgroup directory"
fi
sleep 1

---
2016-07-18 16:55:23,371 INFOCLI  centos7: /usr/local/bin/qstat -f 375.centos7
2016-07-18 16:55:23,465 INFO     expect on server centos7: job_state = R job 375.centos7  got: job_state = Q
2016-07-18 16:55:23,967 INFOCLI  centos7: /usr/local/bin/qmgr -c set server scheduling=True
2016-07-18 16:55:23,985 INFOCLI  centos7: /usr/local/bin/qstat -f 375.centos7
2016-07-18 16:55:24,081 INFO     expect on server centos7: job_state = R job 375.centos7 attempt: 2 ...  OK
2016-07-18 16:55:24,081 INFO     status on centos7: job 375.centos7 ['Output_Path', 'Error_Path', 'Keep_Files']
2016-07-18 16:55:24,081 INFOCLI  centos7: /usr/local/bin/qstat -f 375.centos7
2016-07-18 16:55:24,174 INFO     Job .o file: centos7.usa.org:/tmp/test_t2.o375
2016-07-18 16:55:25,176 INFO     test_t1b: ['Unable to find the pbs directory in the cgroup directory', '']
2016-07-18 16:55:25,177 INFO     FAILED

2016-07-18 16:55:25,177 INFO     ==============================
2016-07-18 16:55:25,177 INFO      Entered CgroupsTests tearDown
2016-07-18 16:55:25,177 INFO     ==============================
2016-07-18 16:55:25,177 INFO     ===============================
2016-07-18 16:55:25,177 INFO     Completed CgroupsTests tearDown
2016-07-18 16:55:25,178 INFO     ===============================
2016-07-18 16:55:25,178 INFO     test name: test_t2 (tests.cgroups_tests.CgroupsTests)...
2016-07-18 16:55:25,178 INFO     test start time: Mon Jul 18 16:55:25 2016
2016-07-18 16:55:25,178 INFO     test docstring: 
 Test to verify that the job is killed when it tries to 
            use more memory then it requested
         
2016-07-18 16:55:25,178 INFO     ===========================
2016-07-18 16:55:25,178 INFO      Entered CgroupsTests setUp
2016-07-18 16:55:25,178 INFO     ===========================
2016-07-18 16:55:25,179 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:25,231 INFO     status on centos7: server
2016-07-18 16:55:25,231 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:25,282 INFO     manager on centos7: set server {'managers': (3, 'root@*')}
2016-07-18 16:55:25,282 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers-=root@*
2016-07-18 16:55:25,326 INFO     manager on centos7: set server {'managers': (2, 'root@*')}
2016-07-18 16:55:25,327 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers+=root@*
2016-07-18 16:55:25,353 INFO     server centos7: reverting configuration to defaults
2016-07-18 16:55:25,354 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:25,404 INFO     select on centos7: __ALL__
2016-07-18 16:55:25,405 INFOCLI  centos7: /usr/local/bin/qselect
2016-07-18 16:55:25,414 INFOCLI  centos7: /usr/local/bin/qstat -f @centos7.usa.org
2016-07-18 16:55:25,472 INFO     expect on server centos7: job_state set 0 job ...  OK
2016-07-18 16:55:25,472 INFOCLI  centos7: /usr/local/bin/pbs_rstat -f
2016-07-18 16:55:25,490 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:25,521 INFO     manager on centos7: delete hook t1_l1
2016-07-18 16:55:25,521 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c delete hook t1_l1
2016-07-18 16:55:25,551 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:55:25,577 INFO     expect on server centos7:  unset  hook t1_l1 ...  OK
2016-07-18 16:55:25,577 INFOCLI  centos7: /usr/local/bin/qstat -Qf @centos7.usa.org
2016-07-18 16:55:25,600 INFO     manager on centos7: delete queue workq
2016-07-18 16:55:25,601 INFOCLI  centos7: /usr/local/bin/qmgr -c delete queue workq
2016-07-18 16:55:25,616 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:55:25,628 INFO     expect on server centos7:  unset  queue workq ...  OK
2016-07-18 16:55:25,629 INFO     manager on centos7: create queue workq {'started': 'True', 'queue_type': 'Execution', 'enabled': 'True'}
2016-07-18 16:55:25,629 INFOCLI  centos7: /usr/local/bin/qmgr -c create queue workq started=True,queue_type=Execution,enabled=True
2016-07-18 16:55:25,649 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:55:25,665 INFO     expect on server centos7: started set True || queue_type set Execution || enabled set True queue workq ...  OK
2016-07-18 16:55:25,665 INFO     manager on centos7: set server {'log_events': '511', 'default_queue': 'workq'}
2016-07-18 16:55:25,665 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=511,default_queue=workq
2016-07-18 16:55:25,681 INFO     status on centos7: resource
2016-07-18 16:55:25,681 INFO     manager on centos7: list resource
2016-07-18 16:55:25,682 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource
2016-07-18 16:55:25,734 INFO     manager on centos7: delete resource nmics,ngpus
2016-07-18 16:55:25,735 INFOCLI  centos7: /usr/local/bin/qmgr -c delete resource nmics,ngpus
2016-07-18 16:55:25,799 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource nmics
2016-07-18 16:55:25,817 INFO     expect on server centos7:  unset  resource nmics ...  OK
2016-07-18 16:55:25,818 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource ngpus
2016-07-18 16:55:25,832 INFO     expect on server centos7:  unset  resource ngpus ...  OK
2016-07-18 16:55:25,832 INFOCLI  status on centos7: server license_count
2016-07-18 16:55:25,832 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:25,885 INFO     server: centos7.usa.org licensed
2016-07-18 16:55:25,924 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/server_priv/comm.lock
2016-07-18 16:55:25,964 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:55:25,982 INFO     scheduler centos7: reverting configuration to defaults
2016-07-18 16:55:25,982 INFO     manager on centos7: list sched
2016-07-18 16:55:25,982 INFOCLI  centos7: /usr/local/bin/qmgr -c list sched
2016-07-18 16:55:25,994 INFO     manager on centos7: unset sched ['sched_cycle_length']
2016-07-18 16:55:25,994 INFOCLI  centos7: /usr/local/bin/qmgr -c unset sched sched_cycle_length
2016-07-18 16:55:26,007 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/dedicated_time
2016-07-18 16:55:26,019 INFOCLI2 centos7: cmp /usr/local/etc/pbs_resource_group /var/spool/pbs/sched_priv/resource_group
2016-07-18 16:55:26,022 INFO     scheduler centos7: reverting holidays file to default
2016-07-18 16:55:26,022 INFOCLI2 centos7: cmp /usr/local/etc/pbs_holidays /var/spool/pbs/sched_priv/holidays
2016-07-18 16:55:26,025 INFOCLI2 centos7: cmp /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:26,029 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:26,042 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:26,053 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:55:26,054 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:55:26,108 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:55:26,118 INFOCLI2 centos7: sudo -H cmp /usr/local/sbin/pbs_mom /usr/local/sbin/pbs_mom.cpuset
2016-07-18 16:55:26,159 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:55:26,169 INFO     mom centos7: reverting configuration to defaults
2016-07-18 16:55:26,169 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/epilogue
2016-07-18 16:55:26,184 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/prologue
2016-07-18 16:55:26,197 INFOCLI2 centos7: sudo -H ls /var/spool/pbs/mom_priv/config.d
2016-07-18 16:55:26,207 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbs_WXOPC /var/spool/pbs/mom_priv/config
2016-07-18 16:55:26,220 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/mom_priv/config
2016-07-18 16:55:26,234 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/mom_priv/config
2016-07-18 16:55:26,245 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/mom_priv/config
2016-07-18 16:55:26,256 INFO     mom centos7: sent signal -HUP
2016-07-18 16:55:26,256 INFOCLI2 centos7: sudo -H kill -HUP 42976
2016-07-18 16:55:26,303 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:55:26,317 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 16:55:26,353 INFO     status on centos7: node centos7
2016-07-18 16:55:26,353 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:55:26,365 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:55:26,381 INFO     expect on server centos7: state = free node centos7 ...  OK
2016-07-18 16:55:26,381 INFO     ============================
2016-07-18 16:55:26,381 INFO     Completed CgroupsTests setUp
2016-07-18 16:55:26,381 INFO     ============================
2016-07-18 16:55:26,381 INFO     server centos7: server operating mode set to cli
2016-07-18 16:55:26,381 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:26,433 INFO     expect on server centos7: pbs_version ~ .*14.* server centos7.usa.org ...  OK
2016-07-18 16:55:26,433 INFO     manager on centos7: set server {'log_events': '2047'}
2016-07-18 16:55:26,433 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=2047
2016-07-18 16:55:26,446 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:26,502 INFO     expect on server centos7: log_events set 2047 server centos7.usa.org ...  OK
2016-07-18 16:55:26,503 INFO     scheduler centos7: config {'resources': 'ncpus,mem,host,vnode,vmem'}
2016-07-18 16:55:26,504 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /var/spool/pbs/sched_priv/sched_config /var/spool/pbs/sched_priv/sched_config.bak
2016-07-18 16:55:26,531 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbs_ZSsJU /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:26,545 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:26,561 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:26,572 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:26,597 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:55:26,598 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:55:26,610 INFO     scheduler centos7 log match: searching for "Error reading line" - No match
2016-07-18 16:55:27,612 INFO     manager on centos7 as root: create resource nmics {'flag': 'nh', 'type': 'long'}
2016-07-18 16:55:27,612 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource nmics flag=nh,type=long
2016-07-18 16:55:27,650 INFO     manager on centos7 as root: create resource ngpus {'flag': 'nh', 'type': 'long'}
2016-07-18 16:55:27,651 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource ngpus flag=nh,type=long
2016-07-18 16:55:27,695 INFO     Test Summary: test_t1_l1 tests "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" cgroup hook
2016-07-18 16:55:27,695 INFO     status on centos7: hook
2016-07-18 16:55:27,695 INFO     manager on centos7: list hook
2016-07-18 16:55:27,695 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:27,742 INFO     status on centos7: hook
2016-07-18 16:55:27,742 INFO     manager on centos7: list hook
2016-07-18 16:55:27,743 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:27,776 INFO     manager on centos7: create hook t1_l1
2016-07-18 16:55:27,776 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c create hook t1_l1
2016-07-18 16:55:27,797 INFO     manager on centos7: set hook t1_l1 {'freq': 2, 'enabled': 'True', 'event': '"execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"'}
2016-07-18 16:55:27,797 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set hook t1_l1 freq=2,enabled=True,event="execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"
2016-07-18 16:55:27,833 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:55:27,854 INFO     expect on server centos7: freq set 2 || enabled set True || event set "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" hook t1_l1 ...  OK
2016-07-18 16:55:27,854 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-python', 'input-file': '/tmp/PtlPbsubzXH8', 'content-encoding': 'default'}
2016-07-18 16:55:27,855 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-python default /tmp/PtlPbsubzXH8
2016-07-18 16:55:27,906 INFO     server centos7: imported hook body
---
#  Copyright (C) 2003-2016 Altair Engineering, Inc. All rights reserved.
#
#  ALTAIR ENGINEERING INC. Proprietary and Confidential. Contains Trade Secret
#  Information. Not for use or disclosure outside ALTAIR and its licensed
#  clients. Information contained herein shall not be decompiled, disassembled,
#  duplicated or disclosed in whole or in part for any purpose. Usage of the
#  software is only as explicitly permitted in the end user software license
#  agreement.
#
#  Copyright notice does not imply publication.

# Last Updated
# Date: 07-06-2016
#
# When soft_limit is true for memory, memsw represents the hard limit.
#
# The resources value in sched_config must contain entries for mem and
# vmem if those subsystems are enabled in the hook configuration file. The
# amount of resource requested will not be avaiable to the hook if they
# are not present.

# This hook handles all of the operations necessary for PBS to support
# cgroups on linux hosts that support them (kernel 2.6.28 and higher)
#
# This hook services the following events:
# - exechost_periodic
# - exechost_startup
# - execjob_attach
# - execjob_begin
# - execjob_end
# - execjob_epilogue
# - execjob_launch

# Imports from __future__ must happen first
from __future__ import with_statement

# Additional imports
import sys
import os
import errno
import signal
import subprocess
import re
import glob
import time
import string
import platform
import traceback
import copy
from stat import S_ISBLK, S_ISCHR
import operator
import pwd
import pbs
import fnmatch
import socket

try:
    import json
except:
    import simplejson as json

CGROUP_KILL_ATTEMPTS = 12

# Flag to turn off features not in 12.2.40X branch
CRAY_12_BRANCH = False


# ============================================================================
# Derived error classes
# ============================================================================

# Base class for errors fixable only by administrative action.


class AdminError(Exception):
    pass

# Base class for errors in processing, unknown cause.


class ProcessingError(Exception):
    pass

# Base class for errors fixable by the user.


class UserError(Exception):
    pass

# Errors in PBS job resource values.


class JobValueError(UserError):
    pass

# Errors when the cgroup is busy.


class CgroupBusyError(ProcessingError):
    pass

# Errors in configuring cgroup.


class CgroupConfigError(AdminError):
    pass

# Errors in configuring cgroup.


class CgroupLimitError(AdminError):
    pass

# Errors processing cgroup.


class CgroupProcessingError(ProcessingError):
    pass

# ============================================================================
# Utility functions
# ============================================================================

#
# FUNCTION caller_name
#
# Return the name of the calling function or method.
#


def caller_name():
    return str(sys._getframe(1).f_code.co_name)

#
# FUNCTION convert_size
#
# Convert a string containing a size specification (e.g. "1m") to a
# string using different units (e.g. "1024k").
#
# This function only interprets a decimal number at the start of the string,
# stopping at any unrecognized character and ignoring the rest of the string.
#
# When down-converting (e.g. MB to KB), all calculations involve integers and
# the result returned is exact. When up-converting (e.g. KB to MB) floating
# point numbers are involved. The result is rounded up. For example:
#
# 1023MB -> GB yields 1g
# 1024MB -> GB yields 1g
# 1025MB -> GB yields 2g  <-- This value was rounded up
#
# Pattern matching or conversion may result in exceptions.
#


def convert_size(value, units='b'):
    logs = {'b': 0, 'k': 10, 'm': 20, 'g': 30,
            't': 40, 'p': 50, 'e': 60, 'z': 70, 'y': 80}
    try:
        new = units[0].lower()
        if new not in logs.keys():
            new = 'b'
        val, old = re.match('([-+]?\d+)([bkmgtpezy]?)',
                            str(value).lower()).groups()
        val = int(val)
        if val < 0:
            raise ValueError('Value may not be negative')
        if old not in logs.keys():
            old = 'b'
        factor = logs[old] - logs[new]
        val *= 2 ** factor
        slop = val - int(val)
        val = int(val)
        if slop > 0:
            val += 1
        # pbs.size() does not like units following zero
        if val <= 0:
            return '0'
        else:
            return str(val) + new
    except:
        return None

#
# FUNCTION size_as_int
#
# Convert a size string to an integer representation of size in bytes
#


def size_as_int(value):
    return int(convert_size(value).rstrip(string.ascii_lowercase))

#
# FUNCTION convert_time
#
# Converts a integer value for time into the value of the return unit
#
# A valid decimal number, with optional sign, may be followed by a character
# representing a scaling factor.  Scaling factors may be either upper or
# lower case. Examples include:
# 250ms
# 40s
# +15min
#
# Valid scaling factors are:
# ns  = 10**-9
# us  = 10**-6
# ms  = 10**-3
# s   =      1
# min =     60
# hr  =   3600
#
# Pattern matching or conversion may result in exceptions.
#


def convert_time(value, return_unit='s'):
    multipliers = {'':  1, 'ns': 10 ** -9, 'us': 10 ** -6,
                   'ms': 10 ** -3, 's': 1, 'min': 60, 'hr': 3600}
    n, factor = re.match('([-+]?\d+)(\w*[a-zA-Z])?',
                         str(value).lower()).groups(0)

    # Check to see if there was not unit of time specified
    if factor == 0:
        factor = ''

    # Check to see if the unit is valid
    if str.lower(factor) not in multipliers.keys():
        raise ValueError('Time unit not recognized.')

    # Convert the value to seconds
    value_in_sec = float(n) * float(multipliers[str.lower(factor)])
    if return_unit != 's':
        return value_in_sec / multipliers[str.lower(return_unit)]
    else:
        return float('%lf' % value_in_sec)

# simplejson hook to convert lists from unicode to utf-8


def decode_list(data):
    rv = []
    for item in data:
        if isinstance(item, unicode):
            item = item.encode('utf-8')
        elif isinstance(item, list):
            item = decode_list(item)
        elif isinstance(item, dict):
            item = decode_dict(item)
        rv.append(item)
    return rv

# simplejson hook to convert dictionaries from unicode to utf-8


def decode_dict(data):
    rv = {}
    for key, value in data.iteritems():
        if isinstance(key, unicode):
            key = key.encode('utf-8')
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        elif isinstance(value, list):
            value = decode_list(value)
        elif isinstance(value, dict):
            value = decode_dict(value)
        rv[key] = value
    return rv

# Convert CPU list format (with ranges) to a Python list


def cpus2list(s):
    # The input string is a comma separated list of digits and ranges.
    # Examples include:
    # 0-3,8-11
    # 0,2,4,6
    # 2,5-7,10
    cpus = []
    if s.strip() == '':
        return cpus
    for r in s.split(','):
        if '-' in r[1:]:
            start, end = r.split('-', 1)
            for i in range(int(start), int(end) + 1):
                cpus.append(int(i))
        else:
            cpus.append(int(r))
    return cpus


# Helper function to ignore suspended jobs when removing
# "already used" resources
def job_to_be_ignored(jobid):
    # Get job substate from small utility that calls printjob
    pbs_exec = ''
    if not CRAY_12_BRANCH:
        pbs_conf = pbs.get_pbs_conf()
        pbs_exec = pbs_conf['PBS_EXEC']
    else:
        if 'PBS_EXEC' in self.cfg:
            pbs_exec = self.cfg['PBS_EXEC']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "PBS_EXEC needs to be defined in the config file")
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Exiting the cgroups hook")
            pbs.accept()

    printjob_cmd = pbs_exec+os.sep+'bin'+os.sep+'printjob'

    cmd = [printjob_cmd, jobid]

    substate = None
    try:
        pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
        # Collect the job substate information
        process = subprocess.Popen(
                                   cmd,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        (out, err) = process.communicate()
        # Find the job substate
        substate_re = re.compile(r"substate:\s+(?P<jobstate>\S+)\s+")
        substate = substate_re.search(out)
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Unexpected error in job_to_be_ignored: %s" %
                   ' '.join([repr(sys.exc_info()[0]),
                            repr(sys.exc_info()[1])]))
        out = "Unknown: failed to run " + printjob_cmd

    if substate is not None:
        substate = substate.group().split(':')[1].strip()
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Job %s has substate %s" % (jobid, substate))

        suspended_substates = ['0x2b', '0x2d', 'unknown']
        return (substate in suspended_substates)
    else:
        return False

# ============================================================================
# Utility classes
# ============================================================================

#
# CLASS HookUtils
#


class HookUtils:

    def __init__(self, hook_events=None):
        if hook_events is not None:
            self.hook_events = hook_events
        else:
            self.hook_events = {
                # Defined in the order they appear in module_pbs_v1.c
                pbs.QUEUEJOB: {
                    'name': 'queuejob',
                    'handler': None
                },
                pbs.MODIFYJOB: {
                    'name': 'modifyjob',
                    'handler': None
                },
                pbs.RESVSUB: {
                    'name': 'resvsub',
                    'handler': None
                },
                pbs.MOVEJOB: {
                    'name': 'movejob',
                    'handler': None
                },
                pbs.RUNJOB: {
                    'name': 'runjob',
                    'handler': None
                },
                pbs.PROVISION: {
                    'name': 'provision',
                    'handler': None
                },
                pbs.EXECJOB_BEGIN: {
                    'name': 'execjob_begin',
                    'handler': self.__execjob_begin_handler
                },
                pbs.EXECJOB_PROLOGUE: {
                    'name': 'execjob_prologue',
                    'handler': None
                },
                pbs.EXECJOB_EPILOGUE: {
                    'name': 'execjob_epilogue',
                    'handler': self.__execjob_epilogue_handler
                },
                pbs.EXECJOB_PRETERM: {
                    'name': 'execjob_preterm',
                    'handler': None
                },
                pbs.EXECJOB_END: {
                    'name': 'execjob_end',
                    'handler': self.__execjob_end_handler
                },
                pbs.EXECJOB_LAUNCH: {
                    'name': 'execjob_launch',
                    'handler': self.__execjob_launch_handler
                },
                pbs.EXECHOST_PERIODIC: {
                    'name': 'exechost_periodic',
                    'handler': self.__exechost_periodic_handler
                },
                pbs.EXECHOST_STARTUP: {
                    'name': 'exechost_startup',
                    'handler': self.__exechost_startup_handler
                },
                pbs.MOM_EVENTS: {
                    'name': 'mom_events',
                    'handler': None
                },
            }

            if not CRAY_12_BRANCH:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "CRAY 12 Branch: %s" % (CRAY_12_BRANCH))
                self.hook_events[pbs.EXECJOB_ATTACH] = {
                    'name': 'execjob_attach',
                    'handler': self.__execjob_attach_handler
                }

    def __repr__(self):
        return "HookUtils(%s)" % (repr(self.hook_events))

    def event_name(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['name']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Type: %s not found" % (caller_name(), type))
            return None

    def hashandler(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['handler'] is not None
        else:
            return None

    def invoke_handler(self, event, cgroup, jobutil, *args):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: UID: real=%d, effective=%d" %
                   (caller_name(), os.getuid(), os.geteuid()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: GID: real=%d, effective=%d" %
                   (caller_name(), os.getgid(), os.getegid()))
        if self.hashandler(event.type):
            result = self.hook_events[event.type][
                'handler'](event, cgroup, jobutil, *args)
            return result
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: %s event not handled by this hook." %
                       (caller_name(), self.event_name(event.type)))

    def __execjob_begin_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Instantiate the NodeConfig class for get_memory_on_node and
        # get_vmem_on node
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Host assigned job resources: %s" %
                   (caller_name(), jobutil.host_resources))

        # Make sure the parent cgroup directories exist
        cgroup.create_paths()

        # Make sure that any old cgroups created in an earlier attempt
        # at creating or running the job are gone
        cgroup.delete(e.job.id)

        # Gather the assigned resources
        cgroup.gather_assigned_resources()
        # Create the cgroup(s) for the job
        cgroup.create_job(e.job.id, node)
        # Configure the new cgroup
        cgroup.configure_job(e.job.id, jobutil.host_resources, node)
        # Initialize resource usage for the job
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        # Write out the assigned resources
        cgroup.write_out_cgroup_host_assigned_resources(e.job.id)
        # Write out the environment variable for the host (pbs_attach)
        if 'devices_name' in cgroup.host_assigned_resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Devices: %s" %
                       cgroup.host_assigned_resources['devices_name'])
            env_list = list()
            if len(cgroup.host_assigned_resources['devices_name']) > 0:
                line = str()
                mics = list()
                gpus = list()
                for key in cgroup.host_assigned_resources['devices_name']:
                    if key.startswith('mic'):
                        mics.append(key[3:])
                    elif key.startswith('nvidia'):
                        gpus.append(key[6:])
                if len(mics) > 0:
                    env_list.append('OFFLOAD_DEVICES="%s"' %
                                    string.join(mics, ','))
                if len(gpus) > 0:
                    # don't put quotes around the values. ex "0" or "0,1".
                    # This will cause it to fail.
                    env_list.append('CUDA_VISIBLE_DEVICES=%s' %
                                    string.join(gpus, ','))

            pbs.logmsg(pbs.EVENT_DEBUG, "ENV_LIST: %s" % env_list)
            cgroup.write_out_cgroup_host_job_env_file(e.job.id, env_list)
        return True

    def __execjob_epilogue_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # The resources_used information has a base type of pbs_resource
        # Set the usage data
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        return True

    def __execjob_end_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Delete the cgroup(s) for the job
        cgroup.delete(e.job.id)
        # Remove the host_assigned_resources and job_env file
        for filename in [cgroup.hook_storage_dir+os.sep+e.job.id,
                         cgroup.host_job_env_filename % e.job.id]:
            try:
                os.remove(filename)
            except OSError:
                pbs.logmsg(pbs.EVENT_DEBUG3, "File: %s not found" % (filename))
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Error removing file: %s" % (filename))
                pass

        return True

    def __execjob_launch_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Add the parent process id to the appropriate cgroups.
        cgroup.add_pids(os.getppid(), jobutil.job.id)
        # FUTURE: Add environment variable to the job environment
        # if job requested mic or gpu
        cgroup.read_in_cgroup_host_assigned_resources(e.job.id)
        if cgroup.host_assigned_resources is not None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "host_assigned_resources: %s" %
                       (cgroup.host_assigned_resources))
            cgroup.setup_job_devices_env()
        return True

    def __exechost_periodic_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Cleanup cgroups for jobs not present on this node
        count = cgroup.cleanup_orphans(e.job_list)
        if ('periodic_resc_update' in cgroup.cfg and
                cgroup.cfg['periodic_resc_update']):
            for job in e.job_list.keys():
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: job is %s" %
                           (caller_name(), job))
                cgroup.update_job_usage(job, e.job_list[job].resources_used)
        # Online nodes that were offlined due to a cgroup not cleaning up
        if count == 0 and cgroup.cfg['online_offlined_nodes']:
            vnode = pbs.event().vnode_list[cgroup.hostname]
            if os.path.isfile(cgroup.offline_file):
                line = 'Orphan cgroup(s) have been cleaned up. '

                # Check with the pbs server to see if the node comment matches
                try:
                    tmp_comment = pbs.server().vnode(cgroup.hostname).comment
                except:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Unable to contact server for node comment")
                    tmp_comment = None
                if tmp_comment == cgroup.offline_msg:
                    line += 'Will bring the node back online'
                    vnode.state = pbs.ND_FREE
                    vnode.comment = None
                else:
                    line += 'However, the comment has changed since the node '
                    line += 'was offlined. Node will remain offline'

                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: %s" % (caller_name(), line))
                # Remove file
                os.remove(cgroup.offline_file)
        return True

    def __exechost_startup_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        cgroup.create_paths()
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))

        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        node.create_vnodes()
        if 'memory' in cgroup.subsystems:
            val = node.get_memory_on_node(cgroup.cfg)
            if val is not None:
                if 'vnode_per_numa_node' in cgroup.cfg:
                    if not cgroup.cfg['vnode_per_numa_node']:
                        hn = node.hostname
                        e.vnode_list[hn].resources_available['mem'] = \
                            pbs.size(val)
                        val_cpus = node.totalcpus
                        if val_cpus is not None:
                            e.vnode_list[hn].resources_available['ncpus'] = \
                                int(val_cpus)
                cgroup.set_node_limit('mem', val)
        if 'memsw' in cgroup.subsystems:
            val = node.get_vmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['vmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('vmem', val)
        if 'hugetlb' in cgroup.subsystems:
            val = node.get_hpmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['hpmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('hpmem', val)
        return True

    def __execjob_attach_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logjobmsg(jobutil.job.id, "%s: Attaching PID %s" %
                      (caller_name(), e.pid))
        # Add the job process id to the appropriate cgroups.
        cgroup.add_pids(e.pid, jobutil.job.id)
        return True

#
# CLASS ShallIRunUtils
#


class ShallIRunUtils:

    def __init__(self, hostname, cfg):
        self.cfg = cfg
        self.hostname = hostname

    def allow_users(self, allow_users):
        """ This still needs to be defined """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pass

    def exclude_hosts(self, exclude_hosts):
        """
        Gets the hostname for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        hostname = pbs.get_local_nodename()
        # check to see if hostname is in the exclude_hosts list
        if self.hostname in exclude_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: exclude host %s is in %s" %
                       (caller_name(), self.hostname, exclude_hosts))
            pbs.event().accept()

        # check to see if it regex syntax is used
        pass

    def exclude_vntypes(self, exclude_vntypes):
        """
        Gets the vntype for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        vntype = None

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'
        if os.path.isfile(vntype_file):
            fdata = open(vntype_file).readlines()
            vntype = fdata[0].strip()
            pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
        if vntype is None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype is set to %s" % (vntype))
        elif vntype in exclude_vntypes:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s is in %s" % (vntype, exclude_vntypes))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Exiting Hook")
            pbs.event().accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s not in global exclude: %s" %
                       (vntype, exclude_vntypes))
        pass

    def run_only_on_hosts(self, approved_hosts):
        """
        Gets the list of approved nodes to run on and checks to see if the hook
        should run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Approved hosts: %s, %s" %
                   (type(approved_hosts), approved_hosts))

        # check to see if hostname is in the approved_hosts list
        if approved_hosts == []:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "approved hosts list is empty: %s" % approved_hosts)
        elif self.hostname not in approved_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s is not in the approved list of hosts: %s" %
                       (self.hostname, approved_hosts))
            pbs.event().accept()

        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s in list of approved hosts: %s" %
                       (self.hostname, approved_hosts))

#
# CLASS JobUtils
#


class JobUtils:

    def __init__(self, job, hostname=None, resources=None):
        self.job = job
        self.host_vnodes = list()
        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if resources is not None:
            self.host_resources = resources
        else:
            self.host_resources = self.__host_assigned_resources()

    def __repr__(self):
        return "JobUtils(%s, %s, %s)" % (repr(self.job), repr(self.hostname),
                                         repr(self.host_resources))

    # Return a dictionary of resources assigned to the local node
    def __host_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Bail out if no hostname was provided
        if self.hostname is None:
            raise CgroupProcessingError('No hostname available')
        # Bail out if no job information is present
        if self.job is None:
            raise CgroupProcessingError('No job information available')
        # Determine which vnodes are on this host, if any
        if self.hostname is not None:
            vnhost_pattern = "%s\[[\d+]\]" % self.hostname
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnhost pattern: %s" %
                       (caller_name(), vnhost_pattern))
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job exec_vnode list: %s" %
                       (caller_name(), self.job.exec_vnode))
            for match in re.findall(vnhost_pattern, str(self.job.exec_vnode)):
                self.host_vnodes.append(match)
            if self.host_vnodes is not None:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Associate %s to vnodes on %s" %
                           (caller_name(), self.host_vnodes, self.hostname))

        # Collect host assigned resources
        resources = {}
        for chunk in self.job.exec_vnode.chunks:
            vnode = False
            if self.host_vnodes == [] and chunk.vnode_name != self.hostname:
                continue
            elif self.host_vnodes != [] and \
                    chunk.vnode_name not in self.host_vnodes:
                continue
            elif chunk.vnode_name in self.host_vnodes:
                vnode = True
                if 'vnodes' not in resources:
                    resources['vnodes'] = {}
                if chunk.vnode_name not in resources['vnodes']:
                    resources['vnodes'][chunk.vnode_name] = {}
                # This check is needed since not all resources are
                # required to be in each chunk of a job
                # i.e. exec_vnodes =
                # (node1[0]:ncpus=4:mem=4gb+node1[1]:mem=2gb) +
                # (node1[1]:ncpus=3+node[0]:ncpus=1:mem=4gb)
                if all(resc in resources['vnodes'][chunk.vnode_name]
                       for resc in chunk.chunk_resources.keys()):
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: resources already defined" %
                               (caller_name()))
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s,\t%s" %
                               (caller_name(),
                                chunk.chunk_resources.keys(),
                                resources['vnodes'][chunk.vnode_name].keys()))
                else:
                    # Initialize resources for each vnode
                    for resc in chunk.chunk_resources.keys():
                        # Initialize the new value
                        if isinstance(chunk.chunk_resources[resc],
                                      pbs.pbs_int):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_int(0)
                        elif isinstance(chunk.chunk_resources[resc],
                                        pbs.pbs_float):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_float(0)
                        elif isinstance(chunk.chunk_resources[resc], pbs.size):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.size('0')

            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Chunk %s" %
                       (caller_name(), chunk.vnode_name))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resources: %s" %
                       (caller_name(), resources))
            for resc in chunk.chunk_resources.keys():
                if resc not in resources.keys():
                    # Initialize the new value
                    if isinstance(chunk.chunk_resources[resc], pbs.pbs_int):
                        resources[resc] = pbs.pbs_int(0)
                    elif isinstance(chunk.chunk_resources[resc],
                                    pbs.pbs_float):
                        resources[resc] = pbs.pbs_float(0.0)
                    elif isinstance(chunk.chunk_resources[resc], pbs.size):
                        resources[resc] = pbs.size('0')
                # Add resource value to total
                if type(chunk.chunk_resources[resc]) in \
                        [pbs.pbs_int, pbs.pbs_float, pbs.size]:
                    resources[resc] += chunk.chunk_resources[resc]
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s: resources[%s][%s] is now %s" %
                               (caller_name(), self.hostname, resc,
                                resources[resc]))
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] += \
                                  chunk.chunk_resources[resc]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Setting resource %s to string %s" %
                               (caller_name(), resc,
                                str(chunk.chunk_resources[resc])))
                    resources[resc] = str(chunk.chunk_resources[resc])
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] = \
                                  str(chunk.chunk_resources[resc])
        if not resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: No resources assigned to host %s" %
                       (caller_name(), self.hostname))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources for %s: %s" %
                       (caller_name(), self.hostname, repr(resources)))

        # Return assigned resources for specified host
        pbs.logmsg(pbs.EVENT_DEBUG3, "Job Resources: %s" % (resources))
        return resources

    # Write a message to the job stdout file
    def write_to_stderr(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stderr_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

    # Write a message to the job stdout file
    def write_to_stdout(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stdout_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

#
# CLASS NodeConfig
#


class NodeConfig:

    def __init__(self, cfg, **kwargs):

        self.cfg = cfg
        hostname = None
        meminfo = None
        numa_nodes = None
        devices = None
        hyperthreading = None
        self.hyperthreads_per_core = 1
        self.totalcpus = self.__discover_cpus()

        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        self.host_mom_jobdir = pbs_home+'/mom_priv/jobs'

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'hostname':
                    hostname = val
                elif arg == 'meminfo':
                    meminfo = val
                elif arg == 'numa_nodes':
                    numa_nodes = val
                elif arg == 'devices':
                    devices = val
                elif arg == 'hyperthreading':
                    hyperthreading = val

        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if meminfo is not None:
            self.meminfo = meminfo
        else:
            self.meminfo = self.__discover_meminfo()
        if numa_nodes is not None:
            self.numa_nodes = numa_nodes
        else:
            self.numa_nodes = self.__discover_numa_nodes()
        if devices is not None:
            self.devices = devices
        else:
            self.devices = self.__discover_devices()
        if hyperthreading is not None:
            self.hyperthreading = hyperthreading
        else:
            self.hyperthreading = self.__hyperthreading_enabled()

        # Add the devices count i.e. nmics and ngpus to the numa nodes
        self.__add_devices_to_numa_node()

    def __repr__(self):
        return ("NodeConfig(%s, %s, %s, %s, %s, %s)" %
                (repr(self.cfg), repr(self.hostname), repr(self.meminfo),
                 repr(self.numa_nodes), repr(self.devices),
                 repr(self.hyperthreading)))

    def __add_devices_to_numa_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (self.devices.keys()))
        for device in self.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" % (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" %
                           (self.devices[device].keys()))
                for device_name in self.devices[device]:
                    device_socket = \
                        self.devices[device][device_name]['numa_node']
                    if device == 'mic':
                        if 'nmics' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['nmics'] = 1
                        else:
                            self.numa_nodes[device_socket]['nmics'] += 1
                    elif device == 'gpu':
                        if 'ngpus' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['ngpus'] = 1
                        else:
                            self.numa_nodes[device_socket]['ngpus'] += 1

        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (self.numa_nodes))

    # Discover what type of hardware is on this node and how is it partitioned
    def __discover_numa_nodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        numa_nodes = {}
        for dir in glob.glob(os.path.join(os.sep,
                             "sys", "devices", "system", "node", "node*")):
            id = int(dir.split(os.sep)[5][4:])
            if id not in numa_nodes.keys():
                numa_nodes[id] = {}
                numa_nodes[id]['devices'] = list()
            numa_nodes[id]['cpus'] = open(os.path.join(dir, "cpulist"),
                                          'r').readline().strip()
            with open(os.path.join(dir, "meminfo"), 'r') as fd:
                for line in fd:
                    # Each line will contain four or five fields. Examples:
                    # Node 0 MemTotal:       32995028 kB
                    # Node 0 HugePages_Total:     0
                    entries = line.split()
                    if len(entries) < 4:
                        continue
                    if entries[2] == "MemTotal:":
                        numa_nodes[id][entries[2].rstrip(':')] = \
                            convert_size(entries[3] + entries[4], 'kb')
                    elif entries[2] == "HugePages_Total:":
                        numa_nodes[id][entries[2].rstrip(':')] = entries[3]
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover Numa Nodes: %s" % numa_nodes)
        return numa_nodes

    # Identify devices and to which numa nodes they are attached
    def __discover_devices(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        devices = {}
        for file in glob.glob(os.path.join(os.sep, "sys", "class", "*", "*",
                                           "device", "numa_node")):
            # The file should contain a single integer
            numa_node = int(open(file, 'r').readline().strip())
            if numa_node < 0:
                numa_node = 0
            dirs = file.split(os.sep)
            name = dirs[3]
            instance = dirs[4]
            if name not in devices.keys():
                devices[name] = {}
            devices[name][instance] = {}
            devices[name][instance]['numa_node'] = numa_node
            if name == 'mic':
                s = os.stat('/dev/%s' % instance)
                devices[name][instance]['major'] = os.major(s.st_rdev)
                devices[name][instance]['minor'] = os.minor(s.st_rdev)
                devices[name][instance]['device_type'] = 'c'

        # Check to see if there are gpus on the node
        gpus = self.discover_gpus()
        if isinstance(gpus, dict) and len(gpus.keys()) > 0:
            devices['gpu'] = gpus['gpu']

        pbs.logmsg(pbs.EVENT_DEBUG3, "Discovered devices: %s" % (devices))
        return devices

    def discover_gpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Check to see if nvidia-smi exists and produces valid output
            cmd = ['/usr/bin/nvidia-smi', '-q', '-x']
            if 'nvidia-smi' in self.cfg:
                cmd[0] = self.cfg['nvidia-smi']

            pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
            # Collect the gpu information
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.PIPE)
            output, err = process.communicate()

            # pbs.logmsg(pbs.EVENT_DEBUG3,"output: %s" % output)
            # import the xml library and parse the output
            import xml.etree.ElementTree as ET

            # Parse the data
            name = 'gpu'
            gpu_data = dict()
            gpu_data[name] = {}
            root = ET.fromstring(output)
            pbs.logmsg(pbs.EVENT_DEBUG3, "root.tag: %s" % root.tag)
            for child in root:
                if child.tag == "attached_gpus":
                    # gpu_data['ngpus'] = int(child.text)
                    pass
                if child.tag == "gpu":
                    device_id = child.get('id')
                    number = 'nvidia%s' % child.find('minor_number').text
                    gpu_data[name][number] = dict()
                    gpu_data[name][number]['id'] = device_id

                    # Determine which socket the device is on
                    filename = '/sys/bus/pci/devices/%s/numa_node' % \
                        device_id.lower()
                    isfile = os.path.isfile(filename)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s, %s" % (filename, isfile))
                    if isfile:
                        numa_node = open(filename).read().strip()
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "numa_node: %s" % (numa_node))
                        if int(numa_node) == -1:
                            numa_node = 0
                        gpu_data[name][number]['numa_node'] = int(numa_node)
                        try:
                            dev_filename = '/dev/%s' % number
                            s = os.stat(dev_filename)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find %s" % dev_filename)
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                        gpu_data[name][number]['major'] = os.major(s.st_rdev)
                        gpu_data[name][number]['minor'] = os.minor(s.st_rdev)
                        gpu_data[name][number]['device_type'] = 'c'
            pbs.logmsg(pbs.EVENT_DEBUG, "%s" % gpu_data)
            return gpu_data
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unable to find %s" % string.join(cmd, " "))
            return {'gpu': {}}
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        return {'gpu': {}}

    # Get the memory info on this host
    def __discover_meminfo(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        meminfo = {}
        with open(os.path.join(os.sep, "proc", "meminfo"), 'r') as fd:
            for line in fd:
                entries = line.split()
                if entries[0] == "MemTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "SwapTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "Hugepagesize:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "HugePages_Total:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
                elif entries[0] == "HugePages_Rsvd:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover meminfo: %s" % meminfo)
        return meminfo

    # Determine if the cpus have hyperthreading enabled
    def __hyperthreading_enabled(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            siblings = 0
            cpu_cores = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "siblings":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    siblings = entries[1].strip()
                elif entries[0].strip() == "cpu cores":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_cores = entries[1].strip()
                elif entries[0].strip() == "flags":
                    flag_options = entries[1].split()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: flag_options: %s" %
                               (caller_name(), flag_options))
                    if "ht" in flag_options:
                        rc = True
                        break
        if rc is True:
            self.hyperthreads_per_core = int(siblings)/int(cpu_cores)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: hyperthreads/core: %d" %
                       (caller_name(), self.hyperthreads_per_core))
            if self.hyperthreads_per_core == 1:
                rc = False
        return rc

    def __discover_cpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            cpu_threads = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "processor":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_threads += 1
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: processors found: %d" %
                   (caller_name(), cpu_threads))

        return cpu_threads

    # Gather the jobs running on this node
    def gather_jobs_on_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        # Get the job list from the server
        jobs = None
        try:
            jobs = pbs.server().vnode(self.hostname).jobs
        except:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Unable to contact server to get jobs")
            return None

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs from server for %s: %s" % (self.hostname, jobs))

        if jobs is None:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "No jobs found on %s " % (self.hostname))
            return list()

        jobs_list = dict()
        for job in jobs.split(','):
            # The job list from the node has a space in front of the job id
            # This must be removed or it will not match the cgroup dir
            jobs_list[job.split('/')[0].strip()] = 0
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs on %s: %s" % (self.hostname, jobs_list.keys()))

        return jobs_list.keys()

    # Gather the jobs running on this node
    def gather_jobs_on_node_local(self):
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Method called" % (caller_name()))
        # Get the job list from the jobs directory
        jobs_list = list()
        try:
            for file in os.listdir(self.host_mom_jobdir):
                if fnmatch.fnmatch(file, "*.JB"):
                    jobs_list.append(file[:-3])
            if not jobs_list:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "No jobs found on %s " % (self.hostname))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Jobs from server for %s: %s" %
                           (self.hostname, repr(jobs_list)))

            return jobs_list
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Could not get job list from mom_priv/jobs for %s" %
                       self.hostname)

            return self.gather_jobs_on_node()

    # Get the memory resource on this mom
    def get_memory_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Calculate memory
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
            pbs.logmsg(pbs.EVENT_DEBUG3, "val: %s" % val)
        else:
            val = size_as_int(MemTotal)
        if 'percent_reserve' in config['cgroup']['memory']:
            percent_reserve = config['cgroup']['memory']['percent_reserve']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memory'].keys():
            reserve_memory = config['cgroup']['memory']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                reserve_mem = size_as_int(reserve_memory)
                if val > reserve_mem:
                    val -= reserve_mem
                else:
                    val -= size_as_int("100mb")
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Unable to reserve more memory than " +
                               "available on the node. Available %s, " +
                               "Tried to reserve %s. Reserving 100mb" %
                               (val, reserve_mem))

            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))

        pbs.logmsg(pbs.EVENT_DEBUG3, "Return val: %s" % val)
        return convert_size(str(val), 'kb')

    # Get the virtual memory resource on this mom
    def get_vmem_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Return the value for memory if no swap is configured
        if size_as_int(self.meminfo['SwapTotal']) <= 0:
            return self.get_memory_on_node(config)
        # Calculate vmem
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
        else:
            val = size_as_int(MemTotal)

        val += size_as_int(self.meminfo['SwapTotal'])
        # Determine if memory needs to be reserved
        if 'percent_reserve' in config['cgroup']['memsw']:
            percent_reserve = config['cgroup']['memsw']['reserve_memory']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memsw']:
            reserve_memory = config['cgroup']['memsw']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                val -= size_as_int(reserve_memory)
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))
        return convert_size(str(val), 'kb')

    # Get the huge page memory resource on this mom
    def get_hpmem_on_node(self, config):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        if 'percent_reserve' in config['cgroup']['hugetlb'].keys():
            percent_reserve = config['cgroup']['hugetlb']['percent_reserve']
        else:
            percent_reserve = 0
        # Calculate vmem
        val = size_as_int(self.meminfo['Hugepagesize'])
        val *= (self.meminfo['HugePages_Total'] -
                self.meminfo['HugePages_Rsvd'])
        val -= (val * percent_reserve) / 100
        return convert_size(str(val), 'kb')

    # Create individual vnodes per socket
    def create_vnodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        key = "vnode_per_numa_node"
        if key in self.cfg.keys():
            if not self.cfg[key]:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s is false" %
                           (caller_name(), key))
                return
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s not configured" %
                       (caller_name(), key))
            return

        # Create one vnode per numa node
        vnode_list = pbs.event().vnode_list
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: numa nodes: %s" %
                   (caller_name(), self.numa_nodes))
        # Define the vnodes
        vnode_name = self.hostname
        vnode_list[vnode_name] = pbs.vnode(vnode_name)
        for id in self.numa_nodes.keys():
            vnode_name = self.hostname + "[%d]" % id
            vnode_list[vnode_name] = pbs.vnode(vnode_name)
            for key, val in sorted(self.numa_nodes[id].iteritems()):
                if key == 'cpus':
                    vnode_list[vnode_name].resources_available['ncpus'] = \
                        len(cpus2list(val))/self.hyperthreads_per_core
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['ncpus'] = 0
                elif key == 'MemTotal':
                    mem = self.get_memory_on_node(self.cfg, val)
                    vnode_list[vnode_name].resources_available['mem'] = \
                        pbs.size(mem)
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['mem'] = \
                        pbs.size('0kb')
                elif key == 'HugePages_Total':
                    pass
                elif isinstance(val, list):
                    pass
                elif isinstance(val, dict):
                    pass
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s=%s" %
                               (caller_name(), key, val))
                    vnode_list[vnode_name].resources_available[key] = val

                    if isinstance(val, int):
                        vnode_list[self.hostname].resources_available[key] = 0
                    elif isinstance(val, float):
                        vnode_list[self.hostname].resources_available[key] = \
                            0.0
                    elif isinstance(val, pbs.size):
                        vnode_list[self.hostname].resources_available[key] = \
                            pbs.size('0kb')
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnode list: %s" %
                   (caller_name(), vnode_list))
        return True

#
# CLASS CgroupUtils
#


class CgroupUtils:
    def __init__(self, hostname, vnode, **kwargs):
        cfg = None
        subsystems = None
        paths = None
        vntype = None
        event = None
        self.assigned_resources = dict()
        self.existing_cgroups = dict()
        self.host_assigned_resources = None
        self.pid_lower_limit = 3

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'subsystems':
                    subsystems = val
                elif arg == 'paths':
                    paths = val
                elif arg == 'vntype':
                    vntype = val
                elif arg == 'event':
                    event = val

        try:
            self.hostname = hostname
            self.vnode = vnode
            # __check_os will raise an exception if cgroups are not present
            self.__check_os()
            # Read in the config file
            if cfg is not None:
                self.cfg = cfg
            else:
                self.cfg = self.__parse_config_file()

            # Determine if the hook should run or exit
            siru = ShallIRunUtils(self.hostname, self.cfg)
            siru.exclude_hosts(self.cfg['exclude_hosts'])
            siru.exclude_vntypes(self.cfg['exclude_vntypes'])
            siru.run_only_on_hosts(self.cfg['run_only_on_hosts'])

            # Collect the mount points
            if paths is not None:
                self.paths = paths
            else:
                self.paths = self.__set_paths()
            # Define the local vnode type
            if vntype is not None:
                self.vntype = vntype
            else:
                self.vntype = self.__get_vnode_type()
            # Determine which subsystems we care about
            if subsystems is not None:
                self.subsystems = subsystems
            else:
                self.subsystems = self.__target_subsystems()

            # location to store information for the different hook events
            pbs_home = ''
            if not CRAY_12_BRANCH:
                pbs_conf = pbs.get_pbs_conf()
                pbs_home = pbs_conf['PBS_HOME']
            else:
                if 'PBS_HOME' in self.cfg:
                    pbs_home = self.cfg['PBS_HOME']
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "PBS_HOME needs to be defined in the " +
                               "config file")
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Exiting the cgroups hook")
                    pbs.accept()

            self.hook_storage_dir = pbs_home+'/mom_priv/hooks/hook_data'
            self.host_job_env_dir = pbs_home+'/aux'
            self.host_job_env_filename = self.host_job_env_dir+os.sep+"%s.env"

            # information for offlining nodes
            self.offline_file = \
                pbs_home+os.sep+'mom_priv'+os.sep+'hooks'+os.sep
            self.offline_file += "%s.offline" % pbs.event().hook_name
            self.offline_msg = "Hook %s: " % pbs.event().hook_name
            self.offline_msg += "Unable to clean up one or more cgroups"

        except:
            raise

    def __repr__(self):
        return ("CgroupUtils(%s, %s, %s, %s, %s, %s)" %
                (repr(self.hostname), repr(self.vnode), repr(self.cfg),
                 repr(self.subsystems), repr(self.paths), repr(self.vntype)))

    # Validate the OS type and version
    def __check_os(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check to see if the platform is linux and the kernel is new enough
        if platform.system() != 'Linux':
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: OS does not support cgroups" %
                       (caller_name()))
            raise CgroupConfigError("OS type not supported")
        rel = map(int,
                  string.split(string.split(platform.release(), '-')[0], '.'))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Detected Linux kernel version %d.%d.%d" %
                   (caller_name(), rel[0], rel[1], rel[2]))
        supported = False
        if rel[0] > 2:
            supported = True
        elif rel[0] == 2:
            if rel[1] > 6:
                supported = True
            elif rel[1] == 6:
                if rel[2] >= 28:
                    supported = True
        if not supported:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Kernel needs to be >= 2.6.28: %s" %
                       (caller_name(), system_info[2]))
            raise CgroupConfigError("OS version not supported")
        return supported

    # Read the config file in json format
    def __parse_config_file(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        config = {}
        # Identify the config file and read in the data
        if 'PBS_HOOK_CONFIG_FILE' in os.environ:
            config_file = os.environ["PBS_HOOK_CONFIG_FILE"]
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Config file is %s" %
                       (caller_name(), config_file))
            try:
                config = json.load(open(config_file, 'r'),
                                   object_hook=decode_dict)
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
        else:
            raise CgroupConfigError("No configuration file present")
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Initial Cgroup Config: %s" %
                   (caller_name(), config))
        # Set some defaults if they are not present
        if 'cgroup_prefix' not in config.keys():
            config['cgroup_prefix'] = 'pbspro'
        if 'periodic_resc_update' not in config.keys():
            config['periodic_resc_update'] = False
        if 'vnode_per_numa_node' not in config.keys():
            config['vnode_per_numa_node'] = False
        if 'online_offlined_nodes' not in config.keys():
            config['online_offlined_nodes'] = False
        if 'exclude_hosts' not in config.keys():
            config['exclude_hosts'] = []
        if 'run_only_on_hosts' not in config.keys():
            config['run_only_on_hosts'] = []
        if 'exclude_vntypes' not in config.keys():
            config['exclude_vntypes'] = []
        if 'cgroup' not in config.keys():
            config['cgroup'] = {}
        else:
            for subsys in config['cgroup']:
                subsystem = config['cgroup'][subsys]
                if 'enabled' not in subsystem.keys():
                    config['cgroup'][subsys]['enabled'] = False
                if 'exclude_hosts' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_hosts'] = []
                if 'exclude_vntypes' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_vntypes'] = []

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Cgroup Config with defaults: %s" %
                   (caller_name(), config))
        return config

    # Determine which subsystems are being requested
    def __target_subsystems(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        subsystems = []
        for key in self.cfg['cgroup'].keys():
            if self.enabled(key):
                subsystems.append(key)
        if 'memory' not in subsystems:
            if 'memsw' in subsystems:
                # Remove memsw since it must be greater then
                # or equal to memory
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing memsw from enabled subsystems")
                subsystems.remove('memsw')
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Enabled subsystems: %s" %
                   (caller_name(), subsystems))
        # It is not an error for all subsystems to be disabled.
        # This host or vnode type may be in the excluded list.
        return subsystems

    # Copy a setting from the parent cgroup
    def __copy_from_parent(self, dest):
        try:
            file = os.path.basename(dest)
            dir = os.path.dirname(dest)
            parent = os.path.dirname(dir)
            source = os.path.join(parent, file)
            if not os.path.isfile(source):
                raise CgroupConfigError('Failed to read %s' % (source))
            self.write_value(dest, open(source, 'r').read().strip())
        except:
            raise

    # Create a dictionary of the cgroup subsystems and their corresponding
    # directories
    def __set_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        paths = {}
        try:
            # Loop through the mounts and collect the ones for cgroups
            with open(os.path.join(os.sep, "proc", "mounts"), 'r') as fd:
                for line in fd:
                    entries = line.split()
                    if len(entries) > 3 and entries[2] == "cgroup":
                        flags = entries[3].split(',')
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "subsysdir: %s (flags=%s)" %
                                   (entries[1], flags))
                        for subsys in flags:
                            paths[subsys] = os.path.join(
                                entries[1], self.cfg["cgroup_prefix"])
        except:
            raise
        if len(paths.keys()) < 1:
            raise CgroupConfigError("Cgroup paths not detected")
        # Both the memory and memsw cgroups share the same mount point
        if "memory" in paths.keys():
            paths["memsw"] = paths["memory"]
        return paths

    # Create the cgroup parent directories that will contain the jobs
    def create_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Create the directories that PBS will use to house the jobs
            old_umask = os.umask(0022)
            for subsys in self.subsystems:
                if subsys not in self.paths.keys():
                    raise CgroupConfigError('No path for subsystem: %s' %
                                            (subsys))
                if not os.path.exists(self.paths[subsys]):
                    os.makedirs(self.paths[subsys], 0755)
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Created directory %s" %
                               (caller_name(), self.paths[subsys]))
                    if subsys == 'memory' or subsys == 'memsw':
                        # Set file to
                        # <cgroup>/memory/pbspro/memory.use_hierarchy
                        file = os.path.join(self.paths[subsys],
                                            'memory.use_hierarchy')
                        if not os.path.isfile(file):
                            raise CgroupConfigError('Failed to configure %s' %
                                                    (file))
                        self.write_value(file, 1)
                    elif subsys == 'cpuset':
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.cpus'))
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.mems'))

        except:
            raise CgroupConfigError("Failed to create directory: %s" %
                                    (self.paths[subsys]))
        finally:
            os.umask(old_umask)

    # Return the vnode type of the local node
    def __get_vnode_type(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")

        # File to check for if it is not defined in the hook input file
        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'

        # self.vnode is None for pbs_attach events
        pbs.logmsg(pbs.EVENT_DEBUG3, "vnode: %s" % self.vnode)
        if self.vnode is not None:
            if 'vntype' in self.vnode.resources_available and \
                    self.vnode.resources_available['vntype'] is not None:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "vntype: %s" %
                           self.vnode.resources_available['vntype'])
                return self.vnode.resources_available['vntype']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3, "vntype file: %s" % vntype_file)
                if os.path.isfile(vntype_file):
                    fdata = open(vntype_file).readlines()
                    vntype = fdata[0].strip()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
                    return vntype
        # If vntype was not set then log a message. It is too expensive
        # to have all moms query the server for large jobs.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: resources_available.vntype is " % caller_name() +
                   "not set for this vnode")
        return None

    # Gather resources that are already assigned
    def gather_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        assigned_resources = {}

        # Gather the cpus and memory sockets assigned
        directories = [x[0] for x in os.walk(self.paths['cpuset'])]

        # Add the existing cgroups to a list to check if assigning
        # resources fail
        for dir in directories[1:]:
            if dir.find(pbs.event().job.id) == -1:
                self.existing_cgroups[dir] = []

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'cpuset' in self.subsystems and \
                self.cfg['cgroup']['cpuset']['enabled']:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather cpuset resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['cpuset'], tmp_dir)
                    with open(path+os.sep+'cpuset.cpus') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.cpus'] = \
                            cpus2list(tmp_val.strip())
                    with open(path+os.sep+'cpuset.mems') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.mems'] = \
                            cpus2list(tmp_val.strip())

        # Check to see if the subsystem is enabled before trying
        # to gather resources
        if 'memory' in self.subsystems and \
                self.cfg['cgroup']['memory']['enabled']:
            # Gather the memory assigned for each socket
            directories = [x[0] for x in os.walk(self.paths['memory'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather memory resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['memory'], tmp_dir)
                    with open(path+os.sep+'memory.limit_in_bytes') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memory.limit'] = \
                            tmp_val.strip()
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    if os.path.isfile(tmp_filename) is False:
                        continue
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    with open(tmp_filename) as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memsw.limit'] = \
                            tmp_val.strip()

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'devices' in self.subsystems and \
                self.cfg['cgroup']['devices']['enabled']:
            # Gather the devices assigned
            directories = [x[0] for x in os.walk(self.paths['devices'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather device resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    pbs.logmsg(pbs.EVENT_DEBUG3, "Dir: %s" % tmp_dir)
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['devices'], tmp_dir)
                    with open(path+os.sep+'devices.list') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['devices.list'] = \
                            tmp_val.strip().split('\n')
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Dir: %s\n\tValue: %s" %
                                   (path, tmp_val.replace('\n', ',')))

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Gathered assigned resources: %s" % assigned_resources)
        self.assigned_resources = assigned_resources

    # Return whether a subsystem is enabled
    def enabled(self, subsystem):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check whether the subsystem is enabled in the configuration file
        if subsystem not in self.cfg['cgroup'].keys():
            return False
        if 'enabled' not in self.cfg['cgroup'][subsystem].keys():
            return False
        if not self.cfg['cgroup'][subsystem]['enabled']:
            return False
        # Check whether the cgroup is mounted for this subsystem
        if subsystem not in self.paths.keys():
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup not mounted for %s" %
                       (caller_name(), subsystem))
            return False
        # Check whether this host is excluded
        # if self.hostname in self.cfg['exclude_hosts']:
        #    pbs.logmsg(pbs.EVENT_DEBUG, "%s: cgroup excluded on host %s" %
        #               (caller_name(), self.hostname))
        #    pbs.event().accept()
        if self.hostname in self.cfg['cgroup'][subsystem]['exclude_hosts']:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup excluded for subsystem %s on host %s" %
                       (caller_name(), subsystem, self.hostname))
            return False
        # Check whether the vnode type is excluded
        if self.vntype is not None:
            # if self.vntype in self.cfg['exclude_vntypes']:
            #    pbs.logmsg(pbs.EVENT_DEBUG,
            #               "%s: cgroup excluded on vnode type %s" %
            #               (caller_name(), self.vntype))
            #    pbs.event().accept()
            if self.vntype in self.cfg['cgroup'][subsystem]['exclude_vntypes']:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           ("%s: cgroup excluded for " +
                            "subsystem %s on vnode type %s") %
                           (caller_name(), subsystem, self.vntype))
                return False
        return True

    # Return the default value for a subsystem
    def default(self, subsystem):
        if subsystem in self.cfg['cgroup']:
            if 'default' in self.cfg['cgroup'][subsystem]:
                return self.cfg['cgroup'][subsystem]['default']
        return None

    # Check to see if the pid matches the job owners uid
    def is_pid_owned_by_job_owner(self, pid):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        try:
            proc_uid = os.stat('/proc/%d' % pid).st_uid
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       '/proc/%d uid:%d' % (pid, proc_uid))

            job_owner_uid = pwd.getpwnam(pbs.event().job.euser)[2]
            pbs.logmsg(pbs.EVENT_DEBUG3, "Job uid: %d" % job_owner_uid)

            if proc_uid == job_owner_uid:
                return True
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Proc uid: %d != Job owner:  %d" %
                           (proc_uid, job_owner_uid))
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG, "Unknown pid: %d" % pid)
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        # If we got to this point something did not match up
        return False

    # Add some number of PIDs to the cgroup tasks files for each subsystem
    def add_pids(self, pids, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # make pids a list
        if isinstance(pids, int):
            pids = [pids]

        if pbs.event().type == pbs.EXECJOB_LAUNCH:
            if 1 in pids:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "1 is not a valid pid to add")
                # Hold the job for further review
                e.reject("Hook tried to add pid 1 to the tasks file " +
                         "for job %s" % jobid)

        # check pids to make sure that they are owned by the job owner
        if not CRAY_12_BRANCH:
            pbs.logmsg(pbs.EVENT_DEBUG3, "Not in the Cray 12 Branch")
            pbs.logmsg(pbs.EVENT_DEBUG3, "check to see if execjob_attach")
            if pbs.event().type == pbs.EXECJOB_ATTACH:
                pbs.logmsg(pbs.EVENT_DEBUG3, "event type: attach or launch")
                tmp_pids = list()
                for p in pids:
                    if self.is_pid_owned_by_job_owner(p):
                        tmp_pids.append(p)
                if len(tmp_pids) == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%d is not a valid pids to add" % p)
                    return False
                else:
                    pids = tmp_pids

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: subsys = %s" %
                       (caller_name(), subsys))
            # memsw and memory use the same tasks file
            if subsys == "memsw":
                if "memory" in self.subsystems:
                    continue
            path = os.path.join(self.paths[subsys], jobid)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path = %s" %
                       (caller_name(), path))
            try:
                tasks_path = os.path.join(path, "tasks")
                for p in pids:
                    self.write_value(tasks_path, p, 'a')
            except IOError, exc:
                raise CgroupLimitError("Failed to add PIDs %s to %s (%s)" %
                                       (str(pids), tasks_path,
                                        errno.errorcode[exc.errno]))
            except:
                raise

    # Set a node limit
    def set_node_limit(self, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s = %s" %
                   (caller_name(), resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'],
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Node resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    def setup_job_devices_env(self):
        """ Setup the job environment for the devices assigned to the job for an
            execjob_launch hook
         """
        if 'devices_name' in self.host_assigned_resources:
            names = self.host_assigned_resources['devices_name']
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "devices: %s" % (names))
            offload_devices = list()
            cuda_visible_devices = list()
            for name in names:
                if name.startswith('mic'):
                    offload_devices.append(name[3:])
                elif name.startswith('nvidia'):
                    cuda_visible_devices.append(name[6:])
            if len(offload_devices) > 0:
                value = '"%s"' % string.join(offload_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['OFFLOAD_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "offload_devices: %s" % offload_devices)
            if len(cuda_visible_devices) > 0:
                value = '"%s"' % string.join(cuda_visible_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['CUDA_VISIBLE_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "cuda_visible_devices: %s" % cuda_visible_devices)
            return [offload_devices, cuda_visible_devices]
        else:
            return False

    def setup_subsys_devices(self, path, node):
        subsys = 'devices'
        # Add the devices that the user is allowed to use
        if subsys in self.cfg['cgroup']:
            devices_allowed = open(os.path.join(path,
                                   "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Initial devices.list: %s" %
                       devices_allowed)
            # Deny access to mic and gpu devices
            devices_list = list()
            devices = node.devices
            for device in devices:
                if device == 'mic' or device == 'gpu':
                    for name in devices[device]:
                        d = devices[device][name]
                        devices_list.append("%d:%d" % (d['major'], d['minor']))
            # For CentOS 7 we need to remove a *:* rwm from devices.list we
            # before can add anything to devices.allow. Otherwise our changes
            # are ignored. Check to see if a *:* rwm is in devices.list.
            # If so remove it
            if "a *:* rwm\n" in devices_allowed:
                value = "a *:* rwm"
                self.write_value(os.path.join(path, 'devices.deny'), value)
            else:
                # Verify that the following devices are not in devices.list
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing access to the following: %s" %
                           devices_list)
            for device_str in devices_list:
                value = "c %s rwm" % device_str
                self.write_value(os.path.join(path, 'devices.deny'), value)
            # Add devices back to the list
            devices_allow = list()
            if 'allow' in self.cfg['cgroup'][subsys]:
                devices_allow = self.cfg['cgroup'][subsys]['allow']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Allowing access to the following: %s" %
                           devices_allow)
                for item in devices_allow:
                    if isinstance(item, str):
                        pbs.logmsg(pbs.EVENT_DEBUG3, "string item: %s" % item)
                        value = "%s" % item
                        self.write_value(os.path.join(
                                         path, 'devices.allow'), value)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "write_value: %s" % value)
                    elif isinstance(item, list):
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "list item: %s" % item)
                        try:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Device allow: %s" % item)
                            stat_filename = '/dev/%s' % item[0]
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Stat file: %s" % stat_filename)
                            s = os.stat(stat_filename)
                            device_type = None
                            if S_ISBLK(s.st_mode):
                                device_type = "b"
                            elif S_ISCHR(s.st_mode):
                                device_type = "c"
                            if device_type is not None:
                                if len(item) == 3 and isinstance(item[2], str):
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % item[2]
                                    value += "%s" % item[1]
                                else:
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % os.minor(s.st_rdev)
                                    value += "%s" % item[1]
                                self.write_value(os.path.join(
                                                path, 'devices.allow'), value)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "write_value: %s" % value)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find /dev/%s. " % item[0] +
                                       "Not added to devices.allow!")
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                    else:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Not sure what to do with: %s" % item)
            devices_allowed = open(os.path.join(
                                   path, "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "After setup devices.list: %s" % devices_allowed)

    # Select devices to assign to the job
    def assign_devices(
                       self,
                       device_type,
                       device_list,
                       number_of_devices,
                       node):
        devices = device_list[:number_of_devices]
        device_ids = list()
        device_names = list()
        for device in devices:
            device_info = node.devices[device_type][device]
            if len(device_ids) == 0:
                if device.find("mic") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:0 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                    device_ids.append("%s %d:1 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                elif device.find("nvidia") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:255 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
            device_ids.append("%s %d:%d rwm" %
                              (device_info['device_type'],
                               device_info['major'],
                               device_info['minor']))
            device_names.append(device)
        return device_names, device_ids

    # Find the device name
    def get_device_name(self, node, available, socket, major, minor):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Get device name: major: %s, minor: %s" % (major, minor))
        avail_device = None
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Possible devices: %s" % (available[socket]['devices']))
        for avail_device in available[socket]['devices']:
            avail_major = None
            avail_minor = None
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Checking device: %s" % (avail_device))
            if avail_device.find('mic') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check mic device: %s" % (avail_device))
                avail_major = node.devices['mic'][avail_device]['major']
                avail_minor = node.devices['mic'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            elif avail_device.find('nvidia') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check gpu device: %s" % (avail_device))
                avail_major = node.devices['gpu'][avail_device]['major']
                avail_minor = node.devices['gpu'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            if int(avail_major) == int(major) and \
                    int(avail_minor) == int(minor):
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device match: name: %s, major: %s, minor: %s" %
                           (avail_device, major, minor))
                return avail_device
        pbs.logmsg(pbs.EVENT_DEBUG3, "No match found")
        return None

    # Assign resources to the job based off the vnodes assignments
    def assign_job_resources_by_vnode(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # Loop through the vnodes and assign resources
        assigned = dict()
        assigned['cpuset.cpus'] = list()
        assigned['cpuset.mems'] = list()
        room_on_socket = True

        for vnode in resources['vnodes']:
            regex = re.compile(".*\[(\d+)\].*")
            socket = int(regex.search(vnode).group(1))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current Vnode: %s" % vnode)
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current socket: %d" % socket)

            if 'ncpus' in resources['vnodes'][vnode]:
                tmp_ncpus = int(resources['vnodes'][vnode]['ncpus'])
                if int(resources['vnodes'][vnode]['ncpus']) <= \
                        len(available[socket]['cpus']):
                    assigned['cpuset.cpus'] += \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] += [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_ncpus: %s" % (tmp_ncpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "available[%d]: %s" %
                               (socket, available[socket]))
                    room_on_socket = False
            if ('nmics' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['nmics']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(mic).*")
                tmp_nmics = int(resources['vnodes'][vnode]['nmics'])
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['nmics']) > 0 and
                   int(resources['vnodes'][vnode]['nmics']) <= len(mics)):
                    tmp_names, tmp_devices = self.assign_devices(
                             'mic', mics[:tmp_nmics], tmp_nmics, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_nmics: %s" % (tmp_nmics))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "mics: %s" % (mics))
                    room_on_socket = False
            if ('ngpus' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['ngpus']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(nvidia).*")
                tmp_ngpus = int(resources['vnodes'][vnode]['ngpus'])
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['ngpus']) > 0 and
                   int(resources['vnodes'][vnode]['ngpus']) <= len(gpus)):
                    tmp_names, tmp_devices = self.assign_devices(
                        'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "tmp_ngpus: %s" % (tmp_ngpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "gpus: %s" % (gpus))
                    room_on_socket = False

        if room_on_socket:
            assigned['cpuset.cpus'].sort()
            assigned['cpuset.mems'].sort()
            if 'devices' in assigned:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids pre set and sort: %s" %
                           (assigned['devices']))
                assigned['devices'] = list(set(assigned['devices']))
                assigned['devices'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids post set and sort: %s" %
                           (assigned['devices']))
                assigned['devices_name'] = list(set(assigned['devices_name']))
                assigned['devices_name'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

    # Assign resources to the job
    def assign_job_resources(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # What would we need to do different for a large smp (UV) system?
        # See if requested resources can fit on a single socket
        assigned = dict()
        pbs.logmsg(pbs.EVENT_DEBUG3, "Requested: %s" % (resources))

        # Determine the order that we should evaluate the sockets.
        socket_list = list()
        if 'placement_type' in self.cfg:
            if self.cfg['placement_type'] == 'round_robin':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Requested round_robin placement")
                # Look at assigned_resources and determine which socket
                # to start with
                jobs_per_socket_cnt = dict()
                for socket in available.keys():
                    jobs_per_socket_cnt[socket] = 0
                for job in self.assigned_resources:
                    if 'cpuset.mems' in self.assigned_resources[job]:
                        for socket in \
                                self.assigned_resources[job]['cpuset.mems']:
                            jobs_per_socket_cnt[socket] += 1

                sorted_dict = sorted(jobs_per_socket_cnt.items(),
                                     key=operator.itemgetter(1))
                for x in sorted_dict:
                    socket_list.append(x[0])
        else:
            socket_list = available.keys()

        # Loop through the socket list and determine if the job
        # can fit on the socket
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Evaluate sockets in the following order: %s" %
                   (socket_list))
        for socket in socket_list:
            room_on_socket = True
            if 'ncpus' in resources:
                if int(resources['ncpus']) <= len(available[socket]['cpus']):
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Requested: %s, Available: %s" %
                               (resources['ncpus'], available[socket]['cpus']))
                    tmp_ncpus = int(resources['ncpus'])
                    assigned['cpuset.cpus'] = \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] = [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient ncpus on socket %d: R:%d,A:%s" %
                               (socket, resources['ncpus'],
                                available[socket]['cpus']))
                    room_on_socket = False
            # Check to see that nmics has been requested and not equal to 0
            if 'nmics' in resources and int(resources['nmics']) > 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'nmics' in resources and int(resources['nmics']) > 0 \
                        and int(resources['nmics']) <= len(mics):
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient nmics on socket %d: R:%s,A:%s" %
                               (socket, resources['nmics'], mics))
                    room_on_socket = False
            # Check to see that ngpus has been requested and not equal to 0
            if 'ngpus' in resources and int(resources['ngpus']) > 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'ngpus' in resources and int(resources['ngpus']) > 0 \
                        and int(resources['ngpus']) <= len(gpus):
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient gpus on socket %d: R:%s,A:%s" %
                               (socket, resources['ngpus'], gpus))
                    room_on_socket = False
            if 'vmem' in resources:
                if size_as_int(resources['vmem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient vmem on socket %d: R:%s,A:%s" %
                               (socket, resources['vmem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if 'mem' in resources:
                if size_as_int(resources['mem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient memory on socket %d: R:%s,A:%s" %
                               (socket, resources['mem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if room_on_socket:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
                self.host_assigned_resources = assigned
                return assigned
        # If we made it here then the requested resources did not fit
        # on a single socket
        # Future: take distance into account when selecting sockets?
        # Combine available resources into a single list
        # This should work for a dual socket machine.
        # Needs additional work for machines with more than 2 sockets
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Unable to find requested resources: %s" % resources +
                   " on a socket. Checking the entire node")
        available_copy = copy.deepcopy(available)
        available_on_node = dict()
        socket_keys = available.keys()
        socket_keys.sort()
        available_on_node['sockets'] = socket_keys
        for socket in socket_keys:
            for key in available_copy[socket].keys():
                if isinstance(available[socket][key], int):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]
                if isinstance(available_copy[socket][key], str):
                    try:
                        if key in available_on_node is False:
                            available_on_node[key] = \
                                size_as_int(available_copy[socket][key])
                        else:
                            available_on_node[key] += \
                                size_as_int(available_copy[socket][key])
                    except:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Unable to convert to int: %s" %
                                   (available_copy[socket][key]))

                if isinstance(available_copy[socket][key], list):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available on node: %s" % (available_on_node))
        assigned = dict()
        room_on_node = False
        if 'ncpus' in resources:
            if int(resources['ncpus']) <= len(available_on_node['cpus']):
                room_on_node = True
                tmp_ncpus = int(resources['ncpus'])
                assigned['cpuset.cpus'] = available_on_node['cpus'][:tmp_ncpus]
                assigned['cpuset.mems'] = available_on_node['sockets']
            if 'nmics' in resources and int(resources['nmics']) != 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['nmics']) <= len(mics) and \
                        int(resources['nmics']) > 0:
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
            if 'ngpus' in resources and int(resources['ngpus']) != 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['ngpus']) <= len(gpus) and \
                        int(resources['ngpus']) > 0:
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
        if room_on_node:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

        # Consolidate resources if needed to place the job

    # Determine which resources are available
    def available_node_resources(self, node):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))

        available = copy.deepcopy(node.numa_nodes)
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Keys: %s" % (available[0].keys()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        for socket in available.keys():
            if 'cpus' in available[socket]:
                # Find the physical cores
                if available[socket]['cpus'].find('-') != -1 and \
                        node.hyperthreading:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Splitting %s at ','" %
                               (available[socket]['cpus']))
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'].split(',')[0])
                else:
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'])
            if 'MemTotal' in available[socket]:
                # Find the memory on the socket in bites.
                # Remove the 'b' to simply math
                available[socket]['memory'] = size_as_int(
                    available[socket]['MemTotal'])

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Pre device add: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (node.devices.keys()))
        for device in node.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" %
                       (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Devices: %s" %
                           (node.devices[device].keys()))
                for device_name in node.devices[device].keys():
                    device_socket = \
                        node.devices[device][device_name]['numa_node']
                    if 'devices' not in available[device_socket]:
                        available[device_socket]['devices'] = list()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Device: %s, Socket: %s" %
                               (device, device_socket))
                    available[device_socket]['devices'].append(device_name)
            # pbs.logmsg(pbs.EVENT_DEBUG3,
            #            "Available on socket %s: %s" %
            #            (socket,available[socket]))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))

        # Remove all of the resources that are assigned to other jobs
        for job in self.assigned_resources.keys():
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            if (not job_to_be_ignored(job)):
                cpus = list()
                sockets = list()
                devices = list()
                memory = 0
                if 'cpuset.cpus' in self.assigned_resources[job]:
                    cpus = self.assigned_resources[job]['cpuset.cpus']
                if 'cpuset.mems' in self.assigned_resources[job]:
                    sockets = self.assigned_resources[job]['cpuset.mems']
                if 'devices.list' in self.assigned_resources[job]:
                    devices = self.assigned_resources[job]['devices.list']
                if 'memory.limit' in self.assigned_resources[job]:
                    memory = size_as_int(
                                self.assigned_resources[job]['memory.limit'])

                # Loop through the sockets and remove cpus that are
                # assigned to other cgroups
                for socket in sockets:
                    for cpu in cpus:
                        try:
                            available[socket]['cpus'].remove(cpu)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %d from %s" %
                                       (cpu, available[socket]['cpus']))

                if len(sockets) == 1:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Decrementing memory: %d by %d" %
                               (size_as_int(available[sockets[0]]['memory']),
                                memory))
                    available[sockets[0]]['memory'] -= memory

                # Loop throught the available sockets
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned device to %s: %s" % (job, devices))
                for socket in available.keys():
                    for device in devices:
                        try:
                            # loop through know devices and see if they match
                            if len(available[socket]['devices']) != 0:
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Check device: %s" % (device))
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Available device: %s" %
                                           (available[socket]['devices']))
                                major, minor = device.split()[1].split(':')
                                avail_device = self.get_device_name(
                                                   node, available, socket,
                                                   major, minor)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Returned device: %s" %
                                           (avail_device))
                                if avail_device is not None:
                                    pbs.logmsg(pbs.EVENT_DEBUG3,
                                               "socket: %d,\t" % socket +
                                               "devices: %s,\t" %
                                               available[socket]['devices'] +
                                               "device to remove: %s" %
                                               (avail_device))
                                    available[socket]['devices'].remove(
                                        avail_device)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %s from %s" %
                                       (device, available[socket]['devices']))
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Job %s res not removed from host " % job +
                           "available res: suspended job")
        pbs.logmsg(pbs.EVENT_DEBUG, "Available resources: %s" % (available))
        return available

    # Set a job limit
    def set_job_limit(self, jobid, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s: %s = %s" %
                   (caller_name(), jobid, resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'softmem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.soft_limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     jobid, 'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'], jobid,
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            elif resource == 'ncpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = self.select_cpus(path, value)
                    if cpus is None:
                        raise CgroupLimitError("Failed to configure cpuset.")
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
                    self.__copy_from_parent(os.path.join(self.paths['cpuset'],
                                            jobid, 'cpuset.mems'))
            elif resource == 'cpuset.cpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = value
                    if cpus is None:
                        msg = "Failed to configure cpus in cpuset."
                        raise CgroupLimitError(msg)
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
            elif resource == 'cpuset.mems':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.mems')
                    mems = value
                    if mems is None:
                        msg = "Failed to configure mems in cpuset."
                        raise CgroupLimitError(msg)
                    mems = ','.join(map(str, mems))
                    self.write_value(path, mems)
            elif resource == 'devices':
                if 'devices' in self.subsystems and \
                        self.cfg['cgroup']['devices']['enabled']:

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.allow')
                    devices = value
                    if devices is None:
                        msg = "Failed to configure device(s)."
                        raise CgroupLimitError(msg)
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Setting devices: %s for %s" % (devices, jobid))
                    for device in devices:
                        self.write_value(path, device)

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.list')
                    output = open(path, 'r').read()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "devices.list: %s" % output.replace("\n", ","))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    # Update resource usage for a job
    def update_job_usage(self, jobid, resc_used):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resc_used = %s" %
                   (caller_name(), str(resc_used)))
        # Sort the subsystems so that we consistently look at the subsystems
        # in the same order every time
        self.subsystems.sort()
        for subsys in self.subsystems:
            path = os.path.join(self.paths[subsys], jobid)
            if subsys == "memory":
                max_mem = self.__get_max_mem_usage(path)
                if max_mem is None:
                    pbs.logjobmsg(jobid, "%s: No max mem data" %
                                  (caller_name()))
                else:
                    resc_used['mem'] = pbs.size(convert_size(max_mem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: mem=%s" %
                                  (caller_name(), resc_used['mem']))
                mem_failcnt = self.__get_mem_failcnt(path)
                if mem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No mem fail count data" %
                                  (caller_name()))
                else:
                    # Check to see if the job exceeded its resource limits
                    if mem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memory limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "memsw":
                max_vmem = self.__get_max_memsw_usage(path)
                if max_vmem is None:
                    pbs.logjobmsg(jobid, "%s: No max vmem data" %
                                  (caller_name()))
                else:
                    resc_used['vmem'] = pbs.size(convert_size(max_vmem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: vmem=%s" %
                                  (caller_name(), resc_used['vmem']))

                vmem_failcnt = self.__get_memsw_failcnt(path)
                if vmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No vmem fail count data" %
                                  (caller_name()))
                else:
                    pbs.logjobmsg(jobid, "%s: vmem fail count: %d " %
                                  (caller_name(), vmem_failcnt))
                    if vmem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memsw limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "hugetlb":
                max_hpmem = self.__get_max_hugetlb_usage(path)
                if max_hpmem is None:
                    pbs.logjobmsg(jobid, "%s: No max hpmem data" %
                                  (caller_name()))
                    return
                hpmem_failcnt = self.__get_hugetlb_failcnt(path)
                if hpmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No hpmem fail count data" %
                                  (caller_name()))
                    return
                if hpmem_failcnt > 0:
                    err_msg = self.__get_error_msg(jobid)
                    pbs.logjobmsg(jobid, "Cgroup hugetlb limit exceeded: %s" %
                                  (err_msg))
                resc_used['hpmem'] = pbs.size(convert_size(max_hpmem, 'kb'))
                pbs.logjobmsg(jobid, "%s: Hugepage usage: %s" %
                              (caller_name(), resc_used['hpmem']))
            elif subsys == "cpuacct":
                cpu_usage = self.__get_cpu_usage(path)
                if cpu_usage is None:
                    pbs.logjobmsg(jobid, "%s: No CPU usage data" %
                                  (caller_name()))
                    return
                cpu_usage = convert_time(str(cpu_usage) + "ns")
                pbs.logjobmsg(jobid, "%s: CPU usage: %.3lf secs" %
                              (caller_name(), cpu_usage))
                resc_used['cput'] = pbs.duration(cpu_usage)

    # Creates the cgroup if it doesn't exists
    def create_job(self, jobid, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Iterate over the subsystems required
        for subsys in self.subsystems:
            # Create a directory for the job
            try:
                old_umask = os.umask(0022)
                path = self.paths[subsys]
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                path = os.path.join(self.paths[subsys], jobid)
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                if subsys == 'devices':
                    self.setup_subsys_devices(path, node)

            except OSError, exc:
                raise CgroupConfigError("Failed to create directory: %s (%s)" %
                                        (path, errno.errorcode[exc.errno]))
            except:
                raise
            finally:
                os.umask(old_umask)

    # Determine the cgroup limits and configure the cgroups
    def configure_job(self, jobid, hostresc, node):
        mem_enabled = 'memory' in self.subsystems
        vmem_enabled = 'memsw' in self.subsystems
        if mem_enabled or vmem_enabled:
            # Initialize mem variables
            mem_avail = node.get_memory_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "mem_avail %s" % mem_avail)
            mem_requested = None
            if 'mem' in hostresc.keys():
                mem_requested = convert_size(hostresc['mem'], 'kb')
            mem_default = None
            if mem_enabled:
                mem_default = self.default('memory')
            # Initialize vmem variables
            vmem_avail = node.get_vmem_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "vmem_avail %s" % vmem_avail)
            vmem_requested = None
            if 'vmem' in hostresc.keys():
                vmem_requested = convert_size(hostresc['vmem'], 'kb')
            vmem_default = None
            if vmem_enabled:
                vmem_default = self.default('memsw')
            # Initialize softmem variables
            if 'soft_limit' in self.cfg['cgroup']['memory'].keys():
                softmem_enabled = self.cfg['cgroup']['memory']['soft_limit']
            else:
                softmem_enabled = False
            # Sanity check
            if size_as_int(mem_avail) > size_as_int(vmem_avail):
                if size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                    raise CgroupLimitError(
                        'mem available (%s) exceeds vmem available (%s)' %
                        (mem_avail, vmem_avail))
            # Determine the mem limit
            if mem_requested is not None:
                # mem requested may not exceed available
                if size_as_int(mem_requested) > size_as_int(mem_avail):
                    raise JobValueError(
                        'mem requested (%s) exceeds mem available (%s)' %
                        (mem_requested, mem_avail))
                mem_limit = mem_requested
            else:
                # mem was not requested
                if mem_default is None:
                    mem_limit = mem_avail
                else:
                    mem_limit = mem_default
            # Determine the vmem limit
            if vmem_requested is not None:
                # vmem requested may not exceed available
                if size_as_int(vmem_requested) > size_as_int(vmem_avail):
                    raise JobValueError(
                        'vmem requested (%s) exceeds vmem available (%s)' %
                        (vmem_requested, vmem_avail))
                vmem_limit = vmem_requested
            else:
                # vmem was not requested
                if vmem_default is None:
                    vmem_limit = vmem_avail
                else:
                    vmem_limit = vmem_default
            # Ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Adjust for soft limits if enabled
            if mem_enabled and softmem_enabled:
                softmem_limit = mem_limit
                # The hard memory limit is assigned the lesser of the vmem
                # limit and available memory
                if size_as_int(vmem_limit) < size_as_int(mem_avail):
                    mem_limit = vmem_limit
                else:
                    mem_limit = mem_avail
            # Again, ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Sanity checks when both memory and memsw are enabled
            if mem_enabled and vmem_enabled:
                if vmem_requested is not None:
                    if size_as_int(vmem_limit) > size_as_int(vmem_requested):
                        # The user requested an invalid limit
                        raise JobValueError(
                            'vmem limit (%s) exceeds vmem requested (%s)' %
                            (vmem_limit, vmem_requested))
                if size_as_int(vmem_limit) > size_as_int(mem_limit):
                    # This job may utilize swap
                    if size_as_int(vmem_avail) <= size_as_int(mem_avail) and \
                      size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                        # No swap available
                        raise CgroupLimitError(
                            ('Job might utilize swap ' +
                             'and no swap space available'))
            # Assign mem and vmem
            if mem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: mem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), mem_limit))
                hostresc['mem'] = pbs.size(mem_limit)
                if softmem_enabled:
                    hostresc['softmem'] = pbs.size(softmem_limit)
            if vmem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: vmem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), vmem_limit))
                hostresc['vmem'] = pbs.size(vmem_limit)
        # Initialize hpmem variables
        hpmem_enabled = 'hugetlb' in self.subsystems
        if hpmem_enabled:
            hpmem_avail = node.get_hpmem_on_node(self.cfg)
            hpmem_limit = None
            hpmem_default = self.default('hugetlb')
            if hpmem_default is None:
                hpmem_default = hpmem_avail
            if 'hpmem' in hostresc.keys():
                hpmem_limit = convert_size(hostresc['hpmem'], 'kb')
            else:
                hpmem_limit = hpmem_default
            # Assign hpmem
            if size_as_int(hpmem_limit) > size_as_int(hpmem_avail):
                raise JobValueError('hpmem limit (%s) exceeds available (%s)' %
                                    (hpmem_limit, hpmem_avail))
            hostresc['hpmem'] = pbs.size(hpmem_limit)
        # Initialize cpuset variables
        cpuset_enabled = 'cpuset' in self.subsystems
        if cpuset_enabled:
            cpu_limit = 1
            if 'ncpus' in hostresc.keys():
                cpu_limit = hostresc['ncpus']
            if cpu_limit < 1:
                cpu_limit = 1
            hostresc['ncpus'] = pbs.pbs_int(cpu_limit)

        # Find the available resources and assign the right ones to the job

        attempt = 0
        assign_resources = False

        # Two attempts since self.cleanup_orphans may actually fix the
        # problem we see in a first attempt
        while attempt < 2:
            attempt += 1
            available_resources = self.available_node_resources(node)
            if 'vnodes' in hostresc:
                assign_resources = self.assign_job_resources_by_vnode(
                    hostresc, available_resources, node)
            else:
                assign_resources = self.assign_job_resources(
                    hostresc, available_resources, node)

            if (assign_resources is False) and (attempt < 2):
                # No resources were assigned to the job.
                # Most likely cause was that a cgroup has
                # not been cleaned up yet
                pbs.logmsg(pbs.EVENT_DEBUG, "Failed to assign resources. " +
                           "Checking the following cgroups on the node " +
                           "with the server: %s" % self.existing_cgroups)

                # Collect the jobs on the node (try reading mom_priv/jobs,
                # if not successful ask server)
                jobs_list = node.gather_jobs_on_node_local()

                if jobs_list is not None:
                    # Don't clean up the cgroup we just made for the
                    # job just yet
                    if jobid not in jobs_list:
                        jobs_list.append(jobid)

                    self.cleanup_orphans(jobs_list)

                    # Recheck what is now assigned after things were cleaned up
                    self.gather_assigned_resources()

        if assign_resources is False:
            # Cleanup cgroups for jobs not present on this node
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Assign resources failed. Attempting to cleanup all " +
                       "leftover cgroups (including event job %s)" % jobid)

            # Remove the current job from the jobs list so it will be
            # cleaned up as well.
            jobs_list.remove(jobid)

            self.cleanup_orphans(jobs_list)

            # Rerun the job and log the message
            line = 'Unable to assign resources to job. '
            line += 'Will requeue the job and try again.'
            line += 'job run_count: %d' % pbs.event().job.run_count
            pbs.event().job.rerun()
            pbs.event().reject(line)

        # Print out the assigned resources
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Assigned resources: %s" % (assign_resources))

        if cpuset_enabled:
            hostresc.pop('ncpus', None)
            for key in ['cpuset.cpus', 'cpuset.mems']:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Initialize devices variables
        devices_enabled = 'devices' in self.subsystems
        if devices_enabled:
            key = 'devices'
            if key in assign_resources:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Apply the resource limits to the cgroups
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Setting cgroup limits for: %s" %
                   (caller_name(), hostresc))

        # The vmem limit must be set after the mem limit, so sort the keys
        for resc in sorted(hostresc.keys()):
            self.set_job_limit(jobid, resc, hostresc[resc])

    # Kill any processes contained within a tasks file
    def __kill_tasks(self, tasks_file):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        with open(tasks_file, 'r') as fd:
            for line in fd:
                os.kill(int(line.strip()), signal.SIGKILL)
        fd.close()
        count = 0
        with open(tasks_file, 'r') as fd:
            for line in fd:
                count += 1
                pid = line.strip()
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Pid: %s did not clean up" %
                           (caller_name(), pid))
                if os.path.isfile('/proc/%s/status' % pid):
                    tmp = open('/proc/%s/status' % pid).readlines()
                    tmp_line = ''
                    for entry in tmp:
                        if entry.find('Name:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('State:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('Uid:') != -1:
                            tmp_line += entry.strip()
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: %s" % (caller_name(), tmp_line))

        return count

    # Perform the actual removal of the cgroup directory
    # by default, only one attempt at killing tasks in cgroup, without sleeping
    # since this method could be called many times
    # (for N directories times M jobs)
    def __remove_cgroup(self, path, tasks_kill_attempts=1):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            if os.path.exists(path):
                tasks_file = os.path.join(path, 'tasks')
                task_cnt = self.__kill_tasks(tasks_file)
                kill_attempts = 0
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Task Kill Attempts: %d" % tasks_kill_attempts)
                while (task_cnt > 0) and (kill_attempts < tasks_kill_attempts):
                    kill_attempts += 1
                    count = 0

                    with open(tasks_file, 'r') as fd:
                        for line in fd:
                            count += 1
                        task_cnt = count

                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "Attempt: %d - cgroup still has %d tasks: %s" %
                               (kill_attempts, task_cnt, path))
                    if kill_attempts < tasks_kill_attempts:
                        time.sleep(1)
                        # Added since there are race conditions where tasks are
                        # being added at the same time we get the initial
                        # task count
                        tasks_file = os.path.join(path, 'tasks')
                        task_cnt = self.__kill_tasks(tasks_file)

                if task_cnt == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Removing directory %s" %
                               (caller_name(), path))
                    os.rmdir(path)
                    if os.path.exists(path):
                        time.sleep(.25)
                        if os.path.exists(path):
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "%s: 2nd Try Removing directory %s" %
                                       (caller_name(), path))
                            os.rmdir(path)
                            time.sleep(.25)
                        if os.path.exists(path):
                            return False
                else:
                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "cgroup still has %d tasks: %s" %
                               (task_cnt, path))

                    # Check to see if the offline file is already present
                    # on the mom
                    offline_file_exists = False
                    if os.path.isfile(self.offline_file):
                        msg = "Cgroup(s) not cleaning up but the node already "
                        msg += "has the offline file."

                        pbs.logmsg(pbs.EVENT_DEBUG, msg)
                    else:
                        # Check to see that the node is not already offline.
                        try:
                            tmp_state = pbs.server().vnode(self.hostname).state
                        except:
                            msg = "Unable to contact server for node state"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            tmp_state = None
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Current Node State: %d" % tmp_state)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Offline Node State: %d" % pbs.ND_OFFLINE)

                        if tmp_state == pbs.ND_OFFLINE:
                            msg = "Cgroup(s) not cleaning up but the node is "
                            msg += "already offline."
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                        elif tmp_state is not None:
                            # Offline the node(s)
                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                            msg = "Offlining node since cgroup(s) are not "
                            msg += "cleaning up"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            vnode = pbs.event().vnode_list[self.hostname]

                            vnode.state = pbs.ND_OFFLINE

                            # Write a file locally to reduce server traffic
                            # when it comes time to online the node
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Offline file: %s" % self.offline_file)
                            open(self.offline_file, 'a').close()
                            vnode.comment = self.offline_msg

                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                        else:
                            pass

                    return False

        except OSError, exc:
            pbs.logmsg(pbs.EVENT_SYSTEM,
                       "Failed to remove cgroup path: %s (%s)" %
                       (path, errno.errorcode[exc.errno]))
        except:
            pbs.logmsg(pbs.EVENT_SYSTEM, "Failed to remove cgroup path: %s" %
                       (path))

        return True

    # Removes cgroup directories that are not associated with a local job
    def cleanup_orphans(self, local_jobs):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        orphan_cnt = 0
        for key in self.paths.keys():
            for dir in glob.glob(os.path.join(self.paths[key], '[0-9]*')):
                jobid = os.path.basename(dir)
                if jobid not in local_jobs:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Removing orphaned cgroup: %s" %
                               (caller_name(), dir))
                    # default number of attempts at killing tasks
                    status = self.__remove_cgroup(dir)
                    if not status:
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "%s: Removing orphaned cgroup %s failed " %
                                   (caller_name(), dir))
                        orphan_cnt += 1
        return orphan_cnt

    # Removes the cgroup directories for a job
    def delete(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        tasks_kill_attempted = False

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            if not tasks_kill_attempted:
                # Make multiple attempts at killing tasks in cgroups on
                # first subsystem encountered since it is "normal" for
                # a cgroup.delete to encounter processes hard to kill
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                                           jobid),
                                              tasks_kill_attempts=(
                                              CGROUP_KILL_ATTEMPTS))
                tasks_kill_attempted = True
            else:
                # We tried getting rid of job processes earlier,
                # so don't try N times with sleeps
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                              jobid), tasks_kill_attempts=1)

            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Status: %s" %
                       (caller_name(), status))
            if status is False:
                # Rerun the job and log the message
                line = 'Unable to cleanup a cgroup. '
                line += 'Will offline the node and requeue the job '
                line += 'and try again. job run_count: %d' % \
                        pbs.event().job.run_count
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Failed to remove cgroup: %s" %
                           (caller_name(), line))
                pbs.event().accept()

    # Write a value to a limit file
    def write_value(self, file, value, mode='w'):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: writing %s to %s" %
                   (caller_name(), value, file))
        try:
            with open(file, mode) as fd:
                fd.write(str(value) + '\n')
        except IOError, exc:
            if exc.errno == errno.ENOENT:
                pbs.logmsg(pbs.EVENT_SYSTEM,
                           "Trying to set limit to unknown file %s" % file)
            elif exc.errno in [errno.EACCES, errno.EPERM]:
                pbs.logmsg(pbs.EVENT_SYSTEM, "Permission denied on file %s" %
                           file)
            elif exc.errno == errno.EBUSY:
                raise CgroupBusyError("Limit rejected")
            elif exc.errno == errno.EINVAL:
                raise CgroupLimitError("Invalid limit value: %s" % (value))
            else:
                raise
        except:
            raise

    # Return memory failcount
    def __get_mem_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.failcnt"), 'r').read().strip())
        except:
            return None

    # Return vmem failcount
    def __get_memsw_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.failcnt"), 'r').read().strip())
        except:
            return None

    # Return hpmem failcount
    def __get_hugetlb_failcnt(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.failcnt"))[0], 'r').read().strip())
        except:
            return None

    # Return the max usage of memory in bytes
    def __get_max_mem_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of memsw in bytes
    def __get_max_memsw_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of hugetlb in bytes
    def __get_max_hugetlb_usage(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.max_usage_in_bytes"))[0],
                            'r').read().strip())
        except:
            return None

    # Return the cpuacct.usage in cpu seconds
    def __get_cpu_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "cpuacct.usage"), 'r').read().strip())
        except:
            return None

    # Assign CPUs to the cpuset
    def select_cpus(self, path, ncpus):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path is %s" % (caller_name(), path))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: ncpus is %s" %
                   (caller_name(), ncpus))
        if ncpus < 1:
            ncpus = 1
        try:
            # Must select from those currently available
            cpufile = os.path.basename(path)
            base = os.path.dirname(path)
            parent = os.path.dirname(base)
            avail = cpus2list(open(os.path.join(parent, cpufile),
                              'r').read().strip())
            if len(avail) < 1:
                raise CgroupProcessingError("No CPUs avaialble in cgroup.")
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Available CPUs: %s" %
                       (caller_name(), avail))
            assigned = []
            for file in glob.glob(os.path.join(parent, '[0-9]*', cpufile)):
                cpus = cpus2list(open(file, 'r').read().strip())
                for id in cpus:
                    if id in avail:
                        avail.remove(id)
            if len(avail) < ncpus:
                raise CgroupProcessingError(
                    "Insufficient CPUs avaialble in cgroup.")
            if len(avail) == ncpus:
                return avail
            # FUTURE: Try to minimize NUMA nodes based on memory requirement
            return avail[0:ncpus]
        except:
            raise

    # Return the error message in system message file
    def __get_error_msg(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        proc = subprocess.Popen(['dmesg'],
                                shell=False, stdout=subprocess.PIPE)
        stdout = proc.communicate()[0].splitlines()
        stdout.reverse()
        # Check to see if the job id is found in dmesg otherwise
        # you could get the information for another job that was killed
        # the line will look like Memory cgroup stats for /pbspro/279.centos7
        kill_line = ""

        for line in stdout:
            start = line.find('Killed process ')
            if start >= 0:
                kill_line = line[start:]
            job_start = line.find(jobid)
            if job_start >= 0:
                # pbs.logmsg(pbs.EVENT_DEBUG3,
                #           "%s: Jobid: %s, Line: %s" %
                #           (caller_name(), jobid, line))
                return kill_line
        return ""

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_job_env_file(self, jobid, env_list):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        # if not os.path.exists(self.host_job_env_dir):
        #    os.makedirs(self.host_job_env_dir, 0755)

        # Write out assigned_resources
        try:
            lines = string.join(env_list, '\n')
            filename = self.host_job_env_filename % jobid
            outfile = open(filename, "w")
            outfile.write(lines)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" % (filename))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (lines))
            return True
        except:
            return False

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        if not os.path.exists(self.hook_storage_dir):
            os.makedirs(self.hook_storage_dir, 0755)

        # Write out assigned_resources
        try:
            json_str = json.dumps(self.host_assigned_resources)
            outfile = open(self.hook_storage_dir+os.sep+jobid, "w")
            outfile.write(json_str)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" %
                       (self.hook_storage_dir+os.sep+jobid))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (json_str))
            return True
        except:
            return False

    def read_in_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        pbs.logmsg(pbs.EVENT_DEBUG3, "Host assigned resources: %s" %
                   (self.host_assigned_resources))
        if os.path.isfile(self.hook_storage_dir+os.sep+jobid):
            # Write out assigned_resources
            try:
                infile = open(self.hook_storage_dir+os.sep+jobid, 'r')
                json_data = json.load(infile, object_hook=decode_dict)
                self.host_assigned_resources = json_data
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Host assigned resources: %s" %
                           (self.host_assigned_resources))
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
            finally:
                infile.close()

        if self.host_assigned_resources is not None:
            return True
        else:
            return False

#
#
# FUNCTION main
#


def main():
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Function called" % (caller_name()))
    hostname = pbs.get_local_nodename()
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Host is %s" % (caller_name(), hostname))

    # Log the hook event type
    e = pbs.event()
    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook name is %s" %
               (caller_name(), e.hook_name))

    try:
        # Instantiate the hook utility class
        hooks = HookUtils()
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Event type is %s" %
                   (caller_name(), hooks.event_name(e.type)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(hooks)))
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: Failed to instantiate hook utility class" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        e.accept()

    if not hooks.hashandler(e.type):
        # Bail out if there is no handler for this event
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s event not handled by this hook" %
                   (caller_name(), hooks.event_name(e.type)))
        e.accept()

    try:
        # If an exception occurs, jobutil must be set
        jobutil = None
        # Instantiate the job utility class first so jobutil can be accessed
        # by the exception handlers.
        if hasattr(e, 'job'):
            jobutil = JobUtils(e.job)
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Job information class instantiated" %
                       (caller_name()))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" %
                       (caller_name(), repr(jobutil)))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Event does not include a job" %
                       (caller_name()))
        # Instantiate the cgroup utility class
        vnode = None
        if hasattr(e, 'vnode_list'):
            if hostname in e.vnode_list.keys():
                vnode = e.vnode_list[hostname]
        cgroup = CgroupUtils(hostname, vnode)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Cgroup utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(cgroup)))
        # Bail out if there is nothing to do
        if len(cgroup.subsystems) < 1:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: All cgroups disabled" %
                       (caller_name()))
            e.accept()

        # Call the appropriate handler
        if hooks.invoke_handler(e, cgroup, jobutil) is True:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned success" %
                       (caller_name()))
            e.accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned failure" %
                       (caller_name()))
            e.reject()

    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass

    except AdminError, exc:
        # Something on the system is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Admin error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except UserError, exc:
        # User must correct problem and resubmit job
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("User error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.delete()
                msg += " (deleted)"
            except:
                msg += " (delete failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except JobValueError, exc:
        # Something in PBS is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Job value error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except Exception, exc:
        # Catch all other exceptions and report them.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Unexpected error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

# line below required for unittesting
if __name__ == "__builtin__":
    start_time = time.time()
    try:
        main()
    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
    finally:
        pbs.logmsg(pbs.EVENT_DEBUG, "Elapsed time: %0.4lf" %
                   (time.time() - start_time))
---
2016-07-18 16:55:27,932 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups.json', 'content-encoding': 'default'}
2016-07-18 16:55:27,933 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups.json
2016-07-18 16:55:28,019 INFO     job: executable set to /bin/sleep with arguments: 100
2016-07-18 16:55:28,020 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0755 /tmp/PtlPbsJobScriptzfbsef
2016-07-18 16:55:28,033 INFOCLI  centos7: sudo -H -u pbsuser /usr/local/bin/qsub -N test_t2 -l select=1:ncpus=1:mem=300mb /tmp/PtlPbsJobScriptzfbsef
2016-07-18 16:55:28,073 INFO     submit to centos7 as pbsuser: job 376.centos7 OrderedDict([('Job_Name', 'test_t2'), ('Resource_List.select', '1:ncpus=1:mem=300mb')])
2016-07-18 16:55:28,073 INFOCLI  job script /tmp/PtlPbsJobScriptzfbsef
---
#!/usr/bin/env python

import os
import subprocess
from subprocess import Popen, PIPE
import time

eat_mem_c = """
#include <stdio.h>
#include <stdlib.h>
#include <string.h>


#define ONE_GB (0x40000000L)
#define ONE_MB (0x100000L)

void
eatmem(total_mb, block_mb)
{
	int i;
	char *buf;
	size_t block;

	for (i=1, buf=NULL, block=(block_mb*ONE_MB); (i*block) < (total_mb*ONE_MB); i++) {
		buf = realloc((void *)buf, (i*block));
		memset(buf+((i-1)*block), 0, block);
	}
	return;
}

int
main(int argc, char *argv[])
{
	int total_mb = 8;
	int block_mb = 1;

	if (argc > 1)
		total_mb = atoi(argv[1]);

	if (argc > 2)
		block_mb = atoi(argv[2]);

	eatmem(total_mb, block_mb);
	printf("Initialized %dMB in blocks of %dMB\\n", total_mb, block_mb);
}
"""

fname = "/tmp/pbs_pp325_eat_mem.c"
fname_exe = "/tmp/pbs_pp325_eat_mem"
fout = open(fname, "w")
fout.write(eat_mem_c)
fout.close()

print os.getppid()

p = Popen(["gcc", "-o", fname_exe, fname], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

p = Popen([fname_exe, "400", "10"], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

os.remove(fname)
os.remove(fname_exe)
time.sleep(1)

---
2016-07-18 16:55:28,074 INFOCLI  centos7: /usr/local/bin/qstat -f 376.centos7
2016-07-18 16:55:28,170 INFO     expect on server centos7: job_state = R job 376.centos7  got: job_state = Q
2016-07-18 16:55:28,671 INFOCLI  centos7: /usr/local/bin/qmgr -c set server scheduling=True
2016-07-18 16:55:28,686 INFOCLI  centos7: /usr/local/bin/qstat -f 376.centos7
2016-07-18 16:55:28,781 INFO     expect on server centos7: job_state = R job 376.centos7 attempt: 2 ...  OK
2016-07-18 16:55:28,787 INFO     mom centos7 log match: searching for ".*376.centos7;Cgroup mem\w+ limit exceeded.*" - using regular expression  - No match
2016-07-18 16:55:39,215 INFO     mom centos7 log match: searching for ".*376.centos7;Cgroup mem\w+ limit exceeded.*" - using regular expression ... OK
2016-07-18 16:55:39,215 INFO     ==============================
2016-07-18 16:55:39,216 INFO      Entered CgroupsTests tearDown
2016-07-18 16:55:39,216 INFO     ==============================
2016-07-18 16:55:39,216 INFO     ===============================
2016-07-18 16:55:39,216 INFO     Completed CgroupsTests tearDown
2016-07-18 16:55:39,216 INFO     ===============================
2016-07-18 16:55:39,216 INFO     ok

2016-07-18 16:55:39,216 INFO     test name: test_t2b (tests.cgroups_tests.CgroupsTests)...
2016-07-18 16:55:39,216 INFO     test start time: Mon Jul 18 16:55:39 2016
2016-07-18 16:55:39,217 INFO     test docstring: 
 Test to verify that the job is killed when it tries to 
            use more swap then it requested
         
2016-07-18 16:55:39,217 INFO     ===========================
2016-07-18 16:55:39,217 INFO      Entered CgroupsTests setUp
2016-07-18 16:55:39,217 INFO     ===========================
2016-07-18 16:55:39,217 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:39,268 INFO     status on centos7: server
2016-07-18 16:55:39,268 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:39,329 INFO     manager on centos7: set server {'managers': (3, 'root@*')}
2016-07-18 16:55:39,330 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers-=root@*
2016-07-18 16:55:39,359 INFO     manager on centos7: set server {'managers': (2, 'root@*')}
2016-07-18 16:55:39,360 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers+=root@*
2016-07-18 16:55:39,392 INFO     server centos7: reverting configuration to defaults
2016-07-18 16:55:39,392 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:39,443 INFO     select on centos7: __ALL__
2016-07-18 16:55:39,443 INFOCLI  centos7: /usr/local/bin/qselect
2016-07-18 16:55:39,463 INFOCLI  centos7: /usr/local/bin/qstat -f @centos7.usa.org
2016-07-18 16:55:39,516 INFO     expect on server centos7: job_state set 0 job ...  OK
2016-07-18 16:55:39,517 INFOCLI  centos7: /usr/local/bin/pbs_rstat -f
2016-07-18 16:55:39,527 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:39,553 INFO     manager on centos7: delete hook t1_l1
2016-07-18 16:55:39,553 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c delete hook t1_l1
2016-07-18 16:55:39,598 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:55:39,624 INFO     expect on server centos7:  unset  hook t1_l1 ...  OK
2016-07-18 16:55:39,625 INFOCLI  centos7: /usr/local/bin/qstat -Qf @centos7.usa.org
2016-07-18 16:55:39,639 INFO     manager on centos7: delete queue workq
2016-07-18 16:55:39,639 INFOCLI  centos7: /usr/local/bin/qmgr -c delete queue workq
2016-07-18 16:55:39,659 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:55:39,670 INFO     expect on server centos7:  unset  queue workq ...  OK
2016-07-18 16:55:39,671 INFO     manager on centos7: create queue workq {'started': 'True', 'queue_type': 'Execution', 'enabled': 'True'}
2016-07-18 16:55:39,671 INFOCLI  centos7: /usr/local/bin/qmgr -c create queue workq started=True,queue_type=Execution,enabled=True
2016-07-18 16:55:39,683 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:55:39,696 INFO     expect on server centos7: started set True || queue_type set Execution || enabled set True queue workq ...  OK
2016-07-18 16:55:39,696 INFO     manager on centos7: set server {'log_events': '511', 'default_queue': 'workq'}
2016-07-18 16:55:39,696 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=511,default_queue=workq
2016-07-18 16:55:39,712 INFO     status on centos7: resource
2016-07-18 16:55:39,712 INFO     manager on centos7: list resource
2016-07-18 16:55:39,712 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource
2016-07-18 16:55:39,784 INFO     manager on centos7: delete resource nmics,ngpus
2016-07-18 16:55:39,785 INFOCLI  centos7: /usr/local/bin/qmgr -c delete resource nmics,ngpus
2016-07-18 16:55:39,840 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource nmics
2016-07-18 16:55:39,859 INFO     expect on server centos7:  unset  resource nmics ...  OK
2016-07-18 16:55:39,860 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource ngpus
2016-07-18 16:55:39,884 INFO     expect on server centos7:  unset  resource ngpus ...  OK
2016-07-18 16:55:39,885 INFOCLI  status on centos7: server license_count
2016-07-18 16:55:39,886 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:39,941 INFO     server: centos7.usa.org licensed
2016-07-18 16:55:39,970 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/server_priv/comm.lock
2016-07-18 16:55:40,016 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:55:40,035 INFO     scheduler centos7: reverting configuration to defaults
2016-07-18 16:55:40,036 INFO     manager on centos7: list sched
2016-07-18 16:55:40,036 INFOCLI  centos7: /usr/local/bin/qmgr -c list sched
2016-07-18 16:55:40,051 INFO     manager on centos7: unset sched ['sched_cycle_length']
2016-07-18 16:55:40,051 INFOCLI  centos7: /usr/local/bin/qmgr -c unset sched sched_cycle_length
2016-07-18 16:55:40,083 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/dedicated_time
2016-07-18 16:55:40,093 INFOCLI2 centos7: cmp /usr/local/etc/pbs_resource_group /var/spool/pbs/sched_priv/resource_group
2016-07-18 16:55:40,099 INFO     scheduler centos7: reverting holidays file to default
2016-07-18 16:55:40,100 INFOCLI2 centos7: cmp /usr/local/etc/pbs_holidays /var/spool/pbs/sched_priv/holidays
2016-07-18 16:55:40,104 INFOCLI2 centos7: cmp /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:40,108 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:40,137 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:40,149 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:55:40,150 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:55:40,206 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:55:40,218 INFOCLI2 centos7: sudo -H cmp /usr/local/sbin/pbs_mom /usr/local/sbin/pbs_mom.cpuset
2016-07-18 16:55:40,273 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:55:40,289 INFO     mom centos7: reverting configuration to defaults
2016-07-18 16:55:40,289 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/epilogue
2016-07-18 16:55:40,304 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/prologue
2016-07-18 16:55:40,316 INFOCLI2 centos7: sudo -H ls /var/spool/pbs/mom_priv/config.d
2016-07-18 16:55:40,332 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsIn7UN2 /var/spool/pbs/mom_priv/config
2016-07-18 16:55:40,343 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/mom_priv/config
2016-07-18 16:55:40,357 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/mom_priv/config
2016-07-18 16:55:40,368 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/mom_priv/config
2016-07-18 16:55:40,378 INFO     mom centos7: sent signal -HUP
2016-07-18 16:55:40,378 INFOCLI2 centos7: sudo -H kill -HUP 42976
2016-07-18 16:55:40,426 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:55:40,437 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 16:55:40,459 INFO     status on centos7: node centos7
2016-07-18 16:55:40,459 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:55:40,481 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:55:40,517 INFO     expect on server centos7: state = free node centos7 ...  OK
2016-07-18 16:55:40,517 INFO     ============================
2016-07-18 16:55:40,518 INFO     Completed CgroupsTests setUp
2016-07-18 16:55:40,518 INFO     ============================
2016-07-18 16:55:40,518 INFO     server centos7: server operating mode set to cli
2016-07-18 16:55:40,518 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:40,586 INFO     expect on server centos7: pbs_version ~ .*14.* server centos7.usa.org ...  OK
2016-07-18 16:55:40,586 INFO     manager on centos7: set server {'log_events': '2047'}
2016-07-18 16:55:40,586 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=2047
2016-07-18 16:55:40,606 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:40,664 INFO     expect on server centos7: log_events set 2047 server centos7.usa.org ...  OK
2016-07-18 16:55:40,664 INFO     scheduler centos7: config {'resources': 'ncpus,mem,host,vnode,vmem'}
2016-07-18 16:55:40,666 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /var/spool/pbs/sched_priv/sched_config /var/spool/pbs/sched_priv/sched_config.bak
2016-07-18 16:55:40,690 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbse7ARZJ /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:40,715 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:40,724 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:40,741 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:40,762 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:55:40,763 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:55:40,785 INFO     scheduler centos7 log match: searching for "Error reading line" - No match
2016-07-18 16:55:41,788 INFO     manager on centos7 as root: create resource nmics {'flag': 'nh', 'type': 'long'}
2016-07-18 16:55:41,789 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource nmics flag=nh,type=long
2016-07-18 16:55:41,818 INFO     manager on centos7 as root: create resource ngpus {'flag': 'nh', 'type': 'long'}
2016-07-18 16:55:41,818 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource ngpus flag=nh,type=long
2016-07-18 16:55:41,874 INFO     Test Summary: test_t1_l1 tests "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" cgroup hook
2016-07-18 16:55:41,874 INFO     status on centos7: hook
2016-07-18 16:55:41,874 INFO     manager on centos7: list hook
2016-07-18 16:55:41,874 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:41,895 INFO     status on centos7: hook
2016-07-18 16:55:41,896 INFO     manager on centos7: list hook
2016-07-18 16:55:41,896 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:41,913 INFO     manager on centos7: create hook t1_l1
2016-07-18 16:55:41,914 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c create hook t1_l1
2016-07-18 16:55:41,937 INFO     manager on centos7: set hook t1_l1 {'freq': 2, 'enabled': 'True', 'event': '"execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"'}
2016-07-18 16:55:41,938 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set hook t1_l1 freq=2,enabled=True,event="execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"
2016-07-18 16:55:41,981 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:55:42,004 INFO     expect on server centos7: freq set 2 || enabled set True || event set "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" hook t1_l1 ...  OK
2016-07-18 16:55:42,005 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-python', 'input-file': '/tmp/PtlPbseBHcKP', 'content-encoding': 'default'}
2016-07-18 16:55:42,005 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-python default /tmp/PtlPbseBHcKP
2016-07-18 16:55:42,060 INFO     server centos7: imported hook body
---
#  Copyright (C) 2003-2016 Altair Engineering, Inc. All rights reserved.
#
#  ALTAIR ENGINEERING INC. Proprietary and Confidential. Contains Trade Secret
#  Information. Not for use or disclosure outside ALTAIR and its licensed
#  clients. Information contained herein shall not be decompiled, disassembled,
#  duplicated or disclosed in whole or in part for any purpose. Usage of the
#  software is only as explicitly permitted in the end user software license
#  agreement.
#
#  Copyright notice does not imply publication.

# Last Updated
# Date: 07-06-2016
#
# When soft_limit is true for memory, memsw represents the hard limit.
#
# The resources value in sched_config must contain entries for mem and
# vmem if those subsystems are enabled in the hook configuration file. The
# amount of resource requested will not be avaiable to the hook if they
# are not present.

# This hook handles all of the operations necessary for PBS to support
# cgroups on linux hosts that support them (kernel 2.6.28 and higher)
#
# This hook services the following events:
# - exechost_periodic
# - exechost_startup
# - execjob_attach
# - execjob_begin
# - execjob_end
# - execjob_epilogue
# - execjob_launch

# Imports from __future__ must happen first
from __future__ import with_statement

# Additional imports
import sys
import os
import errno
import signal
import subprocess
import re
import glob
import time
import string
import platform
import traceback
import copy
from stat import S_ISBLK, S_ISCHR
import operator
import pwd
import pbs
import fnmatch
import socket

try:
    import json
except:
    import simplejson as json

CGROUP_KILL_ATTEMPTS = 12

# Flag to turn off features not in 12.2.40X branch
CRAY_12_BRANCH = False


# ============================================================================
# Derived error classes
# ============================================================================

# Base class for errors fixable only by administrative action.


class AdminError(Exception):
    pass

# Base class for errors in processing, unknown cause.


class ProcessingError(Exception):
    pass

# Base class for errors fixable by the user.


class UserError(Exception):
    pass

# Errors in PBS job resource values.


class JobValueError(UserError):
    pass

# Errors when the cgroup is busy.


class CgroupBusyError(ProcessingError):
    pass

# Errors in configuring cgroup.


class CgroupConfigError(AdminError):
    pass

# Errors in configuring cgroup.


class CgroupLimitError(AdminError):
    pass

# Errors processing cgroup.


class CgroupProcessingError(ProcessingError):
    pass

# ============================================================================
# Utility functions
# ============================================================================

#
# FUNCTION caller_name
#
# Return the name of the calling function or method.
#


def caller_name():
    return str(sys._getframe(1).f_code.co_name)

#
# FUNCTION convert_size
#
# Convert a string containing a size specification (e.g. "1m") to a
# string using different units (e.g. "1024k").
#
# This function only interprets a decimal number at the start of the string,
# stopping at any unrecognized character and ignoring the rest of the string.
#
# When down-converting (e.g. MB to KB), all calculations involve integers and
# the result returned is exact. When up-converting (e.g. KB to MB) floating
# point numbers are involved. The result is rounded up. For example:
#
# 1023MB -> GB yields 1g
# 1024MB -> GB yields 1g
# 1025MB -> GB yields 2g  <-- This value was rounded up
#
# Pattern matching or conversion may result in exceptions.
#


def convert_size(value, units='b'):
    logs = {'b': 0, 'k': 10, 'm': 20, 'g': 30,
            't': 40, 'p': 50, 'e': 60, 'z': 70, 'y': 80}
    try:
        new = units[0].lower()
        if new not in logs.keys():
            new = 'b'
        val, old = re.match('([-+]?\d+)([bkmgtpezy]?)',
                            str(value).lower()).groups()
        val = int(val)
        if val < 0:
            raise ValueError('Value may not be negative')
        if old not in logs.keys():
            old = 'b'
        factor = logs[old] - logs[new]
        val *= 2 ** factor
        slop = val - int(val)
        val = int(val)
        if slop > 0:
            val += 1
        # pbs.size() does not like units following zero
        if val <= 0:
            return '0'
        else:
            return str(val) + new
    except:
        return None

#
# FUNCTION size_as_int
#
# Convert a size string to an integer representation of size in bytes
#


def size_as_int(value):
    return int(convert_size(value).rstrip(string.ascii_lowercase))

#
# FUNCTION convert_time
#
# Converts a integer value for time into the value of the return unit
#
# A valid decimal number, with optional sign, may be followed by a character
# representing a scaling factor.  Scaling factors may be either upper or
# lower case. Examples include:
# 250ms
# 40s
# +15min
#
# Valid scaling factors are:
# ns  = 10**-9
# us  = 10**-6
# ms  = 10**-3
# s   =      1
# min =     60
# hr  =   3600
#
# Pattern matching or conversion may result in exceptions.
#


def convert_time(value, return_unit='s'):
    multipliers = {'':  1, 'ns': 10 ** -9, 'us': 10 ** -6,
                   'ms': 10 ** -3, 's': 1, 'min': 60, 'hr': 3600}
    n, factor = re.match('([-+]?\d+)(\w*[a-zA-Z])?',
                         str(value).lower()).groups(0)

    # Check to see if there was not unit of time specified
    if factor == 0:
        factor = ''

    # Check to see if the unit is valid
    if str.lower(factor) not in multipliers.keys():
        raise ValueError('Time unit not recognized.')

    # Convert the value to seconds
    value_in_sec = float(n) * float(multipliers[str.lower(factor)])
    if return_unit != 's':
        return value_in_sec / multipliers[str.lower(return_unit)]
    else:
        return float('%lf' % value_in_sec)

# simplejson hook to convert lists from unicode to utf-8


def decode_list(data):
    rv = []
    for item in data:
        if isinstance(item, unicode):
            item = item.encode('utf-8')
        elif isinstance(item, list):
            item = decode_list(item)
        elif isinstance(item, dict):
            item = decode_dict(item)
        rv.append(item)
    return rv

# simplejson hook to convert dictionaries from unicode to utf-8


def decode_dict(data):
    rv = {}
    for key, value in data.iteritems():
        if isinstance(key, unicode):
            key = key.encode('utf-8')
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        elif isinstance(value, list):
            value = decode_list(value)
        elif isinstance(value, dict):
            value = decode_dict(value)
        rv[key] = value
    return rv

# Convert CPU list format (with ranges) to a Python list


def cpus2list(s):
    # The input string is a comma separated list of digits and ranges.
    # Examples include:
    # 0-3,8-11
    # 0,2,4,6
    # 2,5-7,10
    cpus = []
    if s.strip() == '':
        return cpus
    for r in s.split(','):
        if '-' in r[1:]:
            start, end = r.split('-', 1)
            for i in range(int(start), int(end) + 1):
                cpus.append(int(i))
        else:
            cpus.append(int(r))
    return cpus


# Helper function to ignore suspended jobs when removing
# "already used" resources
def job_to_be_ignored(jobid):
    # Get job substate from small utility that calls printjob
    pbs_exec = ''
    if not CRAY_12_BRANCH:
        pbs_conf = pbs.get_pbs_conf()
        pbs_exec = pbs_conf['PBS_EXEC']
    else:
        if 'PBS_EXEC' in self.cfg:
            pbs_exec = self.cfg['PBS_EXEC']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "PBS_EXEC needs to be defined in the config file")
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Exiting the cgroups hook")
            pbs.accept()

    printjob_cmd = pbs_exec+os.sep+'bin'+os.sep+'printjob'

    cmd = [printjob_cmd, jobid]

    substate = None
    try:
        pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
        # Collect the job substate information
        process = subprocess.Popen(
                                   cmd,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        (out, err) = process.communicate()
        # Find the job substate
        substate_re = re.compile(r"substate:\s+(?P<jobstate>\S+)\s+")
        substate = substate_re.search(out)
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Unexpected error in job_to_be_ignored: %s" %
                   ' '.join([repr(sys.exc_info()[0]),
                            repr(sys.exc_info()[1])]))
        out = "Unknown: failed to run " + printjob_cmd

    if substate is not None:
        substate = substate.group().split(':')[1].strip()
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Job %s has substate %s" % (jobid, substate))

        suspended_substates = ['0x2b', '0x2d', 'unknown']
        return (substate in suspended_substates)
    else:
        return False

# ============================================================================
# Utility classes
# ============================================================================

#
# CLASS HookUtils
#


class HookUtils:

    def __init__(self, hook_events=None):
        if hook_events is not None:
            self.hook_events = hook_events
        else:
            self.hook_events = {
                # Defined in the order they appear in module_pbs_v1.c
                pbs.QUEUEJOB: {
                    'name': 'queuejob',
                    'handler': None
                },
                pbs.MODIFYJOB: {
                    'name': 'modifyjob',
                    'handler': None
                },
                pbs.RESVSUB: {
                    'name': 'resvsub',
                    'handler': None
                },
                pbs.MOVEJOB: {
                    'name': 'movejob',
                    'handler': None
                },
                pbs.RUNJOB: {
                    'name': 'runjob',
                    'handler': None
                },
                pbs.PROVISION: {
                    'name': 'provision',
                    'handler': None
                },
                pbs.EXECJOB_BEGIN: {
                    'name': 'execjob_begin',
                    'handler': self.__execjob_begin_handler
                },
                pbs.EXECJOB_PROLOGUE: {
                    'name': 'execjob_prologue',
                    'handler': None
                },
                pbs.EXECJOB_EPILOGUE: {
                    'name': 'execjob_epilogue',
                    'handler': self.__execjob_epilogue_handler
                },
                pbs.EXECJOB_PRETERM: {
                    'name': 'execjob_preterm',
                    'handler': None
                },
                pbs.EXECJOB_END: {
                    'name': 'execjob_end',
                    'handler': self.__execjob_end_handler
                },
                pbs.EXECJOB_LAUNCH: {
                    'name': 'execjob_launch',
                    'handler': self.__execjob_launch_handler
                },
                pbs.EXECHOST_PERIODIC: {
                    'name': 'exechost_periodic',
                    'handler': self.__exechost_periodic_handler
                },
                pbs.EXECHOST_STARTUP: {
                    'name': 'exechost_startup',
                    'handler': self.__exechost_startup_handler
                },
                pbs.MOM_EVENTS: {
                    'name': 'mom_events',
                    'handler': None
                },
            }

            if not CRAY_12_BRANCH:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "CRAY 12 Branch: %s" % (CRAY_12_BRANCH))
                self.hook_events[pbs.EXECJOB_ATTACH] = {
                    'name': 'execjob_attach',
                    'handler': self.__execjob_attach_handler
                }

    def __repr__(self):
        return "HookUtils(%s)" % (repr(self.hook_events))

    def event_name(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['name']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Type: %s not found" % (caller_name(), type))
            return None

    def hashandler(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['handler'] is not None
        else:
            return None

    def invoke_handler(self, event, cgroup, jobutil, *args):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: UID: real=%d, effective=%d" %
                   (caller_name(), os.getuid(), os.geteuid()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: GID: real=%d, effective=%d" %
                   (caller_name(), os.getgid(), os.getegid()))
        if self.hashandler(event.type):
            result = self.hook_events[event.type][
                'handler'](event, cgroup, jobutil, *args)
            return result
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: %s event not handled by this hook." %
                       (caller_name(), self.event_name(event.type)))

    def __execjob_begin_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Instantiate the NodeConfig class for get_memory_on_node and
        # get_vmem_on node
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Host assigned job resources: %s" %
                   (caller_name(), jobutil.host_resources))

        # Make sure the parent cgroup directories exist
        cgroup.create_paths()

        # Make sure that any old cgroups created in an earlier attempt
        # at creating or running the job are gone
        cgroup.delete(e.job.id)

        # Gather the assigned resources
        cgroup.gather_assigned_resources()
        # Create the cgroup(s) for the job
        cgroup.create_job(e.job.id, node)
        # Configure the new cgroup
        cgroup.configure_job(e.job.id, jobutil.host_resources, node)
        # Initialize resource usage for the job
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        # Write out the assigned resources
        cgroup.write_out_cgroup_host_assigned_resources(e.job.id)
        # Write out the environment variable for the host (pbs_attach)
        if 'devices_name' in cgroup.host_assigned_resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Devices: %s" %
                       cgroup.host_assigned_resources['devices_name'])
            env_list = list()
            if len(cgroup.host_assigned_resources['devices_name']) > 0:
                line = str()
                mics = list()
                gpus = list()
                for key in cgroup.host_assigned_resources['devices_name']:
                    if key.startswith('mic'):
                        mics.append(key[3:])
                    elif key.startswith('nvidia'):
                        gpus.append(key[6:])
                if len(mics) > 0:
                    env_list.append('OFFLOAD_DEVICES="%s"' %
                                    string.join(mics, ','))
                if len(gpus) > 0:
                    # don't put quotes around the values. ex "0" or "0,1".
                    # This will cause it to fail.
                    env_list.append('CUDA_VISIBLE_DEVICES=%s' %
                                    string.join(gpus, ','))

            pbs.logmsg(pbs.EVENT_DEBUG, "ENV_LIST: %s" % env_list)
            cgroup.write_out_cgroup_host_job_env_file(e.job.id, env_list)
        return True

    def __execjob_epilogue_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # The resources_used information has a base type of pbs_resource
        # Set the usage data
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        return True

    def __execjob_end_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Delete the cgroup(s) for the job
        cgroup.delete(e.job.id)
        # Remove the host_assigned_resources and job_env file
        for filename in [cgroup.hook_storage_dir+os.sep+e.job.id,
                         cgroup.host_job_env_filename % e.job.id]:
            try:
                os.remove(filename)
            except OSError:
                pbs.logmsg(pbs.EVENT_DEBUG3, "File: %s not found" % (filename))
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Error removing file: %s" % (filename))
                pass

        return True

    def __execjob_launch_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Add the parent process id to the appropriate cgroups.
        cgroup.add_pids(os.getppid(), jobutil.job.id)
        # FUTURE: Add environment variable to the job environment
        # if job requested mic or gpu
        cgroup.read_in_cgroup_host_assigned_resources(e.job.id)
        if cgroup.host_assigned_resources is not None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "host_assigned_resources: %s" %
                       (cgroup.host_assigned_resources))
            cgroup.setup_job_devices_env()
        return True

    def __exechost_periodic_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Cleanup cgroups for jobs not present on this node
        count = cgroup.cleanup_orphans(e.job_list)
        if ('periodic_resc_update' in cgroup.cfg and
                cgroup.cfg['periodic_resc_update']):
            for job in e.job_list.keys():
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: job is %s" %
                           (caller_name(), job))
                cgroup.update_job_usage(job, e.job_list[job].resources_used)
        # Online nodes that were offlined due to a cgroup not cleaning up
        if count == 0 and cgroup.cfg['online_offlined_nodes']:
            vnode = pbs.event().vnode_list[cgroup.hostname]
            if os.path.isfile(cgroup.offline_file):
                line = 'Orphan cgroup(s) have been cleaned up. '

                # Check with the pbs server to see if the node comment matches
                try:
                    tmp_comment = pbs.server().vnode(cgroup.hostname).comment
                except:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Unable to contact server for node comment")
                    tmp_comment = None
                if tmp_comment == cgroup.offline_msg:
                    line += 'Will bring the node back online'
                    vnode.state = pbs.ND_FREE
                    vnode.comment = None
                else:
                    line += 'However, the comment has changed since the node '
                    line += 'was offlined. Node will remain offline'

                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: %s" % (caller_name(), line))
                # Remove file
                os.remove(cgroup.offline_file)
        return True

    def __exechost_startup_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        cgroup.create_paths()
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))

        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        node.create_vnodes()
        if 'memory' in cgroup.subsystems:
            val = node.get_memory_on_node(cgroup.cfg)
            if val is not None:
                if 'vnode_per_numa_node' in cgroup.cfg:
                    if not cgroup.cfg['vnode_per_numa_node']:
                        hn = node.hostname
                        e.vnode_list[hn].resources_available['mem'] = \
                            pbs.size(val)
                        val_cpus = node.totalcpus
                        if val_cpus is not None:
                            e.vnode_list[hn].resources_available['ncpus'] = \
                                int(val_cpus)
                cgroup.set_node_limit('mem', val)
        if 'memsw' in cgroup.subsystems:
            val = node.get_vmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['vmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('vmem', val)
        if 'hugetlb' in cgroup.subsystems:
            val = node.get_hpmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['hpmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('hpmem', val)
        return True

    def __execjob_attach_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logjobmsg(jobutil.job.id, "%s: Attaching PID %s" %
                      (caller_name(), e.pid))
        # Add the job process id to the appropriate cgroups.
        cgroup.add_pids(e.pid, jobutil.job.id)
        return True

#
# CLASS ShallIRunUtils
#


class ShallIRunUtils:

    def __init__(self, hostname, cfg):
        self.cfg = cfg
        self.hostname = hostname

    def allow_users(self, allow_users):
        """ This still needs to be defined """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pass

    def exclude_hosts(self, exclude_hosts):
        """
        Gets the hostname for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        hostname = pbs.get_local_nodename()
        # check to see if hostname is in the exclude_hosts list
        if self.hostname in exclude_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: exclude host %s is in %s" %
                       (caller_name(), self.hostname, exclude_hosts))
            pbs.event().accept()

        # check to see if it regex syntax is used
        pass

    def exclude_vntypes(self, exclude_vntypes):
        """
        Gets the vntype for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        vntype = None

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'
        if os.path.isfile(vntype_file):
            fdata = open(vntype_file).readlines()
            vntype = fdata[0].strip()
            pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
        if vntype is None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype is set to %s" % (vntype))
        elif vntype in exclude_vntypes:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s is in %s" % (vntype, exclude_vntypes))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Exiting Hook")
            pbs.event().accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s not in global exclude: %s" %
                       (vntype, exclude_vntypes))
        pass

    def run_only_on_hosts(self, approved_hosts):
        """
        Gets the list of approved nodes to run on and checks to see if the hook
        should run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Approved hosts: %s, %s" %
                   (type(approved_hosts), approved_hosts))

        # check to see if hostname is in the approved_hosts list
        if approved_hosts == []:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "approved hosts list is empty: %s" % approved_hosts)
        elif self.hostname not in approved_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s is not in the approved list of hosts: %s" %
                       (self.hostname, approved_hosts))
            pbs.event().accept()

        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s in list of approved hosts: %s" %
                       (self.hostname, approved_hosts))

#
# CLASS JobUtils
#


class JobUtils:

    def __init__(self, job, hostname=None, resources=None):
        self.job = job
        self.host_vnodes = list()
        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if resources is not None:
            self.host_resources = resources
        else:
            self.host_resources = self.__host_assigned_resources()

    def __repr__(self):
        return "JobUtils(%s, %s, %s)" % (repr(self.job), repr(self.hostname),
                                         repr(self.host_resources))

    # Return a dictionary of resources assigned to the local node
    def __host_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Bail out if no hostname was provided
        if self.hostname is None:
            raise CgroupProcessingError('No hostname available')
        # Bail out if no job information is present
        if self.job is None:
            raise CgroupProcessingError('No job information available')
        # Determine which vnodes are on this host, if any
        if self.hostname is not None:
            vnhost_pattern = "%s\[[\d+]\]" % self.hostname
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnhost pattern: %s" %
                       (caller_name(), vnhost_pattern))
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job exec_vnode list: %s" %
                       (caller_name(), self.job.exec_vnode))
            for match in re.findall(vnhost_pattern, str(self.job.exec_vnode)):
                self.host_vnodes.append(match)
            if self.host_vnodes is not None:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Associate %s to vnodes on %s" %
                           (caller_name(), self.host_vnodes, self.hostname))

        # Collect host assigned resources
        resources = {}
        for chunk in self.job.exec_vnode.chunks:
            vnode = False
            if self.host_vnodes == [] and chunk.vnode_name != self.hostname:
                continue
            elif self.host_vnodes != [] and \
                    chunk.vnode_name not in self.host_vnodes:
                continue
            elif chunk.vnode_name in self.host_vnodes:
                vnode = True
                if 'vnodes' not in resources:
                    resources['vnodes'] = {}
                if chunk.vnode_name not in resources['vnodes']:
                    resources['vnodes'][chunk.vnode_name] = {}
                # This check is needed since not all resources are
                # required to be in each chunk of a job
                # i.e. exec_vnodes =
                # (node1[0]:ncpus=4:mem=4gb+node1[1]:mem=2gb) +
                # (node1[1]:ncpus=3+node[0]:ncpus=1:mem=4gb)
                if all(resc in resources['vnodes'][chunk.vnode_name]
                       for resc in chunk.chunk_resources.keys()):
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: resources already defined" %
                               (caller_name()))
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s,\t%s" %
                               (caller_name(),
                                chunk.chunk_resources.keys(),
                                resources['vnodes'][chunk.vnode_name].keys()))
                else:
                    # Initialize resources for each vnode
                    for resc in chunk.chunk_resources.keys():
                        # Initialize the new value
                        if isinstance(chunk.chunk_resources[resc],
                                      pbs.pbs_int):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_int(0)
                        elif isinstance(chunk.chunk_resources[resc],
                                        pbs.pbs_float):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_float(0)
                        elif isinstance(chunk.chunk_resources[resc], pbs.size):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.size('0')

            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Chunk %s" %
                       (caller_name(), chunk.vnode_name))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resources: %s" %
                       (caller_name(), resources))
            for resc in chunk.chunk_resources.keys():
                if resc not in resources.keys():
                    # Initialize the new value
                    if isinstance(chunk.chunk_resources[resc], pbs.pbs_int):
                        resources[resc] = pbs.pbs_int(0)
                    elif isinstance(chunk.chunk_resources[resc],
                                    pbs.pbs_float):
                        resources[resc] = pbs.pbs_float(0.0)
                    elif isinstance(chunk.chunk_resources[resc], pbs.size):
                        resources[resc] = pbs.size('0')
                # Add resource value to total
                if type(chunk.chunk_resources[resc]) in \
                        [pbs.pbs_int, pbs.pbs_float, pbs.size]:
                    resources[resc] += chunk.chunk_resources[resc]
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s: resources[%s][%s] is now %s" %
                               (caller_name(), self.hostname, resc,
                                resources[resc]))
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] += \
                                  chunk.chunk_resources[resc]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Setting resource %s to string %s" %
                               (caller_name(), resc,
                                str(chunk.chunk_resources[resc])))
                    resources[resc] = str(chunk.chunk_resources[resc])
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] = \
                                  str(chunk.chunk_resources[resc])
        if not resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: No resources assigned to host %s" %
                       (caller_name(), self.hostname))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources for %s: %s" %
                       (caller_name(), self.hostname, repr(resources)))

        # Return assigned resources for specified host
        pbs.logmsg(pbs.EVENT_DEBUG3, "Job Resources: %s" % (resources))
        return resources

    # Write a message to the job stdout file
    def write_to_stderr(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stderr_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

    # Write a message to the job stdout file
    def write_to_stdout(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stdout_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

#
# CLASS NodeConfig
#


class NodeConfig:

    def __init__(self, cfg, **kwargs):

        self.cfg = cfg
        hostname = None
        meminfo = None
        numa_nodes = None
        devices = None
        hyperthreading = None
        self.hyperthreads_per_core = 1
        self.totalcpus = self.__discover_cpus()

        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        self.host_mom_jobdir = pbs_home+'/mom_priv/jobs'

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'hostname':
                    hostname = val
                elif arg == 'meminfo':
                    meminfo = val
                elif arg == 'numa_nodes':
                    numa_nodes = val
                elif arg == 'devices':
                    devices = val
                elif arg == 'hyperthreading':
                    hyperthreading = val

        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if meminfo is not None:
            self.meminfo = meminfo
        else:
            self.meminfo = self.__discover_meminfo()
        if numa_nodes is not None:
            self.numa_nodes = numa_nodes
        else:
            self.numa_nodes = self.__discover_numa_nodes()
        if devices is not None:
            self.devices = devices
        else:
            self.devices = self.__discover_devices()
        if hyperthreading is not None:
            self.hyperthreading = hyperthreading
        else:
            self.hyperthreading = self.__hyperthreading_enabled()

        # Add the devices count i.e. nmics and ngpus to the numa nodes
        self.__add_devices_to_numa_node()

    def __repr__(self):
        return ("NodeConfig(%s, %s, %s, %s, %s, %s)" %
                (repr(self.cfg), repr(self.hostname), repr(self.meminfo),
                 repr(self.numa_nodes), repr(self.devices),
                 repr(self.hyperthreading)))

    def __add_devices_to_numa_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (self.devices.keys()))
        for device in self.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" % (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" %
                           (self.devices[device].keys()))
                for device_name in self.devices[device]:
                    device_socket = \
                        self.devices[device][device_name]['numa_node']
                    if device == 'mic':
                        if 'nmics' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['nmics'] = 1
                        else:
                            self.numa_nodes[device_socket]['nmics'] += 1
                    elif device == 'gpu':
                        if 'ngpus' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['ngpus'] = 1
                        else:
                            self.numa_nodes[device_socket]['ngpus'] += 1

        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (self.numa_nodes))

    # Discover what type of hardware is on this node and how is it partitioned
    def __discover_numa_nodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        numa_nodes = {}
        for dir in glob.glob(os.path.join(os.sep,
                             "sys", "devices", "system", "node", "node*")):
            id = int(dir.split(os.sep)[5][4:])
            if id not in numa_nodes.keys():
                numa_nodes[id] = {}
                numa_nodes[id]['devices'] = list()
            numa_nodes[id]['cpus'] = open(os.path.join(dir, "cpulist"),
                                          'r').readline().strip()
            with open(os.path.join(dir, "meminfo"), 'r') as fd:
                for line in fd:
                    # Each line will contain four or five fields. Examples:
                    # Node 0 MemTotal:       32995028 kB
                    # Node 0 HugePages_Total:     0
                    entries = line.split()
                    if len(entries) < 4:
                        continue
                    if entries[2] == "MemTotal:":
                        numa_nodes[id][entries[2].rstrip(':')] = \
                            convert_size(entries[3] + entries[4], 'kb')
                    elif entries[2] == "HugePages_Total:":
                        numa_nodes[id][entries[2].rstrip(':')] = entries[3]
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover Numa Nodes: %s" % numa_nodes)
        return numa_nodes

    # Identify devices and to which numa nodes they are attached
    def __discover_devices(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        devices = {}
        for file in glob.glob(os.path.join(os.sep, "sys", "class", "*", "*",
                                           "device", "numa_node")):
            # The file should contain a single integer
            numa_node = int(open(file, 'r').readline().strip())
            if numa_node < 0:
                numa_node = 0
            dirs = file.split(os.sep)
            name = dirs[3]
            instance = dirs[4]
            if name not in devices.keys():
                devices[name] = {}
            devices[name][instance] = {}
            devices[name][instance]['numa_node'] = numa_node
            if name == 'mic':
                s = os.stat('/dev/%s' % instance)
                devices[name][instance]['major'] = os.major(s.st_rdev)
                devices[name][instance]['minor'] = os.minor(s.st_rdev)
                devices[name][instance]['device_type'] = 'c'

        # Check to see if there are gpus on the node
        gpus = self.discover_gpus()
        if isinstance(gpus, dict) and len(gpus.keys()) > 0:
            devices['gpu'] = gpus['gpu']

        pbs.logmsg(pbs.EVENT_DEBUG3, "Discovered devices: %s" % (devices))
        return devices

    def discover_gpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Check to see if nvidia-smi exists and produces valid output
            cmd = ['/usr/bin/nvidia-smi', '-q', '-x']
            if 'nvidia-smi' in self.cfg:
                cmd[0] = self.cfg['nvidia-smi']

            pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
            # Collect the gpu information
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.PIPE)
            output, err = process.communicate()

            # pbs.logmsg(pbs.EVENT_DEBUG3,"output: %s" % output)
            # import the xml library and parse the output
            import xml.etree.ElementTree as ET

            # Parse the data
            name = 'gpu'
            gpu_data = dict()
            gpu_data[name] = {}
            root = ET.fromstring(output)
            pbs.logmsg(pbs.EVENT_DEBUG3, "root.tag: %s" % root.tag)
            for child in root:
                if child.tag == "attached_gpus":
                    # gpu_data['ngpus'] = int(child.text)
                    pass
                if child.tag == "gpu":
                    device_id = child.get('id')
                    number = 'nvidia%s' % child.find('minor_number').text
                    gpu_data[name][number] = dict()
                    gpu_data[name][number]['id'] = device_id

                    # Determine which socket the device is on
                    filename = '/sys/bus/pci/devices/%s/numa_node' % \
                        device_id.lower()
                    isfile = os.path.isfile(filename)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s, %s" % (filename, isfile))
                    if isfile:
                        numa_node = open(filename).read().strip()
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "numa_node: %s" % (numa_node))
                        if int(numa_node) == -1:
                            numa_node = 0
                        gpu_data[name][number]['numa_node'] = int(numa_node)
                        try:
                            dev_filename = '/dev/%s' % number
                            s = os.stat(dev_filename)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find %s" % dev_filename)
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                        gpu_data[name][number]['major'] = os.major(s.st_rdev)
                        gpu_data[name][number]['minor'] = os.minor(s.st_rdev)
                        gpu_data[name][number]['device_type'] = 'c'
            pbs.logmsg(pbs.EVENT_DEBUG, "%s" % gpu_data)
            return gpu_data
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unable to find %s" % string.join(cmd, " "))
            return {'gpu': {}}
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        return {'gpu': {}}

    # Get the memory info on this host
    def __discover_meminfo(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        meminfo = {}
        with open(os.path.join(os.sep, "proc", "meminfo"), 'r') as fd:
            for line in fd:
                entries = line.split()
                if entries[0] == "MemTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "SwapTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "Hugepagesize:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "HugePages_Total:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
                elif entries[0] == "HugePages_Rsvd:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover meminfo: %s" % meminfo)
        return meminfo

    # Determine if the cpus have hyperthreading enabled
    def __hyperthreading_enabled(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            siblings = 0
            cpu_cores = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "siblings":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    siblings = entries[1].strip()
                elif entries[0].strip() == "cpu cores":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_cores = entries[1].strip()
                elif entries[0].strip() == "flags":
                    flag_options = entries[1].split()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: flag_options: %s" %
                               (caller_name(), flag_options))
                    if "ht" in flag_options:
                        rc = True
                        break
        if rc is True:
            self.hyperthreads_per_core = int(siblings)/int(cpu_cores)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: hyperthreads/core: %d" %
                       (caller_name(), self.hyperthreads_per_core))
            if self.hyperthreads_per_core == 1:
                rc = False
        return rc

    def __discover_cpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            cpu_threads = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "processor":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_threads += 1
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: processors found: %d" %
                   (caller_name(), cpu_threads))

        return cpu_threads

    # Gather the jobs running on this node
    def gather_jobs_on_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        # Get the job list from the server
        jobs = None
        try:
            jobs = pbs.server().vnode(self.hostname).jobs
        except:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Unable to contact server to get jobs")
            return None

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs from server for %s: %s" % (self.hostname, jobs))

        if jobs is None:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "No jobs found on %s " % (self.hostname))
            return list()

        jobs_list = dict()
        for job in jobs.split(','):
            # The job list from the node has a space in front of the job id
            # This must be removed or it will not match the cgroup dir
            jobs_list[job.split('/')[0].strip()] = 0
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs on %s: %s" % (self.hostname, jobs_list.keys()))

        return jobs_list.keys()

    # Gather the jobs running on this node
    def gather_jobs_on_node_local(self):
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Method called" % (caller_name()))
        # Get the job list from the jobs directory
        jobs_list = list()
        try:
            for file in os.listdir(self.host_mom_jobdir):
                if fnmatch.fnmatch(file, "*.JB"):
                    jobs_list.append(file[:-3])
            if not jobs_list:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "No jobs found on %s " % (self.hostname))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Jobs from server for %s: %s" %
                           (self.hostname, repr(jobs_list)))

            return jobs_list
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Could not get job list from mom_priv/jobs for %s" %
                       self.hostname)

            return self.gather_jobs_on_node()

    # Get the memory resource on this mom
    def get_memory_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Calculate memory
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
            pbs.logmsg(pbs.EVENT_DEBUG3, "val: %s" % val)
        else:
            val = size_as_int(MemTotal)
        if 'percent_reserve' in config['cgroup']['memory']:
            percent_reserve = config['cgroup']['memory']['percent_reserve']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memory'].keys():
            reserve_memory = config['cgroup']['memory']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                reserve_mem = size_as_int(reserve_memory)
                if val > reserve_mem:
                    val -= reserve_mem
                else:
                    val -= size_as_int("100mb")
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Unable to reserve more memory than " +
                               "available on the node. Available %s, " +
                               "Tried to reserve %s. Reserving 100mb" %
                               (val, reserve_mem))

            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))

        pbs.logmsg(pbs.EVENT_DEBUG3, "Return val: %s" % val)
        return convert_size(str(val), 'kb')

    # Get the virtual memory resource on this mom
    def get_vmem_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Return the value for memory if no swap is configured
        if size_as_int(self.meminfo['SwapTotal']) <= 0:
            return self.get_memory_on_node(config)
        # Calculate vmem
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
        else:
            val = size_as_int(MemTotal)

        val += size_as_int(self.meminfo['SwapTotal'])
        # Determine if memory needs to be reserved
        if 'percent_reserve' in config['cgroup']['memsw']:
            percent_reserve = config['cgroup']['memsw']['reserve_memory']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memsw']:
            reserve_memory = config['cgroup']['memsw']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                val -= size_as_int(reserve_memory)
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))
        return convert_size(str(val), 'kb')

    # Get the huge page memory resource on this mom
    def get_hpmem_on_node(self, config):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        if 'percent_reserve' in config['cgroup']['hugetlb'].keys():
            percent_reserve = config['cgroup']['hugetlb']['percent_reserve']
        else:
            percent_reserve = 0
        # Calculate vmem
        val = size_as_int(self.meminfo['Hugepagesize'])
        val *= (self.meminfo['HugePages_Total'] -
                self.meminfo['HugePages_Rsvd'])
        val -= (val * percent_reserve) / 100
        return convert_size(str(val), 'kb')

    # Create individual vnodes per socket
    def create_vnodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        key = "vnode_per_numa_node"
        if key in self.cfg.keys():
            if not self.cfg[key]:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s is false" %
                           (caller_name(), key))
                return
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s not configured" %
                       (caller_name(), key))
            return

        # Create one vnode per numa node
        vnode_list = pbs.event().vnode_list
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: numa nodes: %s" %
                   (caller_name(), self.numa_nodes))
        # Define the vnodes
        vnode_name = self.hostname
        vnode_list[vnode_name] = pbs.vnode(vnode_name)
        for id in self.numa_nodes.keys():
            vnode_name = self.hostname + "[%d]" % id
            vnode_list[vnode_name] = pbs.vnode(vnode_name)
            for key, val in sorted(self.numa_nodes[id].iteritems()):
                if key == 'cpus':
                    vnode_list[vnode_name].resources_available['ncpus'] = \
                        len(cpus2list(val))/self.hyperthreads_per_core
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['ncpus'] = 0
                elif key == 'MemTotal':
                    mem = self.get_memory_on_node(self.cfg, val)
                    vnode_list[vnode_name].resources_available['mem'] = \
                        pbs.size(mem)
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['mem'] = \
                        pbs.size('0kb')
                elif key == 'HugePages_Total':
                    pass
                elif isinstance(val, list):
                    pass
                elif isinstance(val, dict):
                    pass
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s=%s" %
                               (caller_name(), key, val))
                    vnode_list[vnode_name].resources_available[key] = val

                    if isinstance(val, int):
                        vnode_list[self.hostname].resources_available[key] = 0
                    elif isinstance(val, float):
                        vnode_list[self.hostname].resources_available[key] = \
                            0.0
                    elif isinstance(val, pbs.size):
                        vnode_list[self.hostname].resources_available[key] = \
                            pbs.size('0kb')
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnode list: %s" %
                   (caller_name(), vnode_list))
        return True

#
# CLASS CgroupUtils
#


class CgroupUtils:
    def __init__(self, hostname, vnode, **kwargs):
        cfg = None
        subsystems = None
        paths = None
        vntype = None
        event = None
        self.assigned_resources = dict()
        self.existing_cgroups = dict()
        self.host_assigned_resources = None
        self.pid_lower_limit = 3

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'subsystems':
                    subsystems = val
                elif arg == 'paths':
                    paths = val
                elif arg == 'vntype':
                    vntype = val
                elif arg == 'event':
                    event = val

        try:
            self.hostname = hostname
            self.vnode = vnode
            # __check_os will raise an exception if cgroups are not present
            self.__check_os()
            # Read in the config file
            if cfg is not None:
                self.cfg = cfg
            else:
                self.cfg = self.__parse_config_file()

            # Determine if the hook should run or exit
            siru = ShallIRunUtils(self.hostname, self.cfg)
            siru.exclude_hosts(self.cfg['exclude_hosts'])
            siru.exclude_vntypes(self.cfg['exclude_vntypes'])
            siru.run_only_on_hosts(self.cfg['run_only_on_hosts'])

            # Collect the mount points
            if paths is not None:
                self.paths = paths
            else:
                self.paths = self.__set_paths()
            # Define the local vnode type
            if vntype is not None:
                self.vntype = vntype
            else:
                self.vntype = self.__get_vnode_type()
            # Determine which subsystems we care about
            if subsystems is not None:
                self.subsystems = subsystems
            else:
                self.subsystems = self.__target_subsystems()

            # location to store information for the different hook events
            pbs_home = ''
            if not CRAY_12_BRANCH:
                pbs_conf = pbs.get_pbs_conf()
                pbs_home = pbs_conf['PBS_HOME']
            else:
                if 'PBS_HOME' in self.cfg:
                    pbs_home = self.cfg['PBS_HOME']
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "PBS_HOME needs to be defined in the " +
                               "config file")
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Exiting the cgroups hook")
                    pbs.accept()

            self.hook_storage_dir = pbs_home+'/mom_priv/hooks/hook_data'
            self.host_job_env_dir = pbs_home+'/aux'
            self.host_job_env_filename = self.host_job_env_dir+os.sep+"%s.env"

            # information for offlining nodes
            self.offline_file = \
                pbs_home+os.sep+'mom_priv'+os.sep+'hooks'+os.sep
            self.offline_file += "%s.offline" % pbs.event().hook_name
            self.offline_msg = "Hook %s: " % pbs.event().hook_name
            self.offline_msg += "Unable to clean up one or more cgroups"

        except:
            raise

    def __repr__(self):
        return ("CgroupUtils(%s, %s, %s, %s, %s, %s)" %
                (repr(self.hostname), repr(self.vnode), repr(self.cfg),
                 repr(self.subsystems), repr(self.paths), repr(self.vntype)))

    # Validate the OS type and version
    def __check_os(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check to see if the platform is linux and the kernel is new enough
        if platform.system() != 'Linux':
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: OS does not support cgroups" %
                       (caller_name()))
            raise CgroupConfigError("OS type not supported")
        rel = map(int,
                  string.split(string.split(platform.release(), '-')[0], '.'))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Detected Linux kernel version %d.%d.%d" %
                   (caller_name(), rel[0], rel[1], rel[2]))
        supported = False
        if rel[0] > 2:
            supported = True
        elif rel[0] == 2:
            if rel[1] > 6:
                supported = True
            elif rel[1] == 6:
                if rel[2] >= 28:
                    supported = True
        if not supported:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Kernel needs to be >= 2.6.28: %s" %
                       (caller_name(), system_info[2]))
            raise CgroupConfigError("OS version not supported")
        return supported

    # Read the config file in json format
    def __parse_config_file(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        config = {}
        # Identify the config file and read in the data
        if 'PBS_HOOK_CONFIG_FILE' in os.environ:
            config_file = os.environ["PBS_HOOK_CONFIG_FILE"]
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Config file is %s" %
                       (caller_name(), config_file))
            try:
                config = json.load(open(config_file, 'r'),
                                   object_hook=decode_dict)
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
        else:
            raise CgroupConfigError("No configuration file present")
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Initial Cgroup Config: %s" %
                   (caller_name(), config))
        # Set some defaults if they are not present
        if 'cgroup_prefix' not in config.keys():
            config['cgroup_prefix'] = 'pbspro'
        if 'periodic_resc_update' not in config.keys():
            config['periodic_resc_update'] = False
        if 'vnode_per_numa_node' not in config.keys():
            config['vnode_per_numa_node'] = False
        if 'online_offlined_nodes' not in config.keys():
            config['online_offlined_nodes'] = False
        if 'exclude_hosts' not in config.keys():
            config['exclude_hosts'] = []
        if 'run_only_on_hosts' not in config.keys():
            config['run_only_on_hosts'] = []
        if 'exclude_vntypes' not in config.keys():
            config['exclude_vntypes'] = []
        if 'cgroup' not in config.keys():
            config['cgroup'] = {}
        else:
            for subsys in config['cgroup']:
                subsystem = config['cgroup'][subsys]
                if 'enabled' not in subsystem.keys():
                    config['cgroup'][subsys]['enabled'] = False
                if 'exclude_hosts' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_hosts'] = []
                if 'exclude_vntypes' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_vntypes'] = []

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Cgroup Config with defaults: %s" %
                   (caller_name(), config))
        return config

    # Determine which subsystems are being requested
    def __target_subsystems(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        subsystems = []
        for key in self.cfg['cgroup'].keys():
            if self.enabled(key):
                subsystems.append(key)
        if 'memory' not in subsystems:
            if 'memsw' in subsystems:
                # Remove memsw since it must be greater then
                # or equal to memory
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing memsw from enabled subsystems")
                subsystems.remove('memsw')
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Enabled subsystems: %s" %
                   (caller_name(), subsystems))
        # It is not an error for all subsystems to be disabled.
        # This host or vnode type may be in the excluded list.
        return subsystems

    # Copy a setting from the parent cgroup
    def __copy_from_parent(self, dest):
        try:
            file = os.path.basename(dest)
            dir = os.path.dirname(dest)
            parent = os.path.dirname(dir)
            source = os.path.join(parent, file)
            if not os.path.isfile(source):
                raise CgroupConfigError('Failed to read %s' % (source))
            self.write_value(dest, open(source, 'r').read().strip())
        except:
            raise

    # Create a dictionary of the cgroup subsystems and their corresponding
    # directories
    def __set_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        paths = {}
        try:
            # Loop through the mounts and collect the ones for cgroups
            with open(os.path.join(os.sep, "proc", "mounts"), 'r') as fd:
                for line in fd:
                    entries = line.split()
                    if len(entries) > 3 and entries[2] == "cgroup":
                        flags = entries[3].split(',')
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "subsysdir: %s (flags=%s)" %
                                   (entries[1], flags))
                        for subsys in flags:
                            paths[subsys] = os.path.join(
                                entries[1], self.cfg["cgroup_prefix"])
        except:
            raise
        if len(paths.keys()) < 1:
            raise CgroupConfigError("Cgroup paths not detected")
        # Both the memory and memsw cgroups share the same mount point
        if "memory" in paths.keys():
            paths["memsw"] = paths["memory"]
        return paths

    # Create the cgroup parent directories that will contain the jobs
    def create_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Create the directories that PBS will use to house the jobs
            old_umask = os.umask(0022)
            for subsys in self.subsystems:
                if subsys not in self.paths.keys():
                    raise CgroupConfigError('No path for subsystem: %s' %
                                            (subsys))
                if not os.path.exists(self.paths[subsys]):
                    os.makedirs(self.paths[subsys], 0755)
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Created directory %s" %
                               (caller_name(), self.paths[subsys]))
                    if subsys == 'memory' or subsys == 'memsw':
                        # Set file to
                        # <cgroup>/memory/pbspro/memory.use_hierarchy
                        file = os.path.join(self.paths[subsys],
                                            'memory.use_hierarchy')
                        if not os.path.isfile(file):
                            raise CgroupConfigError('Failed to configure %s' %
                                                    (file))
                        self.write_value(file, 1)
                    elif subsys == 'cpuset':
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.cpus'))
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.mems'))

        except:
            raise CgroupConfigError("Failed to create directory: %s" %
                                    (self.paths[subsys]))
        finally:
            os.umask(old_umask)

    # Return the vnode type of the local node
    def __get_vnode_type(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")

        # File to check for if it is not defined in the hook input file
        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'

        # self.vnode is None for pbs_attach events
        pbs.logmsg(pbs.EVENT_DEBUG3, "vnode: %s" % self.vnode)
        if self.vnode is not None:
            if 'vntype' in self.vnode.resources_available and \
                    self.vnode.resources_available['vntype'] is not None:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "vntype: %s" %
                           self.vnode.resources_available['vntype'])
                return self.vnode.resources_available['vntype']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3, "vntype file: %s" % vntype_file)
                if os.path.isfile(vntype_file):
                    fdata = open(vntype_file).readlines()
                    vntype = fdata[0].strip()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
                    return vntype
        # If vntype was not set then log a message. It is too expensive
        # to have all moms query the server for large jobs.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: resources_available.vntype is " % caller_name() +
                   "not set for this vnode")
        return None

    # Gather resources that are already assigned
    def gather_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        assigned_resources = {}

        # Gather the cpus and memory sockets assigned
        directories = [x[0] for x in os.walk(self.paths['cpuset'])]

        # Add the existing cgroups to a list to check if assigning
        # resources fail
        for dir in directories[1:]:
            if dir.find(pbs.event().job.id) == -1:
                self.existing_cgroups[dir] = []

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'cpuset' in self.subsystems and \
                self.cfg['cgroup']['cpuset']['enabled']:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather cpuset resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['cpuset'], tmp_dir)
                    with open(path+os.sep+'cpuset.cpus') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.cpus'] = \
                            cpus2list(tmp_val.strip())
                    with open(path+os.sep+'cpuset.mems') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.mems'] = \
                            cpus2list(tmp_val.strip())

        # Check to see if the subsystem is enabled before trying
        # to gather resources
        if 'memory' in self.subsystems and \
                self.cfg['cgroup']['memory']['enabled']:
            # Gather the memory assigned for each socket
            directories = [x[0] for x in os.walk(self.paths['memory'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather memory resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['memory'], tmp_dir)
                    with open(path+os.sep+'memory.limit_in_bytes') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memory.limit'] = \
                            tmp_val.strip()
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    if os.path.isfile(tmp_filename) is False:
                        continue
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    with open(tmp_filename) as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memsw.limit'] = \
                            tmp_val.strip()

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'devices' in self.subsystems and \
                self.cfg['cgroup']['devices']['enabled']:
            # Gather the devices assigned
            directories = [x[0] for x in os.walk(self.paths['devices'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather device resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    pbs.logmsg(pbs.EVENT_DEBUG3, "Dir: %s" % tmp_dir)
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['devices'], tmp_dir)
                    with open(path+os.sep+'devices.list') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['devices.list'] = \
                            tmp_val.strip().split('\n')
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Dir: %s\n\tValue: %s" %
                                   (path, tmp_val.replace('\n', ',')))

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Gathered assigned resources: %s" % assigned_resources)
        self.assigned_resources = assigned_resources

    # Return whether a subsystem is enabled
    def enabled(self, subsystem):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check whether the subsystem is enabled in the configuration file
        if subsystem not in self.cfg['cgroup'].keys():
            return False
        if 'enabled' not in self.cfg['cgroup'][subsystem].keys():
            return False
        if not self.cfg['cgroup'][subsystem]['enabled']:
            return False
        # Check whether the cgroup is mounted for this subsystem
        if subsystem not in self.paths.keys():
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup not mounted for %s" %
                       (caller_name(), subsystem))
            return False
        # Check whether this host is excluded
        # if self.hostname in self.cfg['exclude_hosts']:
        #    pbs.logmsg(pbs.EVENT_DEBUG, "%s: cgroup excluded on host %s" %
        #               (caller_name(), self.hostname))
        #    pbs.event().accept()
        if self.hostname in self.cfg['cgroup'][subsystem]['exclude_hosts']:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup excluded for subsystem %s on host %s" %
                       (caller_name(), subsystem, self.hostname))
            return False
        # Check whether the vnode type is excluded
        if self.vntype is not None:
            # if self.vntype in self.cfg['exclude_vntypes']:
            #    pbs.logmsg(pbs.EVENT_DEBUG,
            #               "%s: cgroup excluded on vnode type %s" %
            #               (caller_name(), self.vntype))
            #    pbs.event().accept()
            if self.vntype in self.cfg['cgroup'][subsystem]['exclude_vntypes']:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           ("%s: cgroup excluded for " +
                            "subsystem %s on vnode type %s") %
                           (caller_name(), subsystem, self.vntype))
                return False
        return True

    # Return the default value for a subsystem
    def default(self, subsystem):
        if subsystem in self.cfg['cgroup']:
            if 'default' in self.cfg['cgroup'][subsystem]:
                return self.cfg['cgroup'][subsystem]['default']
        return None

    # Check to see if the pid matches the job owners uid
    def is_pid_owned_by_job_owner(self, pid):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        try:
            proc_uid = os.stat('/proc/%d' % pid).st_uid
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       '/proc/%d uid:%d' % (pid, proc_uid))

            job_owner_uid = pwd.getpwnam(pbs.event().job.euser)[2]
            pbs.logmsg(pbs.EVENT_DEBUG3, "Job uid: %d" % job_owner_uid)

            if proc_uid == job_owner_uid:
                return True
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Proc uid: %d != Job owner:  %d" %
                           (proc_uid, job_owner_uid))
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG, "Unknown pid: %d" % pid)
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        # If we got to this point something did not match up
        return False

    # Add some number of PIDs to the cgroup tasks files for each subsystem
    def add_pids(self, pids, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # make pids a list
        if isinstance(pids, int):
            pids = [pids]

        if pbs.event().type == pbs.EXECJOB_LAUNCH:
            if 1 in pids:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "1 is not a valid pid to add")
                # Hold the job for further review
                e.reject("Hook tried to add pid 1 to the tasks file " +
                         "for job %s" % jobid)

        # check pids to make sure that they are owned by the job owner
        if not CRAY_12_BRANCH:
            pbs.logmsg(pbs.EVENT_DEBUG3, "Not in the Cray 12 Branch")
            pbs.logmsg(pbs.EVENT_DEBUG3, "check to see if execjob_attach")
            if pbs.event().type == pbs.EXECJOB_ATTACH:
                pbs.logmsg(pbs.EVENT_DEBUG3, "event type: attach or launch")
                tmp_pids = list()
                for p in pids:
                    if self.is_pid_owned_by_job_owner(p):
                        tmp_pids.append(p)
                if len(tmp_pids) == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%d is not a valid pids to add" % p)
                    return False
                else:
                    pids = tmp_pids

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: subsys = %s" %
                       (caller_name(), subsys))
            # memsw and memory use the same tasks file
            if subsys == "memsw":
                if "memory" in self.subsystems:
                    continue
            path = os.path.join(self.paths[subsys], jobid)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path = %s" %
                       (caller_name(), path))
            try:
                tasks_path = os.path.join(path, "tasks")
                for p in pids:
                    self.write_value(tasks_path, p, 'a')
            except IOError, exc:
                raise CgroupLimitError("Failed to add PIDs %s to %s (%s)" %
                                       (str(pids), tasks_path,
                                        errno.errorcode[exc.errno]))
            except:
                raise

    # Set a node limit
    def set_node_limit(self, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s = %s" %
                   (caller_name(), resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'],
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Node resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    def setup_job_devices_env(self):
        """ Setup the job environment for the devices assigned to the job for an
            execjob_launch hook
         """
        if 'devices_name' in self.host_assigned_resources:
            names = self.host_assigned_resources['devices_name']
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "devices: %s" % (names))
            offload_devices = list()
            cuda_visible_devices = list()
            for name in names:
                if name.startswith('mic'):
                    offload_devices.append(name[3:])
                elif name.startswith('nvidia'):
                    cuda_visible_devices.append(name[6:])
            if len(offload_devices) > 0:
                value = '"%s"' % string.join(offload_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['OFFLOAD_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "offload_devices: %s" % offload_devices)
            if len(cuda_visible_devices) > 0:
                value = '"%s"' % string.join(cuda_visible_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['CUDA_VISIBLE_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "cuda_visible_devices: %s" % cuda_visible_devices)
            return [offload_devices, cuda_visible_devices]
        else:
            return False

    def setup_subsys_devices(self, path, node):
        subsys = 'devices'
        # Add the devices that the user is allowed to use
        if subsys in self.cfg['cgroup']:
            devices_allowed = open(os.path.join(path,
                                   "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Initial devices.list: %s" %
                       devices_allowed)
            # Deny access to mic and gpu devices
            devices_list = list()
            devices = node.devices
            for device in devices:
                if device == 'mic' or device == 'gpu':
                    for name in devices[device]:
                        d = devices[device][name]
                        devices_list.append("%d:%d" % (d['major'], d['minor']))
            # For CentOS 7 we need to remove a *:* rwm from devices.list we
            # before can add anything to devices.allow. Otherwise our changes
            # are ignored. Check to see if a *:* rwm is in devices.list.
            # If so remove it
            if "a *:* rwm\n" in devices_allowed:
                value = "a *:* rwm"
                self.write_value(os.path.join(path, 'devices.deny'), value)
            else:
                # Verify that the following devices are not in devices.list
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing access to the following: %s" %
                           devices_list)
            for device_str in devices_list:
                value = "c %s rwm" % device_str
                self.write_value(os.path.join(path, 'devices.deny'), value)
            # Add devices back to the list
            devices_allow = list()
            if 'allow' in self.cfg['cgroup'][subsys]:
                devices_allow = self.cfg['cgroup'][subsys]['allow']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Allowing access to the following: %s" %
                           devices_allow)
                for item in devices_allow:
                    if isinstance(item, str):
                        pbs.logmsg(pbs.EVENT_DEBUG3, "string item: %s" % item)
                        value = "%s" % item
                        self.write_value(os.path.join(
                                         path, 'devices.allow'), value)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "write_value: %s" % value)
                    elif isinstance(item, list):
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "list item: %s" % item)
                        try:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Device allow: %s" % item)
                            stat_filename = '/dev/%s' % item[0]
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Stat file: %s" % stat_filename)
                            s = os.stat(stat_filename)
                            device_type = None
                            if S_ISBLK(s.st_mode):
                                device_type = "b"
                            elif S_ISCHR(s.st_mode):
                                device_type = "c"
                            if device_type is not None:
                                if len(item) == 3 and isinstance(item[2], str):
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % item[2]
                                    value += "%s" % item[1]
                                else:
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % os.minor(s.st_rdev)
                                    value += "%s" % item[1]
                                self.write_value(os.path.join(
                                                path, 'devices.allow'), value)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "write_value: %s" % value)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find /dev/%s. " % item[0] +
                                       "Not added to devices.allow!")
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                    else:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Not sure what to do with: %s" % item)
            devices_allowed = open(os.path.join(
                                   path, "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "After setup devices.list: %s" % devices_allowed)

    # Select devices to assign to the job
    def assign_devices(
                       self,
                       device_type,
                       device_list,
                       number_of_devices,
                       node):
        devices = device_list[:number_of_devices]
        device_ids = list()
        device_names = list()
        for device in devices:
            device_info = node.devices[device_type][device]
            if len(device_ids) == 0:
                if device.find("mic") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:0 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                    device_ids.append("%s %d:1 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                elif device.find("nvidia") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:255 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
            device_ids.append("%s %d:%d rwm" %
                              (device_info['device_type'],
                               device_info['major'],
                               device_info['minor']))
            device_names.append(device)
        return device_names, device_ids

    # Find the device name
    def get_device_name(self, node, available, socket, major, minor):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Get device name: major: %s, minor: %s" % (major, minor))
        avail_device = None
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Possible devices: %s" % (available[socket]['devices']))
        for avail_device in available[socket]['devices']:
            avail_major = None
            avail_minor = None
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Checking device: %s" % (avail_device))
            if avail_device.find('mic') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check mic device: %s" % (avail_device))
                avail_major = node.devices['mic'][avail_device]['major']
                avail_minor = node.devices['mic'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            elif avail_device.find('nvidia') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check gpu device: %s" % (avail_device))
                avail_major = node.devices['gpu'][avail_device]['major']
                avail_minor = node.devices['gpu'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            if int(avail_major) == int(major) and \
                    int(avail_minor) == int(minor):
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device match: name: %s, major: %s, minor: %s" %
                           (avail_device, major, minor))
                return avail_device
        pbs.logmsg(pbs.EVENT_DEBUG3, "No match found")
        return None

    # Assign resources to the job based off the vnodes assignments
    def assign_job_resources_by_vnode(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # Loop through the vnodes and assign resources
        assigned = dict()
        assigned['cpuset.cpus'] = list()
        assigned['cpuset.mems'] = list()
        room_on_socket = True

        for vnode in resources['vnodes']:
            regex = re.compile(".*\[(\d+)\].*")
            socket = int(regex.search(vnode).group(1))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current Vnode: %s" % vnode)
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current socket: %d" % socket)

            if 'ncpus' in resources['vnodes'][vnode]:
                tmp_ncpus = int(resources['vnodes'][vnode]['ncpus'])
                if int(resources['vnodes'][vnode]['ncpus']) <= \
                        len(available[socket]['cpus']):
                    assigned['cpuset.cpus'] += \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] += [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_ncpus: %s" % (tmp_ncpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "available[%d]: %s" %
                               (socket, available[socket]))
                    room_on_socket = False
            if ('nmics' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['nmics']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(mic).*")
                tmp_nmics = int(resources['vnodes'][vnode]['nmics'])
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['nmics']) > 0 and
                   int(resources['vnodes'][vnode]['nmics']) <= len(mics)):
                    tmp_names, tmp_devices = self.assign_devices(
                             'mic', mics[:tmp_nmics], tmp_nmics, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_nmics: %s" % (tmp_nmics))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "mics: %s" % (mics))
                    room_on_socket = False
            if ('ngpus' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['ngpus']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(nvidia).*")
                tmp_ngpus = int(resources['vnodes'][vnode]['ngpus'])
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['ngpus']) > 0 and
                   int(resources['vnodes'][vnode]['ngpus']) <= len(gpus)):
                    tmp_names, tmp_devices = self.assign_devices(
                        'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "tmp_ngpus: %s" % (tmp_ngpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "gpus: %s" % (gpus))
                    room_on_socket = False

        if room_on_socket:
            assigned['cpuset.cpus'].sort()
            assigned['cpuset.mems'].sort()
            if 'devices' in assigned:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids pre set and sort: %s" %
                           (assigned['devices']))
                assigned['devices'] = list(set(assigned['devices']))
                assigned['devices'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids post set and sort: %s" %
                           (assigned['devices']))
                assigned['devices_name'] = list(set(assigned['devices_name']))
                assigned['devices_name'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

    # Assign resources to the job
    def assign_job_resources(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # What would we need to do different for a large smp (UV) system?
        # See if requested resources can fit on a single socket
        assigned = dict()
        pbs.logmsg(pbs.EVENT_DEBUG3, "Requested: %s" % (resources))

        # Determine the order that we should evaluate the sockets.
        socket_list = list()
        if 'placement_type' in self.cfg:
            if self.cfg['placement_type'] == 'round_robin':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Requested round_robin placement")
                # Look at assigned_resources and determine which socket
                # to start with
                jobs_per_socket_cnt = dict()
                for socket in available.keys():
                    jobs_per_socket_cnt[socket] = 0
                for job in self.assigned_resources:
                    if 'cpuset.mems' in self.assigned_resources[job]:
                        for socket in \
                                self.assigned_resources[job]['cpuset.mems']:
                            jobs_per_socket_cnt[socket] += 1

                sorted_dict = sorted(jobs_per_socket_cnt.items(),
                                     key=operator.itemgetter(1))
                for x in sorted_dict:
                    socket_list.append(x[0])
        else:
            socket_list = available.keys()

        # Loop through the socket list and determine if the job
        # can fit on the socket
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Evaluate sockets in the following order: %s" %
                   (socket_list))
        for socket in socket_list:
            room_on_socket = True
            if 'ncpus' in resources:
                if int(resources['ncpus']) <= len(available[socket]['cpus']):
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Requested: %s, Available: %s" %
                               (resources['ncpus'], available[socket]['cpus']))
                    tmp_ncpus = int(resources['ncpus'])
                    assigned['cpuset.cpus'] = \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] = [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient ncpus on socket %d: R:%d,A:%s" %
                               (socket, resources['ncpus'],
                                available[socket]['cpus']))
                    room_on_socket = False
            # Check to see that nmics has been requested and not equal to 0
            if 'nmics' in resources and int(resources['nmics']) > 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'nmics' in resources and int(resources['nmics']) > 0 \
                        and int(resources['nmics']) <= len(mics):
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient nmics on socket %d: R:%s,A:%s" %
                               (socket, resources['nmics'], mics))
                    room_on_socket = False
            # Check to see that ngpus has been requested and not equal to 0
            if 'ngpus' in resources and int(resources['ngpus']) > 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'ngpus' in resources and int(resources['ngpus']) > 0 \
                        and int(resources['ngpus']) <= len(gpus):
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient gpus on socket %d: R:%s,A:%s" %
                               (socket, resources['ngpus'], gpus))
                    room_on_socket = False
            if 'vmem' in resources:
                if size_as_int(resources['vmem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient vmem on socket %d: R:%s,A:%s" %
                               (socket, resources['vmem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if 'mem' in resources:
                if size_as_int(resources['mem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient memory on socket %d: R:%s,A:%s" %
                               (socket, resources['mem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if room_on_socket:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
                self.host_assigned_resources = assigned
                return assigned
        # If we made it here then the requested resources did not fit
        # on a single socket
        # Future: take distance into account when selecting sockets?
        # Combine available resources into a single list
        # This should work for a dual socket machine.
        # Needs additional work for machines with more than 2 sockets
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Unable to find requested resources: %s" % resources +
                   " on a socket. Checking the entire node")
        available_copy = copy.deepcopy(available)
        available_on_node = dict()
        socket_keys = available.keys()
        socket_keys.sort()
        available_on_node['sockets'] = socket_keys
        for socket in socket_keys:
            for key in available_copy[socket].keys():
                if isinstance(available[socket][key], int):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]
                if isinstance(available_copy[socket][key], str):
                    try:
                        if key in available_on_node is False:
                            available_on_node[key] = \
                                size_as_int(available_copy[socket][key])
                        else:
                            available_on_node[key] += \
                                size_as_int(available_copy[socket][key])
                    except:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Unable to convert to int: %s" %
                                   (available_copy[socket][key]))

                if isinstance(available_copy[socket][key], list):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available on node: %s" % (available_on_node))
        assigned = dict()
        room_on_node = False
        if 'ncpus' in resources:
            if int(resources['ncpus']) <= len(available_on_node['cpus']):
                room_on_node = True
                tmp_ncpus = int(resources['ncpus'])
                assigned['cpuset.cpus'] = available_on_node['cpus'][:tmp_ncpus]
                assigned['cpuset.mems'] = available_on_node['sockets']
            if 'nmics' in resources and int(resources['nmics']) != 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['nmics']) <= len(mics) and \
                        int(resources['nmics']) > 0:
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
            if 'ngpus' in resources and int(resources['ngpus']) != 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['ngpus']) <= len(gpus) and \
                        int(resources['ngpus']) > 0:
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
        if room_on_node:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

        # Consolidate resources if needed to place the job

    # Determine which resources are available
    def available_node_resources(self, node):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))

        available = copy.deepcopy(node.numa_nodes)
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Keys: %s" % (available[0].keys()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        for socket in available.keys():
            if 'cpus' in available[socket]:
                # Find the physical cores
                if available[socket]['cpus'].find('-') != -1 and \
                        node.hyperthreading:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Splitting %s at ','" %
                               (available[socket]['cpus']))
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'].split(',')[0])
                else:
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'])
            if 'MemTotal' in available[socket]:
                # Find the memory on the socket in bites.
                # Remove the 'b' to simply math
                available[socket]['memory'] = size_as_int(
                    available[socket]['MemTotal'])

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Pre device add: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (node.devices.keys()))
        for device in node.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" %
                       (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Devices: %s" %
                           (node.devices[device].keys()))
                for device_name in node.devices[device].keys():
                    device_socket = \
                        node.devices[device][device_name]['numa_node']
                    if 'devices' not in available[device_socket]:
                        available[device_socket]['devices'] = list()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Device: %s, Socket: %s" %
                               (device, device_socket))
                    available[device_socket]['devices'].append(device_name)
            # pbs.logmsg(pbs.EVENT_DEBUG3,
            #            "Available on socket %s: %s" %
            #            (socket,available[socket]))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))

        # Remove all of the resources that are assigned to other jobs
        for job in self.assigned_resources.keys():
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            if (not job_to_be_ignored(job)):
                cpus = list()
                sockets = list()
                devices = list()
                memory = 0
                if 'cpuset.cpus' in self.assigned_resources[job]:
                    cpus = self.assigned_resources[job]['cpuset.cpus']
                if 'cpuset.mems' in self.assigned_resources[job]:
                    sockets = self.assigned_resources[job]['cpuset.mems']
                if 'devices.list' in self.assigned_resources[job]:
                    devices = self.assigned_resources[job]['devices.list']
                if 'memory.limit' in self.assigned_resources[job]:
                    memory = size_as_int(
                                self.assigned_resources[job]['memory.limit'])

                # Loop through the sockets and remove cpus that are
                # assigned to other cgroups
                for socket in sockets:
                    for cpu in cpus:
                        try:
                            available[socket]['cpus'].remove(cpu)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %d from %s" %
                                       (cpu, available[socket]['cpus']))

                if len(sockets) == 1:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Decrementing memory: %d by %d" %
                               (size_as_int(available[sockets[0]]['memory']),
                                memory))
                    available[sockets[0]]['memory'] -= memory

                # Loop throught the available sockets
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned device to %s: %s" % (job, devices))
                for socket in available.keys():
                    for device in devices:
                        try:
                            # loop through know devices and see if they match
                            if len(available[socket]['devices']) != 0:
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Check device: %s" % (device))
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Available device: %s" %
                                           (available[socket]['devices']))
                                major, minor = device.split()[1].split(':')
                                avail_device = self.get_device_name(
                                                   node, available, socket,
                                                   major, minor)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Returned device: %s" %
                                           (avail_device))
                                if avail_device is not None:
                                    pbs.logmsg(pbs.EVENT_DEBUG3,
                                               "socket: %d,\t" % socket +
                                               "devices: %s,\t" %
                                               available[socket]['devices'] +
                                               "device to remove: %s" %
                                               (avail_device))
                                    available[socket]['devices'].remove(
                                        avail_device)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %s from %s" %
                                       (device, available[socket]['devices']))
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Job %s res not removed from host " % job +
                           "available res: suspended job")
        pbs.logmsg(pbs.EVENT_DEBUG, "Available resources: %s" % (available))
        return available

    # Set a job limit
    def set_job_limit(self, jobid, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s: %s = %s" %
                   (caller_name(), jobid, resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'softmem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.soft_limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     jobid, 'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'], jobid,
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            elif resource == 'ncpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = self.select_cpus(path, value)
                    if cpus is None:
                        raise CgroupLimitError("Failed to configure cpuset.")
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
                    self.__copy_from_parent(os.path.join(self.paths['cpuset'],
                                            jobid, 'cpuset.mems'))
            elif resource == 'cpuset.cpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = value
                    if cpus is None:
                        msg = "Failed to configure cpus in cpuset."
                        raise CgroupLimitError(msg)
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
            elif resource == 'cpuset.mems':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.mems')
                    mems = value
                    if mems is None:
                        msg = "Failed to configure mems in cpuset."
                        raise CgroupLimitError(msg)
                    mems = ','.join(map(str, mems))
                    self.write_value(path, mems)
            elif resource == 'devices':
                if 'devices' in self.subsystems and \
                        self.cfg['cgroup']['devices']['enabled']:

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.allow')
                    devices = value
                    if devices is None:
                        msg = "Failed to configure device(s)."
                        raise CgroupLimitError(msg)
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Setting devices: %s for %s" % (devices, jobid))
                    for device in devices:
                        self.write_value(path, device)

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.list')
                    output = open(path, 'r').read()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "devices.list: %s" % output.replace("\n", ","))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    # Update resource usage for a job
    def update_job_usage(self, jobid, resc_used):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resc_used = %s" %
                   (caller_name(), str(resc_used)))
        # Sort the subsystems so that we consistently look at the subsystems
        # in the same order every time
        self.subsystems.sort()
        for subsys in self.subsystems:
            path = os.path.join(self.paths[subsys], jobid)
            if subsys == "memory":
                max_mem = self.__get_max_mem_usage(path)
                if max_mem is None:
                    pbs.logjobmsg(jobid, "%s: No max mem data" %
                                  (caller_name()))
                else:
                    resc_used['mem'] = pbs.size(convert_size(max_mem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: mem=%s" %
                                  (caller_name(), resc_used['mem']))
                mem_failcnt = self.__get_mem_failcnt(path)
                if mem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No mem fail count data" %
                                  (caller_name()))
                else:
                    # Check to see if the job exceeded its resource limits
                    if mem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memory limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "memsw":
                max_vmem = self.__get_max_memsw_usage(path)
                if max_vmem is None:
                    pbs.logjobmsg(jobid, "%s: No max vmem data" %
                                  (caller_name()))
                else:
                    resc_used['vmem'] = pbs.size(convert_size(max_vmem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: vmem=%s" %
                                  (caller_name(), resc_used['vmem']))

                vmem_failcnt = self.__get_memsw_failcnt(path)
                if vmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No vmem fail count data" %
                                  (caller_name()))
                else:
                    pbs.logjobmsg(jobid, "%s: vmem fail count: %d " %
                                  (caller_name(), vmem_failcnt))
                    if vmem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memsw limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "hugetlb":
                max_hpmem = self.__get_max_hugetlb_usage(path)
                if max_hpmem is None:
                    pbs.logjobmsg(jobid, "%s: No max hpmem data" %
                                  (caller_name()))
                    return
                hpmem_failcnt = self.__get_hugetlb_failcnt(path)
                if hpmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No hpmem fail count data" %
                                  (caller_name()))
                    return
                if hpmem_failcnt > 0:
                    err_msg = self.__get_error_msg(jobid)
                    pbs.logjobmsg(jobid, "Cgroup hugetlb limit exceeded: %s" %
                                  (err_msg))
                resc_used['hpmem'] = pbs.size(convert_size(max_hpmem, 'kb'))
                pbs.logjobmsg(jobid, "%s: Hugepage usage: %s" %
                              (caller_name(), resc_used['hpmem']))
            elif subsys == "cpuacct":
                cpu_usage = self.__get_cpu_usage(path)
                if cpu_usage is None:
                    pbs.logjobmsg(jobid, "%s: No CPU usage data" %
                                  (caller_name()))
                    return
                cpu_usage = convert_time(str(cpu_usage) + "ns")
                pbs.logjobmsg(jobid, "%s: CPU usage: %.3lf secs" %
                              (caller_name(), cpu_usage))
                resc_used['cput'] = pbs.duration(cpu_usage)

    # Creates the cgroup if it doesn't exists
    def create_job(self, jobid, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Iterate over the subsystems required
        for subsys in self.subsystems:
            # Create a directory for the job
            try:
                old_umask = os.umask(0022)
                path = self.paths[subsys]
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                path = os.path.join(self.paths[subsys], jobid)
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                if subsys == 'devices':
                    self.setup_subsys_devices(path, node)

            except OSError, exc:
                raise CgroupConfigError("Failed to create directory: %s (%s)" %
                                        (path, errno.errorcode[exc.errno]))
            except:
                raise
            finally:
                os.umask(old_umask)

    # Determine the cgroup limits and configure the cgroups
    def configure_job(self, jobid, hostresc, node):
        mem_enabled = 'memory' in self.subsystems
        vmem_enabled = 'memsw' in self.subsystems
        if mem_enabled or vmem_enabled:
            # Initialize mem variables
            mem_avail = node.get_memory_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "mem_avail %s" % mem_avail)
            mem_requested = None
            if 'mem' in hostresc.keys():
                mem_requested = convert_size(hostresc['mem'], 'kb')
            mem_default = None
            if mem_enabled:
                mem_default = self.default('memory')
            # Initialize vmem variables
            vmem_avail = node.get_vmem_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "vmem_avail %s" % vmem_avail)
            vmem_requested = None
            if 'vmem' in hostresc.keys():
                vmem_requested = convert_size(hostresc['vmem'], 'kb')
            vmem_default = None
            if vmem_enabled:
                vmem_default = self.default('memsw')
            # Initialize softmem variables
            if 'soft_limit' in self.cfg['cgroup']['memory'].keys():
                softmem_enabled = self.cfg['cgroup']['memory']['soft_limit']
            else:
                softmem_enabled = False
            # Sanity check
            if size_as_int(mem_avail) > size_as_int(vmem_avail):
                if size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                    raise CgroupLimitError(
                        'mem available (%s) exceeds vmem available (%s)' %
                        (mem_avail, vmem_avail))
            # Determine the mem limit
            if mem_requested is not None:
                # mem requested may not exceed available
                if size_as_int(mem_requested) > size_as_int(mem_avail):
                    raise JobValueError(
                        'mem requested (%s) exceeds mem available (%s)' %
                        (mem_requested, mem_avail))
                mem_limit = mem_requested
            else:
                # mem was not requested
                if mem_default is None:
                    mem_limit = mem_avail
                else:
                    mem_limit = mem_default
            # Determine the vmem limit
            if vmem_requested is not None:
                # vmem requested may not exceed available
                if size_as_int(vmem_requested) > size_as_int(vmem_avail):
                    raise JobValueError(
                        'vmem requested (%s) exceeds vmem available (%s)' %
                        (vmem_requested, vmem_avail))
                vmem_limit = vmem_requested
            else:
                # vmem was not requested
                if vmem_default is None:
                    vmem_limit = vmem_avail
                else:
                    vmem_limit = vmem_default
            # Ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Adjust for soft limits if enabled
            if mem_enabled and softmem_enabled:
                softmem_limit = mem_limit
                # The hard memory limit is assigned the lesser of the vmem
                # limit and available memory
                if size_as_int(vmem_limit) < size_as_int(mem_avail):
                    mem_limit = vmem_limit
                else:
                    mem_limit = mem_avail
            # Again, ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Sanity checks when both memory and memsw are enabled
            if mem_enabled and vmem_enabled:
                if vmem_requested is not None:
                    if size_as_int(vmem_limit) > size_as_int(vmem_requested):
                        # The user requested an invalid limit
                        raise JobValueError(
                            'vmem limit (%s) exceeds vmem requested (%s)' %
                            (vmem_limit, vmem_requested))
                if size_as_int(vmem_limit) > size_as_int(mem_limit):
                    # This job may utilize swap
                    if size_as_int(vmem_avail) <= size_as_int(mem_avail) and \
                      size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                        # No swap available
                        raise CgroupLimitError(
                            ('Job might utilize swap ' +
                             'and no swap space available'))
            # Assign mem and vmem
            if mem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: mem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), mem_limit))
                hostresc['mem'] = pbs.size(mem_limit)
                if softmem_enabled:
                    hostresc['softmem'] = pbs.size(softmem_limit)
            if vmem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: vmem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), vmem_limit))
                hostresc['vmem'] = pbs.size(vmem_limit)
        # Initialize hpmem variables
        hpmem_enabled = 'hugetlb' in self.subsystems
        if hpmem_enabled:
            hpmem_avail = node.get_hpmem_on_node(self.cfg)
            hpmem_limit = None
            hpmem_default = self.default('hugetlb')
            if hpmem_default is None:
                hpmem_default = hpmem_avail
            if 'hpmem' in hostresc.keys():
                hpmem_limit = convert_size(hostresc['hpmem'], 'kb')
            else:
                hpmem_limit = hpmem_default
            # Assign hpmem
            if size_as_int(hpmem_limit) > size_as_int(hpmem_avail):
                raise JobValueError('hpmem limit (%s) exceeds available (%s)' %
                                    (hpmem_limit, hpmem_avail))
            hostresc['hpmem'] = pbs.size(hpmem_limit)
        # Initialize cpuset variables
        cpuset_enabled = 'cpuset' in self.subsystems
        if cpuset_enabled:
            cpu_limit = 1
            if 'ncpus' in hostresc.keys():
                cpu_limit = hostresc['ncpus']
            if cpu_limit < 1:
                cpu_limit = 1
            hostresc['ncpus'] = pbs.pbs_int(cpu_limit)

        # Find the available resources and assign the right ones to the job

        attempt = 0
        assign_resources = False

        # Two attempts since self.cleanup_orphans may actually fix the
        # problem we see in a first attempt
        while attempt < 2:
            attempt += 1
            available_resources = self.available_node_resources(node)
            if 'vnodes' in hostresc:
                assign_resources = self.assign_job_resources_by_vnode(
                    hostresc, available_resources, node)
            else:
                assign_resources = self.assign_job_resources(
                    hostresc, available_resources, node)

            if (assign_resources is False) and (attempt < 2):
                # No resources were assigned to the job.
                # Most likely cause was that a cgroup has
                # not been cleaned up yet
                pbs.logmsg(pbs.EVENT_DEBUG, "Failed to assign resources. " +
                           "Checking the following cgroups on the node " +
                           "with the server: %s" % self.existing_cgroups)

                # Collect the jobs on the node (try reading mom_priv/jobs,
                # if not successful ask server)
                jobs_list = node.gather_jobs_on_node_local()

                if jobs_list is not None:
                    # Don't clean up the cgroup we just made for the
                    # job just yet
                    if jobid not in jobs_list:
                        jobs_list.append(jobid)

                    self.cleanup_orphans(jobs_list)

                    # Recheck what is now assigned after things were cleaned up
                    self.gather_assigned_resources()

        if assign_resources is False:
            # Cleanup cgroups for jobs not present on this node
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Assign resources failed. Attempting to cleanup all " +
                       "leftover cgroups (including event job %s)" % jobid)

            # Remove the current job from the jobs list so it will be
            # cleaned up as well.
            jobs_list.remove(jobid)

            self.cleanup_orphans(jobs_list)

            # Rerun the job and log the message
            line = 'Unable to assign resources to job. '
            line += 'Will requeue the job and try again.'
            line += 'job run_count: %d' % pbs.event().job.run_count
            pbs.event().job.rerun()
            pbs.event().reject(line)

        # Print out the assigned resources
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Assigned resources: %s" % (assign_resources))

        if cpuset_enabled:
            hostresc.pop('ncpus', None)
            for key in ['cpuset.cpus', 'cpuset.mems']:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Initialize devices variables
        devices_enabled = 'devices' in self.subsystems
        if devices_enabled:
            key = 'devices'
            if key in assign_resources:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Apply the resource limits to the cgroups
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Setting cgroup limits for: %s" %
                   (caller_name(), hostresc))

        # The vmem limit must be set after the mem limit, so sort the keys
        for resc in sorted(hostresc.keys()):
            self.set_job_limit(jobid, resc, hostresc[resc])

    # Kill any processes contained within a tasks file
    def __kill_tasks(self, tasks_file):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        with open(tasks_file, 'r') as fd:
            for line in fd:
                os.kill(int(line.strip()), signal.SIGKILL)
        fd.close()
        count = 0
        with open(tasks_file, 'r') as fd:
            for line in fd:
                count += 1
                pid = line.strip()
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Pid: %s did not clean up" %
                           (caller_name(), pid))
                if os.path.isfile('/proc/%s/status' % pid):
                    tmp = open('/proc/%s/status' % pid).readlines()
                    tmp_line = ''
                    for entry in tmp:
                        if entry.find('Name:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('State:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('Uid:') != -1:
                            tmp_line += entry.strip()
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: %s" % (caller_name(), tmp_line))

        return count

    # Perform the actual removal of the cgroup directory
    # by default, only one attempt at killing tasks in cgroup, without sleeping
    # since this method could be called many times
    # (for N directories times M jobs)
    def __remove_cgroup(self, path, tasks_kill_attempts=1):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            if os.path.exists(path):
                tasks_file = os.path.join(path, 'tasks')
                task_cnt = self.__kill_tasks(tasks_file)
                kill_attempts = 0
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Task Kill Attempts: %d" % tasks_kill_attempts)
                while (task_cnt > 0) and (kill_attempts < tasks_kill_attempts):
                    kill_attempts += 1
                    count = 0

                    with open(tasks_file, 'r') as fd:
                        for line in fd:
                            count += 1
                        task_cnt = count

                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "Attempt: %d - cgroup still has %d tasks: %s" %
                               (kill_attempts, task_cnt, path))
                    if kill_attempts < tasks_kill_attempts:
                        time.sleep(1)
                        # Added since there are race conditions where tasks are
                        # being added at the same time we get the initial
                        # task count
                        tasks_file = os.path.join(path, 'tasks')
                        task_cnt = self.__kill_tasks(tasks_file)

                if task_cnt == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Removing directory %s" %
                               (caller_name(), path))
                    os.rmdir(path)
                    if os.path.exists(path):
                        time.sleep(.25)
                        if os.path.exists(path):
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "%s: 2nd Try Removing directory %s" %
                                       (caller_name(), path))
                            os.rmdir(path)
                            time.sleep(.25)
                        if os.path.exists(path):
                            return False
                else:
                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "cgroup still has %d tasks: %s" %
                               (task_cnt, path))

                    # Check to see if the offline file is already present
                    # on the mom
                    offline_file_exists = False
                    if os.path.isfile(self.offline_file):
                        msg = "Cgroup(s) not cleaning up but the node already "
                        msg += "has the offline file."

                        pbs.logmsg(pbs.EVENT_DEBUG, msg)
                    else:
                        # Check to see that the node is not already offline.
                        try:
                            tmp_state = pbs.server().vnode(self.hostname).state
                        except:
                            msg = "Unable to contact server for node state"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            tmp_state = None
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Current Node State: %d" % tmp_state)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Offline Node State: %d" % pbs.ND_OFFLINE)

                        if tmp_state == pbs.ND_OFFLINE:
                            msg = "Cgroup(s) not cleaning up but the node is "
                            msg += "already offline."
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                        elif tmp_state is not None:
                            # Offline the node(s)
                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                            msg = "Offlining node since cgroup(s) are not "
                            msg += "cleaning up"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            vnode = pbs.event().vnode_list[self.hostname]

                            vnode.state = pbs.ND_OFFLINE

                            # Write a file locally to reduce server traffic
                            # when it comes time to online the node
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Offline file: %s" % self.offline_file)
                            open(self.offline_file, 'a').close()
                            vnode.comment = self.offline_msg

                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                        else:
                            pass

                    return False

        except OSError, exc:
            pbs.logmsg(pbs.EVENT_SYSTEM,
                       "Failed to remove cgroup path: %s (%s)" %
                       (path, errno.errorcode[exc.errno]))
        except:
            pbs.logmsg(pbs.EVENT_SYSTEM, "Failed to remove cgroup path: %s" %
                       (path))

        return True

    # Removes cgroup directories that are not associated with a local job
    def cleanup_orphans(self, local_jobs):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        orphan_cnt = 0
        for key in self.paths.keys():
            for dir in glob.glob(os.path.join(self.paths[key], '[0-9]*')):
                jobid = os.path.basename(dir)
                if jobid not in local_jobs:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Removing orphaned cgroup: %s" %
                               (caller_name(), dir))
                    # default number of attempts at killing tasks
                    status = self.__remove_cgroup(dir)
                    if not status:
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "%s: Removing orphaned cgroup %s failed " %
                                   (caller_name(), dir))
                        orphan_cnt += 1
        return orphan_cnt

    # Removes the cgroup directories for a job
    def delete(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        tasks_kill_attempted = False

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            if not tasks_kill_attempted:
                # Make multiple attempts at killing tasks in cgroups on
                # first subsystem encountered since it is "normal" for
                # a cgroup.delete to encounter processes hard to kill
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                                           jobid),
                                              tasks_kill_attempts=(
                                              CGROUP_KILL_ATTEMPTS))
                tasks_kill_attempted = True
            else:
                # We tried getting rid of job processes earlier,
                # so don't try N times with sleeps
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                              jobid), tasks_kill_attempts=1)

            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Status: %s" %
                       (caller_name(), status))
            if status is False:
                # Rerun the job and log the message
                line = 'Unable to cleanup a cgroup. '
                line += 'Will offline the node and requeue the job '
                line += 'and try again. job run_count: %d' % \
                        pbs.event().job.run_count
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Failed to remove cgroup: %s" %
                           (caller_name(), line))
                pbs.event().accept()

    # Write a value to a limit file
    def write_value(self, file, value, mode='w'):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: writing %s to %s" %
                   (caller_name(), value, file))
        try:
            with open(file, mode) as fd:
                fd.write(str(value) + '\n')
        except IOError, exc:
            if exc.errno == errno.ENOENT:
                pbs.logmsg(pbs.EVENT_SYSTEM,
                           "Trying to set limit to unknown file %s" % file)
            elif exc.errno in [errno.EACCES, errno.EPERM]:
                pbs.logmsg(pbs.EVENT_SYSTEM, "Permission denied on file %s" %
                           file)
            elif exc.errno == errno.EBUSY:
                raise CgroupBusyError("Limit rejected")
            elif exc.errno == errno.EINVAL:
                raise CgroupLimitError("Invalid limit value: %s" % (value))
            else:
                raise
        except:
            raise

    # Return memory failcount
    def __get_mem_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.failcnt"), 'r').read().strip())
        except:
            return None

    # Return vmem failcount
    def __get_memsw_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.failcnt"), 'r').read().strip())
        except:
            return None

    # Return hpmem failcount
    def __get_hugetlb_failcnt(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.failcnt"))[0], 'r').read().strip())
        except:
            return None

    # Return the max usage of memory in bytes
    def __get_max_mem_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of memsw in bytes
    def __get_max_memsw_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of hugetlb in bytes
    def __get_max_hugetlb_usage(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.max_usage_in_bytes"))[0],
                            'r').read().strip())
        except:
            return None

    # Return the cpuacct.usage in cpu seconds
    def __get_cpu_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "cpuacct.usage"), 'r').read().strip())
        except:
            return None

    # Assign CPUs to the cpuset
    def select_cpus(self, path, ncpus):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path is %s" % (caller_name(), path))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: ncpus is %s" %
                   (caller_name(), ncpus))
        if ncpus < 1:
            ncpus = 1
        try:
            # Must select from those currently available
            cpufile = os.path.basename(path)
            base = os.path.dirname(path)
            parent = os.path.dirname(base)
            avail = cpus2list(open(os.path.join(parent, cpufile),
                              'r').read().strip())
            if len(avail) < 1:
                raise CgroupProcessingError("No CPUs avaialble in cgroup.")
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Available CPUs: %s" %
                       (caller_name(), avail))
            assigned = []
            for file in glob.glob(os.path.join(parent, '[0-9]*', cpufile)):
                cpus = cpus2list(open(file, 'r').read().strip())
                for id in cpus:
                    if id in avail:
                        avail.remove(id)
            if len(avail) < ncpus:
                raise CgroupProcessingError(
                    "Insufficient CPUs avaialble in cgroup.")
            if len(avail) == ncpus:
                return avail
            # FUTURE: Try to minimize NUMA nodes based on memory requirement
            return avail[0:ncpus]
        except:
            raise

    # Return the error message in system message file
    def __get_error_msg(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        proc = subprocess.Popen(['dmesg'],
                                shell=False, stdout=subprocess.PIPE)
        stdout = proc.communicate()[0].splitlines()
        stdout.reverse()
        # Check to see if the job id is found in dmesg otherwise
        # you could get the information for another job that was killed
        # the line will look like Memory cgroup stats for /pbspro/279.centos7
        kill_line = ""

        for line in stdout:
            start = line.find('Killed process ')
            if start >= 0:
                kill_line = line[start:]
            job_start = line.find(jobid)
            if job_start >= 0:
                # pbs.logmsg(pbs.EVENT_DEBUG3,
                #           "%s: Jobid: %s, Line: %s" %
                #           (caller_name(), jobid, line))
                return kill_line
        return ""

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_job_env_file(self, jobid, env_list):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        # if not os.path.exists(self.host_job_env_dir):
        #    os.makedirs(self.host_job_env_dir, 0755)

        # Write out assigned_resources
        try:
            lines = string.join(env_list, '\n')
            filename = self.host_job_env_filename % jobid
            outfile = open(filename, "w")
            outfile.write(lines)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" % (filename))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (lines))
            return True
        except:
            return False

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        if not os.path.exists(self.hook_storage_dir):
            os.makedirs(self.hook_storage_dir, 0755)

        # Write out assigned_resources
        try:
            json_str = json.dumps(self.host_assigned_resources)
            outfile = open(self.hook_storage_dir+os.sep+jobid, "w")
            outfile.write(json_str)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" %
                       (self.hook_storage_dir+os.sep+jobid))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (json_str))
            return True
        except:
            return False

    def read_in_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        pbs.logmsg(pbs.EVENT_DEBUG3, "Host assigned resources: %s" %
                   (self.host_assigned_resources))
        if os.path.isfile(self.hook_storage_dir+os.sep+jobid):
            # Write out assigned_resources
            try:
                infile = open(self.hook_storage_dir+os.sep+jobid, 'r')
                json_data = json.load(infile, object_hook=decode_dict)
                self.host_assigned_resources = json_data
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Host assigned resources: %s" %
                           (self.host_assigned_resources))
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
            finally:
                infile.close()

        if self.host_assigned_resources is not None:
            return True
        else:
            return False

#
#
# FUNCTION main
#


def main():
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Function called" % (caller_name()))
    hostname = pbs.get_local_nodename()
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Host is %s" % (caller_name(), hostname))

    # Log the hook event type
    e = pbs.event()
    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook name is %s" %
               (caller_name(), e.hook_name))

    try:
        # Instantiate the hook utility class
        hooks = HookUtils()
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Event type is %s" %
                   (caller_name(), hooks.event_name(e.type)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(hooks)))
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: Failed to instantiate hook utility class" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        e.accept()

    if not hooks.hashandler(e.type):
        # Bail out if there is no handler for this event
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s event not handled by this hook" %
                   (caller_name(), hooks.event_name(e.type)))
        e.accept()

    try:
        # If an exception occurs, jobutil must be set
        jobutil = None
        # Instantiate the job utility class first so jobutil can be accessed
        # by the exception handlers.
        if hasattr(e, 'job'):
            jobutil = JobUtils(e.job)
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Job information class instantiated" %
                       (caller_name()))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" %
                       (caller_name(), repr(jobutil)))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Event does not include a job" %
                       (caller_name()))
        # Instantiate the cgroup utility class
        vnode = None
        if hasattr(e, 'vnode_list'):
            if hostname in e.vnode_list.keys():
                vnode = e.vnode_list[hostname]
        cgroup = CgroupUtils(hostname, vnode)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Cgroup utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(cgroup)))
        # Bail out if there is nothing to do
        if len(cgroup.subsystems) < 1:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: All cgroups disabled" %
                       (caller_name()))
            e.accept()

        # Call the appropriate handler
        if hooks.invoke_handler(e, cgroup, jobutil) is True:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned success" %
                       (caller_name()))
            e.accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned failure" %
                       (caller_name()))
            e.reject()

    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass

    except AdminError, exc:
        # Something on the system is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Admin error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except UserError, exc:
        # User must correct problem and resubmit job
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("User error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.delete()
                msg += " (deleted)"
            except:
                msg += " (delete failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except JobValueError, exc:
        # Something in PBS is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Job value error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except Exception, exc:
        # Catch all other exceptions and report them.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Unexpected error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

# line below required for unittesting
if __name__ == "__builtin__":
    start_time = time.time()
    try:
        main()
    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
    finally:
        pbs.logmsg(pbs.EVENT_DEBUG, "Elapsed time: %0.4lf" %
                   (time.time() - start_time))
---
2016-07-18 16:55:42,084 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups.json', 'content-encoding': 'default'}
2016-07-18 16:55:42,085 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups.json
2016-07-18 16:55:42,141 INFO     job: executable set to /bin/sleep with arguments: 100
2016-07-18 16:55:42,142 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0755 /tmp/PtlPbsJobScript9HTPa0
2016-07-18 16:55:42,152 INFOCLI  centos7: sudo -H -u pbsuser /usr/local/bin/qsub -N test_t2 -l select=1:ncpus=1:mem=300mb:vmem=320mb /tmp/PtlPbsJobScript9HTPa0
2016-07-18 16:55:42,183 INFO     submit to centos7 as pbsuser: job 377.centos7 OrderedDict([('Job_Name', 'test_t2'), ('Resource_List.select', '1:ncpus=1:mem=300mb:vmem=320mb')])
2016-07-18 16:55:42,183 INFOCLI  job script /tmp/PtlPbsJobScript9HTPa0
---
#!/usr/bin/env python

import os
import subprocess
from subprocess import Popen, PIPE
import time

eat_mem_c = """
#include <stdio.h>
#include <stdlib.h>
#include <string.h>


#define ONE_GB (0x40000000L)
#define ONE_MB (0x100000L)

void
eatmem(total_mb, block_mb)
{
	int i;
	char *buf;
	size_t block;

	for (i=1, buf=NULL, block=(block_mb*ONE_MB); (i*block) < (total_mb*ONE_MB); i++) {
		buf = realloc((void *)buf, (i*block));
		memset(buf+((i-1)*block), 0, block);
	}
	return;
}

int
main(int argc, char *argv[])
{
	int total_mb = 8;
	int block_mb = 1;

	if (argc > 1)
		total_mb = atoi(argv[1]);

	if (argc > 2)
		block_mb = atoi(argv[2]);

	eatmem(total_mb, block_mb);
	printf("Initialized %dMB in blocks of %dMB\\n", total_mb, block_mb);
}
"""

fname = "/tmp/pbs_pp325_eat_mem.c"
fname_exe = "/tmp/pbs_pp325_eat_mem"
fout = open(fname, "w")
fout.write(eat_mem_c)
fout.close()

print os.getppid()

p = Popen(["gcc", "-o", fname_exe, fname], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

p = Popen([fname_exe, "400", "10"], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

os.remove(fname)
os.remove(fname_exe)
time.sleep(1)

---
2016-07-18 16:55:42,183 INFOCLI  centos7: /usr/local/bin/qstat -f 377.centos7
2016-07-18 16:55:42,283 INFO     expect on server centos7: job_state = R job 377.centos7  got: job_state = Q
2016-07-18 16:55:42,784 INFOCLI  centos7: /usr/local/bin/qmgr -c set server scheduling=True
2016-07-18 16:55:42,820 INFOCLI  centos7: /usr/local/bin/qstat -f 377.centos7
2016-07-18 16:55:42,951 INFO     expect on server centos7: job_state = R job 377.centos7 attempt: 2 ...  OK
2016-07-18 16:55:42,951 INFO     Time: 1468886142
2016-07-18 16:55:42,951 INFO     Ctime: 1468886108, type: 1468886108
2016-07-18 16:55:42,957 INFO     mom centos7 log match: searching for ".*377.centos7;update_job_usage: Memory usage: vmem=3208\d{2}kb.*" - using regular expression  - No match
2016-07-18 16:55:53,284 INFO     mom centos7 log match: searching for ".*377.centos7;update_job_usage: Memory usage: vmem=3208\d{2}kb.*" - using regular expression ... OK
2016-07-18 16:55:53,285 INFO     ==============================
2016-07-18 16:55:53,285 INFO      Entered CgroupsTests tearDown
2016-07-18 16:55:53,285 INFO     ==============================
2016-07-18 16:55:53,285 INFO     ===============================
2016-07-18 16:55:53,285 INFO     Completed CgroupsTests tearDown
2016-07-18 16:55:53,285 INFO     ===============================
2016-07-18 16:55:53,285 INFO     ok

2016-07-18 16:55:53,286 INFO     test name: test_t3 (tests.cgroups_tests.CgroupsTests)...
2016-07-18 16:55:53,286 INFO     test start time: Mon Jul 18 16:55:53 2016
2016-07-18 16:55:53,286 INFO     test docstring: 
 Test to verify that the node is offlined when it can't clean up
            the cgroup and brought back online once the cgroup is cleaned up
        
2016-07-18 16:55:53,286 INFO     ===========================
2016-07-18 16:55:53,286 INFO      Entered CgroupsTests setUp
2016-07-18 16:55:53,286 INFO     ===========================
2016-07-18 16:55:53,286 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:53,345 INFO     status on centos7: server
2016-07-18 16:55:53,345 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:53,399 INFO     manager on centos7: set server {'managers': (3, 'root@*')}
2016-07-18 16:55:53,399 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers-=root@*
2016-07-18 16:55:53,425 INFO     manager on centos7: set server {'managers': (2, 'root@*')}
2016-07-18 16:55:53,425 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers+=root@*
2016-07-18 16:55:53,472 INFO     server centos7: reverting configuration to defaults
2016-07-18 16:55:53,472 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:53,527 INFO     select on centos7: __ALL__
2016-07-18 16:55:53,527 INFOCLI  centos7: /usr/local/bin/qselect
2016-07-18 16:55:53,551 INFOCLI  centos7: /usr/local/bin/qstat -f @centos7.usa.org
2016-07-18 16:55:53,604 INFO     expect on server centos7: job_state set 0 job ...  OK
2016-07-18 16:55:53,604 INFOCLI  centos7: /usr/local/bin/pbs_rstat -f
2016-07-18 16:55:53,620 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:53,641 INFO     manager on centos7: delete hook t1_l1
2016-07-18 16:55:53,641 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c delete hook t1_l1
2016-07-18 16:55:53,679 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:55:53,722 INFO     expect on server centos7:  unset  hook t1_l1 ...  OK
2016-07-18 16:55:53,722 INFOCLI  centos7: /usr/local/bin/qstat -Qf @centos7.usa.org
2016-07-18 16:55:53,753 INFO     manager on centos7: delete queue workq
2016-07-18 16:55:53,753 INFOCLI  centos7: /usr/local/bin/qmgr -c delete queue workq
2016-07-18 16:55:53,766 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:55:53,780 INFO     expect on server centos7:  unset  queue workq ...  OK
2016-07-18 16:55:53,780 INFO     manager on centos7: create queue workq {'started': 'True', 'queue_type': 'Execution', 'enabled': 'True'}
2016-07-18 16:55:53,780 INFOCLI  centos7: /usr/local/bin/qmgr -c create queue workq started=True,queue_type=Execution,enabled=True
2016-07-18 16:55:53,796 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:55:53,806 INFO     expect on server centos7: started set True || queue_type set Execution || enabled set True queue workq ...  OK
2016-07-18 16:55:53,806 INFO     manager on centos7: set server {'log_events': '511', 'default_queue': 'workq'}
2016-07-18 16:55:53,806 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=511,default_queue=workq
2016-07-18 16:55:53,829 INFO     status on centos7: resource
2016-07-18 16:55:53,830 INFO     manager on centos7: list resource
2016-07-18 16:55:53,830 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource
2016-07-18 16:55:53,882 INFO     manager on centos7: delete resource nmics,ngpus
2016-07-18 16:55:53,882 INFOCLI  centos7: /usr/local/bin/qmgr -c delete resource nmics,ngpus
2016-07-18 16:55:53,935 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource nmics
2016-07-18 16:55:53,948 INFO     expect on server centos7:  unset  resource nmics ...  OK
2016-07-18 16:55:53,948 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource ngpus
2016-07-18 16:55:53,960 INFO     expect on server centos7:  unset  resource ngpus ...  OK
2016-07-18 16:55:53,960 INFOCLI  status on centos7: server license_count
2016-07-18 16:55:53,960 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:54,013 INFO     server: centos7.usa.org licensed
2016-07-18 16:55:54,047 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/server_priv/comm.lock
2016-07-18 16:55:54,081 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:55:54,092 INFO     scheduler centos7: reverting configuration to defaults
2016-07-18 16:55:54,092 INFO     manager on centos7: list sched
2016-07-18 16:55:54,092 INFOCLI  centos7: /usr/local/bin/qmgr -c list sched
2016-07-18 16:55:54,128 INFO     manager on centos7: unset sched ['sched_cycle_length']
2016-07-18 16:55:54,129 INFOCLI  centos7: /usr/local/bin/qmgr -c unset sched sched_cycle_length
2016-07-18 16:55:54,145 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/dedicated_time
2016-07-18 16:55:54,176 INFOCLI2 centos7: cmp /usr/local/etc/pbs_resource_group /var/spool/pbs/sched_priv/resource_group
2016-07-18 16:55:54,187 INFO     scheduler centos7: reverting holidays file to default
2016-07-18 16:55:54,188 INFOCLI2 centos7: cmp /usr/local/etc/pbs_holidays /var/spool/pbs/sched_priv/holidays
2016-07-18 16:55:54,198 INFOCLI2 centos7: cmp /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:54,204 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:54,221 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:54,233 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:55:54,233 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:55:54,287 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:55:54,298 INFOCLI2 centos7: sudo -H cmp /usr/local/sbin/pbs_mom /usr/local/sbin/pbs_mom.cpuset
2016-07-18 16:55:54,347 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:55:54,357 INFO     mom centos7: reverting configuration to defaults
2016-07-18 16:55:54,357 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/epilogue
2016-07-18 16:55:54,377 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/prologue
2016-07-18 16:55:54,387 INFOCLI2 centos7: sudo -H ls /var/spool/pbs/mom_priv/config.d
2016-07-18 16:55:54,400 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsZKHmnx /var/spool/pbs/mom_priv/config
2016-07-18 16:55:54,412 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/mom_priv/config
2016-07-18 16:55:54,423 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/mom_priv/config
2016-07-18 16:55:54,443 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/mom_priv/config
2016-07-18 16:55:54,461 INFO     mom centos7: sent signal -HUP
2016-07-18 16:55:54,461 INFOCLI2 centos7: sudo -H kill -HUP 42976
2016-07-18 16:55:54,503 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:55:54,513 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 16:55:54,530 INFO     status on centos7: node centos7
2016-07-18 16:55:54,530 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:55:54,544 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:55:54,558 INFO     expect on server centos7: state = free node centos7 ...  OK
2016-07-18 16:55:54,558 INFO     ============================
2016-07-18 16:55:54,558 INFO     Completed CgroupsTests setUp
2016-07-18 16:55:54,559 INFO     ============================
2016-07-18 16:55:54,559 INFO     server centos7: server operating mode set to cli
2016-07-18 16:55:54,559 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:54,610 INFO     expect on server centos7: pbs_version ~ .*14.* server centos7.usa.org ...  OK
2016-07-18 16:55:54,611 INFO     manager on centos7: set server {'log_events': '2047'}
2016-07-18 16:55:54,611 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=2047
2016-07-18 16:55:54,636 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:55:54,691 INFO     expect on server centos7: log_events set 2047 server centos7.usa.org ...  OK
2016-07-18 16:55:54,692 INFO     scheduler centos7: config {'resources': 'ncpus,mem,host,vnode,vmem'}
2016-07-18 16:55:54,693 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /var/spool/pbs/sched_priv/sched_config /var/spool/pbs/sched_priv/sched_config.bak
2016-07-18 16:55:54,715 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsI59pkU /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:54,726 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:54,739 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:54,755 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:55:54,768 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:55:54,769 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:55:54,779 INFO     scheduler centos7 log match: searching for "Error reading line" - No match
2016-07-18 16:55:55,781 INFO     manager on centos7 as root: create resource nmics {'flag': 'nh', 'type': 'long'}
2016-07-18 16:55:55,782 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource nmics flag=nh,type=long
2016-07-18 16:55:55,806 INFO     manager on centos7 as root: create resource ngpus {'flag': 'nh', 'type': 'long'}
2016-07-18 16:55:55,806 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource ngpus flag=nh,type=long
2016-07-18 16:55:55,836 INFO     Test Summary: test_t1_l1 tests "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" cgroup hook
2016-07-18 16:55:55,836 INFO     status on centos7: hook
2016-07-18 16:55:55,836 INFO     manager on centos7: list hook
2016-07-18 16:55:55,837 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:55,855 INFO     status on centos7: hook
2016-07-18 16:55:55,855 INFO     manager on centos7: list hook
2016-07-18 16:55:55,855 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:55:55,889 INFO     manager on centos7: create hook t1_l1
2016-07-18 16:55:55,889 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c create hook t1_l1
2016-07-18 16:55:55,910 INFO     manager on centos7: set hook t1_l1 {'freq': 2, 'enabled': 'True', 'event': '"execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"'}
2016-07-18 16:55:55,910 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set hook t1_l1 freq=2,enabled=True,event="execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"
2016-07-18 16:55:55,952 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:55:55,994 INFO     expect on server centos7: freq set 2 || enabled set True || event set "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" hook t1_l1 ...  OK
2016-07-18 16:55:55,995 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-python', 'input-file': '/tmp/PtlPbsRxv6Ds', 'content-encoding': 'default'}
2016-07-18 16:55:55,995 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-python default /tmp/PtlPbsRxv6Ds
2016-07-18 16:55:56,048 INFO     server centos7: imported hook body
---
#  Copyright (C) 2003-2016 Altair Engineering, Inc. All rights reserved.
#
#  ALTAIR ENGINEERING INC. Proprietary and Confidential. Contains Trade Secret
#  Information. Not for use or disclosure outside ALTAIR and its licensed
#  clients. Information contained herein shall not be decompiled, disassembled,
#  duplicated or disclosed in whole or in part for any purpose. Usage of the
#  software is only as explicitly permitted in the end user software license
#  agreement.
#
#  Copyright notice does not imply publication.

# Last Updated
# Date: 07-06-2016
#
# When soft_limit is true for memory, memsw represents the hard limit.
#
# The resources value in sched_config must contain entries for mem and
# vmem if those subsystems are enabled in the hook configuration file. The
# amount of resource requested will not be avaiable to the hook if they
# are not present.

# This hook handles all of the operations necessary for PBS to support
# cgroups on linux hosts that support them (kernel 2.6.28 and higher)
#
# This hook services the following events:
# - exechost_periodic
# - exechost_startup
# - execjob_attach
# - execjob_begin
# - execjob_end
# - execjob_epilogue
# - execjob_launch

# Imports from __future__ must happen first
from __future__ import with_statement

# Additional imports
import sys
import os
import errno
import signal
import subprocess
import re
import glob
import time
import string
import platform
import traceback
import copy
from stat import S_ISBLK, S_ISCHR
import operator
import pwd
import pbs
import fnmatch
import socket

try:
    import json
except:
    import simplejson as json

CGROUP_KILL_ATTEMPTS = 12

# Flag to turn off features not in 12.2.40X branch
CRAY_12_BRANCH = False


# ============================================================================
# Derived error classes
# ============================================================================

# Base class for errors fixable only by administrative action.


class AdminError(Exception):
    pass

# Base class for errors in processing, unknown cause.


class ProcessingError(Exception):
    pass

# Base class for errors fixable by the user.


class UserError(Exception):
    pass

# Errors in PBS job resource values.


class JobValueError(UserError):
    pass

# Errors when the cgroup is busy.


class CgroupBusyError(ProcessingError):
    pass

# Errors in configuring cgroup.


class CgroupConfigError(AdminError):
    pass

# Errors in configuring cgroup.


class CgroupLimitError(AdminError):
    pass

# Errors processing cgroup.


class CgroupProcessingError(ProcessingError):
    pass

# ============================================================================
# Utility functions
# ============================================================================

#
# FUNCTION caller_name
#
# Return the name of the calling function or method.
#


def caller_name():
    return str(sys._getframe(1).f_code.co_name)

#
# FUNCTION convert_size
#
# Convert a string containing a size specification (e.g. "1m") to a
# string using different units (e.g. "1024k").
#
# This function only interprets a decimal number at the start of the string,
# stopping at any unrecognized character and ignoring the rest of the string.
#
# When down-converting (e.g. MB to KB), all calculations involve integers and
# the result returned is exact. When up-converting (e.g. KB to MB) floating
# point numbers are involved. The result is rounded up. For example:
#
# 1023MB -> GB yields 1g
# 1024MB -> GB yields 1g
# 1025MB -> GB yields 2g  <-- This value was rounded up
#
# Pattern matching or conversion may result in exceptions.
#


def convert_size(value, units='b'):
    logs = {'b': 0, 'k': 10, 'm': 20, 'g': 30,
            't': 40, 'p': 50, 'e': 60, 'z': 70, 'y': 80}
    try:
        new = units[0].lower()
        if new not in logs.keys():
            new = 'b'
        val, old = re.match('([-+]?\d+)([bkmgtpezy]?)',
                            str(value).lower()).groups()
        val = int(val)
        if val < 0:
            raise ValueError('Value may not be negative')
        if old not in logs.keys():
            old = 'b'
        factor = logs[old] - logs[new]
        val *= 2 ** factor
        slop = val - int(val)
        val = int(val)
        if slop > 0:
            val += 1
        # pbs.size() does not like units following zero
        if val <= 0:
            return '0'
        else:
            return str(val) + new
    except:
        return None

#
# FUNCTION size_as_int
#
# Convert a size string to an integer representation of size in bytes
#


def size_as_int(value):
    return int(convert_size(value).rstrip(string.ascii_lowercase))

#
# FUNCTION convert_time
#
# Converts a integer value for time into the value of the return unit
#
# A valid decimal number, with optional sign, may be followed by a character
# representing a scaling factor.  Scaling factors may be either upper or
# lower case. Examples include:
# 250ms
# 40s
# +15min
#
# Valid scaling factors are:
# ns  = 10**-9
# us  = 10**-6
# ms  = 10**-3
# s   =      1
# min =     60
# hr  =   3600
#
# Pattern matching or conversion may result in exceptions.
#


def convert_time(value, return_unit='s'):
    multipliers = {'':  1, 'ns': 10 ** -9, 'us': 10 ** -6,
                   'ms': 10 ** -3, 's': 1, 'min': 60, 'hr': 3600}
    n, factor = re.match('([-+]?\d+)(\w*[a-zA-Z])?',
                         str(value).lower()).groups(0)

    # Check to see if there was not unit of time specified
    if factor == 0:
        factor = ''

    # Check to see if the unit is valid
    if str.lower(factor) not in multipliers.keys():
        raise ValueError('Time unit not recognized.')

    # Convert the value to seconds
    value_in_sec = float(n) * float(multipliers[str.lower(factor)])
    if return_unit != 's':
        return value_in_sec / multipliers[str.lower(return_unit)]
    else:
        return float('%lf' % value_in_sec)

# simplejson hook to convert lists from unicode to utf-8


def decode_list(data):
    rv = []
    for item in data:
        if isinstance(item, unicode):
            item = item.encode('utf-8')
        elif isinstance(item, list):
            item = decode_list(item)
        elif isinstance(item, dict):
            item = decode_dict(item)
        rv.append(item)
    return rv

# simplejson hook to convert dictionaries from unicode to utf-8


def decode_dict(data):
    rv = {}
    for key, value in data.iteritems():
        if isinstance(key, unicode):
            key = key.encode('utf-8')
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        elif isinstance(value, list):
            value = decode_list(value)
        elif isinstance(value, dict):
            value = decode_dict(value)
        rv[key] = value
    return rv

# Convert CPU list format (with ranges) to a Python list


def cpus2list(s):
    # The input string is a comma separated list of digits and ranges.
    # Examples include:
    # 0-3,8-11
    # 0,2,4,6
    # 2,5-7,10
    cpus = []
    if s.strip() == '':
        return cpus
    for r in s.split(','):
        if '-' in r[1:]:
            start, end = r.split('-', 1)
            for i in range(int(start), int(end) + 1):
                cpus.append(int(i))
        else:
            cpus.append(int(r))
    return cpus


# Helper function to ignore suspended jobs when removing
# "already used" resources
def job_to_be_ignored(jobid):
    # Get job substate from small utility that calls printjob
    pbs_exec = ''
    if not CRAY_12_BRANCH:
        pbs_conf = pbs.get_pbs_conf()
        pbs_exec = pbs_conf['PBS_EXEC']
    else:
        if 'PBS_EXEC' in self.cfg:
            pbs_exec = self.cfg['PBS_EXEC']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "PBS_EXEC needs to be defined in the config file")
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Exiting the cgroups hook")
            pbs.accept()

    printjob_cmd = pbs_exec+os.sep+'bin'+os.sep+'printjob'

    cmd = [printjob_cmd, jobid]

    substate = None
    try:
        pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
        # Collect the job substate information
        process = subprocess.Popen(
                                   cmd,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        (out, err) = process.communicate()
        # Find the job substate
        substate_re = re.compile(r"substate:\s+(?P<jobstate>\S+)\s+")
        substate = substate_re.search(out)
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Unexpected error in job_to_be_ignored: %s" %
                   ' '.join([repr(sys.exc_info()[0]),
                            repr(sys.exc_info()[1])]))
        out = "Unknown: failed to run " + printjob_cmd

    if substate is not None:
        substate = substate.group().split(':')[1].strip()
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Job %s has substate %s" % (jobid, substate))

        suspended_substates = ['0x2b', '0x2d', 'unknown']
        return (substate in suspended_substates)
    else:
        return False

# ============================================================================
# Utility classes
# ============================================================================

#
# CLASS HookUtils
#


class HookUtils:

    def __init__(self, hook_events=None):
        if hook_events is not None:
            self.hook_events = hook_events
        else:
            self.hook_events = {
                # Defined in the order they appear in module_pbs_v1.c
                pbs.QUEUEJOB: {
                    'name': 'queuejob',
                    'handler': None
                },
                pbs.MODIFYJOB: {
                    'name': 'modifyjob',
                    'handler': None
                },
                pbs.RESVSUB: {
                    'name': 'resvsub',
                    'handler': None
                },
                pbs.MOVEJOB: {
                    'name': 'movejob',
                    'handler': None
                },
                pbs.RUNJOB: {
                    'name': 'runjob',
                    'handler': None
                },
                pbs.PROVISION: {
                    'name': 'provision',
                    'handler': None
                },
                pbs.EXECJOB_BEGIN: {
                    'name': 'execjob_begin',
                    'handler': self.__execjob_begin_handler
                },
                pbs.EXECJOB_PROLOGUE: {
                    'name': 'execjob_prologue',
                    'handler': None
                },
                pbs.EXECJOB_EPILOGUE: {
                    'name': 'execjob_epilogue',
                    'handler': self.__execjob_epilogue_handler
                },
                pbs.EXECJOB_PRETERM: {
                    'name': 'execjob_preterm',
                    'handler': None
                },
                pbs.EXECJOB_END: {
                    'name': 'execjob_end',
                    'handler': self.__execjob_end_handler
                },
                pbs.EXECJOB_LAUNCH: {
                    'name': 'execjob_launch',
                    'handler': self.__execjob_launch_handler
                },
                pbs.EXECHOST_PERIODIC: {
                    'name': 'exechost_periodic',
                    'handler': self.__exechost_periodic_handler
                },
                pbs.EXECHOST_STARTUP: {
                    'name': 'exechost_startup',
                    'handler': self.__exechost_startup_handler
                },
                pbs.MOM_EVENTS: {
                    'name': 'mom_events',
                    'handler': None
                },
            }

            if not CRAY_12_BRANCH:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "CRAY 12 Branch: %s" % (CRAY_12_BRANCH))
                self.hook_events[pbs.EXECJOB_ATTACH] = {
                    'name': 'execjob_attach',
                    'handler': self.__execjob_attach_handler
                }

    def __repr__(self):
        return "HookUtils(%s)" % (repr(self.hook_events))

    def event_name(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['name']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Type: %s not found" % (caller_name(), type))
            return None

    def hashandler(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['handler'] is not None
        else:
            return None

    def invoke_handler(self, event, cgroup, jobutil, *args):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: UID: real=%d, effective=%d" %
                   (caller_name(), os.getuid(), os.geteuid()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: GID: real=%d, effective=%d" %
                   (caller_name(), os.getgid(), os.getegid()))
        if self.hashandler(event.type):
            result = self.hook_events[event.type][
                'handler'](event, cgroup, jobutil, *args)
            return result
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: %s event not handled by this hook." %
                       (caller_name(), self.event_name(event.type)))

    def __execjob_begin_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Instantiate the NodeConfig class for get_memory_on_node and
        # get_vmem_on node
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Host assigned job resources: %s" %
                   (caller_name(), jobutil.host_resources))

        # Make sure the parent cgroup directories exist
        cgroup.create_paths()

        # Make sure that any old cgroups created in an earlier attempt
        # at creating or running the job are gone
        cgroup.delete(e.job.id)

        # Gather the assigned resources
        cgroup.gather_assigned_resources()
        # Create the cgroup(s) for the job
        cgroup.create_job(e.job.id, node)
        # Configure the new cgroup
        cgroup.configure_job(e.job.id, jobutil.host_resources, node)
        # Initialize resource usage for the job
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        # Write out the assigned resources
        cgroup.write_out_cgroup_host_assigned_resources(e.job.id)
        # Write out the environment variable for the host (pbs_attach)
        if 'devices_name' in cgroup.host_assigned_resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Devices: %s" %
                       cgroup.host_assigned_resources['devices_name'])
            env_list = list()
            if len(cgroup.host_assigned_resources['devices_name']) > 0:
                line = str()
                mics = list()
                gpus = list()
                for key in cgroup.host_assigned_resources['devices_name']:
                    if key.startswith('mic'):
                        mics.append(key[3:])
                    elif key.startswith('nvidia'):
                        gpus.append(key[6:])
                if len(mics) > 0:
                    env_list.append('OFFLOAD_DEVICES="%s"' %
                                    string.join(mics, ','))
                if len(gpus) > 0:
                    # don't put quotes around the values. ex "0" or "0,1".
                    # This will cause it to fail.
                    env_list.append('CUDA_VISIBLE_DEVICES=%s' %
                                    string.join(gpus, ','))

            pbs.logmsg(pbs.EVENT_DEBUG, "ENV_LIST: %s" % env_list)
            cgroup.write_out_cgroup_host_job_env_file(e.job.id, env_list)
        return True

    def __execjob_epilogue_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # The resources_used information has a base type of pbs_resource
        # Set the usage data
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        return True

    def __execjob_end_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Delete the cgroup(s) for the job
        cgroup.delete(e.job.id)
        # Remove the host_assigned_resources and job_env file
        for filename in [cgroup.hook_storage_dir+os.sep+e.job.id,
                         cgroup.host_job_env_filename % e.job.id]:
            try:
                os.remove(filename)
            except OSError:
                pbs.logmsg(pbs.EVENT_DEBUG3, "File: %s not found" % (filename))
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Error removing file: %s" % (filename))
                pass

        return True

    def __execjob_launch_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Add the parent process id to the appropriate cgroups.
        cgroup.add_pids(os.getppid(), jobutil.job.id)
        # FUTURE: Add environment variable to the job environment
        # if job requested mic or gpu
        cgroup.read_in_cgroup_host_assigned_resources(e.job.id)
        if cgroup.host_assigned_resources is not None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "host_assigned_resources: %s" %
                       (cgroup.host_assigned_resources))
            cgroup.setup_job_devices_env()
        return True

    def __exechost_periodic_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Cleanup cgroups for jobs not present on this node
        count = cgroup.cleanup_orphans(e.job_list)
        if ('periodic_resc_update' in cgroup.cfg and
                cgroup.cfg['periodic_resc_update']):
            for job in e.job_list.keys():
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: job is %s" %
                           (caller_name(), job))
                cgroup.update_job_usage(job, e.job_list[job].resources_used)
        # Online nodes that were offlined due to a cgroup not cleaning up
        if count == 0 and cgroup.cfg['online_offlined_nodes']:
            vnode = pbs.event().vnode_list[cgroup.hostname]
            if os.path.isfile(cgroup.offline_file):
                line = 'Orphan cgroup(s) have been cleaned up. '

                # Check with the pbs server to see if the node comment matches
                try:
                    tmp_comment = pbs.server().vnode(cgroup.hostname).comment
                except:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Unable to contact server for node comment")
                    tmp_comment = None
                if tmp_comment == cgroup.offline_msg:
                    line += 'Will bring the node back online'
                    vnode.state = pbs.ND_FREE
                    vnode.comment = None
                else:
                    line += 'However, the comment has changed since the node '
                    line += 'was offlined. Node will remain offline'

                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: %s" % (caller_name(), line))
                # Remove file
                os.remove(cgroup.offline_file)
        return True

    def __exechost_startup_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        cgroup.create_paths()
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))

        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        node.create_vnodes()
        if 'memory' in cgroup.subsystems:
            val = node.get_memory_on_node(cgroup.cfg)
            if val is not None:
                if 'vnode_per_numa_node' in cgroup.cfg:
                    if not cgroup.cfg['vnode_per_numa_node']:
                        hn = node.hostname
                        e.vnode_list[hn].resources_available['mem'] = \
                            pbs.size(val)
                        val_cpus = node.totalcpus
                        if val_cpus is not None:
                            e.vnode_list[hn].resources_available['ncpus'] = \
                                int(val_cpus)
                cgroup.set_node_limit('mem', val)
        if 'memsw' in cgroup.subsystems:
            val = node.get_vmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['vmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('vmem', val)
        if 'hugetlb' in cgroup.subsystems:
            val = node.get_hpmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['hpmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('hpmem', val)
        return True

    def __execjob_attach_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logjobmsg(jobutil.job.id, "%s: Attaching PID %s" %
                      (caller_name(), e.pid))
        # Add the job process id to the appropriate cgroups.
        cgroup.add_pids(e.pid, jobutil.job.id)
        return True

#
# CLASS ShallIRunUtils
#


class ShallIRunUtils:

    def __init__(self, hostname, cfg):
        self.cfg = cfg
        self.hostname = hostname

    def allow_users(self, allow_users):
        """ This still needs to be defined """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pass

    def exclude_hosts(self, exclude_hosts):
        """
        Gets the hostname for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        hostname = pbs.get_local_nodename()
        # check to see if hostname is in the exclude_hosts list
        if self.hostname in exclude_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: exclude host %s is in %s" %
                       (caller_name(), self.hostname, exclude_hosts))
            pbs.event().accept()

        # check to see if it regex syntax is used
        pass

    def exclude_vntypes(self, exclude_vntypes):
        """
        Gets the vntype for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        vntype = None

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'
        if os.path.isfile(vntype_file):
            fdata = open(vntype_file).readlines()
            vntype = fdata[0].strip()
            pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
        if vntype is None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype is set to %s" % (vntype))
        elif vntype in exclude_vntypes:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s is in %s" % (vntype, exclude_vntypes))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Exiting Hook")
            pbs.event().accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s not in global exclude: %s" %
                       (vntype, exclude_vntypes))
        pass

    def run_only_on_hosts(self, approved_hosts):
        """
        Gets the list of approved nodes to run on and checks to see if the hook
        should run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Approved hosts: %s, %s" %
                   (type(approved_hosts), approved_hosts))

        # check to see if hostname is in the approved_hosts list
        if approved_hosts == []:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "approved hosts list is empty: %s" % approved_hosts)
        elif self.hostname not in approved_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s is not in the approved list of hosts: %s" %
                       (self.hostname, approved_hosts))
            pbs.event().accept()

        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s in list of approved hosts: %s" %
                       (self.hostname, approved_hosts))

#
# CLASS JobUtils
#


class JobUtils:

    def __init__(self, job, hostname=None, resources=None):
        self.job = job
        self.host_vnodes = list()
        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if resources is not None:
            self.host_resources = resources
        else:
            self.host_resources = self.__host_assigned_resources()

    def __repr__(self):
        return "JobUtils(%s, %s, %s)" % (repr(self.job), repr(self.hostname),
                                         repr(self.host_resources))

    # Return a dictionary of resources assigned to the local node
    def __host_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Bail out if no hostname was provided
        if self.hostname is None:
            raise CgroupProcessingError('No hostname available')
        # Bail out if no job information is present
        if self.job is None:
            raise CgroupProcessingError('No job information available')
        # Determine which vnodes are on this host, if any
        if self.hostname is not None:
            vnhost_pattern = "%s\[[\d+]\]" % self.hostname
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnhost pattern: %s" %
                       (caller_name(), vnhost_pattern))
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job exec_vnode list: %s" %
                       (caller_name(), self.job.exec_vnode))
            for match in re.findall(vnhost_pattern, str(self.job.exec_vnode)):
                self.host_vnodes.append(match)
            if self.host_vnodes is not None:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Associate %s to vnodes on %s" %
                           (caller_name(), self.host_vnodes, self.hostname))

        # Collect host assigned resources
        resources = {}
        for chunk in self.job.exec_vnode.chunks:
            vnode = False
            if self.host_vnodes == [] and chunk.vnode_name != self.hostname:
                continue
            elif self.host_vnodes != [] and \
                    chunk.vnode_name not in self.host_vnodes:
                continue
            elif chunk.vnode_name in self.host_vnodes:
                vnode = True
                if 'vnodes' not in resources:
                    resources['vnodes'] = {}
                if chunk.vnode_name not in resources['vnodes']:
                    resources['vnodes'][chunk.vnode_name] = {}
                # This check is needed since not all resources are
                # required to be in each chunk of a job
                # i.e. exec_vnodes =
                # (node1[0]:ncpus=4:mem=4gb+node1[1]:mem=2gb) +
                # (node1[1]:ncpus=3+node[0]:ncpus=1:mem=4gb)
                if all(resc in resources['vnodes'][chunk.vnode_name]
                       for resc in chunk.chunk_resources.keys()):
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: resources already defined" %
                               (caller_name()))
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s,\t%s" %
                               (caller_name(),
                                chunk.chunk_resources.keys(),
                                resources['vnodes'][chunk.vnode_name].keys()))
                else:
                    # Initialize resources for each vnode
                    for resc in chunk.chunk_resources.keys():
                        # Initialize the new value
                        if isinstance(chunk.chunk_resources[resc],
                                      pbs.pbs_int):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_int(0)
                        elif isinstance(chunk.chunk_resources[resc],
                                        pbs.pbs_float):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_float(0)
                        elif isinstance(chunk.chunk_resources[resc], pbs.size):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.size('0')

            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Chunk %s" %
                       (caller_name(), chunk.vnode_name))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resources: %s" %
                       (caller_name(), resources))
            for resc in chunk.chunk_resources.keys():
                if resc not in resources.keys():
                    # Initialize the new value
                    if isinstance(chunk.chunk_resources[resc], pbs.pbs_int):
                        resources[resc] = pbs.pbs_int(0)
                    elif isinstance(chunk.chunk_resources[resc],
                                    pbs.pbs_float):
                        resources[resc] = pbs.pbs_float(0.0)
                    elif isinstance(chunk.chunk_resources[resc], pbs.size):
                        resources[resc] = pbs.size('0')
                # Add resource value to total
                if type(chunk.chunk_resources[resc]) in \
                        [pbs.pbs_int, pbs.pbs_float, pbs.size]:
                    resources[resc] += chunk.chunk_resources[resc]
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s: resources[%s][%s] is now %s" %
                               (caller_name(), self.hostname, resc,
                                resources[resc]))
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] += \
                                  chunk.chunk_resources[resc]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Setting resource %s to string %s" %
                               (caller_name(), resc,
                                str(chunk.chunk_resources[resc])))
                    resources[resc] = str(chunk.chunk_resources[resc])
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] = \
                                  str(chunk.chunk_resources[resc])
        if not resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: No resources assigned to host %s" %
                       (caller_name(), self.hostname))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources for %s: %s" %
                       (caller_name(), self.hostname, repr(resources)))

        # Return assigned resources for specified host
        pbs.logmsg(pbs.EVENT_DEBUG3, "Job Resources: %s" % (resources))
        return resources

    # Write a message to the job stdout file
    def write_to_stderr(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stderr_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

    # Write a message to the job stdout file
    def write_to_stdout(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stdout_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

#
# CLASS NodeConfig
#


class NodeConfig:

    def __init__(self, cfg, **kwargs):

        self.cfg = cfg
        hostname = None
        meminfo = None
        numa_nodes = None
        devices = None
        hyperthreading = None
        self.hyperthreads_per_core = 1
        self.totalcpus = self.__discover_cpus()

        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        self.host_mom_jobdir = pbs_home+'/mom_priv/jobs'

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'hostname':
                    hostname = val
                elif arg == 'meminfo':
                    meminfo = val
                elif arg == 'numa_nodes':
                    numa_nodes = val
                elif arg == 'devices':
                    devices = val
                elif arg == 'hyperthreading':
                    hyperthreading = val

        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if meminfo is not None:
            self.meminfo = meminfo
        else:
            self.meminfo = self.__discover_meminfo()
        if numa_nodes is not None:
            self.numa_nodes = numa_nodes
        else:
            self.numa_nodes = self.__discover_numa_nodes()
        if devices is not None:
            self.devices = devices
        else:
            self.devices = self.__discover_devices()
        if hyperthreading is not None:
            self.hyperthreading = hyperthreading
        else:
            self.hyperthreading = self.__hyperthreading_enabled()

        # Add the devices count i.e. nmics and ngpus to the numa nodes
        self.__add_devices_to_numa_node()

    def __repr__(self):
        return ("NodeConfig(%s, %s, %s, %s, %s, %s)" %
                (repr(self.cfg), repr(self.hostname), repr(self.meminfo),
                 repr(self.numa_nodes), repr(self.devices),
                 repr(self.hyperthreading)))

    def __add_devices_to_numa_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (self.devices.keys()))
        for device in self.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" % (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" %
                           (self.devices[device].keys()))
                for device_name in self.devices[device]:
                    device_socket = \
                        self.devices[device][device_name]['numa_node']
                    if device == 'mic':
                        if 'nmics' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['nmics'] = 1
                        else:
                            self.numa_nodes[device_socket]['nmics'] += 1
                    elif device == 'gpu':
                        if 'ngpus' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['ngpus'] = 1
                        else:
                            self.numa_nodes[device_socket]['ngpus'] += 1

        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (self.numa_nodes))

    # Discover what type of hardware is on this node and how is it partitioned
    def __discover_numa_nodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        numa_nodes = {}
        for dir in glob.glob(os.path.join(os.sep,
                             "sys", "devices", "system", "node", "node*")):
            id = int(dir.split(os.sep)[5][4:])
            if id not in numa_nodes.keys():
                numa_nodes[id] = {}
                numa_nodes[id]['devices'] = list()
            numa_nodes[id]['cpus'] = open(os.path.join(dir, "cpulist"),
                                          'r').readline().strip()
            with open(os.path.join(dir, "meminfo"), 'r') as fd:
                for line in fd:
                    # Each line will contain four or five fields. Examples:
                    # Node 0 MemTotal:       32995028 kB
                    # Node 0 HugePages_Total:     0
                    entries = line.split()
                    if len(entries) < 4:
                        continue
                    if entries[2] == "MemTotal:":
                        numa_nodes[id][entries[2].rstrip(':')] = \
                            convert_size(entries[3] + entries[4], 'kb')
                    elif entries[2] == "HugePages_Total:":
                        numa_nodes[id][entries[2].rstrip(':')] = entries[3]
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover Numa Nodes: %s" % numa_nodes)
        return numa_nodes

    # Identify devices and to which numa nodes they are attached
    def __discover_devices(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        devices = {}
        for file in glob.glob(os.path.join(os.sep, "sys", "class", "*", "*",
                                           "device", "numa_node")):
            # The file should contain a single integer
            numa_node = int(open(file, 'r').readline().strip())
            if numa_node < 0:
                numa_node = 0
            dirs = file.split(os.sep)
            name = dirs[3]
            instance = dirs[4]
            if name not in devices.keys():
                devices[name] = {}
            devices[name][instance] = {}
            devices[name][instance]['numa_node'] = numa_node
            if name == 'mic':
                s = os.stat('/dev/%s' % instance)
                devices[name][instance]['major'] = os.major(s.st_rdev)
                devices[name][instance]['minor'] = os.minor(s.st_rdev)
                devices[name][instance]['device_type'] = 'c'

        # Check to see if there are gpus on the node
        gpus = self.discover_gpus()
        if isinstance(gpus, dict) and len(gpus.keys()) > 0:
            devices['gpu'] = gpus['gpu']

        pbs.logmsg(pbs.EVENT_DEBUG3, "Discovered devices: %s" % (devices))
        return devices

    def discover_gpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Check to see if nvidia-smi exists and produces valid output
            cmd = ['/usr/bin/nvidia-smi', '-q', '-x']
            if 'nvidia-smi' in self.cfg:
                cmd[0] = self.cfg['nvidia-smi']

            pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
            # Collect the gpu information
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.PIPE)
            output, err = process.communicate()

            # pbs.logmsg(pbs.EVENT_DEBUG3,"output: %s" % output)
            # import the xml library and parse the output
            import xml.etree.ElementTree as ET

            # Parse the data
            name = 'gpu'
            gpu_data = dict()
            gpu_data[name] = {}
            root = ET.fromstring(output)
            pbs.logmsg(pbs.EVENT_DEBUG3, "root.tag: %s" % root.tag)
            for child in root:
                if child.tag == "attached_gpus":
                    # gpu_data['ngpus'] = int(child.text)
                    pass
                if child.tag == "gpu":
                    device_id = child.get('id')
                    number = 'nvidia%s' % child.find('minor_number').text
                    gpu_data[name][number] = dict()
                    gpu_data[name][number]['id'] = device_id

                    # Determine which socket the device is on
                    filename = '/sys/bus/pci/devices/%s/numa_node' % \
                        device_id.lower()
                    isfile = os.path.isfile(filename)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s, %s" % (filename, isfile))
                    if isfile:
                        numa_node = open(filename).read().strip()
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "numa_node: %s" % (numa_node))
                        if int(numa_node) == -1:
                            numa_node = 0
                        gpu_data[name][number]['numa_node'] = int(numa_node)
                        try:
                            dev_filename = '/dev/%s' % number
                            s = os.stat(dev_filename)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find %s" % dev_filename)
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                        gpu_data[name][number]['major'] = os.major(s.st_rdev)
                        gpu_data[name][number]['minor'] = os.minor(s.st_rdev)
                        gpu_data[name][number]['device_type'] = 'c'
            pbs.logmsg(pbs.EVENT_DEBUG, "%s" % gpu_data)
            return gpu_data
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unable to find %s" % string.join(cmd, " "))
            return {'gpu': {}}
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        return {'gpu': {}}

    # Get the memory info on this host
    def __discover_meminfo(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        meminfo = {}
        with open(os.path.join(os.sep, "proc", "meminfo"), 'r') as fd:
            for line in fd:
                entries = line.split()
                if entries[0] == "MemTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "SwapTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "Hugepagesize:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "HugePages_Total:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
                elif entries[0] == "HugePages_Rsvd:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover meminfo: %s" % meminfo)
        return meminfo

    # Determine if the cpus have hyperthreading enabled
    def __hyperthreading_enabled(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            siblings = 0
            cpu_cores = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "siblings":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    siblings = entries[1].strip()
                elif entries[0].strip() == "cpu cores":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_cores = entries[1].strip()
                elif entries[0].strip() == "flags":
                    flag_options = entries[1].split()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: flag_options: %s" %
                               (caller_name(), flag_options))
                    if "ht" in flag_options:
                        rc = True
                        break
        if rc is True:
            self.hyperthreads_per_core = int(siblings)/int(cpu_cores)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: hyperthreads/core: %d" %
                       (caller_name(), self.hyperthreads_per_core))
            if self.hyperthreads_per_core == 1:
                rc = False
        return rc

    def __discover_cpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            cpu_threads = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "processor":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_threads += 1
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: processors found: %d" %
                   (caller_name(), cpu_threads))

        return cpu_threads

    # Gather the jobs running on this node
    def gather_jobs_on_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        # Get the job list from the server
        jobs = None
        try:
            jobs = pbs.server().vnode(self.hostname).jobs
        except:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Unable to contact server to get jobs")
            return None

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs from server for %s: %s" % (self.hostname, jobs))

        if jobs is None:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "No jobs found on %s " % (self.hostname))
            return list()

        jobs_list = dict()
        for job in jobs.split(','):
            # The job list from the node has a space in front of the job id
            # This must be removed or it will not match the cgroup dir
            jobs_list[job.split('/')[0].strip()] = 0
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs on %s: %s" % (self.hostname, jobs_list.keys()))

        return jobs_list.keys()

    # Gather the jobs running on this node
    def gather_jobs_on_node_local(self):
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Method called" % (caller_name()))
        # Get the job list from the jobs directory
        jobs_list = list()
        try:
            for file in os.listdir(self.host_mom_jobdir):
                if fnmatch.fnmatch(file, "*.JB"):
                    jobs_list.append(file[:-3])
            if not jobs_list:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "No jobs found on %s " % (self.hostname))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Jobs from server for %s: %s" %
                           (self.hostname, repr(jobs_list)))

            return jobs_list
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Could not get job list from mom_priv/jobs for %s" %
                       self.hostname)

            return self.gather_jobs_on_node()

    # Get the memory resource on this mom
    def get_memory_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Calculate memory
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
            pbs.logmsg(pbs.EVENT_DEBUG3, "val: %s" % val)
        else:
            val = size_as_int(MemTotal)
        if 'percent_reserve' in config['cgroup']['memory']:
            percent_reserve = config['cgroup']['memory']['percent_reserve']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memory'].keys():
            reserve_memory = config['cgroup']['memory']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                reserve_mem = size_as_int(reserve_memory)
                if val > reserve_mem:
                    val -= reserve_mem
                else:
                    val -= size_as_int("100mb")
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Unable to reserve more memory than " +
                               "available on the node. Available %s, " +
                               "Tried to reserve %s. Reserving 100mb" %
                               (val, reserve_mem))

            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))

        pbs.logmsg(pbs.EVENT_DEBUG3, "Return val: %s" % val)
        return convert_size(str(val), 'kb')

    # Get the virtual memory resource on this mom
    def get_vmem_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Return the value for memory if no swap is configured
        if size_as_int(self.meminfo['SwapTotal']) <= 0:
            return self.get_memory_on_node(config)
        # Calculate vmem
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
        else:
            val = size_as_int(MemTotal)

        val += size_as_int(self.meminfo['SwapTotal'])
        # Determine if memory needs to be reserved
        if 'percent_reserve' in config['cgroup']['memsw']:
            percent_reserve = config['cgroup']['memsw']['reserve_memory']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memsw']:
            reserve_memory = config['cgroup']['memsw']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                val -= size_as_int(reserve_memory)
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))
        return convert_size(str(val), 'kb')

    # Get the huge page memory resource on this mom
    def get_hpmem_on_node(self, config):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        if 'percent_reserve' in config['cgroup']['hugetlb'].keys():
            percent_reserve = config['cgroup']['hugetlb']['percent_reserve']
        else:
            percent_reserve = 0
        # Calculate vmem
        val = size_as_int(self.meminfo['Hugepagesize'])
        val *= (self.meminfo['HugePages_Total'] -
                self.meminfo['HugePages_Rsvd'])
        val -= (val * percent_reserve) / 100
        return convert_size(str(val), 'kb')

    # Create individual vnodes per socket
    def create_vnodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        key = "vnode_per_numa_node"
        if key in self.cfg.keys():
            if not self.cfg[key]:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s is false" %
                           (caller_name(), key))
                return
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s not configured" %
                       (caller_name(), key))
            return

        # Create one vnode per numa node
        vnode_list = pbs.event().vnode_list
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: numa nodes: %s" %
                   (caller_name(), self.numa_nodes))
        # Define the vnodes
        vnode_name = self.hostname
        vnode_list[vnode_name] = pbs.vnode(vnode_name)
        for id in self.numa_nodes.keys():
            vnode_name = self.hostname + "[%d]" % id
            vnode_list[vnode_name] = pbs.vnode(vnode_name)
            for key, val in sorted(self.numa_nodes[id].iteritems()):
                if key == 'cpus':
                    vnode_list[vnode_name].resources_available['ncpus'] = \
                        len(cpus2list(val))/self.hyperthreads_per_core
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['ncpus'] = 0
                elif key == 'MemTotal':
                    mem = self.get_memory_on_node(self.cfg, val)
                    vnode_list[vnode_name].resources_available['mem'] = \
                        pbs.size(mem)
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['mem'] = \
                        pbs.size('0kb')
                elif key == 'HugePages_Total':
                    pass
                elif isinstance(val, list):
                    pass
                elif isinstance(val, dict):
                    pass
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s=%s" %
                               (caller_name(), key, val))
                    vnode_list[vnode_name].resources_available[key] = val

                    if isinstance(val, int):
                        vnode_list[self.hostname].resources_available[key] = 0
                    elif isinstance(val, float):
                        vnode_list[self.hostname].resources_available[key] = \
                            0.0
                    elif isinstance(val, pbs.size):
                        vnode_list[self.hostname].resources_available[key] = \
                            pbs.size('0kb')
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnode list: %s" %
                   (caller_name(), vnode_list))
        return True

#
# CLASS CgroupUtils
#


class CgroupUtils:
    def __init__(self, hostname, vnode, **kwargs):
        cfg = None
        subsystems = None
        paths = None
        vntype = None
        event = None
        self.assigned_resources = dict()
        self.existing_cgroups = dict()
        self.host_assigned_resources = None
        self.pid_lower_limit = 3

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'subsystems':
                    subsystems = val
                elif arg == 'paths':
                    paths = val
                elif arg == 'vntype':
                    vntype = val
                elif arg == 'event':
                    event = val

        try:
            self.hostname = hostname
            self.vnode = vnode
            # __check_os will raise an exception if cgroups are not present
            self.__check_os()
            # Read in the config file
            if cfg is not None:
                self.cfg = cfg
            else:
                self.cfg = self.__parse_config_file()

            # Determine if the hook should run or exit
            siru = ShallIRunUtils(self.hostname, self.cfg)
            siru.exclude_hosts(self.cfg['exclude_hosts'])
            siru.exclude_vntypes(self.cfg['exclude_vntypes'])
            siru.run_only_on_hosts(self.cfg['run_only_on_hosts'])

            # Collect the mount points
            if paths is not None:
                self.paths = paths
            else:
                self.paths = self.__set_paths()
            # Define the local vnode type
            if vntype is not None:
                self.vntype = vntype
            else:
                self.vntype = self.__get_vnode_type()
            # Determine which subsystems we care about
            if subsystems is not None:
                self.subsystems = subsystems
            else:
                self.subsystems = self.__target_subsystems()

            # location to store information for the different hook events
            pbs_home = ''
            if not CRAY_12_BRANCH:
                pbs_conf = pbs.get_pbs_conf()
                pbs_home = pbs_conf['PBS_HOME']
            else:
                if 'PBS_HOME' in self.cfg:
                    pbs_home = self.cfg['PBS_HOME']
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "PBS_HOME needs to be defined in the " +
                               "config file")
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Exiting the cgroups hook")
                    pbs.accept()

            self.hook_storage_dir = pbs_home+'/mom_priv/hooks/hook_data'
            self.host_job_env_dir = pbs_home+'/aux'
            self.host_job_env_filename = self.host_job_env_dir+os.sep+"%s.env"

            # information for offlining nodes
            self.offline_file = \
                pbs_home+os.sep+'mom_priv'+os.sep+'hooks'+os.sep
            self.offline_file += "%s.offline" % pbs.event().hook_name
            self.offline_msg = "Hook %s: " % pbs.event().hook_name
            self.offline_msg += "Unable to clean up one or more cgroups"

        except:
            raise

    def __repr__(self):
        return ("CgroupUtils(%s, %s, %s, %s, %s, %s)" %
                (repr(self.hostname), repr(self.vnode), repr(self.cfg),
                 repr(self.subsystems), repr(self.paths), repr(self.vntype)))

    # Validate the OS type and version
    def __check_os(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check to see if the platform is linux and the kernel is new enough
        if platform.system() != 'Linux':
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: OS does not support cgroups" %
                       (caller_name()))
            raise CgroupConfigError("OS type not supported")
        rel = map(int,
                  string.split(string.split(platform.release(), '-')[0], '.'))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Detected Linux kernel version %d.%d.%d" %
                   (caller_name(), rel[0], rel[1], rel[2]))
        supported = False
        if rel[0] > 2:
            supported = True
        elif rel[0] == 2:
            if rel[1] > 6:
                supported = True
            elif rel[1] == 6:
                if rel[2] >= 28:
                    supported = True
        if not supported:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Kernel needs to be >= 2.6.28: %s" %
                       (caller_name(), system_info[2]))
            raise CgroupConfigError("OS version not supported")
        return supported

    # Read the config file in json format
    def __parse_config_file(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        config = {}
        # Identify the config file and read in the data
        if 'PBS_HOOK_CONFIG_FILE' in os.environ:
            config_file = os.environ["PBS_HOOK_CONFIG_FILE"]
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Config file is %s" %
                       (caller_name(), config_file))
            try:
                config = json.load(open(config_file, 'r'),
                                   object_hook=decode_dict)
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
        else:
            raise CgroupConfigError("No configuration file present")
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Initial Cgroup Config: %s" %
                   (caller_name(), config))
        # Set some defaults if they are not present
        if 'cgroup_prefix' not in config.keys():
            config['cgroup_prefix'] = 'pbspro'
        if 'periodic_resc_update' not in config.keys():
            config['periodic_resc_update'] = False
        if 'vnode_per_numa_node' not in config.keys():
            config['vnode_per_numa_node'] = False
        if 'online_offlined_nodes' not in config.keys():
            config['online_offlined_nodes'] = False
        if 'exclude_hosts' not in config.keys():
            config['exclude_hosts'] = []
        if 'run_only_on_hosts' not in config.keys():
            config['run_only_on_hosts'] = []
        if 'exclude_vntypes' not in config.keys():
            config['exclude_vntypes'] = []
        if 'cgroup' not in config.keys():
            config['cgroup'] = {}
        else:
            for subsys in config['cgroup']:
                subsystem = config['cgroup'][subsys]
                if 'enabled' not in subsystem.keys():
                    config['cgroup'][subsys]['enabled'] = False
                if 'exclude_hosts' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_hosts'] = []
                if 'exclude_vntypes' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_vntypes'] = []

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Cgroup Config with defaults: %s" %
                   (caller_name(), config))
        return config

    # Determine which subsystems are being requested
    def __target_subsystems(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        subsystems = []
        for key in self.cfg['cgroup'].keys():
            if self.enabled(key):
                subsystems.append(key)
        if 'memory' not in subsystems:
            if 'memsw' in subsystems:
                # Remove memsw since it must be greater then
                # or equal to memory
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing memsw from enabled subsystems")
                subsystems.remove('memsw')
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Enabled subsystems: %s" %
                   (caller_name(), subsystems))
        # It is not an error for all subsystems to be disabled.
        # This host or vnode type may be in the excluded list.
        return subsystems

    # Copy a setting from the parent cgroup
    def __copy_from_parent(self, dest):
        try:
            file = os.path.basename(dest)
            dir = os.path.dirname(dest)
            parent = os.path.dirname(dir)
            source = os.path.join(parent, file)
            if not os.path.isfile(source):
                raise CgroupConfigError('Failed to read %s' % (source))
            self.write_value(dest, open(source, 'r').read().strip())
        except:
            raise

    # Create a dictionary of the cgroup subsystems and their corresponding
    # directories
    def __set_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        paths = {}
        try:
            # Loop through the mounts and collect the ones for cgroups
            with open(os.path.join(os.sep, "proc", "mounts"), 'r') as fd:
                for line in fd:
                    entries = line.split()
                    if len(entries) > 3 and entries[2] == "cgroup":
                        flags = entries[3].split(',')
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "subsysdir: %s (flags=%s)" %
                                   (entries[1], flags))
                        for subsys in flags:
                            paths[subsys] = os.path.join(
                                entries[1], self.cfg["cgroup_prefix"])
        except:
            raise
        if len(paths.keys()) < 1:
            raise CgroupConfigError("Cgroup paths not detected")
        # Both the memory and memsw cgroups share the same mount point
        if "memory" in paths.keys():
            paths["memsw"] = paths["memory"]
        return paths

    # Create the cgroup parent directories that will contain the jobs
    def create_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Create the directories that PBS will use to house the jobs
            old_umask = os.umask(0022)
            for subsys in self.subsystems:
                if subsys not in self.paths.keys():
                    raise CgroupConfigError('No path for subsystem: %s' %
                                            (subsys))
                if not os.path.exists(self.paths[subsys]):
                    os.makedirs(self.paths[subsys], 0755)
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Created directory %s" %
                               (caller_name(), self.paths[subsys]))
                    if subsys == 'memory' or subsys == 'memsw':
                        # Set file to
                        # <cgroup>/memory/pbspro/memory.use_hierarchy
                        file = os.path.join(self.paths[subsys],
                                            'memory.use_hierarchy')
                        if not os.path.isfile(file):
                            raise CgroupConfigError('Failed to configure %s' %
                                                    (file))
                        self.write_value(file, 1)
                    elif subsys == 'cpuset':
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.cpus'))
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.mems'))

        except:
            raise CgroupConfigError("Failed to create directory: %s" %
                                    (self.paths[subsys]))
        finally:
            os.umask(old_umask)

    # Return the vnode type of the local node
    def __get_vnode_type(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")

        # File to check for if it is not defined in the hook input file
        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'

        # self.vnode is None for pbs_attach events
        pbs.logmsg(pbs.EVENT_DEBUG3, "vnode: %s" % self.vnode)
        if self.vnode is not None:
            if 'vntype' in self.vnode.resources_available and \
                    self.vnode.resources_available['vntype'] is not None:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "vntype: %s" %
                           self.vnode.resources_available['vntype'])
                return self.vnode.resources_available['vntype']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3, "vntype file: %s" % vntype_file)
                if os.path.isfile(vntype_file):
                    fdata = open(vntype_file).readlines()
                    vntype = fdata[0].strip()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
                    return vntype
        # If vntype was not set then log a message. It is too expensive
        # to have all moms query the server for large jobs.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: resources_available.vntype is " % caller_name() +
                   "not set for this vnode")
        return None

    # Gather resources that are already assigned
    def gather_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        assigned_resources = {}

        # Gather the cpus and memory sockets assigned
        directories = [x[0] for x in os.walk(self.paths['cpuset'])]

        # Add the existing cgroups to a list to check if assigning
        # resources fail
        for dir in directories[1:]:
            if dir.find(pbs.event().job.id) == -1:
                self.existing_cgroups[dir] = []

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'cpuset' in self.subsystems and \
                self.cfg['cgroup']['cpuset']['enabled']:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather cpuset resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['cpuset'], tmp_dir)
                    with open(path+os.sep+'cpuset.cpus') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.cpus'] = \
                            cpus2list(tmp_val.strip())
                    with open(path+os.sep+'cpuset.mems') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.mems'] = \
                            cpus2list(tmp_val.strip())

        # Check to see if the subsystem is enabled before trying
        # to gather resources
        if 'memory' in self.subsystems and \
                self.cfg['cgroup']['memory']['enabled']:
            # Gather the memory assigned for each socket
            directories = [x[0] for x in os.walk(self.paths['memory'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather memory resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['memory'], tmp_dir)
                    with open(path+os.sep+'memory.limit_in_bytes') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memory.limit'] = \
                            tmp_val.strip()
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    if os.path.isfile(tmp_filename) is False:
                        continue
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    with open(tmp_filename) as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memsw.limit'] = \
                            tmp_val.strip()

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'devices' in self.subsystems and \
                self.cfg['cgroup']['devices']['enabled']:
            # Gather the devices assigned
            directories = [x[0] for x in os.walk(self.paths['devices'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather device resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    pbs.logmsg(pbs.EVENT_DEBUG3, "Dir: %s" % tmp_dir)
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['devices'], tmp_dir)
                    with open(path+os.sep+'devices.list') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['devices.list'] = \
                            tmp_val.strip().split('\n')
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Dir: %s\n\tValue: %s" %
                                   (path, tmp_val.replace('\n', ',')))

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Gathered assigned resources: %s" % assigned_resources)
        self.assigned_resources = assigned_resources

    # Return whether a subsystem is enabled
    def enabled(self, subsystem):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check whether the subsystem is enabled in the configuration file
        if subsystem not in self.cfg['cgroup'].keys():
            return False
        if 'enabled' not in self.cfg['cgroup'][subsystem].keys():
            return False
        if not self.cfg['cgroup'][subsystem]['enabled']:
            return False
        # Check whether the cgroup is mounted for this subsystem
        if subsystem not in self.paths.keys():
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup not mounted for %s" %
                       (caller_name(), subsystem))
            return False
        # Check whether this host is excluded
        # if self.hostname in self.cfg['exclude_hosts']:
        #    pbs.logmsg(pbs.EVENT_DEBUG, "%s: cgroup excluded on host %s" %
        #               (caller_name(), self.hostname))
        #    pbs.event().accept()
        if self.hostname in self.cfg['cgroup'][subsystem]['exclude_hosts']:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup excluded for subsystem %s on host %s" %
                       (caller_name(), subsystem, self.hostname))
            return False
        # Check whether the vnode type is excluded
        if self.vntype is not None:
            # if self.vntype in self.cfg['exclude_vntypes']:
            #    pbs.logmsg(pbs.EVENT_DEBUG,
            #               "%s: cgroup excluded on vnode type %s" %
            #               (caller_name(), self.vntype))
            #    pbs.event().accept()
            if self.vntype in self.cfg['cgroup'][subsystem]['exclude_vntypes']:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           ("%s: cgroup excluded for " +
                            "subsystem %s on vnode type %s") %
                           (caller_name(), subsystem, self.vntype))
                return False
        return True

    # Return the default value for a subsystem
    def default(self, subsystem):
        if subsystem in self.cfg['cgroup']:
            if 'default' in self.cfg['cgroup'][subsystem]:
                return self.cfg['cgroup'][subsystem]['default']
        return None

    # Check to see if the pid matches the job owners uid
    def is_pid_owned_by_job_owner(self, pid):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        try:
            proc_uid = os.stat('/proc/%d' % pid).st_uid
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       '/proc/%d uid:%d' % (pid, proc_uid))

            job_owner_uid = pwd.getpwnam(pbs.event().job.euser)[2]
            pbs.logmsg(pbs.EVENT_DEBUG3, "Job uid: %d" % job_owner_uid)

            if proc_uid == job_owner_uid:
                return True
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Proc uid: %d != Job owner:  %d" %
                           (proc_uid, job_owner_uid))
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG, "Unknown pid: %d" % pid)
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        # If we got to this point something did not match up
        return False

    # Add some number of PIDs to the cgroup tasks files for each subsystem
    def add_pids(self, pids, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # make pids a list
        if isinstance(pids, int):
            pids = [pids]

        if pbs.event().type == pbs.EXECJOB_LAUNCH:
            if 1 in pids:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "1 is not a valid pid to add")
                # Hold the job for further review
                e.reject("Hook tried to add pid 1 to the tasks file " +
                         "for job %s" % jobid)

        # check pids to make sure that they are owned by the job owner
        if not CRAY_12_BRANCH:
            pbs.logmsg(pbs.EVENT_DEBUG3, "Not in the Cray 12 Branch")
            pbs.logmsg(pbs.EVENT_DEBUG3, "check to see if execjob_attach")
            if pbs.event().type == pbs.EXECJOB_ATTACH:
                pbs.logmsg(pbs.EVENT_DEBUG3, "event type: attach or launch")
                tmp_pids = list()
                for p in pids:
                    if self.is_pid_owned_by_job_owner(p):
                        tmp_pids.append(p)
                if len(tmp_pids) == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%d is not a valid pids to add" % p)
                    return False
                else:
                    pids = tmp_pids

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: subsys = %s" %
                       (caller_name(), subsys))
            # memsw and memory use the same tasks file
            if subsys == "memsw":
                if "memory" in self.subsystems:
                    continue
            path = os.path.join(self.paths[subsys], jobid)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path = %s" %
                       (caller_name(), path))
            try:
                tasks_path = os.path.join(path, "tasks")
                for p in pids:
                    self.write_value(tasks_path, p, 'a')
            except IOError, exc:
                raise CgroupLimitError("Failed to add PIDs %s to %s (%s)" %
                                       (str(pids), tasks_path,
                                        errno.errorcode[exc.errno]))
            except:
                raise

    # Set a node limit
    def set_node_limit(self, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s = %s" %
                   (caller_name(), resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'],
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Node resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    def setup_job_devices_env(self):
        """ Setup the job environment for the devices assigned to the job for an
            execjob_launch hook
         """
        if 'devices_name' in self.host_assigned_resources:
            names = self.host_assigned_resources['devices_name']
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "devices: %s" % (names))
            offload_devices = list()
            cuda_visible_devices = list()
            for name in names:
                if name.startswith('mic'):
                    offload_devices.append(name[3:])
                elif name.startswith('nvidia'):
                    cuda_visible_devices.append(name[6:])
            if len(offload_devices) > 0:
                value = '"%s"' % string.join(offload_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['OFFLOAD_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "offload_devices: %s" % offload_devices)
            if len(cuda_visible_devices) > 0:
                value = '"%s"' % string.join(cuda_visible_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['CUDA_VISIBLE_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "cuda_visible_devices: %s" % cuda_visible_devices)
            return [offload_devices, cuda_visible_devices]
        else:
            return False

    def setup_subsys_devices(self, path, node):
        subsys = 'devices'
        # Add the devices that the user is allowed to use
        if subsys in self.cfg['cgroup']:
            devices_allowed = open(os.path.join(path,
                                   "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Initial devices.list: %s" %
                       devices_allowed)
            # Deny access to mic and gpu devices
            devices_list = list()
            devices = node.devices
            for device in devices:
                if device == 'mic' or device == 'gpu':
                    for name in devices[device]:
                        d = devices[device][name]
                        devices_list.append("%d:%d" % (d['major'], d['minor']))
            # For CentOS 7 we need to remove a *:* rwm from devices.list we
            # before can add anything to devices.allow. Otherwise our changes
            # are ignored. Check to see if a *:* rwm is in devices.list.
            # If so remove it
            if "a *:* rwm\n" in devices_allowed:
                value = "a *:* rwm"
                self.write_value(os.path.join(path, 'devices.deny'), value)
            else:
                # Verify that the following devices are not in devices.list
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing access to the following: %s" %
                           devices_list)
            for device_str in devices_list:
                value = "c %s rwm" % device_str
                self.write_value(os.path.join(path, 'devices.deny'), value)
            # Add devices back to the list
            devices_allow = list()
            if 'allow' in self.cfg['cgroup'][subsys]:
                devices_allow = self.cfg['cgroup'][subsys]['allow']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Allowing access to the following: %s" %
                           devices_allow)
                for item in devices_allow:
                    if isinstance(item, str):
                        pbs.logmsg(pbs.EVENT_DEBUG3, "string item: %s" % item)
                        value = "%s" % item
                        self.write_value(os.path.join(
                                         path, 'devices.allow'), value)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "write_value: %s" % value)
                    elif isinstance(item, list):
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "list item: %s" % item)
                        try:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Device allow: %s" % item)
                            stat_filename = '/dev/%s' % item[0]
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Stat file: %s" % stat_filename)
                            s = os.stat(stat_filename)
                            device_type = None
                            if S_ISBLK(s.st_mode):
                                device_type = "b"
                            elif S_ISCHR(s.st_mode):
                                device_type = "c"
                            if device_type is not None:
                                if len(item) == 3 and isinstance(item[2], str):
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % item[2]
                                    value += "%s" % item[1]
                                else:
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % os.minor(s.st_rdev)
                                    value += "%s" % item[1]
                                self.write_value(os.path.join(
                                                path, 'devices.allow'), value)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "write_value: %s" % value)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find /dev/%s. " % item[0] +
                                       "Not added to devices.allow!")
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                    else:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Not sure what to do with: %s" % item)
            devices_allowed = open(os.path.join(
                                   path, "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "After setup devices.list: %s" % devices_allowed)

    # Select devices to assign to the job
    def assign_devices(
                       self,
                       device_type,
                       device_list,
                       number_of_devices,
                       node):
        devices = device_list[:number_of_devices]
        device_ids = list()
        device_names = list()
        for device in devices:
            device_info = node.devices[device_type][device]
            if len(device_ids) == 0:
                if device.find("mic") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:0 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                    device_ids.append("%s %d:1 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                elif device.find("nvidia") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:255 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
            device_ids.append("%s %d:%d rwm" %
                              (device_info['device_type'],
                               device_info['major'],
                               device_info['minor']))
            device_names.append(device)
        return device_names, device_ids

    # Find the device name
    def get_device_name(self, node, available, socket, major, minor):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Get device name: major: %s, minor: %s" % (major, minor))
        avail_device = None
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Possible devices: %s" % (available[socket]['devices']))
        for avail_device in available[socket]['devices']:
            avail_major = None
            avail_minor = None
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Checking device: %s" % (avail_device))
            if avail_device.find('mic') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check mic device: %s" % (avail_device))
                avail_major = node.devices['mic'][avail_device]['major']
                avail_minor = node.devices['mic'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            elif avail_device.find('nvidia') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check gpu device: %s" % (avail_device))
                avail_major = node.devices['gpu'][avail_device]['major']
                avail_minor = node.devices['gpu'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            if int(avail_major) == int(major) and \
                    int(avail_minor) == int(minor):
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device match: name: %s, major: %s, minor: %s" %
                           (avail_device, major, minor))
                return avail_device
        pbs.logmsg(pbs.EVENT_DEBUG3, "No match found")
        return None

    # Assign resources to the job based off the vnodes assignments
    def assign_job_resources_by_vnode(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # Loop through the vnodes and assign resources
        assigned = dict()
        assigned['cpuset.cpus'] = list()
        assigned['cpuset.mems'] = list()
        room_on_socket = True

        for vnode in resources['vnodes']:
            regex = re.compile(".*\[(\d+)\].*")
            socket = int(regex.search(vnode).group(1))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current Vnode: %s" % vnode)
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current socket: %d" % socket)

            if 'ncpus' in resources['vnodes'][vnode]:
                tmp_ncpus = int(resources['vnodes'][vnode]['ncpus'])
                if int(resources['vnodes'][vnode]['ncpus']) <= \
                        len(available[socket]['cpus']):
                    assigned['cpuset.cpus'] += \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] += [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_ncpus: %s" % (tmp_ncpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "available[%d]: %s" %
                               (socket, available[socket]))
                    room_on_socket = False
            if ('nmics' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['nmics']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(mic).*")
                tmp_nmics = int(resources['vnodes'][vnode]['nmics'])
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['nmics']) > 0 and
                   int(resources['vnodes'][vnode]['nmics']) <= len(mics)):
                    tmp_names, tmp_devices = self.assign_devices(
                             'mic', mics[:tmp_nmics], tmp_nmics, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_nmics: %s" % (tmp_nmics))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "mics: %s" % (mics))
                    room_on_socket = False
            if ('ngpus' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['ngpus']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(nvidia).*")
                tmp_ngpus = int(resources['vnodes'][vnode]['ngpus'])
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['ngpus']) > 0 and
                   int(resources['vnodes'][vnode]['ngpus']) <= len(gpus)):
                    tmp_names, tmp_devices = self.assign_devices(
                        'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "tmp_ngpus: %s" % (tmp_ngpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "gpus: %s" % (gpus))
                    room_on_socket = False

        if room_on_socket:
            assigned['cpuset.cpus'].sort()
            assigned['cpuset.mems'].sort()
            if 'devices' in assigned:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids pre set and sort: %s" %
                           (assigned['devices']))
                assigned['devices'] = list(set(assigned['devices']))
                assigned['devices'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids post set and sort: %s" %
                           (assigned['devices']))
                assigned['devices_name'] = list(set(assigned['devices_name']))
                assigned['devices_name'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

    # Assign resources to the job
    def assign_job_resources(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # What would we need to do different for a large smp (UV) system?
        # See if requested resources can fit on a single socket
        assigned = dict()
        pbs.logmsg(pbs.EVENT_DEBUG3, "Requested: %s" % (resources))

        # Determine the order that we should evaluate the sockets.
        socket_list = list()
        if 'placement_type' in self.cfg:
            if self.cfg['placement_type'] == 'round_robin':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Requested round_robin placement")
                # Look at assigned_resources and determine which socket
                # to start with
                jobs_per_socket_cnt = dict()
                for socket in available.keys():
                    jobs_per_socket_cnt[socket] = 0
                for job in self.assigned_resources:
                    if 'cpuset.mems' in self.assigned_resources[job]:
                        for socket in \
                                self.assigned_resources[job]['cpuset.mems']:
                            jobs_per_socket_cnt[socket] += 1

                sorted_dict = sorted(jobs_per_socket_cnt.items(),
                                     key=operator.itemgetter(1))
                for x in sorted_dict:
                    socket_list.append(x[0])
        else:
            socket_list = available.keys()

        # Loop through the socket list and determine if the job
        # can fit on the socket
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Evaluate sockets in the following order: %s" %
                   (socket_list))
        for socket in socket_list:
            room_on_socket = True
            if 'ncpus' in resources:
                if int(resources['ncpus']) <= len(available[socket]['cpus']):
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Requested: %s, Available: %s" %
                               (resources['ncpus'], available[socket]['cpus']))
                    tmp_ncpus = int(resources['ncpus'])
                    assigned['cpuset.cpus'] = \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] = [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient ncpus on socket %d: R:%d,A:%s" %
                               (socket, resources['ncpus'],
                                available[socket]['cpus']))
                    room_on_socket = False
            # Check to see that nmics has been requested and not equal to 0
            if 'nmics' in resources and int(resources['nmics']) > 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'nmics' in resources and int(resources['nmics']) > 0 \
                        and int(resources['nmics']) <= len(mics):
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient nmics on socket %d: R:%s,A:%s" %
                               (socket, resources['nmics'], mics))
                    room_on_socket = False
            # Check to see that ngpus has been requested and not equal to 0
            if 'ngpus' in resources and int(resources['ngpus']) > 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'ngpus' in resources and int(resources['ngpus']) > 0 \
                        and int(resources['ngpus']) <= len(gpus):
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient gpus on socket %d: R:%s,A:%s" %
                               (socket, resources['ngpus'], gpus))
                    room_on_socket = False
            if 'vmem' in resources:
                if size_as_int(resources['vmem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient vmem on socket %d: R:%s,A:%s" %
                               (socket, resources['vmem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if 'mem' in resources:
                if size_as_int(resources['mem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient memory on socket %d: R:%s,A:%s" %
                               (socket, resources['mem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if room_on_socket:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
                self.host_assigned_resources = assigned
                return assigned
        # If we made it here then the requested resources did not fit
        # on a single socket
        # Future: take distance into account when selecting sockets?
        # Combine available resources into a single list
        # This should work for a dual socket machine.
        # Needs additional work for machines with more than 2 sockets
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Unable to find requested resources: %s" % resources +
                   " on a socket. Checking the entire node")
        available_copy = copy.deepcopy(available)
        available_on_node = dict()
        socket_keys = available.keys()
        socket_keys.sort()
        available_on_node['sockets'] = socket_keys
        for socket in socket_keys:
            for key in available_copy[socket].keys():
                if isinstance(available[socket][key], int):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]
                if isinstance(available_copy[socket][key], str):
                    try:
                        if key in available_on_node is False:
                            available_on_node[key] = \
                                size_as_int(available_copy[socket][key])
                        else:
                            available_on_node[key] += \
                                size_as_int(available_copy[socket][key])
                    except:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Unable to convert to int: %s" %
                                   (available_copy[socket][key]))

                if isinstance(available_copy[socket][key], list):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available on node: %s" % (available_on_node))
        assigned = dict()
        room_on_node = False
        if 'ncpus' in resources:
            if int(resources['ncpus']) <= len(available_on_node['cpus']):
                room_on_node = True
                tmp_ncpus = int(resources['ncpus'])
                assigned['cpuset.cpus'] = available_on_node['cpus'][:tmp_ncpus]
                assigned['cpuset.mems'] = available_on_node['sockets']
            if 'nmics' in resources and int(resources['nmics']) != 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['nmics']) <= len(mics) and \
                        int(resources['nmics']) > 0:
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
            if 'ngpus' in resources and int(resources['ngpus']) != 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['ngpus']) <= len(gpus) and \
                        int(resources['ngpus']) > 0:
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
        if room_on_node:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

        # Consolidate resources if needed to place the job

    # Determine which resources are available
    def available_node_resources(self, node):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))

        available = copy.deepcopy(node.numa_nodes)
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Keys: %s" % (available[0].keys()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        for socket in available.keys():
            if 'cpus' in available[socket]:
                # Find the physical cores
                if available[socket]['cpus'].find('-') != -1 and \
                        node.hyperthreading:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Splitting %s at ','" %
                               (available[socket]['cpus']))
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'].split(',')[0])
                else:
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'])
            if 'MemTotal' in available[socket]:
                # Find the memory on the socket in bites.
                # Remove the 'b' to simply math
                available[socket]['memory'] = size_as_int(
                    available[socket]['MemTotal'])

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Pre device add: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (node.devices.keys()))
        for device in node.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" %
                       (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Devices: %s" %
                           (node.devices[device].keys()))
                for device_name in node.devices[device].keys():
                    device_socket = \
                        node.devices[device][device_name]['numa_node']
                    if 'devices' not in available[device_socket]:
                        available[device_socket]['devices'] = list()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Device: %s, Socket: %s" %
                               (device, device_socket))
                    available[device_socket]['devices'].append(device_name)
            # pbs.logmsg(pbs.EVENT_DEBUG3,
            #            "Available on socket %s: %s" %
            #            (socket,available[socket]))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))

        # Remove all of the resources that are assigned to other jobs
        for job in self.assigned_resources.keys():
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            if (not job_to_be_ignored(job)):
                cpus = list()
                sockets = list()
                devices = list()
                memory = 0
                if 'cpuset.cpus' in self.assigned_resources[job]:
                    cpus = self.assigned_resources[job]['cpuset.cpus']
                if 'cpuset.mems' in self.assigned_resources[job]:
                    sockets = self.assigned_resources[job]['cpuset.mems']
                if 'devices.list' in self.assigned_resources[job]:
                    devices = self.assigned_resources[job]['devices.list']
                if 'memory.limit' in self.assigned_resources[job]:
                    memory = size_as_int(
                                self.assigned_resources[job]['memory.limit'])

                # Loop through the sockets and remove cpus that are
                # assigned to other cgroups
                for socket in sockets:
                    for cpu in cpus:
                        try:
                            available[socket]['cpus'].remove(cpu)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %d from %s" %
                                       (cpu, available[socket]['cpus']))

                if len(sockets) == 1:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Decrementing memory: %d by %d" %
                               (size_as_int(available[sockets[0]]['memory']),
                                memory))
                    available[sockets[0]]['memory'] -= memory

                # Loop throught the available sockets
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned device to %s: %s" % (job, devices))
                for socket in available.keys():
                    for device in devices:
                        try:
                            # loop through know devices and see if they match
                            if len(available[socket]['devices']) != 0:
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Check device: %s" % (device))
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Available device: %s" %
                                           (available[socket]['devices']))
                                major, minor = device.split()[1].split(':')
                                avail_device = self.get_device_name(
                                                   node, available, socket,
                                                   major, minor)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Returned device: %s" %
                                           (avail_device))
                                if avail_device is not None:
                                    pbs.logmsg(pbs.EVENT_DEBUG3,
                                               "socket: %d,\t" % socket +
                                               "devices: %s,\t" %
                                               available[socket]['devices'] +
                                               "device to remove: %s" %
                                               (avail_device))
                                    available[socket]['devices'].remove(
                                        avail_device)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %s from %s" %
                                       (device, available[socket]['devices']))
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Job %s res not removed from host " % job +
                           "available res: suspended job")
        pbs.logmsg(pbs.EVENT_DEBUG, "Available resources: %s" % (available))
        return available

    # Set a job limit
    def set_job_limit(self, jobid, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s: %s = %s" %
                   (caller_name(), jobid, resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'softmem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.soft_limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     jobid, 'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'], jobid,
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            elif resource == 'ncpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = self.select_cpus(path, value)
                    if cpus is None:
                        raise CgroupLimitError("Failed to configure cpuset.")
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
                    self.__copy_from_parent(os.path.join(self.paths['cpuset'],
                                            jobid, 'cpuset.mems'))
            elif resource == 'cpuset.cpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = value
                    if cpus is None:
                        msg = "Failed to configure cpus in cpuset."
                        raise CgroupLimitError(msg)
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
            elif resource == 'cpuset.mems':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.mems')
                    mems = value
                    if mems is None:
                        msg = "Failed to configure mems in cpuset."
                        raise CgroupLimitError(msg)
                    mems = ','.join(map(str, mems))
                    self.write_value(path, mems)
            elif resource == 'devices':
                if 'devices' in self.subsystems and \
                        self.cfg['cgroup']['devices']['enabled']:

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.allow')
                    devices = value
                    if devices is None:
                        msg = "Failed to configure device(s)."
                        raise CgroupLimitError(msg)
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Setting devices: %s for %s" % (devices, jobid))
                    for device in devices:
                        self.write_value(path, device)

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.list')
                    output = open(path, 'r').read()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "devices.list: %s" % output.replace("\n", ","))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    # Update resource usage for a job
    def update_job_usage(self, jobid, resc_used):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resc_used = %s" %
                   (caller_name(), str(resc_used)))
        # Sort the subsystems so that we consistently look at the subsystems
        # in the same order every time
        self.subsystems.sort()
        for subsys in self.subsystems:
            path = os.path.join(self.paths[subsys], jobid)
            if subsys == "memory":
                max_mem = self.__get_max_mem_usage(path)
                if max_mem is None:
                    pbs.logjobmsg(jobid, "%s: No max mem data" %
                                  (caller_name()))
                else:
                    resc_used['mem'] = pbs.size(convert_size(max_mem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: mem=%s" %
                                  (caller_name(), resc_used['mem']))
                mem_failcnt = self.__get_mem_failcnt(path)
                if mem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No mem fail count data" %
                                  (caller_name()))
                else:
                    # Check to see if the job exceeded its resource limits
                    if mem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memory limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "memsw":
                max_vmem = self.__get_max_memsw_usage(path)
                if max_vmem is None:
                    pbs.logjobmsg(jobid, "%s: No max vmem data" %
                                  (caller_name()))
                else:
                    resc_used['vmem'] = pbs.size(convert_size(max_vmem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: vmem=%s" %
                                  (caller_name(), resc_used['vmem']))

                vmem_failcnt = self.__get_memsw_failcnt(path)
                if vmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No vmem fail count data" %
                                  (caller_name()))
                else:
                    pbs.logjobmsg(jobid, "%s: vmem fail count: %d " %
                                  (caller_name(), vmem_failcnt))
                    if vmem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memsw limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "hugetlb":
                max_hpmem = self.__get_max_hugetlb_usage(path)
                if max_hpmem is None:
                    pbs.logjobmsg(jobid, "%s: No max hpmem data" %
                                  (caller_name()))
                    return
                hpmem_failcnt = self.__get_hugetlb_failcnt(path)
                if hpmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No hpmem fail count data" %
                                  (caller_name()))
                    return
                if hpmem_failcnt > 0:
                    err_msg = self.__get_error_msg(jobid)
                    pbs.logjobmsg(jobid, "Cgroup hugetlb limit exceeded: %s" %
                                  (err_msg))
                resc_used['hpmem'] = pbs.size(convert_size(max_hpmem, 'kb'))
                pbs.logjobmsg(jobid, "%s: Hugepage usage: %s" %
                              (caller_name(), resc_used['hpmem']))
            elif subsys == "cpuacct":
                cpu_usage = self.__get_cpu_usage(path)
                if cpu_usage is None:
                    pbs.logjobmsg(jobid, "%s: No CPU usage data" %
                                  (caller_name()))
                    return
                cpu_usage = convert_time(str(cpu_usage) + "ns")
                pbs.logjobmsg(jobid, "%s: CPU usage: %.3lf secs" %
                              (caller_name(), cpu_usage))
                resc_used['cput'] = pbs.duration(cpu_usage)

    # Creates the cgroup if it doesn't exists
    def create_job(self, jobid, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Iterate over the subsystems required
        for subsys in self.subsystems:
            # Create a directory for the job
            try:
                old_umask = os.umask(0022)
                path = self.paths[subsys]
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                path = os.path.join(self.paths[subsys], jobid)
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                if subsys == 'devices':
                    self.setup_subsys_devices(path, node)

            except OSError, exc:
                raise CgroupConfigError("Failed to create directory: %s (%s)" %
                                        (path, errno.errorcode[exc.errno]))
            except:
                raise
            finally:
                os.umask(old_umask)

    # Determine the cgroup limits and configure the cgroups
    def configure_job(self, jobid, hostresc, node):
        mem_enabled = 'memory' in self.subsystems
        vmem_enabled = 'memsw' in self.subsystems
        if mem_enabled or vmem_enabled:
            # Initialize mem variables
            mem_avail = node.get_memory_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "mem_avail %s" % mem_avail)
            mem_requested = None
            if 'mem' in hostresc.keys():
                mem_requested = convert_size(hostresc['mem'], 'kb')
            mem_default = None
            if mem_enabled:
                mem_default = self.default('memory')
            # Initialize vmem variables
            vmem_avail = node.get_vmem_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "vmem_avail %s" % vmem_avail)
            vmem_requested = None
            if 'vmem' in hostresc.keys():
                vmem_requested = convert_size(hostresc['vmem'], 'kb')
            vmem_default = None
            if vmem_enabled:
                vmem_default = self.default('memsw')
            # Initialize softmem variables
            if 'soft_limit' in self.cfg['cgroup']['memory'].keys():
                softmem_enabled = self.cfg['cgroup']['memory']['soft_limit']
            else:
                softmem_enabled = False
            # Sanity check
            if size_as_int(mem_avail) > size_as_int(vmem_avail):
                if size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                    raise CgroupLimitError(
                        'mem available (%s) exceeds vmem available (%s)' %
                        (mem_avail, vmem_avail))
            # Determine the mem limit
            if mem_requested is not None:
                # mem requested may not exceed available
                if size_as_int(mem_requested) > size_as_int(mem_avail):
                    raise JobValueError(
                        'mem requested (%s) exceeds mem available (%s)' %
                        (mem_requested, mem_avail))
                mem_limit = mem_requested
            else:
                # mem was not requested
                if mem_default is None:
                    mem_limit = mem_avail
                else:
                    mem_limit = mem_default
            # Determine the vmem limit
            if vmem_requested is not None:
                # vmem requested may not exceed available
                if size_as_int(vmem_requested) > size_as_int(vmem_avail):
                    raise JobValueError(
                        'vmem requested (%s) exceeds vmem available (%s)' %
                        (vmem_requested, vmem_avail))
                vmem_limit = vmem_requested
            else:
                # vmem was not requested
                if vmem_default is None:
                    vmem_limit = vmem_avail
                else:
                    vmem_limit = vmem_default
            # Ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Adjust for soft limits if enabled
            if mem_enabled and softmem_enabled:
                softmem_limit = mem_limit
                # The hard memory limit is assigned the lesser of the vmem
                # limit and available memory
                if size_as_int(vmem_limit) < size_as_int(mem_avail):
                    mem_limit = vmem_limit
                else:
                    mem_limit = mem_avail
            # Again, ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Sanity checks when both memory and memsw are enabled
            if mem_enabled and vmem_enabled:
                if vmem_requested is not None:
                    if size_as_int(vmem_limit) > size_as_int(vmem_requested):
                        # The user requested an invalid limit
                        raise JobValueError(
                            'vmem limit (%s) exceeds vmem requested (%s)' %
                            (vmem_limit, vmem_requested))
                if size_as_int(vmem_limit) > size_as_int(mem_limit):
                    # This job may utilize swap
                    if size_as_int(vmem_avail) <= size_as_int(mem_avail) and \
                      size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                        # No swap available
                        raise CgroupLimitError(
                            ('Job might utilize swap ' +
                             'and no swap space available'))
            # Assign mem and vmem
            if mem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: mem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), mem_limit))
                hostresc['mem'] = pbs.size(mem_limit)
                if softmem_enabled:
                    hostresc['softmem'] = pbs.size(softmem_limit)
            if vmem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: vmem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), vmem_limit))
                hostresc['vmem'] = pbs.size(vmem_limit)
        # Initialize hpmem variables
        hpmem_enabled = 'hugetlb' in self.subsystems
        if hpmem_enabled:
            hpmem_avail = node.get_hpmem_on_node(self.cfg)
            hpmem_limit = None
            hpmem_default = self.default('hugetlb')
            if hpmem_default is None:
                hpmem_default = hpmem_avail
            if 'hpmem' in hostresc.keys():
                hpmem_limit = convert_size(hostresc['hpmem'], 'kb')
            else:
                hpmem_limit = hpmem_default
            # Assign hpmem
            if size_as_int(hpmem_limit) > size_as_int(hpmem_avail):
                raise JobValueError('hpmem limit (%s) exceeds available (%s)' %
                                    (hpmem_limit, hpmem_avail))
            hostresc['hpmem'] = pbs.size(hpmem_limit)
        # Initialize cpuset variables
        cpuset_enabled = 'cpuset' in self.subsystems
        if cpuset_enabled:
            cpu_limit = 1
            if 'ncpus' in hostresc.keys():
                cpu_limit = hostresc['ncpus']
            if cpu_limit < 1:
                cpu_limit = 1
            hostresc['ncpus'] = pbs.pbs_int(cpu_limit)

        # Find the available resources and assign the right ones to the job

        attempt = 0
        assign_resources = False

        # Two attempts since self.cleanup_orphans may actually fix the
        # problem we see in a first attempt
        while attempt < 2:
            attempt += 1
            available_resources = self.available_node_resources(node)
            if 'vnodes' in hostresc:
                assign_resources = self.assign_job_resources_by_vnode(
                    hostresc, available_resources, node)
            else:
                assign_resources = self.assign_job_resources(
                    hostresc, available_resources, node)

            if (assign_resources is False) and (attempt < 2):
                # No resources were assigned to the job.
                # Most likely cause was that a cgroup has
                # not been cleaned up yet
                pbs.logmsg(pbs.EVENT_DEBUG, "Failed to assign resources. " +
                           "Checking the following cgroups on the node " +
                           "with the server: %s" % self.existing_cgroups)

                # Collect the jobs on the node (try reading mom_priv/jobs,
                # if not successful ask server)
                jobs_list = node.gather_jobs_on_node_local()

                if jobs_list is not None:
                    # Don't clean up the cgroup we just made for the
                    # job just yet
                    if jobid not in jobs_list:
                        jobs_list.append(jobid)

                    self.cleanup_orphans(jobs_list)

                    # Recheck what is now assigned after things were cleaned up
                    self.gather_assigned_resources()

        if assign_resources is False:
            # Cleanup cgroups for jobs not present on this node
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Assign resources failed. Attempting to cleanup all " +
                       "leftover cgroups (including event job %s)" % jobid)

            # Remove the current job from the jobs list so it will be
            # cleaned up as well.
            jobs_list.remove(jobid)

            self.cleanup_orphans(jobs_list)

            # Rerun the job and log the message
            line = 'Unable to assign resources to job. '
            line += 'Will requeue the job and try again.'
            line += 'job run_count: %d' % pbs.event().job.run_count
            pbs.event().job.rerun()
            pbs.event().reject(line)

        # Print out the assigned resources
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Assigned resources: %s" % (assign_resources))

        if cpuset_enabled:
            hostresc.pop('ncpus', None)
            for key in ['cpuset.cpus', 'cpuset.mems']:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Initialize devices variables
        devices_enabled = 'devices' in self.subsystems
        if devices_enabled:
            key = 'devices'
            if key in assign_resources:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Apply the resource limits to the cgroups
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Setting cgroup limits for: %s" %
                   (caller_name(), hostresc))

        # The vmem limit must be set after the mem limit, so sort the keys
        for resc in sorted(hostresc.keys()):
            self.set_job_limit(jobid, resc, hostresc[resc])

    # Kill any processes contained within a tasks file
    def __kill_tasks(self, tasks_file):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        with open(tasks_file, 'r') as fd:
            for line in fd:
                os.kill(int(line.strip()), signal.SIGKILL)
        fd.close()
        count = 0
        with open(tasks_file, 'r') as fd:
            for line in fd:
                count += 1
                pid = line.strip()
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Pid: %s did not clean up" %
                           (caller_name(), pid))
                if os.path.isfile('/proc/%s/status' % pid):
                    tmp = open('/proc/%s/status' % pid).readlines()
                    tmp_line = ''
                    for entry in tmp:
                        if entry.find('Name:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('State:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('Uid:') != -1:
                            tmp_line += entry.strip()
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: %s" % (caller_name(), tmp_line))

        return count

    # Perform the actual removal of the cgroup directory
    # by default, only one attempt at killing tasks in cgroup, without sleeping
    # since this method could be called many times
    # (for N directories times M jobs)
    def __remove_cgroup(self, path, tasks_kill_attempts=1):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            if os.path.exists(path):
                tasks_file = os.path.join(path, 'tasks')
                task_cnt = self.__kill_tasks(tasks_file)
                kill_attempts = 0
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Task Kill Attempts: %d" % tasks_kill_attempts)
                while (task_cnt > 0) and (kill_attempts < tasks_kill_attempts):
                    kill_attempts += 1
                    count = 0

                    with open(tasks_file, 'r') as fd:
                        for line in fd:
                            count += 1
                        task_cnt = count

                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "Attempt: %d - cgroup still has %d tasks: %s" %
                               (kill_attempts, task_cnt, path))
                    if kill_attempts < tasks_kill_attempts:
                        time.sleep(1)
                        # Added since there are race conditions where tasks are
                        # being added at the same time we get the initial
                        # task count
                        tasks_file = os.path.join(path, 'tasks')
                        task_cnt = self.__kill_tasks(tasks_file)

                if task_cnt == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Removing directory %s" %
                               (caller_name(), path))
                    os.rmdir(path)
                    if os.path.exists(path):
                        time.sleep(.25)
                        if os.path.exists(path):
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "%s: 2nd Try Removing directory %s" %
                                       (caller_name(), path))
                            os.rmdir(path)
                            time.sleep(.25)
                        if os.path.exists(path):
                            return False
                else:
                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "cgroup still has %d tasks: %s" %
                               (task_cnt, path))

                    # Check to see if the offline file is already present
                    # on the mom
                    offline_file_exists = False
                    if os.path.isfile(self.offline_file):
                        msg = "Cgroup(s) not cleaning up but the node already "
                        msg += "has the offline file."

                        pbs.logmsg(pbs.EVENT_DEBUG, msg)
                    else:
                        # Check to see that the node is not already offline.
                        try:
                            tmp_state = pbs.server().vnode(self.hostname).state
                        except:
                            msg = "Unable to contact server for node state"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            tmp_state = None
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Current Node State: %d" % tmp_state)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Offline Node State: %d" % pbs.ND_OFFLINE)

                        if tmp_state == pbs.ND_OFFLINE:
                            msg = "Cgroup(s) not cleaning up but the node is "
                            msg += "already offline."
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                        elif tmp_state is not None:
                            # Offline the node(s)
                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                            msg = "Offlining node since cgroup(s) are not "
                            msg += "cleaning up"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            vnode = pbs.event().vnode_list[self.hostname]

                            vnode.state = pbs.ND_OFFLINE

                            # Write a file locally to reduce server traffic
                            # when it comes time to online the node
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Offline file: %s" % self.offline_file)
                            open(self.offline_file, 'a').close()
                            vnode.comment = self.offline_msg

                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                        else:
                            pass

                    return False

        except OSError, exc:
            pbs.logmsg(pbs.EVENT_SYSTEM,
                       "Failed to remove cgroup path: %s (%s)" %
                       (path, errno.errorcode[exc.errno]))
        except:
            pbs.logmsg(pbs.EVENT_SYSTEM, "Failed to remove cgroup path: %s" %
                       (path))

        return True

    # Removes cgroup directories that are not associated with a local job
    def cleanup_orphans(self, local_jobs):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        orphan_cnt = 0
        for key in self.paths.keys():
            for dir in glob.glob(os.path.join(self.paths[key], '[0-9]*')):
                jobid = os.path.basename(dir)
                if jobid not in local_jobs:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Removing orphaned cgroup: %s" %
                               (caller_name(), dir))
                    # default number of attempts at killing tasks
                    status = self.__remove_cgroup(dir)
                    if not status:
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "%s: Removing orphaned cgroup %s failed " %
                                   (caller_name(), dir))
                        orphan_cnt += 1
        return orphan_cnt

    # Removes the cgroup directories for a job
    def delete(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        tasks_kill_attempted = False

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            if not tasks_kill_attempted:
                # Make multiple attempts at killing tasks in cgroups on
                # first subsystem encountered since it is "normal" for
                # a cgroup.delete to encounter processes hard to kill
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                                           jobid),
                                              tasks_kill_attempts=(
                                              CGROUP_KILL_ATTEMPTS))
                tasks_kill_attempted = True
            else:
                # We tried getting rid of job processes earlier,
                # so don't try N times with sleeps
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                              jobid), tasks_kill_attempts=1)

            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Status: %s" %
                       (caller_name(), status))
            if status is False:
                # Rerun the job and log the message
                line = 'Unable to cleanup a cgroup. '
                line += 'Will offline the node and requeue the job '
                line += 'and try again. job run_count: %d' % \
                        pbs.event().job.run_count
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Failed to remove cgroup: %s" %
                           (caller_name(), line))
                pbs.event().accept()

    # Write a value to a limit file
    def write_value(self, file, value, mode='w'):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: writing %s to %s" %
                   (caller_name(), value, file))
        try:
            with open(file, mode) as fd:
                fd.write(str(value) + '\n')
        except IOError, exc:
            if exc.errno == errno.ENOENT:
                pbs.logmsg(pbs.EVENT_SYSTEM,
                           "Trying to set limit to unknown file %s" % file)
            elif exc.errno in [errno.EACCES, errno.EPERM]:
                pbs.logmsg(pbs.EVENT_SYSTEM, "Permission denied on file %s" %
                           file)
            elif exc.errno == errno.EBUSY:
                raise CgroupBusyError("Limit rejected")
            elif exc.errno == errno.EINVAL:
                raise CgroupLimitError("Invalid limit value: %s" % (value))
            else:
                raise
        except:
            raise

    # Return memory failcount
    def __get_mem_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.failcnt"), 'r').read().strip())
        except:
            return None

    # Return vmem failcount
    def __get_memsw_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.failcnt"), 'r').read().strip())
        except:
            return None

    # Return hpmem failcount
    def __get_hugetlb_failcnt(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.failcnt"))[0], 'r').read().strip())
        except:
            return None

    # Return the max usage of memory in bytes
    def __get_max_mem_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of memsw in bytes
    def __get_max_memsw_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of hugetlb in bytes
    def __get_max_hugetlb_usage(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.max_usage_in_bytes"))[0],
                            'r').read().strip())
        except:
            return None

    # Return the cpuacct.usage in cpu seconds
    def __get_cpu_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "cpuacct.usage"), 'r').read().strip())
        except:
            return None

    # Assign CPUs to the cpuset
    def select_cpus(self, path, ncpus):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path is %s" % (caller_name(), path))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: ncpus is %s" %
                   (caller_name(), ncpus))
        if ncpus < 1:
            ncpus = 1
        try:
            # Must select from those currently available
            cpufile = os.path.basename(path)
            base = os.path.dirname(path)
            parent = os.path.dirname(base)
            avail = cpus2list(open(os.path.join(parent, cpufile),
                              'r').read().strip())
            if len(avail) < 1:
                raise CgroupProcessingError("No CPUs avaialble in cgroup.")
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Available CPUs: %s" %
                       (caller_name(), avail))
            assigned = []
            for file in glob.glob(os.path.join(parent, '[0-9]*', cpufile)):
                cpus = cpus2list(open(file, 'r').read().strip())
                for id in cpus:
                    if id in avail:
                        avail.remove(id)
            if len(avail) < ncpus:
                raise CgroupProcessingError(
                    "Insufficient CPUs avaialble in cgroup.")
            if len(avail) == ncpus:
                return avail
            # FUTURE: Try to minimize NUMA nodes based on memory requirement
            return avail[0:ncpus]
        except:
            raise

    # Return the error message in system message file
    def __get_error_msg(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        proc = subprocess.Popen(['dmesg'],
                                shell=False, stdout=subprocess.PIPE)
        stdout = proc.communicate()[0].splitlines()
        stdout.reverse()
        # Check to see if the job id is found in dmesg otherwise
        # you could get the information for another job that was killed
        # the line will look like Memory cgroup stats for /pbspro/279.centos7
        kill_line = ""

        for line in stdout:
            start = line.find('Killed process ')
            if start >= 0:
                kill_line = line[start:]
            job_start = line.find(jobid)
            if job_start >= 0:
                # pbs.logmsg(pbs.EVENT_DEBUG3,
                #           "%s: Jobid: %s, Line: %s" %
                #           (caller_name(), jobid, line))
                return kill_line
        return ""

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_job_env_file(self, jobid, env_list):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        # if not os.path.exists(self.host_job_env_dir):
        #    os.makedirs(self.host_job_env_dir, 0755)

        # Write out assigned_resources
        try:
            lines = string.join(env_list, '\n')
            filename = self.host_job_env_filename % jobid
            outfile = open(filename, "w")
            outfile.write(lines)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" % (filename))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (lines))
            return True
        except:
            return False

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        if not os.path.exists(self.hook_storage_dir):
            os.makedirs(self.hook_storage_dir, 0755)

        # Write out assigned_resources
        try:
            json_str = json.dumps(self.host_assigned_resources)
            outfile = open(self.hook_storage_dir+os.sep+jobid, "w")
            outfile.write(json_str)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" %
                       (self.hook_storage_dir+os.sep+jobid))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (json_str))
            return True
        except:
            return False

    def read_in_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        pbs.logmsg(pbs.EVENT_DEBUG3, "Host assigned resources: %s" %
                   (self.host_assigned_resources))
        if os.path.isfile(self.hook_storage_dir+os.sep+jobid):
            # Write out assigned_resources
            try:
                infile = open(self.hook_storage_dir+os.sep+jobid, 'r')
                json_data = json.load(infile, object_hook=decode_dict)
                self.host_assigned_resources = json_data
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Host assigned resources: %s" %
                           (self.host_assigned_resources))
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
            finally:
                infile.close()

        if self.host_assigned_resources is not None:
            return True
        else:
            return False

#
#
# FUNCTION main
#


def main():
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Function called" % (caller_name()))
    hostname = pbs.get_local_nodename()
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Host is %s" % (caller_name(), hostname))

    # Log the hook event type
    e = pbs.event()
    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook name is %s" %
               (caller_name(), e.hook_name))

    try:
        # Instantiate the hook utility class
        hooks = HookUtils()
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Event type is %s" %
                   (caller_name(), hooks.event_name(e.type)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(hooks)))
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: Failed to instantiate hook utility class" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        e.accept()

    if not hooks.hashandler(e.type):
        # Bail out if there is no handler for this event
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s event not handled by this hook" %
                   (caller_name(), hooks.event_name(e.type)))
        e.accept()

    try:
        # If an exception occurs, jobutil must be set
        jobutil = None
        # Instantiate the job utility class first so jobutil can be accessed
        # by the exception handlers.
        if hasattr(e, 'job'):
            jobutil = JobUtils(e.job)
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Job information class instantiated" %
                       (caller_name()))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" %
                       (caller_name(), repr(jobutil)))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Event does not include a job" %
                       (caller_name()))
        # Instantiate the cgroup utility class
        vnode = None
        if hasattr(e, 'vnode_list'):
            if hostname in e.vnode_list.keys():
                vnode = e.vnode_list[hostname]
        cgroup = CgroupUtils(hostname, vnode)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Cgroup utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(cgroup)))
        # Bail out if there is nothing to do
        if len(cgroup.subsystems) < 1:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: All cgroups disabled" %
                       (caller_name()))
            e.accept()

        # Call the appropriate handler
        if hooks.invoke_handler(e, cgroup, jobutil) is True:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned success" %
                       (caller_name()))
            e.accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned failure" %
                       (caller_name()))
            e.reject()

    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass

    except AdminError, exc:
        # Something on the system is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Admin error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except UserError, exc:
        # User must correct problem and resubmit job
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("User error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.delete()
                msg += " (deleted)"
            except:
                msg += " (delete failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except JobValueError, exc:
        # Something in PBS is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Job value error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except Exception, exc:
        # Catch all other exceptions and report them.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Unexpected error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

# line below required for unittesting
if __name__ == "__builtin__":
    start_time = time.time()
    try:
        main()
    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
    finally:
        pbs.logmsg(pbs.EVENT_DEBUG, "Elapsed time: %0.4lf" %
                   (time.time() - start_time))
---
2016-07-18 16:55:56,090 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups.json', 'content-encoding': 'default'}
2016-07-18 16:55:56,090 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups.json
2016-07-18 16:55:56,170 INFO     job: executable set to /bin/sleep with arguments: 100
2016-07-18 16:55:56,171 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0755 /tmp/PtlPbsJobScriptMXS9Is
2016-07-18 16:55:56,181 INFOCLI  centos7: sudo -H -u pbsuser /usr/local/bin/qsub -N test_t3 -l walltime=3 -l select=1:ncpus=1:mem=300mb /tmp/PtlPbsJobScriptMXS9Is
2016-07-18 16:55:56,200 INFO     submit to centos7 as pbsuser: job 378.centos7 OrderedDict([('Job_Name', 'test_t3'), ('Resource_List.walltime', 3), ('Resource_List.select', '1:ncpus=1:mem=300mb')])
2016-07-18 16:55:56,200 INFOCLI  job script /tmp/PtlPbsJobScriptMXS9Is
---
#!/bin/bash
sleep 5

---
2016-07-18 16:55:56,200 INFOCLI  centos7: /usr/local/bin/qstat -f 378.centos7
2016-07-18 16:55:56,295 INFO     expect on server centos7: job_state = R job 378.centos7  got: job_state = Q
2016-07-18 16:55:56,797 INFOCLI  centos7: /usr/local/bin/qmgr -c set server scheduling=True
2016-07-18 16:55:56,812 INFOCLI  centos7: /usr/local/bin/qstat -f 378.centos7
2016-07-18 16:55:56,903 INFO     expect on server centos7: job_state = R job 378.centos7 attempt: 2 ...  OK
2016-07-18 16:55:57,906 INFO     Tasks: 58346
58371
58372

2016-07-18 16:55:57,906 INFO     Server name: centos7.usa.org
2016-07-18 16:55:57,906 INFO     /sys/fs/cgroup/freezer/378.centos7/tasks
2016-07-18 16:55:57,906 INFO     tasks: 58346
58371
58372

2016-07-18 16:55:57,907 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:55:57,924 INFO     expect on server centos7: state = offline node centos7  got: state = free
2016-07-18 16:56:00,929 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:00,946 INFO     expect on server centos7: state = offline node centos7 attempt: 2  got: state = free
2016-07-18 16:56:03,951 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:03,967 INFO     expect on server centos7: state = offline node centos7 attempt: 3  got: state = free
2016-07-18 16:56:06,971 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:06,987 INFO     expect on server centos7: state = offline node centos7 attempt: 4  got: state = free
2016-07-18 16:56:09,992 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:10,006 INFO     expect on server centos7: state = offline node centos7 attempt: 5  got: state = free
2016-07-18 16:56:13,010 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:13,027 INFO     expect on server centos7: state = offline node centos7 attempt: 6  got: state = free
2016-07-18 16:56:16,031 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:16,043 INFO     expect on server centos7: state = offline node centos7 attempt: 7  got: state = free
2016-07-18 16:56:19,047 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:19,060 INFO     expect on server centos7: state = offline node centos7 attempt: 8  got: state = free
2016-07-18 16:56:22,064 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:22,076 INFO     expect on server centos7: state = offline node centos7 attempt: 9  got: state = free
2016-07-18 16:56:25,080 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:25,097 INFO     expect on server centos7: state = offline node centos7 attempt: 10  got: state = free
2016-07-18 16:56:28,100 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:28,111 INFO     expect on server centos7: state = offline node centos7 attempt: 11  got: state = free
2016-07-18 16:56:31,116 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:31,127 INFO     expect on server centos7: state = offline node centos7 attempt: 12  got: state = free
2016-07-18 16:56:34,130 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:34,142 INFO     expect on server centos7: state = offline node centos7 attempt: 13  got: state = free
2016-07-18 16:56:37,146 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:37,166 INFO     expect on server centos7: state = offline node centos7 attempt: 14 ...  OK
2016-07-18 16:56:38,169 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:38,184 INFO     expect on server centos7: state = free node centos7  got: state = offline
2016-07-18 16:56:41,189 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:41,203 INFO     expect on server centos7: state = free node centos7 attempt: 2 ...  OK
2016-07-18 16:56:41,203 INFO     ==============================
2016-07-18 16:56:41,203 INFO      Entered CgroupsTests tearDown
2016-07-18 16:56:41,203 INFO     ==============================
2016-07-18 16:56:41,203 INFO     ===============================
2016-07-18 16:56:41,203 INFO     Completed CgroupsTests tearDown
2016-07-18 16:56:41,203 INFO     ===============================
2016-07-18 16:56:41,204 INFO     ok

2016-07-18 16:56:41,204 INFO     test name: test_t4 (tests.cgroups_tests.CgroupsTests)...
2016-07-18 16:56:41,204 INFO     test start time: Mon Jul 18 16:56:41 2016
2016-07-18 16:56:41,204 INFO     test docstring: 
 Test to verify that cgroups are not enforced on nodes
            that have an exclude vntype file set
        
2016-07-18 16:56:41,204 INFO     ===========================
2016-07-18 16:56:41,204 INFO      Entered CgroupsTests setUp
2016-07-18 16:56:41,205 INFO     ===========================
2016-07-18 16:56:41,205 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:41,271 INFO     status on centos7: server
2016-07-18 16:56:41,272 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:41,347 INFO     manager on centos7: set server {'managers': (3, 'root@*')}
2016-07-18 16:56:41,348 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers-=root@*
2016-07-18 16:56:41,388 INFO     manager on centos7: set server {'managers': (2, 'root@*')}
2016-07-18 16:56:41,389 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers+=root@*
2016-07-18 16:56:41,423 INFO     server centos7: reverting configuration to defaults
2016-07-18 16:56:41,423 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:41,474 INFO     select on centos7: __ALL__
2016-07-18 16:56:41,475 INFOCLI  centos7: /usr/local/bin/qselect
2016-07-18 16:56:41,490 INFOCLI  centos7: /usr/local/bin/qstat -f @centos7.usa.org
2016-07-18 16:56:41,542 INFO     expect on server centos7: job_state set 0 job ...  OK
2016-07-18 16:56:41,543 INFOCLI  centos7: /usr/local/bin/pbs_rstat -f
2016-07-18 16:56:41,555 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:56:41,576 INFO     manager on centos7: delete hook t1_l1
2016-07-18 16:56:41,577 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c delete hook t1_l1
2016-07-18 16:56:41,606 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:56:41,635 INFO     expect on server centos7:  unset  hook t1_l1 ...  OK
2016-07-18 16:56:41,635 INFOCLI  centos7: /usr/local/bin/qstat -Qf @centos7.usa.org
2016-07-18 16:56:41,659 INFO     manager on centos7: delete queue workq
2016-07-18 16:56:41,659 INFOCLI  centos7: /usr/local/bin/qmgr -c delete queue workq
2016-07-18 16:56:41,679 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:56:41,700 INFO     expect on server centos7:  unset  queue workq ...  OK
2016-07-18 16:56:41,701 INFO     manager on centos7: create queue workq {'started': 'True', 'queue_type': 'Execution', 'enabled': 'True'}
2016-07-18 16:56:41,701 INFOCLI  centos7: /usr/local/bin/qmgr -c create queue workq started=True,queue_type=Execution,enabled=True
2016-07-18 16:56:41,714 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:56:41,736 INFO     expect on server centos7: started set True || queue_type set Execution || enabled set True queue workq ...  OK
2016-07-18 16:56:41,736 INFO     manager on centos7: set server {'log_events': '511', 'default_queue': 'workq'}
2016-07-18 16:56:41,736 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=511,default_queue=workq
2016-07-18 16:56:41,752 INFO     status on centos7: resource
2016-07-18 16:56:41,752 INFO     manager on centos7: list resource
2016-07-18 16:56:41,752 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource
2016-07-18 16:56:41,809 INFO     manager on centos7: delete resource nmics,ngpus
2016-07-18 16:56:41,809 INFOCLI  centos7: /usr/local/bin/qmgr -c delete resource nmics,ngpus
2016-07-18 16:56:41,866 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource nmics
2016-07-18 16:56:41,878 INFO     expect on server centos7:  unset  resource nmics ...  OK
2016-07-18 16:56:41,878 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource ngpus
2016-07-18 16:56:41,902 INFO     expect on server centos7:  unset  resource ngpus ...  OK
2016-07-18 16:56:41,902 INFOCLI  status on centos7: server license_count
2016-07-18 16:56:41,902 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:41,953 INFO     server: centos7.usa.org licensed
2016-07-18 16:56:41,978 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/server_priv/comm.lock
2016-07-18 16:56:42,024 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:56:42,042 INFO     scheduler centos7: reverting configuration to defaults
2016-07-18 16:56:42,043 INFO     manager on centos7: list sched
2016-07-18 16:56:42,043 INFOCLI  centos7: /usr/local/bin/qmgr -c list sched
2016-07-18 16:56:42,059 INFO     manager on centos7: unset sched ['sched_cycle_length']
2016-07-18 16:56:42,059 INFOCLI  centos7: /usr/local/bin/qmgr -c unset sched sched_cycle_length
2016-07-18 16:56:42,077 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/dedicated_time
2016-07-18 16:56:42,089 INFOCLI2 centos7: cmp /usr/local/etc/pbs_resource_group /var/spool/pbs/sched_priv/resource_group
2016-07-18 16:56:42,092 INFO     scheduler centos7: reverting holidays file to default
2016-07-18 16:56:42,092 INFOCLI2 centos7: cmp /usr/local/etc/pbs_holidays /var/spool/pbs/sched_priv/holidays
2016-07-18 16:56:42,095 INFOCLI2 centos7: cmp /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:42,099 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:42,126 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:42,137 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:56:42,138 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:56:42,217 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:56:42,226 INFOCLI2 centos7: sudo -H cmp /usr/local/sbin/pbs_mom /usr/local/sbin/pbs_mom.cpuset
2016-07-18 16:56:42,266 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:56:42,276 INFO     mom centos7: reverting configuration to defaults
2016-07-18 16:56:42,276 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/epilogue
2016-07-18 16:56:42,287 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/prologue
2016-07-18 16:56:42,300 INFOCLI2 centos7: sudo -H ls /var/spool/pbs/mom_priv/config.d
2016-07-18 16:56:42,319 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsIlUT9G /var/spool/pbs/mom_priv/config
2016-07-18 16:56:42,333 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/mom_priv/config
2016-07-18 16:56:42,346 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/mom_priv/config
2016-07-18 16:56:42,382 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/mom_priv/config
2016-07-18 16:56:42,397 INFO     mom centos7: sent signal -HUP
2016-07-18 16:56:42,397 INFOCLI2 centos7: sudo -H kill -HUP 42976
2016-07-18 16:56:42,434 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:56:42,444 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 16:56:42,455 INFO     status on centos7: node centos7
2016-07-18 16:56:42,455 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:42,470 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:42,482 INFO     expect on server centos7: state = free node centos7 ...  OK
2016-07-18 16:56:42,482 INFO     ============================
2016-07-18 16:56:42,482 INFO     Completed CgroupsTests setUp
2016-07-18 16:56:42,483 INFO     ============================
2016-07-18 16:56:42,483 INFO     server centos7: server operating mode set to cli
2016-07-18 16:56:42,483 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:42,535 INFO     expect on server centos7: pbs_version ~ .*14.* server centos7.usa.org ...  OK
2016-07-18 16:56:42,535 INFO     manager on centos7: set server {'log_events': '2047'}
2016-07-18 16:56:42,535 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=2047
2016-07-18 16:56:42,552 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:42,615 INFO     expect on server centos7: log_events set 2047 server centos7.usa.org ...  OK
2016-07-18 16:56:42,615 INFO     scheduler centos7: config {'resources': 'ncpus,mem,host,vnode,vmem'}
2016-07-18 16:56:42,616 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /var/spool/pbs/sched_priv/sched_config /var/spool/pbs/sched_priv/sched_config.bak
2016-07-18 16:56:42,641 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsEptGCZ /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:42,656 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:42,667 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:42,696 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:42,711 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:56:42,711 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:56:42,726 INFO     scheduler centos7 log match: searching for "Error reading line" - No match
2016-07-18 16:56:43,729 INFO     manager on centos7 as root: create resource nmics {'flag': 'nh', 'type': 'long'}
2016-07-18 16:56:43,729 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource nmics flag=nh,type=long
2016-07-18 16:56:43,756 INFO     manager on centos7 as root: create resource ngpus {'flag': 'nh', 'type': 'long'}
2016-07-18 16:56:43,757 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource ngpus flag=nh,type=long
2016-07-18 16:56:43,783 INFO     Test Summary: test_t1_l1 tests "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" cgroup hook
2016-07-18 16:56:43,783 INFO     status on centos7: hook
2016-07-18 16:56:43,783 INFO     manager on centos7: list hook
2016-07-18 16:56:43,783 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:56:43,800 INFO     status on centos7: hook
2016-07-18 16:56:43,800 INFO     manager on centos7: list hook
2016-07-18 16:56:43,801 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:56:43,819 INFO     manager on centos7: create hook t1_l1
2016-07-18 16:56:43,819 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c create hook t1_l1
2016-07-18 16:56:43,848 INFO     manager on centos7: set hook t1_l1 {'freq': 2, 'enabled': 'True', 'event': '"execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"'}
2016-07-18 16:56:43,848 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set hook t1_l1 freq=2,enabled=True,event="execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"
2016-07-18 16:56:43,891 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:56:43,928 INFO     expect on server centos7: freq set 2 || enabled set True || event set "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" hook t1_l1 ...  OK
2016-07-18 16:56:43,929 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-python', 'input-file': '/tmp/PtlPbsT2Z9FB', 'content-encoding': 'default'}
2016-07-18 16:56:43,929 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-python default /tmp/PtlPbsT2Z9FB
2016-07-18 16:56:43,976 INFO     server centos7: imported hook body
---
#  Copyright (C) 2003-2016 Altair Engineering, Inc. All rights reserved.
#
#  ALTAIR ENGINEERING INC. Proprietary and Confidential. Contains Trade Secret
#  Information. Not for use or disclosure outside ALTAIR and its licensed
#  clients. Information contained herein shall not be decompiled, disassembled,
#  duplicated or disclosed in whole or in part for any purpose. Usage of the
#  software is only as explicitly permitted in the end user software license
#  agreement.
#
#  Copyright notice does not imply publication.

# Last Updated
# Date: 07-06-2016
#
# When soft_limit is true for memory, memsw represents the hard limit.
#
# The resources value in sched_config must contain entries for mem and
# vmem if those subsystems are enabled in the hook configuration file. The
# amount of resource requested will not be avaiable to the hook if they
# are not present.

# This hook handles all of the operations necessary for PBS to support
# cgroups on linux hosts that support them (kernel 2.6.28 and higher)
#
# This hook services the following events:
# - exechost_periodic
# - exechost_startup
# - execjob_attach
# - execjob_begin
# - execjob_end
# - execjob_epilogue
# - execjob_launch

# Imports from __future__ must happen first
from __future__ import with_statement

# Additional imports
import sys
import os
import errno
import signal
import subprocess
import re
import glob
import time
import string
import platform
import traceback
import copy
from stat import S_ISBLK, S_ISCHR
import operator
import pwd
import pbs
import fnmatch
import socket

try:
    import json
except:
    import simplejson as json

CGROUP_KILL_ATTEMPTS = 12

# Flag to turn off features not in 12.2.40X branch
CRAY_12_BRANCH = False


# ============================================================================
# Derived error classes
# ============================================================================

# Base class for errors fixable only by administrative action.


class AdminError(Exception):
    pass

# Base class for errors in processing, unknown cause.


class ProcessingError(Exception):
    pass

# Base class for errors fixable by the user.


class UserError(Exception):
    pass

# Errors in PBS job resource values.


class JobValueError(UserError):
    pass

# Errors when the cgroup is busy.


class CgroupBusyError(ProcessingError):
    pass

# Errors in configuring cgroup.


class CgroupConfigError(AdminError):
    pass

# Errors in configuring cgroup.


class CgroupLimitError(AdminError):
    pass

# Errors processing cgroup.


class CgroupProcessingError(ProcessingError):
    pass

# ============================================================================
# Utility functions
# ============================================================================

#
# FUNCTION caller_name
#
# Return the name of the calling function or method.
#


def caller_name():
    return str(sys._getframe(1).f_code.co_name)

#
# FUNCTION convert_size
#
# Convert a string containing a size specification (e.g. "1m") to a
# string using different units (e.g. "1024k").
#
# This function only interprets a decimal number at the start of the string,
# stopping at any unrecognized character and ignoring the rest of the string.
#
# When down-converting (e.g. MB to KB), all calculations involve integers and
# the result returned is exact. When up-converting (e.g. KB to MB) floating
# point numbers are involved. The result is rounded up. For example:
#
# 1023MB -> GB yields 1g
# 1024MB -> GB yields 1g
# 1025MB -> GB yields 2g  <-- This value was rounded up
#
# Pattern matching or conversion may result in exceptions.
#


def convert_size(value, units='b'):
    logs = {'b': 0, 'k': 10, 'm': 20, 'g': 30,
            't': 40, 'p': 50, 'e': 60, 'z': 70, 'y': 80}
    try:
        new = units[0].lower()
        if new not in logs.keys():
            new = 'b'
        val, old = re.match('([-+]?\d+)([bkmgtpezy]?)',
                            str(value).lower()).groups()
        val = int(val)
        if val < 0:
            raise ValueError('Value may not be negative')
        if old not in logs.keys():
            old = 'b'
        factor = logs[old] - logs[new]
        val *= 2 ** factor
        slop = val - int(val)
        val = int(val)
        if slop > 0:
            val += 1
        # pbs.size() does not like units following zero
        if val <= 0:
            return '0'
        else:
            return str(val) + new
    except:
        return None

#
# FUNCTION size_as_int
#
# Convert a size string to an integer representation of size in bytes
#


def size_as_int(value):
    return int(convert_size(value).rstrip(string.ascii_lowercase))

#
# FUNCTION convert_time
#
# Converts a integer value for time into the value of the return unit
#
# A valid decimal number, with optional sign, may be followed by a character
# representing a scaling factor.  Scaling factors may be either upper or
# lower case. Examples include:
# 250ms
# 40s
# +15min
#
# Valid scaling factors are:
# ns  = 10**-9
# us  = 10**-6
# ms  = 10**-3
# s   =      1
# min =     60
# hr  =   3600
#
# Pattern matching or conversion may result in exceptions.
#


def convert_time(value, return_unit='s'):
    multipliers = {'':  1, 'ns': 10 ** -9, 'us': 10 ** -6,
                   'ms': 10 ** -3, 's': 1, 'min': 60, 'hr': 3600}
    n, factor = re.match('([-+]?\d+)(\w*[a-zA-Z])?',
                         str(value).lower()).groups(0)

    # Check to see if there was not unit of time specified
    if factor == 0:
        factor = ''

    # Check to see if the unit is valid
    if str.lower(factor) not in multipliers.keys():
        raise ValueError('Time unit not recognized.')

    # Convert the value to seconds
    value_in_sec = float(n) * float(multipliers[str.lower(factor)])
    if return_unit != 's':
        return value_in_sec / multipliers[str.lower(return_unit)]
    else:
        return float('%lf' % value_in_sec)

# simplejson hook to convert lists from unicode to utf-8


def decode_list(data):
    rv = []
    for item in data:
        if isinstance(item, unicode):
            item = item.encode('utf-8')
        elif isinstance(item, list):
            item = decode_list(item)
        elif isinstance(item, dict):
            item = decode_dict(item)
        rv.append(item)
    return rv

# simplejson hook to convert dictionaries from unicode to utf-8


def decode_dict(data):
    rv = {}
    for key, value in data.iteritems():
        if isinstance(key, unicode):
            key = key.encode('utf-8')
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        elif isinstance(value, list):
            value = decode_list(value)
        elif isinstance(value, dict):
            value = decode_dict(value)
        rv[key] = value
    return rv

# Convert CPU list format (with ranges) to a Python list


def cpus2list(s):
    # The input string is a comma separated list of digits and ranges.
    # Examples include:
    # 0-3,8-11
    # 0,2,4,6
    # 2,5-7,10
    cpus = []
    if s.strip() == '':
        return cpus
    for r in s.split(','):
        if '-' in r[1:]:
            start, end = r.split('-', 1)
            for i in range(int(start), int(end) + 1):
                cpus.append(int(i))
        else:
            cpus.append(int(r))
    return cpus


# Helper function to ignore suspended jobs when removing
# "already used" resources
def job_to_be_ignored(jobid):
    # Get job substate from small utility that calls printjob
    pbs_exec = ''
    if not CRAY_12_BRANCH:
        pbs_conf = pbs.get_pbs_conf()
        pbs_exec = pbs_conf['PBS_EXEC']
    else:
        if 'PBS_EXEC' in self.cfg:
            pbs_exec = self.cfg['PBS_EXEC']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "PBS_EXEC needs to be defined in the config file")
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Exiting the cgroups hook")
            pbs.accept()

    printjob_cmd = pbs_exec+os.sep+'bin'+os.sep+'printjob'

    cmd = [printjob_cmd, jobid]

    substate = None
    try:
        pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
        # Collect the job substate information
        process = subprocess.Popen(
                                   cmd,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        (out, err) = process.communicate()
        # Find the job substate
        substate_re = re.compile(r"substate:\s+(?P<jobstate>\S+)\s+")
        substate = substate_re.search(out)
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Unexpected error in job_to_be_ignored: %s" %
                   ' '.join([repr(sys.exc_info()[0]),
                            repr(sys.exc_info()[1])]))
        out = "Unknown: failed to run " + printjob_cmd

    if substate is not None:
        substate = substate.group().split(':')[1].strip()
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Job %s has substate %s" % (jobid, substate))

        suspended_substates = ['0x2b', '0x2d', 'unknown']
        return (substate in suspended_substates)
    else:
        return False

# ============================================================================
# Utility classes
# ============================================================================

#
# CLASS HookUtils
#


class HookUtils:

    def __init__(self, hook_events=None):
        if hook_events is not None:
            self.hook_events = hook_events
        else:
            self.hook_events = {
                # Defined in the order they appear in module_pbs_v1.c
                pbs.QUEUEJOB: {
                    'name': 'queuejob',
                    'handler': None
                },
                pbs.MODIFYJOB: {
                    'name': 'modifyjob',
                    'handler': None
                },
                pbs.RESVSUB: {
                    'name': 'resvsub',
                    'handler': None
                },
                pbs.MOVEJOB: {
                    'name': 'movejob',
                    'handler': None
                },
                pbs.RUNJOB: {
                    'name': 'runjob',
                    'handler': None
                },
                pbs.PROVISION: {
                    'name': 'provision',
                    'handler': None
                },
                pbs.EXECJOB_BEGIN: {
                    'name': 'execjob_begin',
                    'handler': self.__execjob_begin_handler
                },
                pbs.EXECJOB_PROLOGUE: {
                    'name': 'execjob_prologue',
                    'handler': None
                },
                pbs.EXECJOB_EPILOGUE: {
                    'name': 'execjob_epilogue',
                    'handler': self.__execjob_epilogue_handler
                },
                pbs.EXECJOB_PRETERM: {
                    'name': 'execjob_preterm',
                    'handler': None
                },
                pbs.EXECJOB_END: {
                    'name': 'execjob_end',
                    'handler': self.__execjob_end_handler
                },
                pbs.EXECJOB_LAUNCH: {
                    'name': 'execjob_launch',
                    'handler': self.__execjob_launch_handler
                },
                pbs.EXECHOST_PERIODIC: {
                    'name': 'exechost_periodic',
                    'handler': self.__exechost_periodic_handler
                },
                pbs.EXECHOST_STARTUP: {
                    'name': 'exechost_startup',
                    'handler': self.__exechost_startup_handler
                },
                pbs.MOM_EVENTS: {
                    'name': 'mom_events',
                    'handler': None
                },
            }

            if not CRAY_12_BRANCH:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "CRAY 12 Branch: %s" % (CRAY_12_BRANCH))
                self.hook_events[pbs.EXECJOB_ATTACH] = {
                    'name': 'execjob_attach',
                    'handler': self.__execjob_attach_handler
                }

    def __repr__(self):
        return "HookUtils(%s)" % (repr(self.hook_events))

    def event_name(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['name']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Type: %s not found" % (caller_name(), type))
            return None

    def hashandler(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['handler'] is not None
        else:
            return None

    def invoke_handler(self, event, cgroup, jobutil, *args):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: UID: real=%d, effective=%d" %
                   (caller_name(), os.getuid(), os.geteuid()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: GID: real=%d, effective=%d" %
                   (caller_name(), os.getgid(), os.getegid()))
        if self.hashandler(event.type):
            result = self.hook_events[event.type][
                'handler'](event, cgroup, jobutil, *args)
            return result
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: %s event not handled by this hook." %
                       (caller_name(), self.event_name(event.type)))

    def __execjob_begin_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Instantiate the NodeConfig class for get_memory_on_node and
        # get_vmem_on node
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Host assigned job resources: %s" %
                   (caller_name(), jobutil.host_resources))

        # Make sure the parent cgroup directories exist
        cgroup.create_paths()

        # Make sure that any old cgroups created in an earlier attempt
        # at creating or running the job are gone
        cgroup.delete(e.job.id)

        # Gather the assigned resources
        cgroup.gather_assigned_resources()
        # Create the cgroup(s) for the job
        cgroup.create_job(e.job.id, node)
        # Configure the new cgroup
        cgroup.configure_job(e.job.id, jobutil.host_resources, node)
        # Initialize resource usage for the job
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        # Write out the assigned resources
        cgroup.write_out_cgroup_host_assigned_resources(e.job.id)
        # Write out the environment variable for the host (pbs_attach)
        if 'devices_name' in cgroup.host_assigned_resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Devices: %s" %
                       cgroup.host_assigned_resources['devices_name'])
            env_list = list()
            if len(cgroup.host_assigned_resources['devices_name']) > 0:
                line = str()
                mics = list()
                gpus = list()
                for key in cgroup.host_assigned_resources['devices_name']:
                    if key.startswith('mic'):
                        mics.append(key[3:])
                    elif key.startswith('nvidia'):
                        gpus.append(key[6:])
                if len(mics) > 0:
                    env_list.append('OFFLOAD_DEVICES="%s"' %
                                    string.join(mics, ','))
                if len(gpus) > 0:
                    # don't put quotes around the values. ex "0" or "0,1".
                    # This will cause it to fail.
                    env_list.append('CUDA_VISIBLE_DEVICES=%s' %
                                    string.join(gpus, ','))

            pbs.logmsg(pbs.EVENT_DEBUG, "ENV_LIST: %s" % env_list)
            cgroup.write_out_cgroup_host_job_env_file(e.job.id, env_list)
        return True

    def __execjob_epilogue_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # The resources_used information has a base type of pbs_resource
        # Set the usage data
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        return True

    def __execjob_end_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Delete the cgroup(s) for the job
        cgroup.delete(e.job.id)
        # Remove the host_assigned_resources and job_env file
        for filename in [cgroup.hook_storage_dir+os.sep+e.job.id,
                         cgroup.host_job_env_filename % e.job.id]:
            try:
                os.remove(filename)
            except OSError:
                pbs.logmsg(pbs.EVENT_DEBUG3, "File: %s not found" % (filename))
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Error removing file: %s" % (filename))
                pass

        return True

    def __execjob_launch_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Add the parent process id to the appropriate cgroups.
        cgroup.add_pids(os.getppid(), jobutil.job.id)
        # FUTURE: Add environment variable to the job environment
        # if job requested mic or gpu
        cgroup.read_in_cgroup_host_assigned_resources(e.job.id)
        if cgroup.host_assigned_resources is not None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "host_assigned_resources: %s" %
                       (cgroup.host_assigned_resources))
            cgroup.setup_job_devices_env()
        return True

    def __exechost_periodic_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Cleanup cgroups for jobs not present on this node
        count = cgroup.cleanup_orphans(e.job_list)
        if ('periodic_resc_update' in cgroup.cfg and
                cgroup.cfg['periodic_resc_update']):
            for job in e.job_list.keys():
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: job is %s" %
                           (caller_name(), job))
                cgroup.update_job_usage(job, e.job_list[job].resources_used)
        # Online nodes that were offlined due to a cgroup not cleaning up
        if count == 0 and cgroup.cfg['online_offlined_nodes']:
            vnode = pbs.event().vnode_list[cgroup.hostname]
            if os.path.isfile(cgroup.offline_file):
                line = 'Orphan cgroup(s) have been cleaned up. '

                # Check with the pbs server to see if the node comment matches
                try:
                    tmp_comment = pbs.server().vnode(cgroup.hostname).comment
                except:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Unable to contact server for node comment")
                    tmp_comment = None
                if tmp_comment == cgroup.offline_msg:
                    line += 'Will bring the node back online'
                    vnode.state = pbs.ND_FREE
                    vnode.comment = None
                else:
                    line += 'However, the comment has changed since the node '
                    line += 'was offlined. Node will remain offline'

                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: %s" % (caller_name(), line))
                # Remove file
                os.remove(cgroup.offline_file)
        return True

    def __exechost_startup_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        cgroup.create_paths()
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))

        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        node.create_vnodes()
        if 'memory' in cgroup.subsystems:
            val = node.get_memory_on_node(cgroup.cfg)
            if val is not None:
                if 'vnode_per_numa_node' in cgroup.cfg:
                    if not cgroup.cfg['vnode_per_numa_node']:
                        hn = node.hostname
                        e.vnode_list[hn].resources_available['mem'] = \
                            pbs.size(val)
                        val_cpus = node.totalcpus
                        if val_cpus is not None:
                            e.vnode_list[hn].resources_available['ncpus'] = \
                                int(val_cpus)
                cgroup.set_node_limit('mem', val)
        if 'memsw' in cgroup.subsystems:
            val = node.get_vmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['vmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('vmem', val)
        if 'hugetlb' in cgroup.subsystems:
            val = node.get_hpmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['hpmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('hpmem', val)
        return True

    def __execjob_attach_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logjobmsg(jobutil.job.id, "%s: Attaching PID %s" %
                      (caller_name(), e.pid))
        # Add the job process id to the appropriate cgroups.
        cgroup.add_pids(e.pid, jobutil.job.id)
        return True

#
# CLASS ShallIRunUtils
#


class ShallIRunUtils:

    def __init__(self, hostname, cfg):
        self.cfg = cfg
        self.hostname = hostname

    def allow_users(self, allow_users):
        """ This still needs to be defined """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pass

    def exclude_hosts(self, exclude_hosts):
        """
        Gets the hostname for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        hostname = pbs.get_local_nodename()
        # check to see if hostname is in the exclude_hosts list
        if self.hostname in exclude_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: exclude host %s is in %s" %
                       (caller_name(), self.hostname, exclude_hosts))
            pbs.event().accept()

        # check to see if it regex syntax is used
        pass

    def exclude_vntypes(self, exclude_vntypes):
        """
        Gets the vntype for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        vntype = None

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'
        if os.path.isfile(vntype_file):
            fdata = open(vntype_file).readlines()
            vntype = fdata[0].strip()
            pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
        if vntype is None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype is set to %s" % (vntype))
        elif vntype in exclude_vntypes:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s is in %s" % (vntype, exclude_vntypes))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Exiting Hook")
            pbs.event().accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s not in global exclude: %s" %
                       (vntype, exclude_vntypes))
        pass

    def run_only_on_hosts(self, approved_hosts):
        """
        Gets the list of approved nodes to run on and checks to see if the hook
        should run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Approved hosts: %s, %s" %
                   (type(approved_hosts), approved_hosts))

        # check to see if hostname is in the approved_hosts list
        if approved_hosts == []:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "approved hosts list is empty: %s" % approved_hosts)
        elif self.hostname not in approved_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s is not in the approved list of hosts: %s" %
                       (self.hostname, approved_hosts))
            pbs.event().accept()

        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s in list of approved hosts: %s" %
                       (self.hostname, approved_hosts))

#
# CLASS JobUtils
#


class JobUtils:

    def __init__(self, job, hostname=None, resources=None):
        self.job = job
        self.host_vnodes = list()
        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if resources is not None:
            self.host_resources = resources
        else:
            self.host_resources = self.__host_assigned_resources()

    def __repr__(self):
        return "JobUtils(%s, %s, %s)" % (repr(self.job), repr(self.hostname),
                                         repr(self.host_resources))

    # Return a dictionary of resources assigned to the local node
    def __host_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Bail out if no hostname was provided
        if self.hostname is None:
            raise CgroupProcessingError('No hostname available')
        # Bail out if no job information is present
        if self.job is None:
            raise CgroupProcessingError('No job information available')
        # Determine which vnodes are on this host, if any
        if self.hostname is not None:
            vnhost_pattern = "%s\[[\d+]\]" % self.hostname
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnhost pattern: %s" %
                       (caller_name(), vnhost_pattern))
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job exec_vnode list: %s" %
                       (caller_name(), self.job.exec_vnode))
            for match in re.findall(vnhost_pattern, str(self.job.exec_vnode)):
                self.host_vnodes.append(match)
            if self.host_vnodes is not None:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Associate %s to vnodes on %s" %
                           (caller_name(), self.host_vnodes, self.hostname))

        # Collect host assigned resources
        resources = {}
        for chunk in self.job.exec_vnode.chunks:
            vnode = False
            if self.host_vnodes == [] and chunk.vnode_name != self.hostname:
                continue
            elif self.host_vnodes != [] and \
                    chunk.vnode_name not in self.host_vnodes:
                continue
            elif chunk.vnode_name in self.host_vnodes:
                vnode = True
                if 'vnodes' not in resources:
                    resources['vnodes'] = {}
                if chunk.vnode_name not in resources['vnodes']:
                    resources['vnodes'][chunk.vnode_name] = {}
                # This check is needed since not all resources are
                # required to be in each chunk of a job
                # i.e. exec_vnodes =
                # (node1[0]:ncpus=4:mem=4gb+node1[1]:mem=2gb) +
                # (node1[1]:ncpus=3+node[0]:ncpus=1:mem=4gb)
                if all(resc in resources['vnodes'][chunk.vnode_name]
                       for resc in chunk.chunk_resources.keys()):
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: resources already defined" %
                               (caller_name()))
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s,\t%s" %
                               (caller_name(),
                                chunk.chunk_resources.keys(),
                                resources['vnodes'][chunk.vnode_name].keys()))
                else:
                    # Initialize resources for each vnode
                    for resc in chunk.chunk_resources.keys():
                        # Initialize the new value
                        if isinstance(chunk.chunk_resources[resc],
                                      pbs.pbs_int):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_int(0)
                        elif isinstance(chunk.chunk_resources[resc],
                                        pbs.pbs_float):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_float(0)
                        elif isinstance(chunk.chunk_resources[resc], pbs.size):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.size('0')

            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Chunk %s" %
                       (caller_name(), chunk.vnode_name))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resources: %s" %
                       (caller_name(), resources))
            for resc in chunk.chunk_resources.keys():
                if resc not in resources.keys():
                    # Initialize the new value
                    if isinstance(chunk.chunk_resources[resc], pbs.pbs_int):
                        resources[resc] = pbs.pbs_int(0)
                    elif isinstance(chunk.chunk_resources[resc],
                                    pbs.pbs_float):
                        resources[resc] = pbs.pbs_float(0.0)
                    elif isinstance(chunk.chunk_resources[resc], pbs.size):
                        resources[resc] = pbs.size('0')
                # Add resource value to total
                if type(chunk.chunk_resources[resc]) in \
                        [pbs.pbs_int, pbs.pbs_float, pbs.size]:
                    resources[resc] += chunk.chunk_resources[resc]
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s: resources[%s][%s] is now %s" %
                               (caller_name(), self.hostname, resc,
                                resources[resc]))
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] += \
                                  chunk.chunk_resources[resc]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Setting resource %s to string %s" %
                               (caller_name(), resc,
                                str(chunk.chunk_resources[resc])))
                    resources[resc] = str(chunk.chunk_resources[resc])
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] = \
                                  str(chunk.chunk_resources[resc])
        if not resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: No resources assigned to host %s" %
                       (caller_name(), self.hostname))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources for %s: %s" %
                       (caller_name(), self.hostname, repr(resources)))

        # Return assigned resources for specified host
        pbs.logmsg(pbs.EVENT_DEBUG3, "Job Resources: %s" % (resources))
        return resources

    # Write a message to the job stdout file
    def write_to_stderr(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stderr_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

    # Write a message to the job stdout file
    def write_to_stdout(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stdout_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

#
# CLASS NodeConfig
#


class NodeConfig:

    def __init__(self, cfg, **kwargs):

        self.cfg = cfg
        hostname = None
        meminfo = None
        numa_nodes = None
        devices = None
        hyperthreading = None
        self.hyperthreads_per_core = 1
        self.totalcpus = self.__discover_cpus()

        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        self.host_mom_jobdir = pbs_home+'/mom_priv/jobs'

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'hostname':
                    hostname = val
                elif arg == 'meminfo':
                    meminfo = val
                elif arg == 'numa_nodes':
                    numa_nodes = val
                elif arg == 'devices':
                    devices = val
                elif arg == 'hyperthreading':
                    hyperthreading = val

        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if meminfo is not None:
            self.meminfo = meminfo
        else:
            self.meminfo = self.__discover_meminfo()
        if numa_nodes is not None:
            self.numa_nodes = numa_nodes
        else:
            self.numa_nodes = self.__discover_numa_nodes()
        if devices is not None:
            self.devices = devices
        else:
            self.devices = self.__discover_devices()
        if hyperthreading is not None:
            self.hyperthreading = hyperthreading
        else:
            self.hyperthreading = self.__hyperthreading_enabled()

        # Add the devices count i.e. nmics and ngpus to the numa nodes
        self.__add_devices_to_numa_node()

    def __repr__(self):
        return ("NodeConfig(%s, %s, %s, %s, %s, %s)" %
                (repr(self.cfg), repr(self.hostname), repr(self.meminfo),
                 repr(self.numa_nodes), repr(self.devices),
                 repr(self.hyperthreading)))

    def __add_devices_to_numa_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (self.devices.keys()))
        for device in self.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" % (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" %
                           (self.devices[device].keys()))
                for device_name in self.devices[device]:
                    device_socket = \
                        self.devices[device][device_name]['numa_node']
                    if device == 'mic':
                        if 'nmics' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['nmics'] = 1
                        else:
                            self.numa_nodes[device_socket]['nmics'] += 1
                    elif device == 'gpu':
                        if 'ngpus' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['ngpus'] = 1
                        else:
                            self.numa_nodes[device_socket]['ngpus'] += 1

        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (self.numa_nodes))

    # Discover what type of hardware is on this node and how is it partitioned
    def __discover_numa_nodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        numa_nodes = {}
        for dir in glob.glob(os.path.join(os.sep,
                             "sys", "devices", "system", "node", "node*")):
            id = int(dir.split(os.sep)[5][4:])
            if id not in numa_nodes.keys():
                numa_nodes[id] = {}
                numa_nodes[id]['devices'] = list()
            numa_nodes[id]['cpus'] = open(os.path.join(dir, "cpulist"),
                                          'r').readline().strip()
            with open(os.path.join(dir, "meminfo"), 'r') as fd:
                for line in fd:
                    # Each line will contain four or five fields. Examples:
                    # Node 0 MemTotal:       32995028 kB
                    # Node 0 HugePages_Total:     0
                    entries = line.split()
                    if len(entries) < 4:
                        continue
                    if entries[2] == "MemTotal:":
                        numa_nodes[id][entries[2].rstrip(':')] = \
                            convert_size(entries[3] + entries[4], 'kb')
                    elif entries[2] == "HugePages_Total:":
                        numa_nodes[id][entries[2].rstrip(':')] = entries[3]
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover Numa Nodes: %s" % numa_nodes)
        return numa_nodes

    # Identify devices and to which numa nodes they are attached
    def __discover_devices(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        devices = {}
        for file in glob.glob(os.path.join(os.sep, "sys", "class", "*", "*",
                                           "device", "numa_node")):
            # The file should contain a single integer
            numa_node = int(open(file, 'r').readline().strip())
            if numa_node < 0:
                numa_node = 0
            dirs = file.split(os.sep)
            name = dirs[3]
            instance = dirs[4]
            if name not in devices.keys():
                devices[name] = {}
            devices[name][instance] = {}
            devices[name][instance]['numa_node'] = numa_node
            if name == 'mic':
                s = os.stat('/dev/%s' % instance)
                devices[name][instance]['major'] = os.major(s.st_rdev)
                devices[name][instance]['minor'] = os.minor(s.st_rdev)
                devices[name][instance]['device_type'] = 'c'

        # Check to see if there are gpus on the node
        gpus = self.discover_gpus()
        if isinstance(gpus, dict) and len(gpus.keys()) > 0:
            devices['gpu'] = gpus['gpu']

        pbs.logmsg(pbs.EVENT_DEBUG3, "Discovered devices: %s" % (devices))
        return devices

    def discover_gpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Check to see if nvidia-smi exists and produces valid output
            cmd = ['/usr/bin/nvidia-smi', '-q', '-x']
            if 'nvidia-smi' in self.cfg:
                cmd[0] = self.cfg['nvidia-smi']

            pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
            # Collect the gpu information
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.PIPE)
            output, err = process.communicate()

            # pbs.logmsg(pbs.EVENT_DEBUG3,"output: %s" % output)
            # import the xml library and parse the output
            import xml.etree.ElementTree as ET

            # Parse the data
            name = 'gpu'
            gpu_data = dict()
            gpu_data[name] = {}
            root = ET.fromstring(output)
            pbs.logmsg(pbs.EVENT_DEBUG3, "root.tag: %s" % root.tag)
            for child in root:
                if child.tag == "attached_gpus":
                    # gpu_data['ngpus'] = int(child.text)
                    pass
                if child.tag == "gpu":
                    device_id = child.get('id')
                    number = 'nvidia%s' % child.find('minor_number').text
                    gpu_data[name][number] = dict()
                    gpu_data[name][number]['id'] = device_id

                    # Determine which socket the device is on
                    filename = '/sys/bus/pci/devices/%s/numa_node' % \
                        device_id.lower()
                    isfile = os.path.isfile(filename)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s, %s" % (filename, isfile))
                    if isfile:
                        numa_node = open(filename).read().strip()
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "numa_node: %s" % (numa_node))
                        if int(numa_node) == -1:
                            numa_node = 0
                        gpu_data[name][number]['numa_node'] = int(numa_node)
                        try:
                            dev_filename = '/dev/%s' % number
                            s = os.stat(dev_filename)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find %s" % dev_filename)
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                        gpu_data[name][number]['major'] = os.major(s.st_rdev)
                        gpu_data[name][number]['minor'] = os.minor(s.st_rdev)
                        gpu_data[name][number]['device_type'] = 'c'
            pbs.logmsg(pbs.EVENT_DEBUG, "%s" % gpu_data)
            return gpu_data
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unable to find %s" % string.join(cmd, " "))
            return {'gpu': {}}
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        return {'gpu': {}}

    # Get the memory info on this host
    def __discover_meminfo(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        meminfo = {}
        with open(os.path.join(os.sep, "proc", "meminfo"), 'r') as fd:
            for line in fd:
                entries = line.split()
                if entries[0] == "MemTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "SwapTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "Hugepagesize:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "HugePages_Total:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
                elif entries[0] == "HugePages_Rsvd:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover meminfo: %s" % meminfo)
        return meminfo

    # Determine if the cpus have hyperthreading enabled
    def __hyperthreading_enabled(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            siblings = 0
            cpu_cores = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "siblings":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    siblings = entries[1].strip()
                elif entries[0].strip() == "cpu cores":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_cores = entries[1].strip()
                elif entries[0].strip() == "flags":
                    flag_options = entries[1].split()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: flag_options: %s" %
                               (caller_name(), flag_options))
                    if "ht" in flag_options:
                        rc = True
                        break
        if rc is True:
            self.hyperthreads_per_core = int(siblings)/int(cpu_cores)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: hyperthreads/core: %d" %
                       (caller_name(), self.hyperthreads_per_core))
            if self.hyperthreads_per_core == 1:
                rc = False
        return rc

    def __discover_cpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            cpu_threads = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "processor":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_threads += 1
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: processors found: %d" %
                   (caller_name(), cpu_threads))

        return cpu_threads

    # Gather the jobs running on this node
    def gather_jobs_on_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        # Get the job list from the server
        jobs = None
        try:
            jobs = pbs.server().vnode(self.hostname).jobs
        except:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Unable to contact server to get jobs")
            return None

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs from server for %s: %s" % (self.hostname, jobs))

        if jobs is None:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "No jobs found on %s " % (self.hostname))
            return list()

        jobs_list = dict()
        for job in jobs.split(','):
            # The job list from the node has a space in front of the job id
            # This must be removed or it will not match the cgroup dir
            jobs_list[job.split('/')[0].strip()] = 0
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs on %s: %s" % (self.hostname, jobs_list.keys()))

        return jobs_list.keys()

    # Gather the jobs running on this node
    def gather_jobs_on_node_local(self):
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Method called" % (caller_name()))
        # Get the job list from the jobs directory
        jobs_list = list()
        try:
            for file in os.listdir(self.host_mom_jobdir):
                if fnmatch.fnmatch(file, "*.JB"):
                    jobs_list.append(file[:-3])
            if not jobs_list:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "No jobs found on %s " % (self.hostname))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Jobs from server for %s: %s" %
                           (self.hostname, repr(jobs_list)))

            return jobs_list
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Could not get job list from mom_priv/jobs for %s" %
                       self.hostname)

            return self.gather_jobs_on_node()

    # Get the memory resource on this mom
    def get_memory_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Calculate memory
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
            pbs.logmsg(pbs.EVENT_DEBUG3, "val: %s" % val)
        else:
            val = size_as_int(MemTotal)
        if 'percent_reserve' in config['cgroup']['memory']:
            percent_reserve = config['cgroup']['memory']['percent_reserve']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memory'].keys():
            reserve_memory = config['cgroup']['memory']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                reserve_mem = size_as_int(reserve_memory)
                if val > reserve_mem:
                    val -= reserve_mem
                else:
                    val -= size_as_int("100mb")
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Unable to reserve more memory than " +
                               "available on the node. Available %s, " +
                               "Tried to reserve %s. Reserving 100mb" %
                               (val, reserve_mem))

            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))

        pbs.logmsg(pbs.EVENT_DEBUG3, "Return val: %s" % val)
        return convert_size(str(val), 'kb')

    # Get the virtual memory resource on this mom
    def get_vmem_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Return the value for memory if no swap is configured
        if size_as_int(self.meminfo['SwapTotal']) <= 0:
            return self.get_memory_on_node(config)
        # Calculate vmem
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
        else:
            val = size_as_int(MemTotal)

        val += size_as_int(self.meminfo['SwapTotal'])
        # Determine if memory needs to be reserved
        if 'percent_reserve' in config['cgroup']['memsw']:
            percent_reserve = config['cgroup']['memsw']['reserve_memory']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memsw']:
            reserve_memory = config['cgroup']['memsw']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                val -= size_as_int(reserve_memory)
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))
        return convert_size(str(val), 'kb')

    # Get the huge page memory resource on this mom
    def get_hpmem_on_node(self, config):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        if 'percent_reserve' in config['cgroup']['hugetlb'].keys():
            percent_reserve = config['cgroup']['hugetlb']['percent_reserve']
        else:
            percent_reserve = 0
        # Calculate vmem
        val = size_as_int(self.meminfo['Hugepagesize'])
        val *= (self.meminfo['HugePages_Total'] -
                self.meminfo['HugePages_Rsvd'])
        val -= (val * percent_reserve) / 100
        return convert_size(str(val), 'kb')

    # Create individual vnodes per socket
    def create_vnodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        key = "vnode_per_numa_node"
        if key in self.cfg.keys():
            if not self.cfg[key]:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s is false" %
                           (caller_name(), key))
                return
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s not configured" %
                       (caller_name(), key))
            return

        # Create one vnode per numa node
        vnode_list = pbs.event().vnode_list
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: numa nodes: %s" %
                   (caller_name(), self.numa_nodes))
        # Define the vnodes
        vnode_name = self.hostname
        vnode_list[vnode_name] = pbs.vnode(vnode_name)
        for id in self.numa_nodes.keys():
            vnode_name = self.hostname + "[%d]" % id
            vnode_list[vnode_name] = pbs.vnode(vnode_name)
            for key, val in sorted(self.numa_nodes[id].iteritems()):
                if key == 'cpus':
                    vnode_list[vnode_name].resources_available['ncpus'] = \
                        len(cpus2list(val))/self.hyperthreads_per_core
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['ncpus'] = 0
                elif key == 'MemTotal':
                    mem = self.get_memory_on_node(self.cfg, val)
                    vnode_list[vnode_name].resources_available['mem'] = \
                        pbs.size(mem)
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['mem'] = \
                        pbs.size('0kb')
                elif key == 'HugePages_Total':
                    pass
                elif isinstance(val, list):
                    pass
                elif isinstance(val, dict):
                    pass
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s=%s" %
                               (caller_name(), key, val))
                    vnode_list[vnode_name].resources_available[key] = val

                    if isinstance(val, int):
                        vnode_list[self.hostname].resources_available[key] = 0
                    elif isinstance(val, float):
                        vnode_list[self.hostname].resources_available[key] = \
                            0.0
                    elif isinstance(val, pbs.size):
                        vnode_list[self.hostname].resources_available[key] = \
                            pbs.size('0kb')
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnode list: %s" %
                   (caller_name(), vnode_list))
        return True

#
# CLASS CgroupUtils
#


class CgroupUtils:
    def __init__(self, hostname, vnode, **kwargs):
        cfg = None
        subsystems = None
        paths = None
        vntype = None
        event = None
        self.assigned_resources = dict()
        self.existing_cgroups = dict()
        self.host_assigned_resources = None
        self.pid_lower_limit = 3

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'subsystems':
                    subsystems = val
                elif arg == 'paths':
                    paths = val
                elif arg == 'vntype':
                    vntype = val
                elif arg == 'event':
                    event = val

        try:
            self.hostname = hostname
            self.vnode = vnode
            # __check_os will raise an exception if cgroups are not present
            self.__check_os()
            # Read in the config file
            if cfg is not None:
                self.cfg = cfg
            else:
                self.cfg = self.__parse_config_file()

            # Determine if the hook should run or exit
            siru = ShallIRunUtils(self.hostname, self.cfg)
            siru.exclude_hosts(self.cfg['exclude_hosts'])
            siru.exclude_vntypes(self.cfg['exclude_vntypes'])
            siru.run_only_on_hosts(self.cfg['run_only_on_hosts'])

            # Collect the mount points
            if paths is not None:
                self.paths = paths
            else:
                self.paths = self.__set_paths()
            # Define the local vnode type
            if vntype is not None:
                self.vntype = vntype
            else:
                self.vntype = self.__get_vnode_type()
            # Determine which subsystems we care about
            if subsystems is not None:
                self.subsystems = subsystems
            else:
                self.subsystems = self.__target_subsystems()

            # location to store information for the different hook events
            pbs_home = ''
            if not CRAY_12_BRANCH:
                pbs_conf = pbs.get_pbs_conf()
                pbs_home = pbs_conf['PBS_HOME']
            else:
                if 'PBS_HOME' in self.cfg:
                    pbs_home = self.cfg['PBS_HOME']
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "PBS_HOME needs to be defined in the " +
                               "config file")
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Exiting the cgroups hook")
                    pbs.accept()

            self.hook_storage_dir = pbs_home+'/mom_priv/hooks/hook_data'
            self.host_job_env_dir = pbs_home+'/aux'
            self.host_job_env_filename = self.host_job_env_dir+os.sep+"%s.env"

            # information for offlining nodes
            self.offline_file = \
                pbs_home+os.sep+'mom_priv'+os.sep+'hooks'+os.sep
            self.offline_file += "%s.offline" % pbs.event().hook_name
            self.offline_msg = "Hook %s: " % pbs.event().hook_name
            self.offline_msg += "Unable to clean up one or more cgroups"

        except:
            raise

    def __repr__(self):
        return ("CgroupUtils(%s, %s, %s, %s, %s, %s)" %
                (repr(self.hostname), repr(self.vnode), repr(self.cfg),
                 repr(self.subsystems), repr(self.paths), repr(self.vntype)))

    # Validate the OS type and version
    def __check_os(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check to see if the platform is linux and the kernel is new enough
        if platform.system() != 'Linux':
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: OS does not support cgroups" %
                       (caller_name()))
            raise CgroupConfigError("OS type not supported")
        rel = map(int,
                  string.split(string.split(platform.release(), '-')[0], '.'))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Detected Linux kernel version %d.%d.%d" %
                   (caller_name(), rel[0], rel[1], rel[2]))
        supported = False
        if rel[0] > 2:
            supported = True
        elif rel[0] == 2:
            if rel[1] > 6:
                supported = True
            elif rel[1] == 6:
                if rel[2] >= 28:
                    supported = True
        if not supported:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Kernel needs to be >= 2.6.28: %s" %
                       (caller_name(), system_info[2]))
            raise CgroupConfigError("OS version not supported")
        return supported

    # Read the config file in json format
    def __parse_config_file(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        config = {}
        # Identify the config file and read in the data
        if 'PBS_HOOK_CONFIG_FILE' in os.environ:
            config_file = os.environ["PBS_HOOK_CONFIG_FILE"]
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Config file is %s" %
                       (caller_name(), config_file))
            try:
                config = json.load(open(config_file, 'r'),
                                   object_hook=decode_dict)
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
        else:
            raise CgroupConfigError("No configuration file present")
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Initial Cgroup Config: %s" %
                   (caller_name(), config))
        # Set some defaults if they are not present
        if 'cgroup_prefix' not in config.keys():
            config['cgroup_prefix'] = 'pbspro'
        if 'periodic_resc_update' not in config.keys():
            config['periodic_resc_update'] = False
        if 'vnode_per_numa_node' not in config.keys():
            config['vnode_per_numa_node'] = False
        if 'online_offlined_nodes' not in config.keys():
            config['online_offlined_nodes'] = False
        if 'exclude_hosts' not in config.keys():
            config['exclude_hosts'] = []
        if 'run_only_on_hosts' not in config.keys():
            config['run_only_on_hosts'] = []
        if 'exclude_vntypes' not in config.keys():
            config['exclude_vntypes'] = []
        if 'cgroup' not in config.keys():
            config['cgroup'] = {}
        else:
            for subsys in config['cgroup']:
                subsystem = config['cgroup'][subsys]
                if 'enabled' not in subsystem.keys():
                    config['cgroup'][subsys]['enabled'] = False
                if 'exclude_hosts' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_hosts'] = []
                if 'exclude_vntypes' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_vntypes'] = []

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Cgroup Config with defaults: %s" %
                   (caller_name(), config))
        return config

    # Determine which subsystems are being requested
    def __target_subsystems(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        subsystems = []
        for key in self.cfg['cgroup'].keys():
            if self.enabled(key):
                subsystems.append(key)
        if 'memory' not in subsystems:
            if 'memsw' in subsystems:
                # Remove memsw since it must be greater then
                # or equal to memory
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing memsw from enabled subsystems")
                subsystems.remove('memsw')
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Enabled subsystems: %s" %
                   (caller_name(), subsystems))
        # It is not an error for all subsystems to be disabled.
        # This host or vnode type may be in the excluded list.
        return subsystems

    # Copy a setting from the parent cgroup
    def __copy_from_parent(self, dest):
        try:
            file = os.path.basename(dest)
            dir = os.path.dirname(dest)
            parent = os.path.dirname(dir)
            source = os.path.join(parent, file)
            if not os.path.isfile(source):
                raise CgroupConfigError('Failed to read %s' % (source))
            self.write_value(dest, open(source, 'r').read().strip())
        except:
            raise

    # Create a dictionary of the cgroup subsystems and their corresponding
    # directories
    def __set_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        paths = {}
        try:
            # Loop through the mounts and collect the ones for cgroups
            with open(os.path.join(os.sep, "proc", "mounts"), 'r') as fd:
                for line in fd:
                    entries = line.split()
                    if len(entries) > 3 and entries[2] == "cgroup":
                        flags = entries[3].split(',')
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "subsysdir: %s (flags=%s)" %
                                   (entries[1], flags))
                        for subsys in flags:
                            paths[subsys] = os.path.join(
                                entries[1], self.cfg["cgroup_prefix"])
        except:
            raise
        if len(paths.keys()) < 1:
            raise CgroupConfigError("Cgroup paths not detected")
        # Both the memory and memsw cgroups share the same mount point
        if "memory" in paths.keys():
            paths["memsw"] = paths["memory"]
        return paths

    # Create the cgroup parent directories that will contain the jobs
    def create_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Create the directories that PBS will use to house the jobs
            old_umask = os.umask(0022)
            for subsys in self.subsystems:
                if subsys not in self.paths.keys():
                    raise CgroupConfigError('No path for subsystem: %s' %
                                            (subsys))
                if not os.path.exists(self.paths[subsys]):
                    os.makedirs(self.paths[subsys], 0755)
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Created directory %s" %
                               (caller_name(), self.paths[subsys]))
                    if subsys == 'memory' or subsys == 'memsw':
                        # Set file to
                        # <cgroup>/memory/pbspro/memory.use_hierarchy
                        file = os.path.join(self.paths[subsys],
                                            'memory.use_hierarchy')
                        if not os.path.isfile(file):
                            raise CgroupConfigError('Failed to configure %s' %
                                                    (file))
                        self.write_value(file, 1)
                    elif subsys == 'cpuset':
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.cpus'))
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.mems'))

        except:
            raise CgroupConfigError("Failed to create directory: %s" %
                                    (self.paths[subsys]))
        finally:
            os.umask(old_umask)

    # Return the vnode type of the local node
    def __get_vnode_type(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")

        # File to check for if it is not defined in the hook input file
        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'

        # self.vnode is None for pbs_attach events
        pbs.logmsg(pbs.EVENT_DEBUG3, "vnode: %s" % self.vnode)
        if self.vnode is not None:
            if 'vntype' in self.vnode.resources_available and \
                    self.vnode.resources_available['vntype'] is not None:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "vntype: %s" %
                           self.vnode.resources_available['vntype'])
                return self.vnode.resources_available['vntype']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3, "vntype file: %s" % vntype_file)
                if os.path.isfile(vntype_file):
                    fdata = open(vntype_file).readlines()
                    vntype = fdata[0].strip()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
                    return vntype
        # If vntype was not set then log a message. It is too expensive
        # to have all moms query the server for large jobs.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: resources_available.vntype is " % caller_name() +
                   "not set for this vnode")
        return None

    # Gather resources that are already assigned
    def gather_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        assigned_resources = {}

        # Gather the cpus and memory sockets assigned
        directories = [x[0] for x in os.walk(self.paths['cpuset'])]

        # Add the existing cgroups to a list to check if assigning
        # resources fail
        for dir in directories[1:]:
            if dir.find(pbs.event().job.id) == -1:
                self.existing_cgroups[dir] = []

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'cpuset' in self.subsystems and \
                self.cfg['cgroup']['cpuset']['enabled']:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather cpuset resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['cpuset'], tmp_dir)
                    with open(path+os.sep+'cpuset.cpus') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.cpus'] = \
                            cpus2list(tmp_val.strip())
                    with open(path+os.sep+'cpuset.mems') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.mems'] = \
                            cpus2list(tmp_val.strip())

        # Check to see if the subsystem is enabled before trying
        # to gather resources
        if 'memory' in self.subsystems and \
                self.cfg['cgroup']['memory']['enabled']:
            # Gather the memory assigned for each socket
            directories = [x[0] for x in os.walk(self.paths['memory'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather memory resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['memory'], tmp_dir)
                    with open(path+os.sep+'memory.limit_in_bytes') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memory.limit'] = \
                            tmp_val.strip()
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    if os.path.isfile(tmp_filename) is False:
                        continue
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    with open(tmp_filename) as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memsw.limit'] = \
                            tmp_val.strip()

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'devices' in self.subsystems and \
                self.cfg['cgroup']['devices']['enabled']:
            # Gather the devices assigned
            directories = [x[0] for x in os.walk(self.paths['devices'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather device resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    pbs.logmsg(pbs.EVENT_DEBUG3, "Dir: %s" % tmp_dir)
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['devices'], tmp_dir)
                    with open(path+os.sep+'devices.list') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['devices.list'] = \
                            tmp_val.strip().split('\n')
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Dir: %s\n\tValue: %s" %
                                   (path, tmp_val.replace('\n', ',')))

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Gathered assigned resources: %s" % assigned_resources)
        self.assigned_resources = assigned_resources

    # Return whether a subsystem is enabled
    def enabled(self, subsystem):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check whether the subsystem is enabled in the configuration file
        if subsystem not in self.cfg['cgroup'].keys():
            return False
        if 'enabled' not in self.cfg['cgroup'][subsystem].keys():
            return False
        if not self.cfg['cgroup'][subsystem]['enabled']:
            return False
        # Check whether the cgroup is mounted for this subsystem
        if subsystem not in self.paths.keys():
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup not mounted for %s" %
                       (caller_name(), subsystem))
            return False
        # Check whether this host is excluded
        # if self.hostname in self.cfg['exclude_hosts']:
        #    pbs.logmsg(pbs.EVENT_DEBUG, "%s: cgroup excluded on host %s" %
        #               (caller_name(), self.hostname))
        #    pbs.event().accept()
        if self.hostname in self.cfg['cgroup'][subsystem]['exclude_hosts']:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup excluded for subsystem %s on host %s" %
                       (caller_name(), subsystem, self.hostname))
            return False
        # Check whether the vnode type is excluded
        if self.vntype is not None:
            # if self.vntype in self.cfg['exclude_vntypes']:
            #    pbs.logmsg(pbs.EVENT_DEBUG,
            #               "%s: cgroup excluded on vnode type %s" %
            #               (caller_name(), self.vntype))
            #    pbs.event().accept()
            if self.vntype in self.cfg['cgroup'][subsystem]['exclude_vntypes']:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           ("%s: cgroup excluded for " +
                            "subsystem %s on vnode type %s") %
                           (caller_name(), subsystem, self.vntype))
                return False
        return True

    # Return the default value for a subsystem
    def default(self, subsystem):
        if subsystem in self.cfg['cgroup']:
            if 'default' in self.cfg['cgroup'][subsystem]:
                return self.cfg['cgroup'][subsystem]['default']
        return None

    # Check to see if the pid matches the job owners uid
    def is_pid_owned_by_job_owner(self, pid):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        try:
            proc_uid = os.stat('/proc/%d' % pid).st_uid
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       '/proc/%d uid:%d' % (pid, proc_uid))

            job_owner_uid = pwd.getpwnam(pbs.event().job.euser)[2]
            pbs.logmsg(pbs.EVENT_DEBUG3, "Job uid: %d" % job_owner_uid)

            if proc_uid == job_owner_uid:
                return True
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Proc uid: %d != Job owner:  %d" %
                           (proc_uid, job_owner_uid))
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG, "Unknown pid: %d" % pid)
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        # If we got to this point something did not match up
        return False

    # Add some number of PIDs to the cgroup tasks files for each subsystem
    def add_pids(self, pids, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # make pids a list
        if isinstance(pids, int):
            pids = [pids]

        if pbs.event().type == pbs.EXECJOB_LAUNCH:
            if 1 in pids:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "1 is not a valid pid to add")
                # Hold the job for further review
                e.reject("Hook tried to add pid 1 to the tasks file " +
                         "for job %s" % jobid)

        # check pids to make sure that they are owned by the job owner
        if not CRAY_12_BRANCH:
            pbs.logmsg(pbs.EVENT_DEBUG3, "Not in the Cray 12 Branch")
            pbs.logmsg(pbs.EVENT_DEBUG3, "check to see if execjob_attach")
            if pbs.event().type == pbs.EXECJOB_ATTACH:
                pbs.logmsg(pbs.EVENT_DEBUG3, "event type: attach or launch")
                tmp_pids = list()
                for p in pids:
                    if self.is_pid_owned_by_job_owner(p):
                        tmp_pids.append(p)
                if len(tmp_pids) == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%d is not a valid pids to add" % p)
                    return False
                else:
                    pids = tmp_pids

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: subsys = %s" %
                       (caller_name(), subsys))
            # memsw and memory use the same tasks file
            if subsys == "memsw":
                if "memory" in self.subsystems:
                    continue
            path = os.path.join(self.paths[subsys], jobid)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path = %s" %
                       (caller_name(), path))
            try:
                tasks_path = os.path.join(path, "tasks")
                for p in pids:
                    self.write_value(tasks_path, p, 'a')
            except IOError, exc:
                raise CgroupLimitError("Failed to add PIDs %s to %s (%s)" %
                                       (str(pids), tasks_path,
                                        errno.errorcode[exc.errno]))
            except:
                raise

    # Set a node limit
    def set_node_limit(self, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s = %s" %
                   (caller_name(), resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'],
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Node resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    def setup_job_devices_env(self):
        """ Setup the job environment for the devices assigned to the job for an
            execjob_launch hook
         """
        if 'devices_name' in self.host_assigned_resources:
            names = self.host_assigned_resources['devices_name']
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "devices: %s" % (names))
            offload_devices = list()
            cuda_visible_devices = list()
            for name in names:
                if name.startswith('mic'):
                    offload_devices.append(name[3:])
                elif name.startswith('nvidia'):
                    cuda_visible_devices.append(name[6:])
            if len(offload_devices) > 0:
                value = '"%s"' % string.join(offload_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['OFFLOAD_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "offload_devices: %s" % offload_devices)
            if len(cuda_visible_devices) > 0:
                value = '"%s"' % string.join(cuda_visible_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['CUDA_VISIBLE_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "cuda_visible_devices: %s" % cuda_visible_devices)
            return [offload_devices, cuda_visible_devices]
        else:
            return False

    def setup_subsys_devices(self, path, node):
        subsys = 'devices'
        # Add the devices that the user is allowed to use
        if subsys in self.cfg['cgroup']:
            devices_allowed = open(os.path.join(path,
                                   "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Initial devices.list: %s" %
                       devices_allowed)
            # Deny access to mic and gpu devices
            devices_list = list()
            devices = node.devices
            for device in devices:
                if device == 'mic' or device == 'gpu':
                    for name in devices[device]:
                        d = devices[device][name]
                        devices_list.append("%d:%d" % (d['major'], d['minor']))
            # For CentOS 7 we need to remove a *:* rwm from devices.list we
            # before can add anything to devices.allow. Otherwise our changes
            # are ignored. Check to see if a *:* rwm is in devices.list.
            # If so remove it
            if "a *:* rwm\n" in devices_allowed:
                value = "a *:* rwm"
                self.write_value(os.path.join(path, 'devices.deny'), value)
            else:
                # Verify that the following devices are not in devices.list
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing access to the following: %s" %
                           devices_list)
            for device_str in devices_list:
                value = "c %s rwm" % device_str
                self.write_value(os.path.join(path, 'devices.deny'), value)
            # Add devices back to the list
            devices_allow = list()
            if 'allow' in self.cfg['cgroup'][subsys]:
                devices_allow = self.cfg['cgroup'][subsys]['allow']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Allowing access to the following: %s" %
                           devices_allow)
                for item in devices_allow:
                    if isinstance(item, str):
                        pbs.logmsg(pbs.EVENT_DEBUG3, "string item: %s" % item)
                        value = "%s" % item
                        self.write_value(os.path.join(
                                         path, 'devices.allow'), value)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "write_value: %s" % value)
                    elif isinstance(item, list):
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "list item: %s" % item)
                        try:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Device allow: %s" % item)
                            stat_filename = '/dev/%s' % item[0]
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Stat file: %s" % stat_filename)
                            s = os.stat(stat_filename)
                            device_type = None
                            if S_ISBLK(s.st_mode):
                                device_type = "b"
                            elif S_ISCHR(s.st_mode):
                                device_type = "c"
                            if device_type is not None:
                                if len(item) == 3 and isinstance(item[2], str):
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % item[2]
                                    value += "%s" % item[1]
                                else:
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % os.minor(s.st_rdev)
                                    value += "%s" % item[1]
                                self.write_value(os.path.join(
                                                path, 'devices.allow'), value)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "write_value: %s" % value)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find /dev/%s. " % item[0] +
                                       "Not added to devices.allow!")
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                    else:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Not sure what to do with: %s" % item)
            devices_allowed = open(os.path.join(
                                   path, "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "After setup devices.list: %s" % devices_allowed)

    # Select devices to assign to the job
    def assign_devices(
                       self,
                       device_type,
                       device_list,
                       number_of_devices,
                       node):
        devices = device_list[:number_of_devices]
        device_ids = list()
        device_names = list()
        for device in devices:
            device_info = node.devices[device_type][device]
            if len(device_ids) == 0:
                if device.find("mic") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:0 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                    device_ids.append("%s %d:1 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                elif device.find("nvidia") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:255 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
            device_ids.append("%s %d:%d rwm" %
                              (device_info['device_type'],
                               device_info['major'],
                               device_info['minor']))
            device_names.append(device)
        return device_names, device_ids

    # Find the device name
    def get_device_name(self, node, available, socket, major, minor):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Get device name: major: %s, minor: %s" % (major, minor))
        avail_device = None
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Possible devices: %s" % (available[socket]['devices']))
        for avail_device in available[socket]['devices']:
            avail_major = None
            avail_minor = None
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Checking device: %s" % (avail_device))
            if avail_device.find('mic') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check mic device: %s" % (avail_device))
                avail_major = node.devices['mic'][avail_device]['major']
                avail_minor = node.devices['mic'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            elif avail_device.find('nvidia') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check gpu device: %s" % (avail_device))
                avail_major = node.devices['gpu'][avail_device]['major']
                avail_minor = node.devices['gpu'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            if int(avail_major) == int(major) and \
                    int(avail_minor) == int(minor):
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device match: name: %s, major: %s, minor: %s" %
                           (avail_device, major, minor))
                return avail_device
        pbs.logmsg(pbs.EVENT_DEBUG3, "No match found")
        return None

    # Assign resources to the job based off the vnodes assignments
    def assign_job_resources_by_vnode(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # Loop through the vnodes and assign resources
        assigned = dict()
        assigned['cpuset.cpus'] = list()
        assigned['cpuset.mems'] = list()
        room_on_socket = True

        for vnode in resources['vnodes']:
            regex = re.compile(".*\[(\d+)\].*")
            socket = int(regex.search(vnode).group(1))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current Vnode: %s" % vnode)
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current socket: %d" % socket)

            if 'ncpus' in resources['vnodes'][vnode]:
                tmp_ncpus = int(resources['vnodes'][vnode]['ncpus'])
                if int(resources['vnodes'][vnode]['ncpus']) <= \
                        len(available[socket]['cpus']):
                    assigned['cpuset.cpus'] += \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] += [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_ncpus: %s" % (tmp_ncpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "available[%d]: %s" %
                               (socket, available[socket]))
                    room_on_socket = False
            if ('nmics' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['nmics']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(mic).*")
                tmp_nmics = int(resources['vnodes'][vnode]['nmics'])
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['nmics']) > 0 and
                   int(resources['vnodes'][vnode]['nmics']) <= len(mics)):
                    tmp_names, tmp_devices = self.assign_devices(
                             'mic', mics[:tmp_nmics], tmp_nmics, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_nmics: %s" % (tmp_nmics))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "mics: %s" % (mics))
                    room_on_socket = False
            if ('ngpus' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['ngpus']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(nvidia).*")
                tmp_ngpus = int(resources['vnodes'][vnode]['ngpus'])
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['ngpus']) > 0 and
                   int(resources['vnodes'][vnode]['ngpus']) <= len(gpus)):
                    tmp_names, tmp_devices = self.assign_devices(
                        'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "tmp_ngpus: %s" % (tmp_ngpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "gpus: %s" % (gpus))
                    room_on_socket = False

        if room_on_socket:
            assigned['cpuset.cpus'].sort()
            assigned['cpuset.mems'].sort()
            if 'devices' in assigned:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids pre set and sort: %s" %
                           (assigned['devices']))
                assigned['devices'] = list(set(assigned['devices']))
                assigned['devices'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids post set and sort: %s" %
                           (assigned['devices']))
                assigned['devices_name'] = list(set(assigned['devices_name']))
                assigned['devices_name'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

    # Assign resources to the job
    def assign_job_resources(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # What would we need to do different for a large smp (UV) system?
        # See if requested resources can fit on a single socket
        assigned = dict()
        pbs.logmsg(pbs.EVENT_DEBUG3, "Requested: %s" % (resources))

        # Determine the order that we should evaluate the sockets.
        socket_list = list()
        if 'placement_type' in self.cfg:
            if self.cfg['placement_type'] == 'round_robin':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Requested round_robin placement")
                # Look at assigned_resources and determine which socket
                # to start with
                jobs_per_socket_cnt = dict()
                for socket in available.keys():
                    jobs_per_socket_cnt[socket] = 0
                for job in self.assigned_resources:
                    if 'cpuset.mems' in self.assigned_resources[job]:
                        for socket in \
                                self.assigned_resources[job]['cpuset.mems']:
                            jobs_per_socket_cnt[socket] += 1

                sorted_dict = sorted(jobs_per_socket_cnt.items(),
                                     key=operator.itemgetter(1))
                for x in sorted_dict:
                    socket_list.append(x[0])
        else:
            socket_list = available.keys()

        # Loop through the socket list and determine if the job
        # can fit on the socket
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Evaluate sockets in the following order: %s" %
                   (socket_list))
        for socket in socket_list:
            room_on_socket = True
            if 'ncpus' in resources:
                if int(resources['ncpus']) <= len(available[socket]['cpus']):
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Requested: %s, Available: %s" %
                               (resources['ncpus'], available[socket]['cpus']))
                    tmp_ncpus = int(resources['ncpus'])
                    assigned['cpuset.cpus'] = \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] = [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient ncpus on socket %d: R:%d,A:%s" %
                               (socket, resources['ncpus'],
                                available[socket]['cpus']))
                    room_on_socket = False
            # Check to see that nmics has been requested and not equal to 0
            if 'nmics' in resources and int(resources['nmics']) > 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'nmics' in resources and int(resources['nmics']) > 0 \
                        and int(resources['nmics']) <= len(mics):
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient nmics on socket %d: R:%s,A:%s" %
                               (socket, resources['nmics'], mics))
                    room_on_socket = False
            # Check to see that ngpus has been requested and not equal to 0
            if 'ngpus' in resources and int(resources['ngpus']) > 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'ngpus' in resources and int(resources['ngpus']) > 0 \
                        and int(resources['ngpus']) <= len(gpus):
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient gpus on socket %d: R:%s,A:%s" %
                               (socket, resources['ngpus'], gpus))
                    room_on_socket = False
            if 'vmem' in resources:
                if size_as_int(resources['vmem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient vmem on socket %d: R:%s,A:%s" %
                               (socket, resources['vmem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if 'mem' in resources:
                if size_as_int(resources['mem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient memory on socket %d: R:%s,A:%s" %
                               (socket, resources['mem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if room_on_socket:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
                self.host_assigned_resources = assigned
                return assigned
        # If we made it here then the requested resources did not fit
        # on a single socket
        # Future: take distance into account when selecting sockets?
        # Combine available resources into a single list
        # This should work for a dual socket machine.
        # Needs additional work for machines with more than 2 sockets
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Unable to find requested resources: %s" % resources +
                   " on a socket. Checking the entire node")
        available_copy = copy.deepcopy(available)
        available_on_node = dict()
        socket_keys = available.keys()
        socket_keys.sort()
        available_on_node['sockets'] = socket_keys
        for socket in socket_keys:
            for key in available_copy[socket].keys():
                if isinstance(available[socket][key], int):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]
                if isinstance(available_copy[socket][key], str):
                    try:
                        if key in available_on_node is False:
                            available_on_node[key] = \
                                size_as_int(available_copy[socket][key])
                        else:
                            available_on_node[key] += \
                                size_as_int(available_copy[socket][key])
                    except:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Unable to convert to int: %s" %
                                   (available_copy[socket][key]))

                if isinstance(available_copy[socket][key], list):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available on node: %s" % (available_on_node))
        assigned = dict()
        room_on_node = False
        if 'ncpus' in resources:
            if int(resources['ncpus']) <= len(available_on_node['cpus']):
                room_on_node = True
                tmp_ncpus = int(resources['ncpus'])
                assigned['cpuset.cpus'] = available_on_node['cpus'][:tmp_ncpus]
                assigned['cpuset.mems'] = available_on_node['sockets']
            if 'nmics' in resources and int(resources['nmics']) != 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['nmics']) <= len(mics) and \
                        int(resources['nmics']) > 0:
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
            if 'ngpus' in resources and int(resources['ngpus']) != 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['ngpus']) <= len(gpus) and \
                        int(resources['ngpus']) > 0:
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
        if room_on_node:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

        # Consolidate resources if needed to place the job

    # Determine which resources are available
    def available_node_resources(self, node):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))

        available = copy.deepcopy(node.numa_nodes)
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Keys: %s" % (available[0].keys()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        for socket in available.keys():
            if 'cpus' in available[socket]:
                # Find the physical cores
                if available[socket]['cpus'].find('-') != -1 and \
                        node.hyperthreading:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Splitting %s at ','" %
                               (available[socket]['cpus']))
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'].split(',')[0])
                else:
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'])
            if 'MemTotal' in available[socket]:
                # Find the memory on the socket in bites.
                # Remove the 'b' to simply math
                available[socket]['memory'] = size_as_int(
                    available[socket]['MemTotal'])

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Pre device add: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (node.devices.keys()))
        for device in node.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" %
                       (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Devices: %s" %
                           (node.devices[device].keys()))
                for device_name in node.devices[device].keys():
                    device_socket = \
                        node.devices[device][device_name]['numa_node']
                    if 'devices' not in available[device_socket]:
                        available[device_socket]['devices'] = list()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Device: %s, Socket: %s" %
                               (device, device_socket))
                    available[device_socket]['devices'].append(device_name)
            # pbs.logmsg(pbs.EVENT_DEBUG3,
            #            "Available on socket %s: %s" %
            #            (socket,available[socket]))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))

        # Remove all of the resources that are assigned to other jobs
        for job in self.assigned_resources.keys():
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            if (not job_to_be_ignored(job)):
                cpus = list()
                sockets = list()
                devices = list()
                memory = 0
                if 'cpuset.cpus' in self.assigned_resources[job]:
                    cpus = self.assigned_resources[job]['cpuset.cpus']
                if 'cpuset.mems' in self.assigned_resources[job]:
                    sockets = self.assigned_resources[job]['cpuset.mems']
                if 'devices.list' in self.assigned_resources[job]:
                    devices = self.assigned_resources[job]['devices.list']
                if 'memory.limit' in self.assigned_resources[job]:
                    memory = size_as_int(
                                self.assigned_resources[job]['memory.limit'])

                # Loop through the sockets and remove cpus that are
                # assigned to other cgroups
                for socket in sockets:
                    for cpu in cpus:
                        try:
                            available[socket]['cpus'].remove(cpu)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %d from %s" %
                                       (cpu, available[socket]['cpus']))

                if len(sockets) == 1:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Decrementing memory: %d by %d" %
                               (size_as_int(available[sockets[0]]['memory']),
                                memory))
                    available[sockets[0]]['memory'] -= memory

                # Loop throught the available sockets
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned device to %s: %s" % (job, devices))
                for socket in available.keys():
                    for device in devices:
                        try:
                            # loop through know devices and see if they match
                            if len(available[socket]['devices']) != 0:
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Check device: %s" % (device))
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Available device: %s" %
                                           (available[socket]['devices']))
                                major, minor = device.split()[1].split(':')
                                avail_device = self.get_device_name(
                                                   node, available, socket,
                                                   major, minor)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Returned device: %s" %
                                           (avail_device))
                                if avail_device is not None:
                                    pbs.logmsg(pbs.EVENT_DEBUG3,
                                               "socket: %d,\t" % socket +
                                               "devices: %s,\t" %
                                               available[socket]['devices'] +
                                               "device to remove: %s" %
                                               (avail_device))
                                    available[socket]['devices'].remove(
                                        avail_device)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %s from %s" %
                                       (device, available[socket]['devices']))
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Job %s res not removed from host " % job +
                           "available res: suspended job")
        pbs.logmsg(pbs.EVENT_DEBUG, "Available resources: %s" % (available))
        return available

    # Set a job limit
    def set_job_limit(self, jobid, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s: %s = %s" %
                   (caller_name(), jobid, resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'softmem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.soft_limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     jobid, 'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'], jobid,
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            elif resource == 'ncpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = self.select_cpus(path, value)
                    if cpus is None:
                        raise CgroupLimitError("Failed to configure cpuset.")
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
                    self.__copy_from_parent(os.path.join(self.paths['cpuset'],
                                            jobid, 'cpuset.mems'))
            elif resource == 'cpuset.cpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = value
                    if cpus is None:
                        msg = "Failed to configure cpus in cpuset."
                        raise CgroupLimitError(msg)
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
            elif resource == 'cpuset.mems':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.mems')
                    mems = value
                    if mems is None:
                        msg = "Failed to configure mems in cpuset."
                        raise CgroupLimitError(msg)
                    mems = ','.join(map(str, mems))
                    self.write_value(path, mems)
            elif resource == 'devices':
                if 'devices' in self.subsystems and \
                        self.cfg['cgroup']['devices']['enabled']:

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.allow')
                    devices = value
                    if devices is None:
                        msg = "Failed to configure device(s)."
                        raise CgroupLimitError(msg)
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Setting devices: %s for %s" % (devices, jobid))
                    for device in devices:
                        self.write_value(path, device)

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.list')
                    output = open(path, 'r').read()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "devices.list: %s" % output.replace("\n", ","))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    # Update resource usage for a job
    def update_job_usage(self, jobid, resc_used):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resc_used = %s" %
                   (caller_name(), str(resc_used)))
        # Sort the subsystems so that we consistently look at the subsystems
        # in the same order every time
        self.subsystems.sort()
        for subsys in self.subsystems:
            path = os.path.join(self.paths[subsys], jobid)
            if subsys == "memory":
                max_mem = self.__get_max_mem_usage(path)
                if max_mem is None:
                    pbs.logjobmsg(jobid, "%s: No max mem data" %
                                  (caller_name()))
                else:
                    resc_used['mem'] = pbs.size(convert_size(max_mem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: mem=%s" %
                                  (caller_name(), resc_used['mem']))
                mem_failcnt = self.__get_mem_failcnt(path)
                if mem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No mem fail count data" %
                                  (caller_name()))
                else:
                    # Check to see if the job exceeded its resource limits
                    if mem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memory limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "memsw":
                max_vmem = self.__get_max_memsw_usage(path)
                if max_vmem is None:
                    pbs.logjobmsg(jobid, "%s: No max vmem data" %
                                  (caller_name()))
                else:
                    resc_used['vmem'] = pbs.size(convert_size(max_vmem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: vmem=%s" %
                                  (caller_name(), resc_used['vmem']))

                vmem_failcnt = self.__get_memsw_failcnt(path)
                if vmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No vmem fail count data" %
                                  (caller_name()))
                else:
                    pbs.logjobmsg(jobid, "%s: vmem fail count: %d " %
                                  (caller_name(), vmem_failcnt))
                    if vmem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memsw limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "hugetlb":
                max_hpmem = self.__get_max_hugetlb_usage(path)
                if max_hpmem is None:
                    pbs.logjobmsg(jobid, "%s: No max hpmem data" %
                                  (caller_name()))
                    return
                hpmem_failcnt = self.__get_hugetlb_failcnt(path)
                if hpmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No hpmem fail count data" %
                                  (caller_name()))
                    return
                if hpmem_failcnt > 0:
                    err_msg = self.__get_error_msg(jobid)
                    pbs.logjobmsg(jobid, "Cgroup hugetlb limit exceeded: %s" %
                                  (err_msg))
                resc_used['hpmem'] = pbs.size(convert_size(max_hpmem, 'kb'))
                pbs.logjobmsg(jobid, "%s: Hugepage usage: %s" %
                              (caller_name(), resc_used['hpmem']))
            elif subsys == "cpuacct":
                cpu_usage = self.__get_cpu_usage(path)
                if cpu_usage is None:
                    pbs.logjobmsg(jobid, "%s: No CPU usage data" %
                                  (caller_name()))
                    return
                cpu_usage = convert_time(str(cpu_usage) + "ns")
                pbs.logjobmsg(jobid, "%s: CPU usage: %.3lf secs" %
                              (caller_name(), cpu_usage))
                resc_used['cput'] = pbs.duration(cpu_usage)

    # Creates the cgroup if it doesn't exists
    def create_job(self, jobid, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Iterate over the subsystems required
        for subsys in self.subsystems:
            # Create a directory for the job
            try:
                old_umask = os.umask(0022)
                path = self.paths[subsys]
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                path = os.path.join(self.paths[subsys], jobid)
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                if subsys == 'devices':
                    self.setup_subsys_devices(path, node)

            except OSError, exc:
                raise CgroupConfigError("Failed to create directory: %s (%s)" %
                                        (path, errno.errorcode[exc.errno]))
            except:
                raise
            finally:
                os.umask(old_umask)

    # Determine the cgroup limits and configure the cgroups
    def configure_job(self, jobid, hostresc, node):
        mem_enabled = 'memory' in self.subsystems
        vmem_enabled = 'memsw' in self.subsystems
        if mem_enabled or vmem_enabled:
            # Initialize mem variables
            mem_avail = node.get_memory_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "mem_avail %s" % mem_avail)
            mem_requested = None
            if 'mem' in hostresc.keys():
                mem_requested = convert_size(hostresc['mem'], 'kb')
            mem_default = None
            if mem_enabled:
                mem_default = self.default('memory')
            # Initialize vmem variables
            vmem_avail = node.get_vmem_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "vmem_avail %s" % vmem_avail)
            vmem_requested = None
            if 'vmem' in hostresc.keys():
                vmem_requested = convert_size(hostresc['vmem'], 'kb')
            vmem_default = None
            if vmem_enabled:
                vmem_default = self.default('memsw')
            # Initialize softmem variables
            if 'soft_limit' in self.cfg['cgroup']['memory'].keys():
                softmem_enabled = self.cfg['cgroup']['memory']['soft_limit']
            else:
                softmem_enabled = False
            # Sanity check
            if size_as_int(mem_avail) > size_as_int(vmem_avail):
                if size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                    raise CgroupLimitError(
                        'mem available (%s) exceeds vmem available (%s)' %
                        (mem_avail, vmem_avail))
            # Determine the mem limit
            if mem_requested is not None:
                # mem requested may not exceed available
                if size_as_int(mem_requested) > size_as_int(mem_avail):
                    raise JobValueError(
                        'mem requested (%s) exceeds mem available (%s)' %
                        (mem_requested, mem_avail))
                mem_limit = mem_requested
            else:
                # mem was not requested
                if mem_default is None:
                    mem_limit = mem_avail
                else:
                    mem_limit = mem_default
            # Determine the vmem limit
            if vmem_requested is not None:
                # vmem requested may not exceed available
                if size_as_int(vmem_requested) > size_as_int(vmem_avail):
                    raise JobValueError(
                        'vmem requested (%s) exceeds vmem available (%s)' %
                        (vmem_requested, vmem_avail))
                vmem_limit = vmem_requested
            else:
                # vmem was not requested
                if vmem_default is None:
                    vmem_limit = vmem_avail
                else:
                    vmem_limit = vmem_default
            # Ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Adjust for soft limits if enabled
            if mem_enabled and softmem_enabled:
                softmem_limit = mem_limit
                # The hard memory limit is assigned the lesser of the vmem
                # limit and available memory
                if size_as_int(vmem_limit) < size_as_int(mem_avail):
                    mem_limit = vmem_limit
                else:
                    mem_limit = mem_avail
            # Again, ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Sanity checks when both memory and memsw are enabled
            if mem_enabled and vmem_enabled:
                if vmem_requested is not None:
                    if size_as_int(vmem_limit) > size_as_int(vmem_requested):
                        # The user requested an invalid limit
                        raise JobValueError(
                            'vmem limit (%s) exceeds vmem requested (%s)' %
                            (vmem_limit, vmem_requested))
                if size_as_int(vmem_limit) > size_as_int(mem_limit):
                    # This job may utilize swap
                    if size_as_int(vmem_avail) <= size_as_int(mem_avail) and \
                      size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                        # No swap available
                        raise CgroupLimitError(
                            ('Job might utilize swap ' +
                             'and no swap space available'))
            # Assign mem and vmem
            if mem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: mem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), mem_limit))
                hostresc['mem'] = pbs.size(mem_limit)
                if softmem_enabled:
                    hostresc['softmem'] = pbs.size(softmem_limit)
            if vmem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: vmem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), vmem_limit))
                hostresc['vmem'] = pbs.size(vmem_limit)
        # Initialize hpmem variables
        hpmem_enabled = 'hugetlb' in self.subsystems
        if hpmem_enabled:
            hpmem_avail = node.get_hpmem_on_node(self.cfg)
            hpmem_limit = None
            hpmem_default = self.default('hugetlb')
            if hpmem_default is None:
                hpmem_default = hpmem_avail
            if 'hpmem' in hostresc.keys():
                hpmem_limit = convert_size(hostresc['hpmem'], 'kb')
            else:
                hpmem_limit = hpmem_default
            # Assign hpmem
            if size_as_int(hpmem_limit) > size_as_int(hpmem_avail):
                raise JobValueError('hpmem limit (%s) exceeds available (%s)' %
                                    (hpmem_limit, hpmem_avail))
            hostresc['hpmem'] = pbs.size(hpmem_limit)
        # Initialize cpuset variables
        cpuset_enabled = 'cpuset' in self.subsystems
        if cpuset_enabled:
            cpu_limit = 1
            if 'ncpus' in hostresc.keys():
                cpu_limit = hostresc['ncpus']
            if cpu_limit < 1:
                cpu_limit = 1
            hostresc['ncpus'] = pbs.pbs_int(cpu_limit)

        # Find the available resources and assign the right ones to the job

        attempt = 0
        assign_resources = False

        # Two attempts since self.cleanup_orphans may actually fix the
        # problem we see in a first attempt
        while attempt < 2:
            attempt += 1
            available_resources = self.available_node_resources(node)
            if 'vnodes' in hostresc:
                assign_resources = self.assign_job_resources_by_vnode(
                    hostresc, available_resources, node)
            else:
                assign_resources = self.assign_job_resources(
                    hostresc, available_resources, node)

            if (assign_resources is False) and (attempt < 2):
                # No resources were assigned to the job.
                # Most likely cause was that a cgroup has
                # not been cleaned up yet
                pbs.logmsg(pbs.EVENT_DEBUG, "Failed to assign resources. " +
                           "Checking the following cgroups on the node " +
                           "with the server: %s" % self.existing_cgroups)

                # Collect the jobs on the node (try reading mom_priv/jobs,
                # if not successful ask server)
                jobs_list = node.gather_jobs_on_node_local()

                if jobs_list is not None:
                    # Don't clean up the cgroup we just made for the
                    # job just yet
                    if jobid not in jobs_list:
                        jobs_list.append(jobid)

                    self.cleanup_orphans(jobs_list)

                    # Recheck what is now assigned after things were cleaned up
                    self.gather_assigned_resources()

        if assign_resources is False:
            # Cleanup cgroups for jobs not present on this node
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Assign resources failed. Attempting to cleanup all " +
                       "leftover cgroups (including event job %s)" % jobid)

            # Remove the current job from the jobs list so it will be
            # cleaned up as well.
            jobs_list.remove(jobid)

            self.cleanup_orphans(jobs_list)

            # Rerun the job and log the message
            line = 'Unable to assign resources to job. '
            line += 'Will requeue the job and try again.'
            line += 'job run_count: %d' % pbs.event().job.run_count
            pbs.event().job.rerun()
            pbs.event().reject(line)

        # Print out the assigned resources
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Assigned resources: %s" % (assign_resources))

        if cpuset_enabled:
            hostresc.pop('ncpus', None)
            for key in ['cpuset.cpus', 'cpuset.mems']:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Initialize devices variables
        devices_enabled = 'devices' in self.subsystems
        if devices_enabled:
            key = 'devices'
            if key in assign_resources:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Apply the resource limits to the cgroups
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Setting cgroup limits for: %s" %
                   (caller_name(), hostresc))

        # The vmem limit must be set after the mem limit, so sort the keys
        for resc in sorted(hostresc.keys()):
            self.set_job_limit(jobid, resc, hostresc[resc])

    # Kill any processes contained within a tasks file
    def __kill_tasks(self, tasks_file):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        with open(tasks_file, 'r') as fd:
            for line in fd:
                os.kill(int(line.strip()), signal.SIGKILL)
        fd.close()
        count = 0
        with open(tasks_file, 'r') as fd:
            for line in fd:
                count += 1
                pid = line.strip()
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Pid: %s did not clean up" %
                           (caller_name(), pid))
                if os.path.isfile('/proc/%s/status' % pid):
                    tmp = open('/proc/%s/status' % pid).readlines()
                    tmp_line = ''
                    for entry in tmp:
                        if entry.find('Name:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('State:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('Uid:') != -1:
                            tmp_line += entry.strip()
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: %s" % (caller_name(), tmp_line))

        return count

    # Perform the actual removal of the cgroup directory
    # by default, only one attempt at killing tasks in cgroup, without sleeping
    # since this method could be called many times
    # (for N directories times M jobs)
    def __remove_cgroup(self, path, tasks_kill_attempts=1):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            if os.path.exists(path):
                tasks_file = os.path.join(path, 'tasks')
                task_cnt = self.__kill_tasks(tasks_file)
                kill_attempts = 0
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Task Kill Attempts: %d" % tasks_kill_attempts)
                while (task_cnt > 0) and (kill_attempts < tasks_kill_attempts):
                    kill_attempts += 1
                    count = 0

                    with open(tasks_file, 'r') as fd:
                        for line in fd:
                            count += 1
                        task_cnt = count

                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "Attempt: %d - cgroup still has %d tasks: %s" %
                               (kill_attempts, task_cnt, path))
                    if kill_attempts < tasks_kill_attempts:
                        time.sleep(1)
                        # Added since there are race conditions where tasks are
                        # being added at the same time we get the initial
                        # task count
                        tasks_file = os.path.join(path, 'tasks')
                        task_cnt = self.__kill_tasks(tasks_file)

                if task_cnt == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Removing directory %s" %
                               (caller_name(), path))
                    os.rmdir(path)
                    if os.path.exists(path):
                        time.sleep(.25)
                        if os.path.exists(path):
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "%s: 2nd Try Removing directory %s" %
                                       (caller_name(), path))
                            os.rmdir(path)
                            time.sleep(.25)
                        if os.path.exists(path):
                            return False
                else:
                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "cgroup still has %d tasks: %s" %
                               (task_cnt, path))

                    # Check to see if the offline file is already present
                    # on the mom
                    offline_file_exists = False
                    if os.path.isfile(self.offline_file):
                        msg = "Cgroup(s) not cleaning up but the node already "
                        msg += "has the offline file."

                        pbs.logmsg(pbs.EVENT_DEBUG, msg)
                    else:
                        # Check to see that the node is not already offline.
                        try:
                            tmp_state = pbs.server().vnode(self.hostname).state
                        except:
                            msg = "Unable to contact server for node state"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            tmp_state = None
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Current Node State: %d" % tmp_state)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Offline Node State: %d" % pbs.ND_OFFLINE)

                        if tmp_state == pbs.ND_OFFLINE:
                            msg = "Cgroup(s) not cleaning up but the node is "
                            msg += "already offline."
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                        elif tmp_state is not None:
                            # Offline the node(s)
                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                            msg = "Offlining node since cgroup(s) are not "
                            msg += "cleaning up"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            vnode = pbs.event().vnode_list[self.hostname]

                            vnode.state = pbs.ND_OFFLINE

                            # Write a file locally to reduce server traffic
                            # when it comes time to online the node
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Offline file: %s" % self.offline_file)
                            open(self.offline_file, 'a').close()
                            vnode.comment = self.offline_msg

                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                        else:
                            pass

                    return False

        except OSError, exc:
            pbs.logmsg(pbs.EVENT_SYSTEM,
                       "Failed to remove cgroup path: %s (%s)" %
                       (path, errno.errorcode[exc.errno]))
        except:
            pbs.logmsg(pbs.EVENT_SYSTEM, "Failed to remove cgroup path: %s" %
                       (path))

        return True

    # Removes cgroup directories that are not associated with a local job
    def cleanup_orphans(self, local_jobs):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        orphan_cnt = 0
        for key in self.paths.keys():
            for dir in glob.glob(os.path.join(self.paths[key], '[0-9]*')):
                jobid = os.path.basename(dir)
                if jobid not in local_jobs:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Removing orphaned cgroup: %s" %
                               (caller_name(), dir))
                    # default number of attempts at killing tasks
                    status = self.__remove_cgroup(dir)
                    if not status:
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "%s: Removing orphaned cgroup %s failed " %
                                   (caller_name(), dir))
                        orphan_cnt += 1
        return orphan_cnt

    # Removes the cgroup directories for a job
    def delete(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        tasks_kill_attempted = False

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            if not tasks_kill_attempted:
                # Make multiple attempts at killing tasks in cgroups on
                # first subsystem encountered since it is "normal" for
                # a cgroup.delete to encounter processes hard to kill
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                                           jobid),
                                              tasks_kill_attempts=(
                                              CGROUP_KILL_ATTEMPTS))
                tasks_kill_attempted = True
            else:
                # We tried getting rid of job processes earlier,
                # so don't try N times with sleeps
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                              jobid), tasks_kill_attempts=1)

            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Status: %s" %
                       (caller_name(), status))
            if status is False:
                # Rerun the job and log the message
                line = 'Unable to cleanup a cgroup. '
                line += 'Will offline the node and requeue the job '
                line += 'and try again. job run_count: %d' % \
                        pbs.event().job.run_count
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Failed to remove cgroup: %s" %
                           (caller_name(), line))
                pbs.event().accept()

    # Write a value to a limit file
    def write_value(self, file, value, mode='w'):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: writing %s to %s" %
                   (caller_name(), value, file))
        try:
            with open(file, mode) as fd:
                fd.write(str(value) + '\n')
        except IOError, exc:
            if exc.errno == errno.ENOENT:
                pbs.logmsg(pbs.EVENT_SYSTEM,
                           "Trying to set limit to unknown file %s" % file)
            elif exc.errno in [errno.EACCES, errno.EPERM]:
                pbs.logmsg(pbs.EVENT_SYSTEM, "Permission denied on file %s" %
                           file)
            elif exc.errno == errno.EBUSY:
                raise CgroupBusyError("Limit rejected")
            elif exc.errno == errno.EINVAL:
                raise CgroupLimitError("Invalid limit value: %s" % (value))
            else:
                raise
        except:
            raise

    # Return memory failcount
    def __get_mem_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.failcnt"), 'r').read().strip())
        except:
            return None

    # Return vmem failcount
    def __get_memsw_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.failcnt"), 'r').read().strip())
        except:
            return None

    # Return hpmem failcount
    def __get_hugetlb_failcnt(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.failcnt"))[0], 'r').read().strip())
        except:
            return None

    # Return the max usage of memory in bytes
    def __get_max_mem_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of memsw in bytes
    def __get_max_memsw_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of hugetlb in bytes
    def __get_max_hugetlb_usage(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.max_usage_in_bytes"))[0],
                            'r').read().strip())
        except:
            return None

    # Return the cpuacct.usage in cpu seconds
    def __get_cpu_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "cpuacct.usage"), 'r').read().strip())
        except:
            return None

    # Assign CPUs to the cpuset
    def select_cpus(self, path, ncpus):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path is %s" % (caller_name(), path))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: ncpus is %s" %
                   (caller_name(), ncpus))
        if ncpus < 1:
            ncpus = 1
        try:
            # Must select from those currently available
            cpufile = os.path.basename(path)
            base = os.path.dirname(path)
            parent = os.path.dirname(base)
            avail = cpus2list(open(os.path.join(parent, cpufile),
                              'r').read().strip())
            if len(avail) < 1:
                raise CgroupProcessingError("No CPUs avaialble in cgroup.")
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Available CPUs: %s" %
                       (caller_name(), avail))
            assigned = []
            for file in glob.glob(os.path.join(parent, '[0-9]*', cpufile)):
                cpus = cpus2list(open(file, 'r').read().strip())
                for id in cpus:
                    if id in avail:
                        avail.remove(id)
            if len(avail) < ncpus:
                raise CgroupProcessingError(
                    "Insufficient CPUs avaialble in cgroup.")
            if len(avail) == ncpus:
                return avail
            # FUTURE: Try to minimize NUMA nodes based on memory requirement
            return avail[0:ncpus]
        except:
            raise

    # Return the error message in system message file
    def __get_error_msg(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        proc = subprocess.Popen(['dmesg'],
                                shell=False, stdout=subprocess.PIPE)
        stdout = proc.communicate()[0].splitlines()
        stdout.reverse()
        # Check to see if the job id is found in dmesg otherwise
        # you could get the information for another job that was killed
        # the line will look like Memory cgroup stats for /pbspro/279.centos7
        kill_line = ""

        for line in stdout:
            start = line.find('Killed process ')
            if start >= 0:
                kill_line = line[start:]
            job_start = line.find(jobid)
            if job_start >= 0:
                # pbs.logmsg(pbs.EVENT_DEBUG3,
                #           "%s: Jobid: %s, Line: %s" %
                #           (caller_name(), jobid, line))
                return kill_line
        return ""

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_job_env_file(self, jobid, env_list):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        # if not os.path.exists(self.host_job_env_dir):
        #    os.makedirs(self.host_job_env_dir, 0755)

        # Write out assigned_resources
        try:
            lines = string.join(env_list, '\n')
            filename = self.host_job_env_filename % jobid
            outfile = open(filename, "w")
            outfile.write(lines)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" % (filename))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (lines))
            return True
        except:
            return False

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        if not os.path.exists(self.hook_storage_dir):
            os.makedirs(self.hook_storage_dir, 0755)

        # Write out assigned_resources
        try:
            json_str = json.dumps(self.host_assigned_resources)
            outfile = open(self.hook_storage_dir+os.sep+jobid, "w")
            outfile.write(json_str)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" %
                       (self.hook_storage_dir+os.sep+jobid))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (json_str))
            return True
        except:
            return False

    def read_in_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        pbs.logmsg(pbs.EVENT_DEBUG3, "Host assigned resources: %s" %
                   (self.host_assigned_resources))
        if os.path.isfile(self.hook_storage_dir+os.sep+jobid):
            # Write out assigned_resources
            try:
                infile = open(self.hook_storage_dir+os.sep+jobid, 'r')
                json_data = json.load(infile, object_hook=decode_dict)
                self.host_assigned_resources = json_data
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Host assigned resources: %s" %
                           (self.host_assigned_resources))
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
            finally:
                infile.close()

        if self.host_assigned_resources is not None:
            return True
        else:
            return False

#
#
# FUNCTION main
#


def main():
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Function called" % (caller_name()))
    hostname = pbs.get_local_nodename()
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Host is %s" % (caller_name(), hostname))

    # Log the hook event type
    e = pbs.event()
    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook name is %s" %
               (caller_name(), e.hook_name))

    try:
        # Instantiate the hook utility class
        hooks = HookUtils()
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Event type is %s" %
                   (caller_name(), hooks.event_name(e.type)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(hooks)))
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: Failed to instantiate hook utility class" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        e.accept()

    if not hooks.hashandler(e.type):
        # Bail out if there is no handler for this event
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s event not handled by this hook" %
                   (caller_name(), hooks.event_name(e.type)))
        e.accept()

    try:
        # If an exception occurs, jobutil must be set
        jobutil = None
        # Instantiate the job utility class first so jobutil can be accessed
        # by the exception handlers.
        if hasattr(e, 'job'):
            jobutil = JobUtils(e.job)
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Job information class instantiated" %
                       (caller_name()))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" %
                       (caller_name(), repr(jobutil)))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Event does not include a job" %
                       (caller_name()))
        # Instantiate the cgroup utility class
        vnode = None
        if hasattr(e, 'vnode_list'):
            if hostname in e.vnode_list.keys():
                vnode = e.vnode_list[hostname]
        cgroup = CgroupUtils(hostname, vnode)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Cgroup utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(cgroup)))
        # Bail out if there is nothing to do
        if len(cgroup.subsystems) < 1:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: All cgroups disabled" %
                       (caller_name()))
            e.accept()

        # Call the appropriate handler
        if hooks.invoke_handler(e, cgroup, jobutil) is True:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned success" %
                       (caller_name()))
            e.accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned failure" %
                       (caller_name()))
            e.reject()

    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass

    except AdminError, exc:
        # Something on the system is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Admin error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except UserError, exc:
        # User must correct problem and resubmit job
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("User error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.delete()
                msg += " (deleted)"
            except:
                msg += " (delete failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except JobValueError, exc:
        # Something in PBS is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Job value error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except Exception, exc:
        # Catch all other exceptions and report them.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Unexpected error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

# line below required for unittesting
if __name__ == "__builtin__":
    start_time = time.time()
    try:
        main()
    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
    finally:
        pbs.logmsg(pbs.EVENT_DEBUG, "Elapsed time: %0.4lf" %
                   (time.time() - start_time))
---
2016-07-18 16:56:44,036 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups.json', 'content-encoding': 'default'}
2016-07-18 16:56:44,036 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups.json
2016-07-18 16:56:44,074 INFO     job: executable set to /bin/sleep with arguments: 100
2016-07-18 16:56:44,075 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0755 /tmp/PtlPbsJobScriptmmR_6K
2016-07-18 16:56:44,087 INFOCLI  centos7: sudo -H -u pbsuser /usr/local/bin/qsub -N test_t4 -l select=1:ncpus=1:mem=300mb /tmp/PtlPbsJobScriptmmR_6K
2016-07-18 16:56:44,103 INFO     submit to centos7 as pbsuser: job 379.centos7 OrderedDict([('Job_Name', 'test_t4'), ('Resource_List.select', '1:ncpus=1:mem=300mb')])
2016-07-18 16:56:44,103 INFOCLI  job script /tmp/PtlPbsJobScriptmmR_6K
---
#!/usr/bin/env python

import os
import subprocess
from subprocess import Popen, PIPE
import time

eat_mem_c = """
#include <stdio.h>
#include <stdlib.h>
#include <string.h>


#define ONE_GB (0x40000000L)
#define ONE_MB (0x100000L)

void
eatmem(total_mb, block_mb)
{
	int i;
	char *buf;
	size_t block;

	for (i=1, buf=NULL, block=(block_mb*ONE_MB); (i*block) < (total_mb*ONE_MB); i++) {
		buf = realloc((void *)buf, (i*block));
		memset(buf+((i-1)*block), 0, block);
	}
	return;
}

int
main(int argc, char *argv[])
{
	int total_mb = 8;
	int block_mb = 1;

	if (argc > 1)
		total_mb = atoi(argv[1]);

	if (argc > 2)
		block_mb = atoi(argv[2]);

	eatmem(total_mb, block_mb);
	printf("Initialized %dMB in blocks of %dMB\\n", total_mb, block_mb);
}
"""

fname = "/tmp/pbs_pp325_eat_mem.c"
fname_exe = "/tmp/pbs_pp325_eat_mem"
fout = open(fname, "w")
fout.write(eat_mem_c)
fout.close()

print os.getppid()

p = Popen(["gcc", "-o", fname_exe, fname], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

p = Popen([fname_exe, "400", "10"], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

os.remove(fname)
os.remove(fname_exe)
time.sleep(1)

---
2016-07-18 16:56:44,104 INFOCLI  centos7: /usr/local/bin/qstat -f 379.centos7
2016-07-18 16:56:44,200 INFO     expect on server centos7: job_state = R job 379.centos7  got: job_state = Q
2016-07-18 16:56:44,703 INFOCLI  centos7: /usr/local/bin/qmgr -c set server scheduling=True
2016-07-18 16:56:44,719 INFOCLI  centos7: /usr/local/bin/qstat -f 379.centos7
2016-07-18 16:56:44,824 INFO     expect on server centos7: job_state = R job 379.centos7 attempt: 2 ...  OK
2016-07-18 16:56:44,825 INFO     mom centos7 log match: searching for ".*vntype: no_cgroups is in \['no_cgroups'\].*" - using regular expression ... OK
2016-07-18 16:56:44,825 INFO     ==============================
2016-07-18 16:56:44,825 INFO      Entered CgroupsTests tearDown
2016-07-18 16:56:44,825 INFO     ==============================
2016-07-18 16:56:44,825 INFO     ===============================
2016-07-18 16:56:44,825 INFO     Completed CgroupsTests tearDown
2016-07-18 16:56:44,826 INFO     ===============================
2016-07-18 16:56:44,826 INFO     ok

2016-07-18 16:56:44,826 INFO     test name: test_t4b (tests.cgroups_tests.CgroupsTests)...
2016-07-18 16:56:44,826 INFO     test start time: Mon Jul 18 16:56:44 2016
2016-07-18 16:56:44,826 INFO     test docstring: 
 Test to verify that cgroups are not enforced on nodes
            that have the exclude_hosts set
        
2016-07-18 16:56:44,826 INFO     ===========================
2016-07-18 16:56:44,826 INFO      Entered CgroupsTests setUp
2016-07-18 16:56:44,826 INFO     ===========================
2016-07-18 16:56:44,827 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:44,881 INFO     status on centos7: server
2016-07-18 16:56:44,882 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:44,940 INFO     manager on centos7: set server {'managers': (3, 'root@*')}
2016-07-18 16:56:44,940 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers-=root@*
2016-07-18 16:56:44,969 INFO     manager on centos7: set server {'managers': (2, 'root@*')}
2016-07-18 16:56:44,970 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers+=root@*
2016-07-18 16:56:44,997 INFO     server centos7: reverting configuration to defaults
2016-07-18 16:56:44,997 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:45,048 INFO     select on centos7: __ALL__
2016-07-18 16:56:45,048 INFOCLI  centos7: /usr/local/bin/qselect
2016-07-18 16:56:45,070 INFO     delete job on centos7: 379.centos7
2016-07-18 16:56:45,070 INFOCLI  centos7: /usr/local/bin/qdel -W force 379.centos7
2016-07-18 16:56:45,152 INFOCLI  centos7: /usr/local/bin/qstat -f 379.centos7
2016-07-18 16:56:45,283 INFOCLI  centos7: /usr/local/bin/qstat -f @centos7.usa.org
2016-07-18 16:56:45,335 INFO     expect on server centos7: job_state set 0 job ...  OK
2016-07-18 16:56:45,336 INFOCLI  centos7: /usr/local/bin/pbs_rstat -f
2016-07-18 16:56:45,349 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:56:45,383 INFO     manager on centos7: delete hook t1_l1
2016-07-18 16:56:45,383 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c delete hook t1_l1
2016-07-18 16:56:45,425 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:56:45,451 INFO     expect on server centos7:  unset  hook t1_l1 ...  OK
2016-07-18 16:56:45,452 INFOCLI  centos7: /usr/local/bin/qstat -Qf @centos7.usa.org
2016-07-18 16:56:45,464 INFO     manager on centos7: delete queue workq
2016-07-18 16:56:45,465 INFOCLI  centos7: /usr/local/bin/qmgr -c delete queue workq
2016-07-18 16:56:45,483 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:56:45,494 INFO     expect on server centos7:  unset  queue workq ...  OK
2016-07-18 16:56:45,495 INFO     manager on centos7: create queue workq {'started': 'True', 'queue_type': 'Execution', 'enabled': 'True'}
2016-07-18 16:56:45,495 INFOCLI  centos7: /usr/local/bin/qmgr -c create queue workq started=True,queue_type=Execution,enabled=True
2016-07-18 16:56:45,510 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:56:45,521 INFO     expect on server centos7: started set True || queue_type set Execution || enabled set True queue workq ...  OK
2016-07-18 16:56:45,521 INFO     manager on centos7: set server {'log_events': '511', 'default_queue': 'workq'}
2016-07-18 16:56:45,521 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=511,default_queue=workq
2016-07-18 16:56:45,545 INFO     status on centos7: resource
2016-07-18 16:56:45,545 INFO     manager on centos7: list resource
2016-07-18 16:56:45,546 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource
2016-07-18 16:56:45,609 INFO     manager on centos7: delete resource nmics,ngpus
2016-07-18 16:56:45,609 INFOCLI  centos7: /usr/local/bin/qmgr -c delete resource nmics,ngpus
2016-07-18 16:56:45,669 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource nmics
2016-07-18 16:56:45,686 INFO     expect on server centos7:  unset  resource nmics ...  OK
2016-07-18 16:56:45,686 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource ngpus
2016-07-18 16:56:45,699 INFO     expect on server centos7:  unset  resource ngpus ...  OK
2016-07-18 16:56:45,699 INFOCLI  status on centos7: server license_count
2016-07-18 16:56:45,699 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:45,755 INFO     server: centos7.usa.org licensed
2016-07-18 16:56:45,794 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/server_priv/comm.lock
2016-07-18 16:56:45,840 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:56:45,853 INFO     scheduler centos7: reverting configuration to defaults
2016-07-18 16:56:45,853 INFO     manager on centos7: list sched
2016-07-18 16:56:45,853 INFOCLI  centos7: /usr/local/bin/qmgr -c list sched
2016-07-18 16:56:45,874 INFO     manager on centos7: unset sched ['sched_cycle_length']
2016-07-18 16:56:45,875 INFOCLI  centos7: /usr/local/bin/qmgr -c unset sched sched_cycle_length
2016-07-18 16:56:45,892 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/dedicated_time
2016-07-18 16:56:45,902 INFOCLI2 centos7: cmp /usr/local/etc/pbs_resource_group /var/spool/pbs/sched_priv/resource_group
2016-07-18 16:56:45,906 INFO     scheduler centos7: reverting holidays file to default
2016-07-18 16:56:45,906 INFOCLI2 centos7: cmp /usr/local/etc/pbs_holidays /var/spool/pbs/sched_priv/holidays
2016-07-18 16:56:45,909 INFOCLI2 centos7: cmp /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:45,912 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:45,938 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:45,947 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:56:45,947 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:56:46,006 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:56:46,021 INFOCLI2 centos7: sudo -H cmp /usr/local/sbin/pbs_mom /usr/local/sbin/pbs_mom.cpuset
2016-07-18 16:56:46,053 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:56:46,066 INFO     mom centos7: reverting configuration to defaults
2016-07-18 16:56:46,066 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/epilogue
2016-07-18 16:56:46,079 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/prologue
2016-07-18 16:56:46,092 INFOCLI2 centos7: sudo -H ls /var/spool/pbs/mom_priv/config.d
2016-07-18 16:56:46,109 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsa7dKSV /var/spool/pbs/mom_priv/config
2016-07-18 16:56:46,125 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/mom_priv/config
2016-07-18 16:56:46,139 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/mom_priv/config
2016-07-18 16:56:46,151 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/mom_priv/config
2016-07-18 16:56:46,172 INFO     mom centos7: sent signal -HUP
2016-07-18 16:56:46,173 INFOCLI2 centos7: sudo -H kill -HUP 42976
2016-07-18 16:56:46,216 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:56:46,225 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 16:56:46,235 INFO     status on centos7: node centos7
2016-07-18 16:56:46,236 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:46,248 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:46,264 INFO     expect on server centos7: state = free node centos7 ...  OK
2016-07-18 16:56:46,264 INFO     ============================
2016-07-18 16:56:46,264 INFO     Completed CgroupsTests setUp
2016-07-18 16:56:46,265 INFO     ============================
2016-07-18 16:56:46,265 INFO     server centos7: server operating mode set to cli
2016-07-18 16:56:46,265 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:46,318 INFO     expect on server centos7: pbs_version ~ .*14.* server centos7.usa.org ...  OK
2016-07-18 16:56:46,318 INFO     manager on centos7: set server {'log_events': '2047'}
2016-07-18 16:56:46,318 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=2047
2016-07-18 16:56:46,346 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:46,402 INFO     expect on server centos7: log_events set 2047 server centos7.usa.org ...  OK
2016-07-18 16:56:46,403 INFO     scheduler centos7: config {'resources': 'ncpus,mem,host,vnode,vmem'}
2016-07-18 16:56:46,403 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /var/spool/pbs/sched_priv/sched_config /var/spool/pbs/sched_priv/sched_config.bak
2016-07-18 16:56:46,415 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsRhXSGx /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:46,425 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:46,436 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:46,452 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:46,465 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:56:46,465 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:56:46,478 INFO     scheduler centos7 log match: searching for "Error reading line" - No match
2016-07-18 16:56:47,481 INFO     manager on centos7 as root: create resource nmics {'flag': 'nh', 'type': 'long'}
2016-07-18 16:56:47,481 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource nmics flag=nh,type=long
2016-07-18 16:56:47,507 INFO     manager on centos7 as root: create resource ngpus {'flag': 'nh', 'type': 'long'}
2016-07-18 16:56:47,507 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource ngpus flag=nh,type=long
2016-07-18 16:56:47,538 INFO     Test Summary: test_t1_l1 tests "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" cgroup hook
2016-07-18 16:56:47,538 INFO     status on centos7: hook
2016-07-18 16:56:47,538 INFO     manager on centos7: list hook
2016-07-18 16:56:47,539 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:56:47,568 INFO     status on centos7: hook
2016-07-18 16:56:47,568 INFO     manager on centos7: list hook
2016-07-18 16:56:47,568 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:56:47,609 INFO     manager on centos7: create hook t1_l1
2016-07-18 16:56:47,609 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c create hook t1_l1
2016-07-18 16:56:47,655 INFO     manager on centos7: set hook t1_l1 {'freq': 2, 'enabled': 'True', 'event': '"execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"'}
2016-07-18 16:56:47,655 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set hook t1_l1 freq=2,enabled=True,event="execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"
2016-07-18 16:56:47,689 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:56:47,709 INFO     expect on server centos7: freq set 2 || enabled set True || event set "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" hook t1_l1 ...  OK
2016-07-18 16:56:47,710 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-python', 'input-file': '/tmp/PtlPbsjh9pbc', 'content-encoding': 'default'}
2016-07-18 16:56:47,710 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-python default /tmp/PtlPbsjh9pbc
2016-07-18 16:56:47,765 INFO     server centos7: imported hook body
---
#  Copyright (C) 2003-2016 Altair Engineering, Inc. All rights reserved.
#
#  ALTAIR ENGINEERING INC. Proprietary and Confidential. Contains Trade Secret
#  Information. Not for use or disclosure outside ALTAIR and its licensed
#  clients. Information contained herein shall not be decompiled, disassembled,
#  duplicated or disclosed in whole or in part for any purpose. Usage of the
#  software is only as explicitly permitted in the end user software license
#  agreement.
#
#  Copyright notice does not imply publication.

# Last Updated
# Date: 07-06-2016
#
# When soft_limit is true for memory, memsw represents the hard limit.
#
# The resources value in sched_config must contain entries for mem and
# vmem if those subsystems are enabled in the hook configuration file. The
# amount of resource requested will not be avaiable to the hook if they
# are not present.

# This hook handles all of the operations necessary for PBS to support
# cgroups on linux hosts that support them (kernel 2.6.28 and higher)
#
# This hook services the following events:
# - exechost_periodic
# - exechost_startup
# - execjob_attach
# - execjob_begin
# - execjob_end
# - execjob_epilogue
# - execjob_launch

# Imports from __future__ must happen first
from __future__ import with_statement

# Additional imports
import sys
import os
import errno
import signal
import subprocess
import re
import glob
import time
import string
import platform
import traceback
import copy
from stat import S_ISBLK, S_ISCHR
import operator
import pwd
import pbs
import fnmatch
import socket

try:
    import json
except:
    import simplejson as json

CGROUP_KILL_ATTEMPTS = 12

# Flag to turn off features not in 12.2.40X branch
CRAY_12_BRANCH = False


# ============================================================================
# Derived error classes
# ============================================================================

# Base class for errors fixable only by administrative action.


class AdminError(Exception):
    pass

# Base class for errors in processing, unknown cause.


class ProcessingError(Exception):
    pass

# Base class for errors fixable by the user.


class UserError(Exception):
    pass

# Errors in PBS job resource values.


class JobValueError(UserError):
    pass

# Errors when the cgroup is busy.


class CgroupBusyError(ProcessingError):
    pass

# Errors in configuring cgroup.


class CgroupConfigError(AdminError):
    pass

# Errors in configuring cgroup.


class CgroupLimitError(AdminError):
    pass

# Errors processing cgroup.


class CgroupProcessingError(ProcessingError):
    pass

# ============================================================================
# Utility functions
# ============================================================================

#
# FUNCTION caller_name
#
# Return the name of the calling function or method.
#


def caller_name():
    return str(sys._getframe(1).f_code.co_name)

#
# FUNCTION convert_size
#
# Convert a string containing a size specification (e.g. "1m") to a
# string using different units (e.g. "1024k").
#
# This function only interprets a decimal number at the start of the string,
# stopping at any unrecognized character and ignoring the rest of the string.
#
# When down-converting (e.g. MB to KB), all calculations involve integers and
# the result returned is exact. When up-converting (e.g. KB to MB) floating
# point numbers are involved. The result is rounded up. For example:
#
# 1023MB -> GB yields 1g
# 1024MB -> GB yields 1g
# 1025MB -> GB yields 2g  <-- This value was rounded up
#
# Pattern matching or conversion may result in exceptions.
#


def convert_size(value, units='b'):
    logs = {'b': 0, 'k': 10, 'm': 20, 'g': 30,
            't': 40, 'p': 50, 'e': 60, 'z': 70, 'y': 80}
    try:
        new = units[0].lower()
        if new not in logs.keys():
            new = 'b'
        val, old = re.match('([-+]?\d+)([bkmgtpezy]?)',
                            str(value).lower()).groups()
        val = int(val)
        if val < 0:
            raise ValueError('Value may not be negative')
        if old not in logs.keys():
            old = 'b'
        factor = logs[old] - logs[new]
        val *= 2 ** factor
        slop = val - int(val)
        val = int(val)
        if slop > 0:
            val += 1
        # pbs.size() does not like units following zero
        if val <= 0:
            return '0'
        else:
            return str(val) + new
    except:
        return None

#
# FUNCTION size_as_int
#
# Convert a size string to an integer representation of size in bytes
#


def size_as_int(value):
    return int(convert_size(value).rstrip(string.ascii_lowercase))

#
# FUNCTION convert_time
#
# Converts a integer value for time into the value of the return unit
#
# A valid decimal number, with optional sign, may be followed by a character
# representing a scaling factor.  Scaling factors may be either upper or
# lower case. Examples include:
# 250ms
# 40s
# +15min
#
# Valid scaling factors are:
# ns  = 10**-9
# us  = 10**-6
# ms  = 10**-3
# s   =      1
# min =     60
# hr  =   3600
#
# Pattern matching or conversion may result in exceptions.
#


def convert_time(value, return_unit='s'):
    multipliers = {'':  1, 'ns': 10 ** -9, 'us': 10 ** -6,
                   'ms': 10 ** -3, 's': 1, 'min': 60, 'hr': 3600}
    n, factor = re.match('([-+]?\d+)(\w*[a-zA-Z])?',
                         str(value).lower()).groups(0)

    # Check to see if there was not unit of time specified
    if factor == 0:
        factor = ''

    # Check to see if the unit is valid
    if str.lower(factor) not in multipliers.keys():
        raise ValueError('Time unit not recognized.')

    # Convert the value to seconds
    value_in_sec = float(n) * float(multipliers[str.lower(factor)])
    if return_unit != 's':
        return value_in_sec / multipliers[str.lower(return_unit)]
    else:
        return float('%lf' % value_in_sec)

# simplejson hook to convert lists from unicode to utf-8


def decode_list(data):
    rv = []
    for item in data:
        if isinstance(item, unicode):
            item = item.encode('utf-8')
        elif isinstance(item, list):
            item = decode_list(item)
        elif isinstance(item, dict):
            item = decode_dict(item)
        rv.append(item)
    return rv

# simplejson hook to convert dictionaries from unicode to utf-8


def decode_dict(data):
    rv = {}
    for key, value in data.iteritems():
        if isinstance(key, unicode):
            key = key.encode('utf-8')
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        elif isinstance(value, list):
            value = decode_list(value)
        elif isinstance(value, dict):
            value = decode_dict(value)
        rv[key] = value
    return rv

# Convert CPU list format (with ranges) to a Python list


def cpus2list(s):
    # The input string is a comma separated list of digits and ranges.
    # Examples include:
    # 0-3,8-11
    # 0,2,4,6
    # 2,5-7,10
    cpus = []
    if s.strip() == '':
        return cpus
    for r in s.split(','):
        if '-' in r[1:]:
            start, end = r.split('-', 1)
            for i in range(int(start), int(end) + 1):
                cpus.append(int(i))
        else:
            cpus.append(int(r))
    return cpus


# Helper function to ignore suspended jobs when removing
# "already used" resources
def job_to_be_ignored(jobid):
    # Get job substate from small utility that calls printjob
    pbs_exec = ''
    if not CRAY_12_BRANCH:
        pbs_conf = pbs.get_pbs_conf()
        pbs_exec = pbs_conf['PBS_EXEC']
    else:
        if 'PBS_EXEC' in self.cfg:
            pbs_exec = self.cfg['PBS_EXEC']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "PBS_EXEC needs to be defined in the config file")
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Exiting the cgroups hook")
            pbs.accept()

    printjob_cmd = pbs_exec+os.sep+'bin'+os.sep+'printjob'

    cmd = [printjob_cmd, jobid]

    substate = None
    try:
        pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
        # Collect the job substate information
        process = subprocess.Popen(
                                   cmd,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        (out, err) = process.communicate()
        # Find the job substate
        substate_re = re.compile(r"substate:\s+(?P<jobstate>\S+)\s+")
        substate = substate_re.search(out)
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Unexpected error in job_to_be_ignored: %s" %
                   ' '.join([repr(sys.exc_info()[0]),
                            repr(sys.exc_info()[1])]))
        out = "Unknown: failed to run " + printjob_cmd

    if substate is not None:
        substate = substate.group().split(':')[1].strip()
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Job %s has substate %s" % (jobid, substate))

        suspended_substates = ['0x2b', '0x2d', 'unknown']
        return (substate in suspended_substates)
    else:
        return False

# ============================================================================
# Utility classes
# ============================================================================

#
# CLASS HookUtils
#


class HookUtils:

    def __init__(self, hook_events=None):
        if hook_events is not None:
            self.hook_events = hook_events
        else:
            self.hook_events = {
                # Defined in the order they appear in module_pbs_v1.c
                pbs.QUEUEJOB: {
                    'name': 'queuejob',
                    'handler': None
                },
                pbs.MODIFYJOB: {
                    'name': 'modifyjob',
                    'handler': None
                },
                pbs.RESVSUB: {
                    'name': 'resvsub',
                    'handler': None
                },
                pbs.MOVEJOB: {
                    'name': 'movejob',
                    'handler': None
                },
                pbs.RUNJOB: {
                    'name': 'runjob',
                    'handler': None
                },
                pbs.PROVISION: {
                    'name': 'provision',
                    'handler': None
                },
                pbs.EXECJOB_BEGIN: {
                    'name': 'execjob_begin',
                    'handler': self.__execjob_begin_handler
                },
                pbs.EXECJOB_PROLOGUE: {
                    'name': 'execjob_prologue',
                    'handler': None
                },
                pbs.EXECJOB_EPILOGUE: {
                    'name': 'execjob_epilogue',
                    'handler': self.__execjob_epilogue_handler
                },
                pbs.EXECJOB_PRETERM: {
                    'name': 'execjob_preterm',
                    'handler': None
                },
                pbs.EXECJOB_END: {
                    'name': 'execjob_end',
                    'handler': self.__execjob_end_handler
                },
                pbs.EXECJOB_LAUNCH: {
                    'name': 'execjob_launch',
                    'handler': self.__execjob_launch_handler
                },
                pbs.EXECHOST_PERIODIC: {
                    'name': 'exechost_periodic',
                    'handler': self.__exechost_periodic_handler
                },
                pbs.EXECHOST_STARTUP: {
                    'name': 'exechost_startup',
                    'handler': self.__exechost_startup_handler
                },
                pbs.MOM_EVENTS: {
                    'name': 'mom_events',
                    'handler': None
                },
            }

            if not CRAY_12_BRANCH:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "CRAY 12 Branch: %s" % (CRAY_12_BRANCH))
                self.hook_events[pbs.EXECJOB_ATTACH] = {
                    'name': 'execjob_attach',
                    'handler': self.__execjob_attach_handler
                }

    def __repr__(self):
        return "HookUtils(%s)" % (repr(self.hook_events))

    def event_name(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['name']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Type: %s not found" % (caller_name(), type))
            return None

    def hashandler(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['handler'] is not None
        else:
            return None

    def invoke_handler(self, event, cgroup, jobutil, *args):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: UID: real=%d, effective=%d" %
                   (caller_name(), os.getuid(), os.geteuid()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: GID: real=%d, effective=%d" %
                   (caller_name(), os.getgid(), os.getegid()))
        if self.hashandler(event.type):
            result = self.hook_events[event.type][
                'handler'](event, cgroup, jobutil, *args)
            return result
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: %s event not handled by this hook." %
                       (caller_name(), self.event_name(event.type)))

    def __execjob_begin_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Instantiate the NodeConfig class for get_memory_on_node and
        # get_vmem_on node
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Host assigned job resources: %s" %
                   (caller_name(), jobutil.host_resources))

        # Make sure the parent cgroup directories exist
        cgroup.create_paths()

        # Make sure that any old cgroups created in an earlier attempt
        # at creating or running the job are gone
        cgroup.delete(e.job.id)

        # Gather the assigned resources
        cgroup.gather_assigned_resources()
        # Create the cgroup(s) for the job
        cgroup.create_job(e.job.id, node)
        # Configure the new cgroup
        cgroup.configure_job(e.job.id, jobutil.host_resources, node)
        # Initialize resource usage for the job
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        # Write out the assigned resources
        cgroup.write_out_cgroup_host_assigned_resources(e.job.id)
        # Write out the environment variable for the host (pbs_attach)
        if 'devices_name' in cgroup.host_assigned_resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Devices: %s" %
                       cgroup.host_assigned_resources['devices_name'])
            env_list = list()
            if len(cgroup.host_assigned_resources['devices_name']) > 0:
                line = str()
                mics = list()
                gpus = list()
                for key in cgroup.host_assigned_resources['devices_name']:
                    if key.startswith('mic'):
                        mics.append(key[3:])
                    elif key.startswith('nvidia'):
                        gpus.append(key[6:])
                if len(mics) > 0:
                    env_list.append('OFFLOAD_DEVICES="%s"' %
                                    string.join(mics, ','))
                if len(gpus) > 0:
                    # don't put quotes around the values. ex "0" or "0,1".
                    # This will cause it to fail.
                    env_list.append('CUDA_VISIBLE_DEVICES=%s' %
                                    string.join(gpus, ','))

            pbs.logmsg(pbs.EVENT_DEBUG, "ENV_LIST: %s" % env_list)
            cgroup.write_out_cgroup_host_job_env_file(e.job.id, env_list)
        return True

    def __execjob_epilogue_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # The resources_used information has a base type of pbs_resource
        # Set the usage data
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        return True

    def __execjob_end_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Delete the cgroup(s) for the job
        cgroup.delete(e.job.id)
        # Remove the host_assigned_resources and job_env file
        for filename in [cgroup.hook_storage_dir+os.sep+e.job.id,
                         cgroup.host_job_env_filename % e.job.id]:
            try:
                os.remove(filename)
            except OSError:
                pbs.logmsg(pbs.EVENT_DEBUG3, "File: %s not found" % (filename))
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Error removing file: %s" % (filename))
                pass

        return True

    def __execjob_launch_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Add the parent process id to the appropriate cgroups.
        cgroup.add_pids(os.getppid(), jobutil.job.id)
        # FUTURE: Add environment variable to the job environment
        # if job requested mic or gpu
        cgroup.read_in_cgroup_host_assigned_resources(e.job.id)
        if cgroup.host_assigned_resources is not None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "host_assigned_resources: %s" %
                       (cgroup.host_assigned_resources))
            cgroup.setup_job_devices_env()
        return True

    def __exechost_periodic_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Cleanup cgroups for jobs not present on this node
        count = cgroup.cleanup_orphans(e.job_list)
        if ('periodic_resc_update' in cgroup.cfg and
                cgroup.cfg['periodic_resc_update']):
            for job in e.job_list.keys():
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: job is %s" %
                           (caller_name(), job))
                cgroup.update_job_usage(job, e.job_list[job].resources_used)
        # Online nodes that were offlined due to a cgroup not cleaning up
        if count == 0 and cgroup.cfg['online_offlined_nodes']:
            vnode = pbs.event().vnode_list[cgroup.hostname]
            if os.path.isfile(cgroup.offline_file):
                line = 'Orphan cgroup(s) have been cleaned up. '

                # Check with the pbs server to see if the node comment matches
                try:
                    tmp_comment = pbs.server().vnode(cgroup.hostname).comment
                except:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Unable to contact server for node comment")
                    tmp_comment = None
                if tmp_comment == cgroup.offline_msg:
                    line += 'Will bring the node back online'
                    vnode.state = pbs.ND_FREE
                    vnode.comment = None
                else:
                    line += 'However, the comment has changed since the node '
                    line += 'was offlined. Node will remain offline'

                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: %s" % (caller_name(), line))
                # Remove file
                os.remove(cgroup.offline_file)
        return True

    def __exechost_startup_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        cgroup.create_paths()
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))

        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        node.create_vnodes()
        if 'memory' in cgroup.subsystems:
            val = node.get_memory_on_node(cgroup.cfg)
            if val is not None:
                if 'vnode_per_numa_node' in cgroup.cfg:
                    if not cgroup.cfg['vnode_per_numa_node']:
                        hn = node.hostname
                        e.vnode_list[hn].resources_available['mem'] = \
                            pbs.size(val)
                        val_cpus = node.totalcpus
                        if val_cpus is not None:
                            e.vnode_list[hn].resources_available['ncpus'] = \
                                int(val_cpus)
                cgroup.set_node_limit('mem', val)
        if 'memsw' in cgroup.subsystems:
            val = node.get_vmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['vmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('vmem', val)
        if 'hugetlb' in cgroup.subsystems:
            val = node.get_hpmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['hpmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('hpmem', val)
        return True

    def __execjob_attach_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logjobmsg(jobutil.job.id, "%s: Attaching PID %s" %
                      (caller_name(), e.pid))
        # Add the job process id to the appropriate cgroups.
        cgroup.add_pids(e.pid, jobutil.job.id)
        return True

#
# CLASS ShallIRunUtils
#


class ShallIRunUtils:

    def __init__(self, hostname, cfg):
        self.cfg = cfg
        self.hostname = hostname

    def allow_users(self, allow_users):
        """ This still needs to be defined """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pass

    def exclude_hosts(self, exclude_hosts):
        """
        Gets the hostname for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        hostname = pbs.get_local_nodename()
        # check to see if hostname is in the exclude_hosts list
        if self.hostname in exclude_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: exclude host %s is in %s" %
                       (caller_name(), self.hostname, exclude_hosts))
            pbs.event().accept()

        # check to see if it regex syntax is used
        pass

    def exclude_vntypes(self, exclude_vntypes):
        """
        Gets the vntype for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        vntype = None

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'
        if os.path.isfile(vntype_file):
            fdata = open(vntype_file).readlines()
            vntype = fdata[0].strip()
            pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
        if vntype is None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype is set to %s" % (vntype))
        elif vntype in exclude_vntypes:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s is in %s" % (vntype, exclude_vntypes))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Exiting Hook")
            pbs.event().accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s not in global exclude: %s" %
                       (vntype, exclude_vntypes))
        pass

    def run_only_on_hosts(self, approved_hosts):
        """
        Gets the list of approved nodes to run on and checks to see if the hook
        should run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Approved hosts: %s, %s" %
                   (type(approved_hosts), approved_hosts))

        # check to see if hostname is in the approved_hosts list
        if approved_hosts == []:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "approved hosts list is empty: %s" % approved_hosts)
        elif self.hostname not in approved_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s is not in the approved list of hosts: %s" %
                       (self.hostname, approved_hosts))
            pbs.event().accept()

        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s in list of approved hosts: %s" %
                       (self.hostname, approved_hosts))

#
# CLASS JobUtils
#


class JobUtils:

    def __init__(self, job, hostname=None, resources=None):
        self.job = job
        self.host_vnodes = list()
        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if resources is not None:
            self.host_resources = resources
        else:
            self.host_resources = self.__host_assigned_resources()

    def __repr__(self):
        return "JobUtils(%s, %s, %s)" % (repr(self.job), repr(self.hostname),
                                         repr(self.host_resources))

    # Return a dictionary of resources assigned to the local node
    def __host_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Bail out if no hostname was provided
        if self.hostname is None:
            raise CgroupProcessingError('No hostname available')
        # Bail out if no job information is present
        if self.job is None:
            raise CgroupProcessingError('No job information available')
        # Determine which vnodes are on this host, if any
        if self.hostname is not None:
            vnhost_pattern = "%s\[[\d+]\]" % self.hostname
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnhost pattern: %s" %
                       (caller_name(), vnhost_pattern))
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job exec_vnode list: %s" %
                       (caller_name(), self.job.exec_vnode))
            for match in re.findall(vnhost_pattern, str(self.job.exec_vnode)):
                self.host_vnodes.append(match)
            if self.host_vnodes is not None:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Associate %s to vnodes on %s" %
                           (caller_name(), self.host_vnodes, self.hostname))

        # Collect host assigned resources
        resources = {}
        for chunk in self.job.exec_vnode.chunks:
            vnode = False
            if self.host_vnodes == [] and chunk.vnode_name != self.hostname:
                continue
            elif self.host_vnodes != [] and \
                    chunk.vnode_name not in self.host_vnodes:
                continue
            elif chunk.vnode_name in self.host_vnodes:
                vnode = True
                if 'vnodes' not in resources:
                    resources['vnodes'] = {}
                if chunk.vnode_name not in resources['vnodes']:
                    resources['vnodes'][chunk.vnode_name] = {}
                # This check is needed since not all resources are
                # required to be in each chunk of a job
                # i.e. exec_vnodes =
                # (node1[0]:ncpus=4:mem=4gb+node1[1]:mem=2gb) +
                # (node1[1]:ncpus=3+node[0]:ncpus=1:mem=4gb)
                if all(resc in resources['vnodes'][chunk.vnode_name]
                       for resc in chunk.chunk_resources.keys()):
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: resources already defined" %
                               (caller_name()))
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s,\t%s" %
                               (caller_name(),
                                chunk.chunk_resources.keys(),
                                resources['vnodes'][chunk.vnode_name].keys()))
                else:
                    # Initialize resources for each vnode
                    for resc in chunk.chunk_resources.keys():
                        # Initialize the new value
                        if isinstance(chunk.chunk_resources[resc],
                                      pbs.pbs_int):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_int(0)
                        elif isinstance(chunk.chunk_resources[resc],
                                        pbs.pbs_float):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_float(0)
                        elif isinstance(chunk.chunk_resources[resc], pbs.size):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.size('0')

            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Chunk %s" %
                       (caller_name(), chunk.vnode_name))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resources: %s" %
                       (caller_name(), resources))
            for resc in chunk.chunk_resources.keys():
                if resc not in resources.keys():
                    # Initialize the new value
                    if isinstance(chunk.chunk_resources[resc], pbs.pbs_int):
                        resources[resc] = pbs.pbs_int(0)
                    elif isinstance(chunk.chunk_resources[resc],
                                    pbs.pbs_float):
                        resources[resc] = pbs.pbs_float(0.0)
                    elif isinstance(chunk.chunk_resources[resc], pbs.size):
                        resources[resc] = pbs.size('0')
                # Add resource value to total
                if type(chunk.chunk_resources[resc]) in \
                        [pbs.pbs_int, pbs.pbs_float, pbs.size]:
                    resources[resc] += chunk.chunk_resources[resc]
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s: resources[%s][%s] is now %s" %
                               (caller_name(), self.hostname, resc,
                                resources[resc]))
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] += \
                                  chunk.chunk_resources[resc]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Setting resource %s to string %s" %
                               (caller_name(), resc,
                                str(chunk.chunk_resources[resc])))
                    resources[resc] = str(chunk.chunk_resources[resc])
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] = \
                                  str(chunk.chunk_resources[resc])
        if not resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: No resources assigned to host %s" %
                       (caller_name(), self.hostname))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources for %s: %s" %
                       (caller_name(), self.hostname, repr(resources)))

        # Return assigned resources for specified host
        pbs.logmsg(pbs.EVENT_DEBUG3, "Job Resources: %s" % (resources))
        return resources

    # Write a message to the job stdout file
    def write_to_stderr(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stderr_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

    # Write a message to the job stdout file
    def write_to_stdout(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stdout_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

#
# CLASS NodeConfig
#


class NodeConfig:

    def __init__(self, cfg, **kwargs):

        self.cfg = cfg
        hostname = None
        meminfo = None
        numa_nodes = None
        devices = None
        hyperthreading = None
        self.hyperthreads_per_core = 1
        self.totalcpus = self.__discover_cpus()

        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        self.host_mom_jobdir = pbs_home+'/mom_priv/jobs'

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'hostname':
                    hostname = val
                elif arg == 'meminfo':
                    meminfo = val
                elif arg == 'numa_nodes':
                    numa_nodes = val
                elif arg == 'devices':
                    devices = val
                elif arg == 'hyperthreading':
                    hyperthreading = val

        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if meminfo is not None:
            self.meminfo = meminfo
        else:
            self.meminfo = self.__discover_meminfo()
        if numa_nodes is not None:
            self.numa_nodes = numa_nodes
        else:
            self.numa_nodes = self.__discover_numa_nodes()
        if devices is not None:
            self.devices = devices
        else:
            self.devices = self.__discover_devices()
        if hyperthreading is not None:
            self.hyperthreading = hyperthreading
        else:
            self.hyperthreading = self.__hyperthreading_enabled()

        # Add the devices count i.e. nmics and ngpus to the numa nodes
        self.__add_devices_to_numa_node()

    def __repr__(self):
        return ("NodeConfig(%s, %s, %s, %s, %s, %s)" %
                (repr(self.cfg), repr(self.hostname), repr(self.meminfo),
                 repr(self.numa_nodes), repr(self.devices),
                 repr(self.hyperthreading)))

    def __add_devices_to_numa_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (self.devices.keys()))
        for device in self.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" % (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" %
                           (self.devices[device].keys()))
                for device_name in self.devices[device]:
                    device_socket = \
                        self.devices[device][device_name]['numa_node']
                    if device == 'mic':
                        if 'nmics' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['nmics'] = 1
                        else:
                            self.numa_nodes[device_socket]['nmics'] += 1
                    elif device == 'gpu':
                        if 'ngpus' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['ngpus'] = 1
                        else:
                            self.numa_nodes[device_socket]['ngpus'] += 1

        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (self.numa_nodes))

    # Discover what type of hardware is on this node and how is it partitioned
    def __discover_numa_nodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        numa_nodes = {}
        for dir in glob.glob(os.path.join(os.sep,
                             "sys", "devices", "system", "node", "node*")):
            id = int(dir.split(os.sep)[5][4:])
            if id not in numa_nodes.keys():
                numa_nodes[id] = {}
                numa_nodes[id]['devices'] = list()
            numa_nodes[id]['cpus'] = open(os.path.join(dir, "cpulist"),
                                          'r').readline().strip()
            with open(os.path.join(dir, "meminfo"), 'r') as fd:
                for line in fd:
                    # Each line will contain four or five fields. Examples:
                    # Node 0 MemTotal:       32995028 kB
                    # Node 0 HugePages_Total:     0
                    entries = line.split()
                    if len(entries) < 4:
                        continue
                    if entries[2] == "MemTotal:":
                        numa_nodes[id][entries[2].rstrip(':')] = \
                            convert_size(entries[3] + entries[4], 'kb')
                    elif entries[2] == "HugePages_Total:":
                        numa_nodes[id][entries[2].rstrip(':')] = entries[3]
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover Numa Nodes: %s" % numa_nodes)
        return numa_nodes

    # Identify devices and to which numa nodes they are attached
    def __discover_devices(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        devices = {}
        for file in glob.glob(os.path.join(os.sep, "sys", "class", "*", "*",
                                           "device", "numa_node")):
            # The file should contain a single integer
            numa_node = int(open(file, 'r').readline().strip())
            if numa_node < 0:
                numa_node = 0
            dirs = file.split(os.sep)
            name = dirs[3]
            instance = dirs[4]
            if name not in devices.keys():
                devices[name] = {}
            devices[name][instance] = {}
            devices[name][instance]['numa_node'] = numa_node
            if name == 'mic':
                s = os.stat('/dev/%s' % instance)
                devices[name][instance]['major'] = os.major(s.st_rdev)
                devices[name][instance]['minor'] = os.minor(s.st_rdev)
                devices[name][instance]['device_type'] = 'c'

        # Check to see if there are gpus on the node
        gpus = self.discover_gpus()
        if isinstance(gpus, dict) and len(gpus.keys()) > 0:
            devices['gpu'] = gpus['gpu']

        pbs.logmsg(pbs.EVENT_DEBUG3, "Discovered devices: %s" % (devices))
        return devices

    def discover_gpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Check to see if nvidia-smi exists and produces valid output
            cmd = ['/usr/bin/nvidia-smi', '-q', '-x']
            if 'nvidia-smi' in self.cfg:
                cmd[0] = self.cfg['nvidia-smi']

            pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
            # Collect the gpu information
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.PIPE)
            output, err = process.communicate()

            # pbs.logmsg(pbs.EVENT_DEBUG3,"output: %s" % output)
            # import the xml library and parse the output
            import xml.etree.ElementTree as ET

            # Parse the data
            name = 'gpu'
            gpu_data = dict()
            gpu_data[name] = {}
            root = ET.fromstring(output)
            pbs.logmsg(pbs.EVENT_DEBUG3, "root.tag: %s" % root.tag)
            for child in root:
                if child.tag == "attached_gpus":
                    # gpu_data['ngpus'] = int(child.text)
                    pass
                if child.tag == "gpu":
                    device_id = child.get('id')
                    number = 'nvidia%s' % child.find('minor_number').text
                    gpu_data[name][number] = dict()
                    gpu_data[name][number]['id'] = device_id

                    # Determine which socket the device is on
                    filename = '/sys/bus/pci/devices/%s/numa_node' % \
                        device_id.lower()
                    isfile = os.path.isfile(filename)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s, %s" % (filename, isfile))
                    if isfile:
                        numa_node = open(filename).read().strip()
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "numa_node: %s" % (numa_node))
                        if int(numa_node) == -1:
                            numa_node = 0
                        gpu_data[name][number]['numa_node'] = int(numa_node)
                        try:
                            dev_filename = '/dev/%s' % number
                            s = os.stat(dev_filename)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find %s" % dev_filename)
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                        gpu_data[name][number]['major'] = os.major(s.st_rdev)
                        gpu_data[name][number]['minor'] = os.minor(s.st_rdev)
                        gpu_data[name][number]['device_type'] = 'c'
            pbs.logmsg(pbs.EVENT_DEBUG, "%s" % gpu_data)
            return gpu_data
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unable to find %s" % string.join(cmd, " "))
            return {'gpu': {}}
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        return {'gpu': {}}

    # Get the memory info on this host
    def __discover_meminfo(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        meminfo = {}
        with open(os.path.join(os.sep, "proc", "meminfo"), 'r') as fd:
            for line in fd:
                entries = line.split()
                if entries[0] == "MemTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "SwapTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "Hugepagesize:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "HugePages_Total:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
                elif entries[0] == "HugePages_Rsvd:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover meminfo: %s" % meminfo)
        return meminfo

    # Determine if the cpus have hyperthreading enabled
    def __hyperthreading_enabled(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            siblings = 0
            cpu_cores = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "siblings":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    siblings = entries[1].strip()
                elif entries[0].strip() == "cpu cores":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_cores = entries[1].strip()
                elif entries[0].strip() == "flags":
                    flag_options = entries[1].split()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: flag_options: %s" %
                               (caller_name(), flag_options))
                    if "ht" in flag_options:
                        rc = True
                        break
        if rc is True:
            self.hyperthreads_per_core = int(siblings)/int(cpu_cores)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: hyperthreads/core: %d" %
                       (caller_name(), self.hyperthreads_per_core))
            if self.hyperthreads_per_core == 1:
                rc = False
        return rc

    def __discover_cpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            cpu_threads = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "processor":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_threads += 1
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: processors found: %d" %
                   (caller_name(), cpu_threads))

        return cpu_threads

    # Gather the jobs running on this node
    def gather_jobs_on_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        # Get the job list from the server
        jobs = None
        try:
            jobs = pbs.server().vnode(self.hostname).jobs
        except:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Unable to contact server to get jobs")
            return None

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs from server for %s: %s" % (self.hostname, jobs))

        if jobs is None:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "No jobs found on %s " % (self.hostname))
            return list()

        jobs_list = dict()
        for job in jobs.split(','):
            # The job list from the node has a space in front of the job id
            # This must be removed or it will not match the cgroup dir
            jobs_list[job.split('/')[0].strip()] = 0
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs on %s: %s" % (self.hostname, jobs_list.keys()))

        return jobs_list.keys()

    # Gather the jobs running on this node
    def gather_jobs_on_node_local(self):
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Method called" % (caller_name()))
        # Get the job list from the jobs directory
        jobs_list = list()
        try:
            for file in os.listdir(self.host_mom_jobdir):
                if fnmatch.fnmatch(file, "*.JB"):
                    jobs_list.append(file[:-3])
            if not jobs_list:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "No jobs found on %s " % (self.hostname))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Jobs from server for %s: %s" %
                           (self.hostname, repr(jobs_list)))

            return jobs_list
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Could not get job list from mom_priv/jobs for %s" %
                       self.hostname)

            return self.gather_jobs_on_node()

    # Get the memory resource on this mom
    def get_memory_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Calculate memory
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
            pbs.logmsg(pbs.EVENT_DEBUG3, "val: %s" % val)
        else:
            val = size_as_int(MemTotal)
        if 'percent_reserve' in config['cgroup']['memory']:
            percent_reserve = config['cgroup']['memory']['percent_reserve']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memory'].keys():
            reserve_memory = config['cgroup']['memory']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                reserve_mem = size_as_int(reserve_memory)
                if val > reserve_mem:
                    val -= reserve_mem
                else:
                    val -= size_as_int("100mb")
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Unable to reserve more memory than " +
                               "available on the node. Available %s, " +
                               "Tried to reserve %s. Reserving 100mb" %
                               (val, reserve_mem))

            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))

        pbs.logmsg(pbs.EVENT_DEBUG3, "Return val: %s" % val)
        return convert_size(str(val), 'kb')

    # Get the virtual memory resource on this mom
    def get_vmem_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Return the value for memory if no swap is configured
        if size_as_int(self.meminfo['SwapTotal']) <= 0:
            return self.get_memory_on_node(config)
        # Calculate vmem
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
        else:
            val = size_as_int(MemTotal)

        val += size_as_int(self.meminfo['SwapTotal'])
        # Determine if memory needs to be reserved
        if 'percent_reserve' in config['cgroup']['memsw']:
            percent_reserve = config['cgroup']['memsw']['reserve_memory']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memsw']:
            reserve_memory = config['cgroup']['memsw']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                val -= size_as_int(reserve_memory)
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))
        return convert_size(str(val), 'kb')

    # Get the huge page memory resource on this mom
    def get_hpmem_on_node(self, config):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        if 'percent_reserve' in config['cgroup']['hugetlb'].keys():
            percent_reserve = config['cgroup']['hugetlb']['percent_reserve']
        else:
            percent_reserve = 0
        # Calculate vmem
        val = size_as_int(self.meminfo['Hugepagesize'])
        val *= (self.meminfo['HugePages_Total'] -
                self.meminfo['HugePages_Rsvd'])
        val -= (val * percent_reserve) / 100
        return convert_size(str(val), 'kb')

    # Create individual vnodes per socket
    def create_vnodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        key = "vnode_per_numa_node"
        if key in self.cfg.keys():
            if not self.cfg[key]:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s is false" %
                           (caller_name(), key))
                return
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s not configured" %
                       (caller_name(), key))
            return

        # Create one vnode per numa node
        vnode_list = pbs.event().vnode_list
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: numa nodes: %s" %
                   (caller_name(), self.numa_nodes))
        # Define the vnodes
        vnode_name = self.hostname
        vnode_list[vnode_name] = pbs.vnode(vnode_name)
        for id in self.numa_nodes.keys():
            vnode_name = self.hostname + "[%d]" % id
            vnode_list[vnode_name] = pbs.vnode(vnode_name)
            for key, val in sorted(self.numa_nodes[id].iteritems()):
                if key == 'cpus':
                    vnode_list[vnode_name].resources_available['ncpus'] = \
                        len(cpus2list(val))/self.hyperthreads_per_core
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['ncpus'] = 0
                elif key == 'MemTotal':
                    mem = self.get_memory_on_node(self.cfg, val)
                    vnode_list[vnode_name].resources_available['mem'] = \
                        pbs.size(mem)
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['mem'] = \
                        pbs.size('0kb')
                elif key == 'HugePages_Total':
                    pass
                elif isinstance(val, list):
                    pass
                elif isinstance(val, dict):
                    pass
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s=%s" %
                               (caller_name(), key, val))
                    vnode_list[vnode_name].resources_available[key] = val

                    if isinstance(val, int):
                        vnode_list[self.hostname].resources_available[key] = 0
                    elif isinstance(val, float):
                        vnode_list[self.hostname].resources_available[key] = \
                            0.0
                    elif isinstance(val, pbs.size):
                        vnode_list[self.hostname].resources_available[key] = \
                            pbs.size('0kb')
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnode list: %s" %
                   (caller_name(), vnode_list))
        return True

#
# CLASS CgroupUtils
#


class CgroupUtils:
    def __init__(self, hostname, vnode, **kwargs):
        cfg = None
        subsystems = None
        paths = None
        vntype = None
        event = None
        self.assigned_resources = dict()
        self.existing_cgroups = dict()
        self.host_assigned_resources = None
        self.pid_lower_limit = 3

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'subsystems':
                    subsystems = val
                elif arg == 'paths':
                    paths = val
                elif arg == 'vntype':
                    vntype = val
                elif arg == 'event':
                    event = val

        try:
            self.hostname = hostname
            self.vnode = vnode
            # __check_os will raise an exception if cgroups are not present
            self.__check_os()
            # Read in the config file
            if cfg is not None:
                self.cfg = cfg
            else:
                self.cfg = self.__parse_config_file()

            # Determine if the hook should run or exit
            siru = ShallIRunUtils(self.hostname, self.cfg)
            siru.exclude_hosts(self.cfg['exclude_hosts'])
            siru.exclude_vntypes(self.cfg['exclude_vntypes'])
            siru.run_only_on_hosts(self.cfg['run_only_on_hosts'])

            # Collect the mount points
            if paths is not None:
                self.paths = paths
            else:
                self.paths = self.__set_paths()
            # Define the local vnode type
            if vntype is not None:
                self.vntype = vntype
            else:
                self.vntype = self.__get_vnode_type()
            # Determine which subsystems we care about
            if subsystems is not None:
                self.subsystems = subsystems
            else:
                self.subsystems = self.__target_subsystems()

            # location to store information for the different hook events
            pbs_home = ''
            if not CRAY_12_BRANCH:
                pbs_conf = pbs.get_pbs_conf()
                pbs_home = pbs_conf['PBS_HOME']
            else:
                if 'PBS_HOME' in self.cfg:
                    pbs_home = self.cfg['PBS_HOME']
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "PBS_HOME needs to be defined in the " +
                               "config file")
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Exiting the cgroups hook")
                    pbs.accept()

            self.hook_storage_dir = pbs_home+'/mom_priv/hooks/hook_data'
            self.host_job_env_dir = pbs_home+'/aux'
            self.host_job_env_filename = self.host_job_env_dir+os.sep+"%s.env"

            # information for offlining nodes
            self.offline_file = \
                pbs_home+os.sep+'mom_priv'+os.sep+'hooks'+os.sep
            self.offline_file += "%s.offline" % pbs.event().hook_name
            self.offline_msg = "Hook %s: " % pbs.event().hook_name
            self.offline_msg += "Unable to clean up one or more cgroups"

        except:
            raise

    def __repr__(self):
        return ("CgroupUtils(%s, %s, %s, %s, %s, %s)" %
                (repr(self.hostname), repr(self.vnode), repr(self.cfg),
                 repr(self.subsystems), repr(self.paths), repr(self.vntype)))

    # Validate the OS type and version
    def __check_os(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check to see if the platform is linux and the kernel is new enough
        if platform.system() != 'Linux':
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: OS does not support cgroups" %
                       (caller_name()))
            raise CgroupConfigError("OS type not supported")
        rel = map(int,
                  string.split(string.split(platform.release(), '-')[0], '.'))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Detected Linux kernel version %d.%d.%d" %
                   (caller_name(), rel[0], rel[1], rel[2]))
        supported = False
        if rel[0] > 2:
            supported = True
        elif rel[0] == 2:
            if rel[1] > 6:
                supported = True
            elif rel[1] == 6:
                if rel[2] >= 28:
                    supported = True
        if not supported:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Kernel needs to be >= 2.6.28: %s" %
                       (caller_name(), system_info[2]))
            raise CgroupConfigError("OS version not supported")
        return supported

    # Read the config file in json format
    def __parse_config_file(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        config = {}
        # Identify the config file and read in the data
        if 'PBS_HOOK_CONFIG_FILE' in os.environ:
            config_file = os.environ["PBS_HOOK_CONFIG_FILE"]
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Config file is %s" %
                       (caller_name(), config_file))
            try:
                config = json.load(open(config_file, 'r'),
                                   object_hook=decode_dict)
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
        else:
            raise CgroupConfigError("No configuration file present")
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Initial Cgroup Config: %s" %
                   (caller_name(), config))
        # Set some defaults if they are not present
        if 'cgroup_prefix' not in config.keys():
            config['cgroup_prefix'] = 'pbspro'
        if 'periodic_resc_update' not in config.keys():
            config['periodic_resc_update'] = False
        if 'vnode_per_numa_node' not in config.keys():
            config['vnode_per_numa_node'] = False
        if 'online_offlined_nodes' not in config.keys():
            config['online_offlined_nodes'] = False
        if 'exclude_hosts' not in config.keys():
            config['exclude_hosts'] = []
        if 'run_only_on_hosts' not in config.keys():
            config['run_only_on_hosts'] = []
        if 'exclude_vntypes' not in config.keys():
            config['exclude_vntypes'] = []
        if 'cgroup' not in config.keys():
            config['cgroup'] = {}
        else:
            for subsys in config['cgroup']:
                subsystem = config['cgroup'][subsys]
                if 'enabled' not in subsystem.keys():
                    config['cgroup'][subsys]['enabled'] = False
                if 'exclude_hosts' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_hosts'] = []
                if 'exclude_vntypes' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_vntypes'] = []

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Cgroup Config with defaults: %s" %
                   (caller_name(), config))
        return config

    # Determine which subsystems are being requested
    def __target_subsystems(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        subsystems = []
        for key in self.cfg['cgroup'].keys():
            if self.enabled(key):
                subsystems.append(key)
        if 'memory' not in subsystems:
            if 'memsw' in subsystems:
                # Remove memsw since it must be greater then
                # or equal to memory
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing memsw from enabled subsystems")
                subsystems.remove('memsw')
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Enabled subsystems: %s" %
                   (caller_name(), subsystems))
        # It is not an error for all subsystems to be disabled.
        # This host or vnode type may be in the excluded list.
        return subsystems

    # Copy a setting from the parent cgroup
    def __copy_from_parent(self, dest):
        try:
            file = os.path.basename(dest)
            dir = os.path.dirname(dest)
            parent = os.path.dirname(dir)
            source = os.path.join(parent, file)
            if not os.path.isfile(source):
                raise CgroupConfigError('Failed to read %s' % (source))
            self.write_value(dest, open(source, 'r').read().strip())
        except:
            raise

    # Create a dictionary of the cgroup subsystems and their corresponding
    # directories
    def __set_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        paths = {}
        try:
            # Loop through the mounts and collect the ones for cgroups
            with open(os.path.join(os.sep, "proc", "mounts"), 'r') as fd:
                for line in fd:
                    entries = line.split()
                    if len(entries) > 3 and entries[2] == "cgroup":
                        flags = entries[3].split(',')
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "subsysdir: %s (flags=%s)" %
                                   (entries[1], flags))
                        for subsys in flags:
                            paths[subsys] = os.path.join(
                                entries[1], self.cfg["cgroup_prefix"])
        except:
            raise
        if len(paths.keys()) < 1:
            raise CgroupConfigError("Cgroup paths not detected")
        # Both the memory and memsw cgroups share the same mount point
        if "memory" in paths.keys():
            paths["memsw"] = paths["memory"]
        return paths

    # Create the cgroup parent directories that will contain the jobs
    def create_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Create the directories that PBS will use to house the jobs
            old_umask = os.umask(0022)
            for subsys in self.subsystems:
                if subsys not in self.paths.keys():
                    raise CgroupConfigError('No path for subsystem: %s' %
                                            (subsys))
                if not os.path.exists(self.paths[subsys]):
                    os.makedirs(self.paths[subsys], 0755)
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Created directory %s" %
                               (caller_name(), self.paths[subsys]))
                    if subsys == 'memory' or subsys == 'memsw':
                        # Set file to
                        # <cgroup>/memory/pbspro/memory.use_hierarchy
                        file = os.path.join(self.paths[subsys],
                                            'memory.use_hierarchy')
                        if not os.path.isfile(file):
                            raise CgroupConfigError('Failed to configure %s' %
                                                    (file))
                        self.write_value(file, 1)
                    elif subsys == 'cpuset':
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.cpus'))
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.mems'))

        except:
            raise CgroupConfigError("Failed to create directory: %s" %
                                    (self.paths[subsys]))
        finally:
            os.umask(old_umask)

    # Return the vnode type of the local node
    def __get_vnode_type(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")

        # File to check for if it is not defined in the hook input file
        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'

        # self.vnode is None for pbs_attach events
        pbs.logmsg(pbs.EVENT_DEBUG3, "vnode: %s" % self.vnode)
        if self.vnode is not None:
            if 'vntype' in self.vnode.resources_available and \
                    self.vnode.resources_available['vntype'] is not None:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "vntype: %s" %
                           self.vnode.resources_available['vntype'])
                return self.vnode.resources_available['vntype']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3, "vntype file: %s" % vntype_file)
                if os.path.isfile(vntype_file):
                    fdata = open(vntype_file).readlines()
                    vntype = fdata[0].strip()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
                    return vntype
        # If vntype was not set then log a message. It is too expensive
        # to have all moms query the server for large jobs.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: resources_available.vntype is " % caller_name() +
                   "not set for this vnode")
        return None

    # Gather resources that are already assigned
    def gather_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        assigned_resources = {}

        # Gather the cpus and memory sockets assigned
        directories = [x[0] for x in os.walk(self.paths['cpuset'])]

        # Add the existing cgroups to a list to check if assigning
        # resources fail
        for dir in directories[1:]:
            if dir.find(pbs.event().job.id) == -1:
                self.existing_cgroups[dir] = []

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'cpuset' in self.subsystems and \
                self.cfg['cgroup']['cpuset']['enabled']:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather cpuset resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['cpuset'], tmp_dir)
                    with open(path+os.sep+'cpuset.cpus') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.cpus'] = \
                            cpus2list(tmp_val.strip())
                    with open(path+os.sep+'cpuset.mems') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.mems'] = \
                            cpus2list(tmp_val.strip())

        # Check to see if the subsystem is enabled before trying
        # to gather resources
        if 'memory' in self.subsystems and \
                self.cfg['cgroup']['memory']['enabled']:
            # Gather the memory assigned for each socket
            directories = [x[0] for x in os.walk(self.paths['memory'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather memory resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['memory'], tmp_dir)
                    with open(path+os.sep+'memory.limit_in_bytes') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memory.limit'] = \
                            tmp_val.strip()
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    if os.path.isfile(tmp_filename) is False:
                        continue
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    with open(tmp_filename) as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memsw.limit'] = \
                            tmp_val.strip()

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'devices' in self.subsystems and \
                self.cfg['cgroup']['devices']['enabled']:
            # Gather the devices assigned
            directories = [x[0] for x in os.walk(self.paths['devices'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather device resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    pbs.logmsg(pbs.EVENT_DEBUG3, "Dir: %s" % tmp_dir)
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['devices'], tmp_dir)
                    with open(path+os.sep+'devices.list') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['devices.list'] = \
                            tmp_val.strip().split('\n')
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Dir: %s\n\tValue: %s" %
                                   (path, tmp_val.replace('\n', ',')))

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Gathered assigned resources: %s" % assigned_resources)
        self.assigned_resources = assigned_resources

    # Return whether a subsystem is enabled
    def enabled(self, subsystem):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check whether the subsystem is enabled in the configuration file
        if subsystem not in self.cfg['cgroup'].keys():
            return False
        if 'enabled' not in self.cfg['cgroup'][subsystem].keys():
            return False
        if not self.cfg['cgroup'][subsystem]['enabled']:
            return False
        # Check whether the cgroup is mounted for this subsystem
        if subsystem not in self.paths.keys():
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup not mounted for %s" %
                       (caller_name(), subsystem))
            return False
        # Check whether this host is excluded
        # if self.hostname in self.cfg['exclude_hosts']:
        #    pbs.logmsg(pbs.EVENT_DEBUG, "%s: cgroup excluded on host %s" %
        #               (caller_name(), self.hostname))
        #    pbs.event().accept()
        if self.hostname in self.cfg['cgroup'][subsystem]['exclude_hosts']:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup excluded for subsystem %s on host %s" %
                       (caller_name(), subsystem, self.hostname))
            return False
        # Check whether the vnode type is excluded
        if self.vntype is not None:
            # if self.vntype in self.cfg['exclude_vntypes']:
            #    pbs.logmsg(pbs.EVENT_DEBUG,
            #               "%s: cgroup excluded on vnode type %s" %
            #               (caller_name(), self.vntype))
            #    pbs.event().accept()
            if self.vntype in self.cfg['cgroup'][subsystem]['exclude_vntypes']:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           ("%s: cgroup excluded for " +
                            "subsystem %s on vnode type %s") %
                           (caller_name(), subsystem, self.vntype))
                return False
        return True

    # Return the default value for a subsystem
    def default(self, subsystem):
        if subsystem in self.cfg['cgroup']:
            if 'default' in self.cfg['cgroup'][subsystem]:
                return self.cfg['cgroup'][subsystem]['default']
        return None

    # Check to see if the pid matches the job owners uid
    def is_pid_owned_by_job_owner(self, pid):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        try:
            proc_uid = os.stat('/proc/%d' % pid).st_uid
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       '/proc/%d uid:%d' % (pid, proc_uid))

            job_owner_uid = pwd.getpwnam(pbs.event().job.euser)[2]
            pbs.logmsg(pbs.EVENT_DEBUG3, "Job uid: %d" % job_owner_uid)

            if proc_uid == job_owner_uid:
                return True
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Proc uid: %d != Job owner:  %d" %
                           (proc_uid, job_owner_uid))
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG, "Unknown pid: %d" % pid)
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        # If we got to this point something did not match up
        return False

    # Add some number of PIDs to the cgroup tasks files for each subsystem
    def add_pids(self, pids, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # make pids a list
        if isinstance(pids, int):
            pids = [pids]

        if pbs.event().type == pbs.EXECJOB_LAUNCH:
            if 1 in pids:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "1 is not a valid pid to add")
                # Hold the job for further review
                e.reject("Hook tried to add pid 1 to the tasks file " +
                         "for job %s" % jobid)

        # check pids to make sure that they are owned by the job owner
        if not CRAY_12_BRANCH:
            pbs.logmsg(pbs.EVENT_DEBUG3, "Not in the Cray 12 Branch")
            pbs.logmsg(pbs.EVENT_DEBUG3, "check to see if execjob_attach")
            if pbs.event().type == pbs.EXECJOB_ATTACH:
                pbs.logmsg(pbs.EVENT_DEBUG3, "event type: attach or launch")
                tmp_pids = list()
                for p in pids:
                    if self.is_pid_owned_by_job_owner(p):
                        tmp_pids.append(p)
                if len(tmp_pids) == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%d is not a valid pids to add" % p)
                    return False
                else:
                    pids = tmp_pids

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: subsys = %s" %
                       (caller_name(), subsys))
            # memsw and memory use the same tasks file
            if subsys == "memsw":
                if "memory" in self.subsystems:
                    continue
            path = os.path.join(self.paths[subsys], jobid)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path = %s" %
                       (caller_name(), path))
            try:
                tasks_path = os.path.join(path, "tasks")
                for p in pids:
                    self.write_value(tasks_path, p, 'a')
            except IOError, exc:
                raise CgroupLimitError("Failed to add PIDs %s to %s (%s)" %
                                       (str(pids), tasks_path,
                                        errno.errorcode[exc.errno]))
            except:
                raise

    # Set a node limit
    def set_node_limit(self, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s = %s" %
                   (caller_name(), resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'],
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Node resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    def setup_job_devices_env(self):
        """ Setup the job environment for the devices assigned to the job for an
            execjob_launch hook
         """
        if 'devices_name' in self.host_assigned_resources:
            names = self.host_assigned_resources['devices_name']
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "devices: %s" % (names))
            offload_devices = list()
            cuda_visible_devices = list()
            for name in names:
                if name.startswith('mic'):
                    offload_devices.append(name[3:])
                elif name.startswith('nvidia'):
                    cuda_visible_devices.append(name[6:])
            if len(offload_devices) > 0:
                value = '"%s"' % string.join(offload_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['OFFLOAD_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "offload_devices: %s" % offload_devices)
            if len(cuda_visible_devices) > 0:
                value = '"%s"' % string.join(cuda_visible_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['CUDA_VISIBLE_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "cuda_visible_devices: %s" % cuda_visible_devices)
            return [offload_devices, cuda_visible_devices]
        else:
            return False

    def setup_subsys_devices(self, path, node):
        subsys = 'devices'
        # Add the devices that the user is allowed to use
        if subsys in self.cfg['cgroup']:
            devices_allowed = open(os.path.join(path,
                                   "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Initial devices.list: %s" %
                       devices_allowed)
            # Deny access to mic and gpu devices
            devices_list = list()
            devices = node.devices
            for device in devices:
                if device == 'mic' or device == 'gpu':
                    for name in devices[device]:
                        d = devices[device][name]
                        devices_list.append("%d:%d" % (d['major'], d['minor']))
            # For CentOS 7 we need to remove a *:* rwm from devices.list we
            # before can add anything to devices.allow. Otherwise our changes
            # are ignored. Check to see if a *:* rwm is in devices.list.
            # If so remove it
            if "a *:* rwm\n" in devices_allowed:
                value = "a *:* rwm"
                self.write_value(os.path.join(path, 'devices.deny'), value)
            else:
                # Verify that the following devices are not in devices.list
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing access to the following: %s" %
                           devices_list)
            for device_str in devices_list:
                value = "c %s rwm" % device_str
                self.write_value(os.path.join(path, 'devices.deny'), value)
            # Add devices back to the list
            devices_allow = list()
            if 'allow' in self.cfg['cgroup'][subsys]:
                devices_allow = self.cfg['cgroup'][subsys]['allow']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Allowing access to the following: %s" %
                           devices_allow)
                for item in devices_allow:
                    if isinstance(item, str):
                        pbs.logmsg(pbs.EVENT_DEBUG3, "string item: %s" % item)
                        value = "%s" % item
                        self.write_value(os.path.join(
                                         path, 'devices.allow'), value)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "write_value: %s" % value)
                    elif isinstance(item, list):
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "list item: %s" % item)
                        try:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Device allow: %s" % item)
                            stat_filename = '/dev/%s' % item[0]
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Stat file: %s" % stat_filename)
                            s = os.stat(stat_filename)
                            device_type = None
                            if S_ISBLK(s.st_mode):
                                device_type = "b"
                            elif S_ISCHR(s.st_mode):
                                device_type = "c"
                            if device_type is not None:
                                if len(item) == 3 and isinstance(item[2], str):
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % item[2]
                                    value += "%s" % item[1]
                                else:
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % os.minor(s.st_rdev)
                                    value += "%s" % item[1]
                                self.write_value(os.path.join(
                                                path, 'devices.allow'), value)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "write_value: %s" % value)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find /dev/%s. " % item[0] +
                                       "Not added to devices.allow!")
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                    else:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Not sure what to do with: %s" % item)
            devices_allowed = open(os.path.join(
                                   path, "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "After setup devices.list: %s" % devices_allowed)

    # Select devices to assign to the job
    def assign_devices(
                       self,
                       device_type,
                       device_list,
                       number_of_devices,
                       node):
        devices = device_list[:number_of_devices]
        device_ids = list()
        device_names = list()
        for device in devices:
            device_info = node.devices[device_type][device]
            if len(device_ids) == 0:
                if device.find("mic") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:0 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                    device_ids.append("%s %d:1 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                elif device.find("nvidia") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:255 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
            device_ids.append("%s %d:%d rwm" %
                              (device_info['device_type'],
                               device_info['major'],
                               device_info['minor']))
            device_names.append(device)
        return device_names, device_ids

    # Find the device name
    def get_device_name(self, node, available, socket, major, minor):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Get device name: major: %s, minor: %s" % (major, minor))
        avail_device = None
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Possible devices: %s" % (available[socket]['devices']))
        for avail_device in available[socket]['devices']:
            avail_major = None
            avail_minor = None
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Checking device: %s" % (avail_device))
            if avail_device.find('mic') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check mic device: %s" % (avail_device))
                avail_major = node.devices['mic'][avail_device]['major']
                avail_minor = node.devices['mic'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            elif avail_device.find('nvidia') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check gpu device: %s" % (avail_device))
                avail_major = node.devices['gpu'][avail_device]['major']
                avail_minor = node.devices['gpu'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            if int(avail_major) == int(major) and \
                    int(avail_minor) == int(minor):
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device match: name: %s, major: %s, minor: %s" %
                           (avail_device, major, minor))
                return avail_device
        pbs.logmsg(pbs.EVENT_DEBUG3, "No match found")
        return None

    # Assign resources to the job based off the vnodes assignments
    def assign_job_resources_by_vnode(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # Loop through the vnodes and assign resources
        assigned = dict()
        assigned['cpuset.cpus'] = list()
        assigned['cpuset.mems'] = list()
        room_on_socket = True

        for vnode in resources['vnodes']:
            regex = re.compile(".*\[(\d+)\].*")
            socket = int(regex.search(vnode).group(1))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current Vnode: %s" % vnode)
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current socket: %d" % socket)

            if 'ncpus' in resources['vnodes'][vnode]:
                tmp_ncpus = int(resources['vnodes'][vnode]['ncpus'])
                if int(resources['vnodes'][vnode]['ncpus']) <= \
                        len(available[socket]['cpus']):
                    assigned['cpuset.cpus'] += \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] += [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_ncpus: %s" % (tmp_ncpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "available[%d]: %s" %
                               (socket, available[socket]))
                    room_on_socket = False
            if ('nmics' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['nmics']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(mic).*")
                tmp_nmics = int(resources['vnodes'][vnode]['nmics'])
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['nmics']) > 0 and
                   int(resources['vnodes'][vnode]['nmics']) <= len(mics)):
                    tmp_names, tmp_devices = self.assign_devices(
                             'mic', mics[:tmp_nmics], tmp_nmics, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_nmics: %s" % (tmp_nmics))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "mics: %s" % (mics))
                    room_on_socket = False
            if ('ngpus' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['ngpus']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(nvidia).*")
                tmp_ngpus = int(resources['vnodes'][vnode]['ngpus'])
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['ngpus']) > 0 and
                   int(resources['vnodes'][vnode]['ngpus']) <= len(gpus)):
                    tmp_names, tmp_devices = self.assign_devices(
                        'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "tmp_ngpus: %s" % (tmp_ngpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "gpus: %s" % (gpus))
                    room_on_socket = False

        if room_on_socket:
            assigned['cpuset.cpus'].sort()
            assigned['cpuset.mems'].sort()
            if 'devices' in assigned:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids pre set and sort: %s" %
                           (assigned['devices']))
                assigned['devices'] = list(set(assigned['devices']))
                assigned['devices'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids post set and sort: %s" %
                           (assigned['devices']))
                assigned['devices_name'] = list(set(assigned['devices_name']))
                assigned['devices_name'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

    # Assign resources to the job
    def assign_job_resources(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # What would we need to do different for a large smp (UV) system?
        # See if requested resources can fit on a single socket
        assigned = dict()
        pbs.logmsg(pbs.EVENT_DEBUG3, "Requested: %s" % (resources))

        # Determine the order that we should evaluate the sockets.
        socket_list = list()
        if 'placement_type' in self.cfg:
            if self.cfg['placement_type'] == 'round_robin':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Requested round_robin placement")
                # Look at assigned_resources and determine which socket
                # to start with
                jobs_per_socket_cnt = dict()
                for socket in available.keys():
                    jobs_per_socket_cnt[socket] = 0
                for job in self.assigned_resources:
                    if 'cpuset.mems' in self.assigned_resources[job]:
                        for socket in \
                                self.assigned_resources[job]['cpuset.mems']:
                            jobs_per_socket_cnt[socket] += 1

                sorted_dict = sorted(jobs_per_socket_cnt.items(),
                                     key=operator.itemgetter(1))
                for x in sorted_dict:
                    socket_list.append(x[0])
        else:
            socket_list = available.keys()

        # Loop through the socket list and determine if the job
        # can fit on the socket
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Evaluate sockets in the following order: %s" %
                   (socket_list))
        for socket in socket_list:
            room_on_socket = True
            if 'ncpus' in resources:
                if int(resources['ncpus']) <= len(available[socket]['cpus']):
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Requested: %s, Available: %s" %
                               (resources['ncpus'], available[socket]['cpus']))
                    tmp_ncpus = int(resources['ncpus'])
                    assigned['cpuset.cpus'] = \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] = [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient ncpus on socket %d: R:%d,A:%s" %
                               (socket, resources['ncpus'],
                                available[socket]['cpus']))
                    room_on_socket = False
            # Check to see that nmics has been requested and not equal to 0
            if 'nmics' in resources and int(resources['nmics']) > 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'nmics' in resources and int(resources['nmics']) > 0 \
                        and int(resources['nmics']) <= len(mics):
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient nmics on socket %d: R:%s,A:%s" %
                               (socket, resources['nmics'], mics))
                    room_on_socket = False
            # Check to see that ngpus has been requested and not equal to 0
            if 'ngpus' in resources and int(resources['ngpus']) > 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'ngpus' in resources and int(resources['ngpus']) > 0 \
                        and int(resources['ngpus']) <= len(gpus):
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient gpus on socket %d: R:%s,A:%s" %
                               (socket, resources['ngpus'], gpus))
                    room_on_socket = False
            if 'vmem' in resources:
                if size_as_int(resources['vmem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient vmem on socket %d: R:%s,A:%s" %
                               (socket, resources['vmem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if 'mem' in resources:
                if size_as_int(resources['mem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient memory on socket %d: R:%s,A:%s" %
                               (socket, resources['mem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if room_on_socket:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
                self.host_assigned_resources = assigned
                return assigned
        # If we made it here then the requested resources did not fit
        # on a single socket
        # Future: take distance into account when selecting sockets?
        # Combine available resources into a single list
        # This should work for a dual socket machine.
        # Needs additional work for machines with more than 2 sockets
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Unable to find requested resources: %s" % resources +
                   " on a socket. Checking the entire node")
        available_copy = copy.deepcopy(available)
        available_on_node = dict()
        socket_keys = available.keys()
        socket_keys.sort()
        available_on_node['sockets'] = socket_keys
        for socket in socket_keys:
            for key in available_copy[socket].keys():
                if isinstance(available[socket][key], int):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]
                if isinstance(available_copy[socket][key], str):
                    try:
                        if key in available_on_node is False:
                            available_on_node[key] = \
                                size_as_int(available_copy[socket][key])
                        else:
                            available_on_node[key] += \
                                size_as_int(available_copy[socket][key])
                    except:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Unable to convert to int: %s" %
                                   (available_copy[socket][key]))

                if isinstance(available_copy[socket][key], list):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available on node: %s" % (available_on_node))
        assigned = dict()
        room_on_node = False
        if 'ncpus' in resources:
            if int(resources['ncpus']) <= len(available_on_node['cpus']):
                room_on_node = True
                tmp_ncpus = int(resources['ncpus'])
                assigned['cpuset.cpus'] = available_on_node['cpus'][:tmp_ncpus]
                assigned['cpuset.mems'] = available_on_node['sockets']
            if 'nmics' in resources and int(resources['nmics']) != 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['nmics']) <= len(mics) and \
                        int(resources['nmics']) > 0:
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
            if 'ngpus' in resources and int(resources['ngpus']) != 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['ngpus']) <= len(gpus) and \
                        int(resources['ngpus']) > 0:
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
        if room_on_node:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

        # Consolidate resources if needed to place the job

    # Determine which resources are available
    def available_node_resources(self, node):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))

        available = copy.deepcopy(node.numa_nodes)
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Keys: %s" % (available[0].keys()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        for socket in available.keys():
            if 'cpus' in available[socket]:
                # Find the physical cores
                if available[socket]['cpus'].find('-') != -1 and \
                        node.hyperthreading:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Splitting %s at ','" %
                               (available[socket]['cpus']))
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'].split(',')[0])
                else:
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'])
            if 'MemTotal' in available[socket]:
                # Find the memory on the socket in bites.
                # Remove the 'b' to simply math
                available[socket]['memory'] = size_as_int(
                    available[socket]['MemTotal'])

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Pre device add: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (node.devices.keys()))
        for device in node.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" %
                       (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Devices: %s" %
                           (node.devices[device].keys()))
                for device_name in node.devices[device].keys():
                    device_socket = \
                        node.devices[device][device_name]['numa_node']
                    if 'devices' not in available[device_socket]:
                        available[device_socket]['devices'] = list()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Device: %s, Socket: %s" %
                               (device, device_socket))
                    available[device_socket]['devices'].append(device_name)
            # pbs.logmsg(pbs.EVENT_DEBUG3,
            #            "Available on socket %s: %s" %
            #            (socket,available[socket]))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))

        # Remove all of the resources that are assigned to other jobs
        for job in self.assigned_resources.keys():
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            if (not job_to_be_ignored(job)):
                cpus = list()
                sockets = list()
                devices = list()
                memory = 0
                if 'cpuset.cpus' in self.assigned_resources[job]:
                    cpus = self.assigned_resources[job]['cpuset.cpus']
                if 'cpuset.mems' in self.assigned_resources[job]:
                    sockets = self.assigned_resources[job]['cpuset.mems']
                if 'devices.list' in self.assigned_resources[job]:
                    devices = self.assigned_resources[job]['devices.list']
                if 'memory.limit' in self.assigned_resources[job]:
                    memory = size_as_int(
                                self.assigned_resources[job]['memory.limit'])

                # Loop through the sockets and remove cpus that are
                # assigned to other cgroups
                for socket in sockets:
                    for cpu in cpus:
                        try:
                            available[socket]['cpus'].remove(cpu)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %d from %s" %
                                       (cpu, available[socket]['cpus']))

                if len(sockets) == 1:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Decrementing memory: %d by %d" %
                               (size_as_int(available[sockets[0]]['memory']),
                                memory))
                    available[sockets[0]]['memory'] -= memory

                # Loop throught the available sockets
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned device to %s: %s" % (job, devices))
                for socket in available.keys():
                    for device in devices:
                        try:
                            # loop through know devices and see if they match
                            if len(available[socket]['devices']) != 0:
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Check device: %s" % (device))
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Available device: %s" %
                                           (available[socket]['devices']))
                                major, minor = device.split()[1].split(':')
                                avail_device = self.get_device_name(
                                                   node, available, socket,
                                                   major, minor)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Returned device: %s" %
                                           (avail_device))
                                if avail_device is not None:
                                    pbs.logmsg(pbs.EVENT_DEBUG3,
                                               "socket: %d,\t" % socket +
                                               "devices: %s,\t" %
                                               available[socket]['devices'] +
                                               "device to remove: %s" %
                                               (avail_device))
                                    available[socket]['devices'].remove(
                                        avail_device)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %s from %s" %
                                       (device, available[socket]['devices']))
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Job %s res not removed from host " % job +
                           "available res: suspended job")
        pbs.logmsg(pbs.EVENT_DEBUG, "Available resources: %s" % (available))
        return available

    # Set a job limit
    def set_job_limit(self, jobid, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s: %s = %s" %
                   (caller_name(), jobid, resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'softmem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.soft_limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     jobid, 'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'], jobid,
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            elif resource == 'ncpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = self.select_cpus(path, value)
                    if cpus is None:
                        raise CgroupLimitError("Failed to configure cpuset.")
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
                    self.__copy_from_parent(os.path.join(self.paths['cpuset'],
                                            jobid, 'cpuset.mems'))
            elif resource == 'cpuset.cpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = value
                    if cpus is None:
                        msg = "Failed to configure cpus in cpuset."
                        raise CgroupLimitError(msg)
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
            elif resource == 'cpuset.mems':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.mems')
                    mems = value
                    if mems is None:
                        msg = "Failed to configure mems in cpuset."
                        raise CgroupLimitError(msg)
                    mems = ','.join(map(str, mems))
                    self.write_value(path, mems)
            elif resource == 'devices':
                if 'devices' in self.subsystems and \
                        self.cfg['cgroup']['devices']['enabled']:

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.allow')
                    devices = value
                    if devices is None:
                        msg = "Failed to configure device(s)."
                        raise CgroupLimitError(msg)
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Setting devices: %s for %s" % (devices, jobid))
                    for device in devices:
                        self.write_value(path, device)

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.list')
                    output = open(path, 'r').read()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "devices.list: %s" % output.replace("\n", ","))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    # Update resource usage for a job
    def update_job_usage(self, jobid, resc_used):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resc_used = %s" %
                   (caller_name(), str(resc_used)))
        # Sort the subsystems so that we consistently look at the subsystems
        # in the same order every time
        self.subsystems.sort()
        for subsys in self.subsystems:
            path = os.path.join(self.paths[subsys], jobid)
            if subsys == "memory":
                max_mem = self.__get_max_mem_usage(path)
                if max_mem is None:
                    pbs.logjobmsg(jobid, "%s: No max mem data" %
                                  (caller_name()))
                else:
                    resc_used['mem'] = pbs.size(convert_size(max_mem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: mem=%s" %
                                  (caller_name(), resc_used['mem']))
                mem_failcnt = self.__get_mem_failcnt(path)
                if mem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No mem fail count data" %
                                  (caller_name()))
                else:
                    # Check to see if the job exceeded its resource limits
                    if mem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memory limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "memsw":
                max_vmem = self.__get_max_memsw_usage(path)
                if max_vmem is None:
                    pbs.logjobmsg(jobid, "%s: No max vmem data" %
                                  (caller_name()))
                else:
                    resc_used['vmem'] = pbs.size(convert_size(max_vmem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: vmem=%s" %
                                  (caller_name(), resc_used['vmem']))

                vmem_failcnt = self.__get_memsw_failcnt(path)
                if vmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No vmem fail count data" %
                                  (caller_name()))
                else:
                    pbs.logjobmsg(jobid, "%s: vmem fail count: %d " %
                                  (caller_name(), vmem_failcnt))
                    if vmem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memsw limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "hugetlb":
                max_hpmem = self.__get_max_hugetlb_usage(path)
                if max_hpmem is None:
                    pbs.logjobmsg(jobid, "%s: No max hpmem data" %
                                  (caller_name()))
                    return
                hpmem_failcnt = self.__get_hugetlb_failcnt(path)
                if hpmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No hpmem fail count data" %
                                  (caller_name()))
                    return
                if hpmem_failcnt > 0:
                    err_msg = self.__get_error_msg(jobid)
                    pbs.logjobmsg(jobid, "Cgroup hugetlb limit exceeded: %s" %
                                  (err_msg))
                resc_used['hpmem'] = pbs.size(convert_size(max_hpmem, 'kb'))
                pbs.logjobmsg(jobid, "%s: Hugepage usage: %s" %
                              (caller_name(), resc_used['hpmem']))
            elif subsys == "cpuacct":
                cpu_usage = self.__get_cpu_usage(path)
                if cpu_usage is None:
                    pbs.logjobmsg(jobid, "%s: No CPU usage data" %
                                  (caller_name()))
                    return
                cpu_usage = convert_time(str(cpu_usage) + "ns")
                pbs.logjobmsg(jobid, "%s: CPU usage: %.3lf secs" %
                              (caller_name(), cpu_usage))
                resc_used['cput'] = pbs.duration(cpu_usage)

    # Creates the cgroup if it doesn't exists
    def create_job(self, jobid, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Iterate over the subsystems required
        for subsys in self.subsystems:
            # Create a directory for the job
            try:
                old_umask = os.umask(0022)
                path = self.paths[subsys]
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                path = os.path.join(self.paths[subsys], jobid)
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                if subsys == 'devices':
                    self.setup_subsys_devices(path, node)

            except OSError, exc:
                raise CgroupConfigError("Failed to create directory: %s (%s)" %
                                        (path, errno.errorcode[exc.errno]))
            except:
                raise
            finally:
                os.umask(old_umask)

    # Determine the cgroup limits and configure the cgroups
    def configure_job(self, jobid, hostresc, node):
        mem_enabled = 'memory' in self.subsystems
        vmem_enabled = 'memsw' in self.subsystems
        if mem_enabled or vmem_enabled:
            # Initialize mem variables
            mem_avail = node.get_memory_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "mem_avail %s" % mem_avail)
            mem_requested = None
            if 'mem' in hostresc.keys():
                mem_requested = convert_size(hostresc['mem'], 'kb')
            mem_default = None
            if mem_enabled:
                mem_default = self.default('memory')
            # Initialize vmem variables
            vmem_avail = node.get_vmem_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "vmem_avail %s" % vmem_avail)
            vmem_requested = None
            if 'vmem' in hostresc.keys():
                vmem_requested = convert_size(hostresc['vmem'], 'kb')
            vmem_default = None
            if vmem_enabled:
                vmem_default = self.default('memsw')
            # Initialize softmem variables
            if 'soft_limit' in self.cfg['cgroup']['memory'].keys():
                softmem_enabled = self.cfg['cgroup']['memory']['soft_limit']
            else:
                softmem_enabled = False
            # Sanity check
            if size_as_int(mem_avail) > size_as_int(vmem_avail):
                if size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                    raise CgroupLimitError(
                        'mem available (%s) exceeds vmem available (%s)' %
                        (mem_avail, vmem_avail))
            # Determine the mem limit
            if mem_requested is not None:
                # mem requested may not exceed available
                if size_as_int(mem_requested) > size_as_int(mem_avail):
                    raise JobValueError(
                        'mem requested (%s) exceeds mem available (%s)' %
                        (mem_requested, mem_avail))
                mem_limit = mem_requested
            else:
                # mem was not requested
                if mem_default is None:
                    mem_limit = mem_avail
                else:
                    mem_limit = mem_default
            # Determine the vmem limit
            if vmem_requested is not None:
                # vmem requested may not exceed available
                if size_as_int(vmem_requested) > size_as_int(vmem_avail):
                    raise JobValueError(
                        'vmem requested (%s) exceeds vmem available (%s)' %
                        (vmem_requested, vmem_avail))
                vmem_limit = vmem_requested
            else:
                # vmem was not requested
                if vmem_default is None:
                    vmem_limit = vmem_avail
                else:
                    vmem_limit = vmem_default
            # Ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Adjust for soft limits if enabled
            if mem_enabled and softmem_enabled:
                softmem_limit = mem_limit
                # The hard memory limit is assigned the lesser of the vmem
                # limit and available memory
                if size_as_int(vmem_limit) < size_as_int(mem_avail):
                    mem_limit = vmem_limit
                else:
                    mem_limit = mem_avail
            # Again, ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Sanity checks when both memory and memsw are enabled
            if mem_enabled and vmem_enabled:
                if vmem_requested is not None:
                    if size_as_int(vmem_limit) > size_as_int(vmem_requested):
                        # The user requested an invalid limit
                        raise JobValueError(
                            'vmem limit (%s) exceeds vmem requested (%s)' %
                            (vmem_limit, vmem_requested))
                if size_as_int(vmem_limit) > size_as_int(mem_limit):
                    # This job may utilize swap
                    if size_as_int(vmem_avail) <= size_as_int(mem_avail) and \
                      size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                        # No swap available
                        raise CgroupLimitError(
                            ('Job might utilize swap ' +
                             'and no swap space available'))
            # Assign mem and vmem
            if mem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: mem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), mem_limit))
                hostresc['mem'] = pbs.size(mem_limit)
                if softmem_enabled:
                    hostresc['softmem'] = pbs.size(softmem_limit)
            if vmem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: vmem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), vmem_limit))
                hostresc['vmem'] = pbs.size(vmem_limit)
        # Initialize hpmem variables
        hpmem_enabled = 'hugetlb' in self.subsystems
        if hpmem_enabled:
            hpmem_avail = node.get_hpmem_on_node(self.cfg)
            hpmem_limit = None
            hpmem_default = self.default('hugetlb')
            if hpmem_default is None:
                hpmem_default = hpmem_avail
            if 'hpmem' in hostresc.keys():
                hpmem_limit = convert_size(hostresc['hpmem'], 'kb')
            else:
                hpmem_limit = hpmem_default
            # Assign hpmem
            if size_as_int(hpmem_limit) > size_as_int(hpmem_avail):
                raise JobValueError('hpmem limit (%s) exceeds available (%s)' %
                                    (hpmem_limit, hpmem_avail))
            hostresc['hpmem'] = pbs.size(hpmem_limit)
        # Initialize cpuset variables
        cpuset_enabled = 'cpuset' in self.subsystems
        if cpuset_enabled:
            cpu_limit = 1
            if 'ncpus' in hostresc.keys():
                cpu_limit = hostresc['ncpus']
            if cpu_limit < 1:
                cpu_limit = 1
            hostresc['ncpus'] = pbs.pbs_int(cpu_limit)

        # Find the available resources and assign the right ones to the job

        attempt = 0
        assign_resources = False

        # Two attempts since self.cleanup_orphans may actually fix the
        # problem we see in a first attempt
        while attempt < 2:
            attempt += 1
            available_resources = self.available_node_resources(node)
            if 'vnodes' in hostresc:
                assign_resources = self.assign_job_resources_by_vnode(
                    hostresc, available_resources, node)
            else:
                assign_resources = self.assign_job_resources(
                    hostresc, available_resources, node)

            if (assign_resources is False) and (attempt < 2):
                # No resources were assigned to the job.
                # Most likely cause was that a cgroup has
                # not been cleaned up yet
                pbs.logmsg(pbs.EVENT_DEBUG, "Failed to assign resources. " +
                           "Checking the following cgroups on the node " +
                           "with the server: %s" % self.existing_cgroups)

                # Collect the jobs on the node (try reading mom_priv/jobs,
                # if not successful ask server)
                jobs_list = node.gather_jobs_on_node_local()

                if jobs_list is not None:
                    # Don't clean up the cgroup we just made for the
                    # job just yet
                    if jobid not in jobs_list:
                        jobs_list.append(jobid)

                    self.cleanup_orphans(jobs_list)

                    # Recheck what is now assigned after things were cleaned up
                    self.gather_assigned_resources()

        if assign_resources is False:
            # Cleanup cgroups for jobs not present on this node
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Assign resources failed. Attempting to cleanup all " +
                       "leftover cgroups (including event job %s)" % jobid)

            # Remove the current job from the jobs list so it will be
            # cleaned up as well.
            jobs_list.remove(jobid)

            self.cleanup_orphans(jobs_list)

            # Rerun the job and log the message
            line = 'Unable to assign resources to job. '
            line += 'Will requeue the job and try again.'
            line += 'job run_count: %d' % pbs.event().job.run_count
            pbs.event().job.rerun()
            pbs.event().reject(line)

        # Print out the assigned resources
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Assigned resources: %s" % (assign_resources))

        if cpuset_enabled:
            hostresc.pop('ncpus', None)
            for key in ['cpuset.cpus', 'cpuset.mems']:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Initialize devices variables
        devices_enabled = 'devices' in self.subsystems
        if devices_enabled:
            key = 'devices'
            if key in assign_resources:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Apply the resource limits to the cgroups
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Setting cgroup limits for: %s" %
                   (caller_name(), hostresc))

        # The vmem limit must be set after the mem limit, so sort the keys
        for resc in sorted(hostresc.keys()):
            self.set_job_limit(jobid, resc, hostresc[resc])

    # Kill any processes contained within a tasks file
    def __kill_tasks(self, tasks_file):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        with open(tasks_file, 'r') as fd:
            for line in fd:
                os.kill(int(line.strip()), signal.SIGKILL)
        fd.close()
        count = 0
        with open(tasks_file, 'r') as fd:
            for line in fd:
                count += 1
                pid = line.strip()
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Pid: %s did not clean up" %
                           (caller_name(), pid))
                if os.path.isfile('/proc/%s/status' % pid):
                    tmp = open('/proc/%s/status' % pid).readlines()
                    tmp_line = ''
                    for entry in tmp:
                        if entry.find('Name:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('State:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('Uid:') != -1:
                            tmp_line += entry.strip()
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: %s" % (caller_name(), tmp_line))

        return count

    # Perform the actual removal of the cgroup directory
    # by default, only one attempt at killing tasks in cgroup, without sleeping
    # since this method could be called many times
    # (for N directories times M jobs)
    def __remove_cgroup(self, path, tasks_kill_attempts=1):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            if os.path.exists(path):
                tasks_file = os.path.join(path, 'tasks')
                task_cnt = self.__kill_tasks(tasks_file)
                kill_attempts = 0
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Task Kill Attempts: %d" % tasks_kill_attempts)
                while (task_cnt > 0) and (kill_attempts < tasks_kill_attempts):
                    kill_attempts += 1
                    count = 0

                    with open(tasks_file, 'r') as fd:
                        for line in fd:
                            count += 1
                        task_cnt = count

                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "Attempt: %d - cgroup still has %d tasks: %s" %
                               (kill_attempts, task_cnt, path))
                    if kill_attempts < tasks_kill_attempts:
                        time.sleep(1)
                        # Added since there are race conditions where tasks are
                        # being added at the same time we get the initial
                        # task count
                        tasks_file = os.path.join(path, 'tasks')
                        task_cnt = self.__kill_tasks(tasks_file)

                if task_cnt == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Removing directory %s" %
                               (caller_name(), path))
                    os.rmdir(path)
                    if os.path.exists(path):
                        time.sleep(.25)
                        if os.path.exists(path):
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "%s: 2nd Try Removing directory %s" %
                                       (caller_name(), path))
                            os.rmdir(path)
                            time.sleep(.25)
                        if os.path.exists(path):
                            return False
                else:
                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "cgroup still has %d tasks: %s" %
                               (task_cnt, path))

                    # Check to see if the offline file is already present
                    # on the mom
                    offline_file_exists = False
                    if os.path.isfile(self.offline_file):
                        msg = "Cgroup(s) not cleaning up but the node already "
                        msg += "has the offline file."

                        pbs.logmsg(pbs.EVENT_DEBUG, msg)
                    else:
                        # Check to see that the node is not already offline.
                        try:
                            tmp_state = pbs.server().vnode(self.hostname).state
                        except:
                            msg = "Unable to contact server for node state"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            tmp_state = None
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Current Node State: %d" % tmp_state)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Offline Node State: %d" % pbs.ND_OFFLINE)

                        if tmp_state == pbs.ND_OFFLINE:
                            msg = "Cgroup(s) not cleaning up but the node is "
                            msg += "already offline."
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                        elif tmp_state is not None:
                            # Offline the node(s)
                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                            msg = "Offlining node since cgroup(s) are not "
                            msg += "cleaning up"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            vnode = pbs.event().vnode_list[self.hostname]

                            vnode.state = pbs.ND_OFFLINE

                            # Write a file locally to reduce server traffic
                            # when it comes time to online the node
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Offline file: %s" % self.offline_file)
                            open(self.offline_file, 'a').close()
                            vnode.comment = self.offline_msg

                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                        else:
                            pass

                    return False

        except OSError, exc:
            pbs.logmsg(pbs.EVENT_SYSTEM,
                       "Failed to remove cgroup path: %s (%s)" %
                       (path, errno.errorcode[exc.errno]))
        except:
            pbs.logmsg(pbs.EVENT_SYSTEM, "Failed to remove cgroup path: %s" %
                       (path))

        return True

    # Removes cgroup directories that are not associated with a local job
    def cleanup_orphans(self, local_jobs):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        orphan_cnt = 0
        for key in self.paths.keys():
            for dir in glob.glob(os.path.join(self.paths[key], '[0-9]*')):
                jobid = os.path.basename(dir)
                if jobid not in local_jobs:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Removing orphaned cgroup: %s" %
                               (caller_name(), dir))
                    # default number of attempts at killing tasks
                    status = self.__remove_cgroup(dir)
                    if not status:
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "%s: Removing orphaned cgroup %s failed " %
                                   (caller_name(), dir))
                        orphan_cnt += 1
        return orphan_cnt

    # Removes the cgroup directories for a job
    def delete(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        tasks_kill_attempted = False

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            if not tasks_kill_attempted:
                # Make multiple attempts at killing tasks in cgroups on
                # first subsystem encountered since it is "normal" for
                # a cgroup.delete to encounter processes hard to kill
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                                           jobid),
                                              tasks_kill_attempts=(
                                              CGROUP_KILL_ATTEMPTS))
                tasks_kill_attempted = True
            else:
                # We tried getting rid of job processes earlier,
                # so don't try N times with sleeps
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                              jobid), tasks_kill_attempts=1)

            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Status: %s" %
                       (caller_name(), status))
            if status is False:
                # Rerun the job and log the message
                line = 'Unable to cleanup a cgroup. '
                line += 'Will offline the node and requeue the job '
                line += 'and try again. job run_count: %d' % \
                        pbs.event().job.run_count
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Failed to remove cgroup: %s" %
                           (caller_name(), line))
                pbs.event().accept()

    # Write a value to a limit file
    def write_value(self, file, value, mode='w'):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: writing %s to %s" %
                   (caller_name(), value, file))
        try:
            with open(file, mode) as fd:
                fd.write(str(value) + '\n')
        except IOError, exc:
            if exc.errno == errno.ENOENT:
                pbs.logmsg(pbs.EVENT_SYSTEM,
                           "Trying to set limit to unknown file %s" % file)
            elif exc.errno in [errno.EACCES, errno.EPERM]:
                pbs.logmsg(pbs.EVENT_SYSTEM, "Permission denied on file %s" %
                           file)
            elif exc.errno == errno.EBUSY:
                raise CgroupBusyError("Limit rejected")
            elif exc.errno == errno.EINVAL:
                raise CgroupLimitError("Invalid limit value: %s" % (value))
            else:
                raise
        except:
            raise

    # Return memory failcount
    def __get_mem_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.failcnt"), 'r').read().strip())
        except:
            return None

    # Return vmem failcount
    def __get_memsw_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.failcnt"), 'r').read().strip())
        except:
            return None

    # Return hpmem failcount
    def __get_hugetlb_failcnt(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.failcnt"))[0], 'r').read().strip())
        except:
            return None

    # Return the max usage of memory in bytes
    def __get_max_mem_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of memsw in bytes
    def __get_max_memsw_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of hugetlb in bytes
    def __get_max_hugetlb_usage(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.max_usage_in_bytes"))[0],
                            'r').read().strip())
        except:
            return None

    # Return the cpuacct.usage in cpu seconds
    def __get_cpu_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "cpuacct.usage"), 'r').read().strip())
        except:
            return None

    # Assign CPUs to the cpuset
    def select_cpus(self, path, ncpus):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path is %s" % (caller_name(), path))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: ncpus is %s" %
                   (caller_name(), ncpus))
        if ncpus < 1:
            ncpus = 1
        try:
            # Must select from those currently available
            cpufile = os.path.basename(path)
            base = os.path.dirname(path)
            parent = os.path.dirname(base)
            avail = cpus2list(open(os.path.join(parent, cpufile),
                              'r').read().strip())
            if len(avail) < 1:
                raise CgroupProcessingError("No CPUs avaialble in cgroup.")
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Available CPUs: %s" %
                       (caller_name(), avail))
            assigned = []
            for file in glob.glob(os.path.join(parent, '[0-9]*', cpufile)):
                cpus = cpus2list(open(file, 'r').read().strip())
                for id in cpus:
                    if id in avail:
                        avail.remove(id)
            if len(avail) < ncpus:
                raise CgroupProcessingError(
                    "Insufficient CPUs avaialble in cgroup.")
            if len(avail) == ncpus:
                return avail
            # FUTURE: Try to minimize NUMA nodes based on memory requirement
            return avail[0:ncpus]
        except:
            raise

    # Return the error message in system message file
    def __get_error_msg(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        proc = subprocess.Popen(['dmesg'],
                                shell=False, stdout=subprocess.PIPE)
        stdout = proc.communicate()[0].splitlines()
        stdout.reverse()
        # Check to see if the job id is found in dmesg otherwise
        # you could get the information for another job that was killed
        # the line will look like Memory cgroup stats for /pbspro/279.centos7
        kill_line = ""

        for line in stdout:
            start = line.find('Killed process ')
            if start >= 0:
                kill_line = line[start:]
            job_start = line.find(jobid)
            if job_start >= 0:
                # pbs.logmsg(pbs.EVENT_DEBUG3,
                #           "%s: Jobid: %s, Line: %s" %
                #           (caller_name(), jobid, line))
                return kill_line
        return ""

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_job_env_file(self, jobid, env_list):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        # if not os.path.exists(self.host_job_env_dir):
        #    os.makedirs(self.host_job_env_dir, 0755)

        # Write out assigned_resources
        try:
            lines = string.join(env_list, '\n')
            filename = self.host_job_env_filename % jobid
            outfile = open(filename, "w")
            outfile.write(lines)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" % (filename))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (lines))
            return True
        except:
            return False

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        if not os.path.exists(self.hook_storage_dir):
            os.makedirs(self.hook_storage_dir, 0755)

        # Write out assigned_resources
        try:
            json_str = json.dumps(self.host_assigned_resources)
            outfile = open(self.hook_storage_dir+os.sep+jobid, "w")
            outfile.write(json_str)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" %
                       (self.hook_storage_dir+os.sep+jobid))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (json_str))
            return True
        except:
            return False

    def read_in_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        pbs.logmsg(pbs.EVENT_DEBUG3, "Host assigned resources: %s" %
                   (self.host_assigned_resources))
        if os.path.isfile(self.hook_storage_dir+os.sep+jobid):
            # Write out assigned_resources
            try:
                infile = open(self.hook_storage_dir+os.sep+jobid, 'r')
                json_data = json.load(infile, object_hook=decode_dict)
                self.host_assigned_resources = json_data
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Host assigned resources: %s" %
                           (self.host_assigned_resources))
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
            finally:
                infile.close()

        if self.host_assigned_resources is not None:
            return True
        else:
            return False

#
#
# FUNCTION main
#


def main():
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Function called" % (caller_name()))
    hostname = pbs.get_local_nodename()
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Host is %s" % (caller_name(), hostname))

    # Log the hook event type
    e = pbs.event()
    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook name is %s" %
               (caller_name(), e.hook_name))

    try:
        # Instantiate the hook utility class
        hooks = HookUtils()
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Event type is %s" %
                   (caller_name(), hooks.event_name(e.type)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(hooks)))
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: Failed to instantiate hook utility class" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        e.accept()

    if not hooks.hashandler(e.type):
        # Bail out if there is no handler for this event
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s event not handled by this hook" %
                   (caller_name(), hooks.event_name(e.type)))
        e.accept()

    try:
        # If an exception occurs, jobutil must be set
        jobutil = None
        # Instantiate the job utility class first so jobutil can be accessed
        # by the exception handlers.
        if hasattr(e, 'job'):
            jobutil = JobUtils(e.job)
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Job information class instantiated" %
                       (caller_name()))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" %
                       (caller_name(), repr(jobutil)))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Event does not include a job" %
                       (caller_name()))
        # Instantiate the cgroup utility class
        vnode = None
        if hasattr(e, 'vnode_list'):
            if hostname in e.vnode_list.keys():
                vnode = e.vnode_list[hostname]
        cgroup = CgroupUtils(hostname, vnode)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Cgroup utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(cgroup)))
        # Bail out if there is nothing to do
        if len(cgroup.subsystems) < 1:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: All cgroups disabled" %
                       (caller_name()))
            e.accept()

        # Call the appropriate handler
        if hooks.invoke_handler(e, cgroup, jobutil) is True:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned success" %
                       (caller_name()))
            e.accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned failure" %
                       (caller_name()))
            e.reject()

    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass

    except AdminError, exc:
        # Something on the system is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Admin error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except UserError, exc:
        # User must correct problem and resubmit job
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("User error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.delete()
                msg += " (deleted)"
            except:
                msg += " (delete failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except JobValueError, exc:
        # Something in PBS is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Job value error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except Exception, exc:
        # Catch all other exceptions and report them.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Unexpected error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

# line below required for unittesting
if __name__ == "__builtin__":
    start_time = time.time()
    try:
        main()
    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
    finally:
        pbs.logmsg(pbs.EVENT_DEBUG, "Elapsed time: %0.4lf" %
                   (time.time() - start_time))
---
2016-07-18 16:56:47,816 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups.json', 'content-encoding': 'default'}
2016-07-18 16:56:47,816 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups.json
2016-07-18 16:56:47,878 INFO     tmp_cfg: {
        "cgroup_prefix"         : "pbspro",
        "periodic_resc_update"  : true, 
        "exclude_hosts"         : ["centos7"],
        "exclude_vntypes"       : [],
        "run_only_on_hosts"     : [],   
        "vnode_per_numa_node"   : false,
        "online_offlined_nodes" : true, 
        "cgroup":
        {
                "cpuacct":
                {
                        "enabled"               : true,
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : []
                },
                "cpuset":
                {
                        "enabled"               : true,
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : []
                },
                "devices":
                {
                        "enabled"               : false,
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : [],
                        "allow" : ["b *:* rwm","c *:* rwm", ["mic/scif","rwm"],["nvidiactl","rwm", "*"],["nvidia-uvm","rwm"]]
                },
                "hugetlb":
                {
                        "enabled"               : false,
                        "default"               : "0MB",
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : []
                },
                "memory":
                {
                        "enabled"               : true,
                        "default"               : "256MB",
                        "reserve_memory"        : "0MB",
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : []
                },
                "memsw":
                {
                        "enabled"               : true,
                        "default"               : "256MB",
                        "reserve_memory"        : "2gb",
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : []
                }
        }
}

2016-07-18 16:56:47,878 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/tmp/t1_l1.json', 'content-encoding': 'default'}
2016-07-18 16:56:47,879 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /tmp/t1_l1.json
2016-07-18 16:56:47,905 ERROR    err: ["hook 't1_l1' contents overwritten"]
2016-07-18 16:56:47,906 INFO     job: executable set to /bin/sleep with arguments: 100
2016-07-18 16:56:47,907 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0755 /tmp/PtlPbsJobScriptm9UfjE
2016-07-18 16:56:47,924 INFOCLI  centos7: sudo -H -u pbsuser /usr/local/bin/qsub -N test_t4b -l select=1:ncpus=1:mem=300mb /tmp/PtlPbsJobScriptm9UfjE
2016-07-18 16:56:47,941 INFO     submit to centos7 as pbsuser: job 380.centos7 OrderedDict([('Job_Name', 'test_t4b'), ('Resource_List.select', '1:ncpus=1:mem=300mb')])
2016-07-18 16:56:47,941 INFOCLI  job script /tmp/PtlPbsJobScriptm9UfjE
---
#!/usr/bin/env python

import os
import subprocess
from subprocess import Popen, PIPE
import time

eat_mem_c = """
#include <stdio.h>
#include <stdlib.h>
#include <string.h>


#define ONE_GB (0x40000000L)
#define ONE_MB (0x100000L)

void
eatmem(total_mb, block_mb)
{
	int i;
	char *buf;
	size_t block;

	for (i=1, buf=NULL, block=(block_mb*ONE_MB); (i*block) < (total_mb*ONE_MB); i++) {
		buf = realloc((void *)buf, (i*block));
		memset(buf+((i-1)*block), 0, block);
	}
	return;
}

int
main(int argc, char *argv[])
{
	int total_mb = 8;
	int block_mb = 1;

	if (argc > 1)
		total_mb = atoi(argv[1]);

	if (argc > 2)
		block_mb = atoi(argv[2]);

	eatmem(total_mb, block_mb);
	printf("Initialized %dMB in blocks of %dMB\\n", total_mb, block_mb);
}
"""

fname = "/tmp/pbs_pp325_eat_mem.c"
fname_exe = "/tmp/pbs_pp325_eat_mem"
fout = open(fname, "w")
fout.write(eat_mem_c)
fout.close()

print os.getppid()

p = Popen(["gcc", "-o", fname_exe, fname], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

p = Popen([fname_exe, "400", "10"], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

os.remove(fname)
os.remove(fname_exe)
time.sleep(1)

---
2016-07-18 16:56:47,942 INFOCLI  centos7: /usr/local/bin/qstat -f 380.centos7
2016-07-18 16:56:48,038 INFO     expect on server centos7: job_state = R job 380.centos7  got: job_state = Q
2016-07-18 16:56:48,540 INFOCLI  centos7: /usr/local/bin/qmgr -c set server scheduling=True
2016-07-18 16:56:48,554 INFOCLI  centos7: /usr/local/bin/qstat -f 380.centos7
2016-07-18 16:56:48,646 INFO     expect on server centos7: job_state = R job 380.centos7 attempt: 2 ...  OK
2016-07-18 16:56:48,647 INFO     mom centos7 log match: searching for ".*exclude host centos7 is in \['centos7'\].*" - using regular expression ... OK
2016-07-18 16:56:48,647 INFO     ==============================
2016-07-18 16:56:48,647 INFO      Entered CgroupsTests tearDown
2016-07-18 16:56:48,648 INFO     ==============================
2016-07-18 16:56:48,648 INFO     ===============================
2016-07-18 16:56:48,648 INFO     Completed CgroupsTests tearDown
2016-07-18 16:56:48,648 INFO     ===============================
2016-07-18 16:56:48,648 INFO     ok

2016-07-18 16:56:48,648 INFO     test name: test_t4c (tests.cgroups_tests.CgroupsTests)...
2016-07-18 16:56:48,648 INFO     test start time: Mon Jul 18 16:56:48 2016
2016-07-18 16:56:48,648 INFO     test docstring: 
 Test to verify that cgroups subsystems are not enforced on nodes
            that have the exclude_hosts set
        
2016-07-18 16:56:48,648 INFO     ===========================
2016-07-18 16:56:48,649 INFO      Entered CgroupsTests setUp
2016-07-18 16:56:48,649 INFO     ===========================
2016-07-18 16:56:48,649 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:48,702 INFO     status on centos7: server
2016-07-18 16:56:48,703 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:48,763 INFO     manager on centos7: set server {'managers': (3, 'root@*')}
2016-07-18 16:56:48,763 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers-=root@*
2016-07-18 16:56:48,785 INFO     manager on centos7: set server {'managers': (2, 'root@*')}
2016-07-18 16:56:48,785 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers+=root@*
2016-07-18 16:56:48,814 INFO     server centos7: reverting configuration to defaults
2016-07-18 16:56:48,814 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:48,870 INFO     select on centos7: __ALL__
2016-07-18 16:56:48,871 INFOCLI  centos7: /usr/local/bin/qselect
2016-07-18 16:56:48,885 INFO     delete job on centos7: 380.centos7
2016-07-18 16:56:48,885 INFOCLI  centos7: /usr/local/bin/qdel -W force 380.centos7
2016-07-18 16:56:48,970 INFOCLI  centos7: /usr/local/bin/qstat -f 380.centos7
2016-07-18 16:56:49,132 INFOCLI  centos7: /usr/local/bin/qstat -f @centos7.usa.org
2016-07-18 16:56:49,198 INFO     expect on server centos7: job_state set 0 job ...  OK
2016-07-18 16:56:49,198 INFOCLI  centos7: /usr/local/bin/pbs_rstat -f
2016-07-18 16:56:49,217 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:56:49,258 INFO     manager on centos7: delete hook t1_l1
2016-07-18 16:56:49,259 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c delete hook t1_l1
2016-07-18 16:56:49,286 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:56:49,312 INFO     expect on server centos7:  unset  hook t1_l1 ...  OK
2016-07-18 16:56:49,313 INFOCLI  centos7: /usr/local/bin/qstat -Qf @centos7.usa.org
2016-07-18 16:56:49,327 INFO     manager on centos7: delete queue workq
2016-07-18 16:56:49,328 INFOCLI  centos7: /usr/local/bin/qmgr -c delete queue workq
2016-07-18 16:56:49,347 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:56:49,360 INFO     expect on server centos7:  unset  queue workq ...  OK
2016-07-18 16:56:49,360 INFO     manager on centos7: create queue workq {'started': 'True', 'queue_type': 'Execution', 'enabled': 'True'}
2016-07-18 16:56:49,361 INFOCLI  centos7: /usr/local/bin/qmgr -c create queue workq started=True,queue_type=Execution,enabled=True
2016-07-18 16:56:49,380 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:56:49,401 INFO     expect on server centos7: started set True || queue_type set Execution || enabled set True queue workq ...  OK
2016-07-18 16:56:49,401 INFO     manager on centos7: set server {'log_events': '511', 'default_queue': 'workq'}
2016-07-18 16:56:49,401 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=511,default_queue=workq
2016-07-18 16:56:49,424 INFO     status on centos7: resource
2016-07-18 16:56:49,424 INFO     manager on centos7: list resource
2016-07-18 16:56:49,425 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource
2016-07-18 16:56:49,490 INFO     manager on centos7: delete resource nmics,ngpus
2016-07-18 16:56:49,490 INFOCLI  centos7: /usr/local/bin/qmgr -c delete resource nmics,ngpus
2016-07-18 16:56:49,571 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource nmics
2016-07-18 16:56:49,591 INFO     expect on server centos7:  unset  resource nmics ...  OK
2016-07-18 16:56:49,591 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource ngpus
2016-07-18 16:56:49,627 INFO     expect on server centos7:  unset  resource ngpus ...  OK
2016-07-18 16:56:49,627 INFOCLI  status on centos7: server license_count
2016-07-18 16:56:49,628 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:49,683 INFO     server: centos7.usa.org licensed
2016-07-18 16:56:49,735 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/server_priv/comm.lock
2016-07-18 16:56:49,791 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:56:49,804 INFO     scheduler centos7: reverting configuration to defaults
2016-07-18 16:56:49,804 INFO     manager on centos7: list sched
2016-07-18 16:56:49,804 INFOCLI  centos7: /usr/local/bin/qmgr -c list sched
2016-07-18 16:56:49,824 INFO     manager on centos7: unset sched ['sched_cycle_length']
2016-07-18 16:56:49,824 INFOCLI  centos7: /usr/local/bin/qmgr -c unset sched sched_cycle_length
2016-07-18 16:56:49,840 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/dedicated_time
2016-07-18 16:56:49,852 INFOCLI2 centos7: cmp /usr/local/etc/pbs_resource_group /var/spool/pbs/sched_priv/resource_group
2016-07-18 16:56:49,855 INFO     scheduler centos7: reverting holidays file to default
2016-07-18 16:56:49,856 INFOCLI2 centos7: cmp /usr/local/etc/pbs_holidays /var/spool/pbs/sched_priv/holidays
2016-07-18 16:56:49,859 INFOCLI2 centos7: cmp /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:49,862 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:49,872 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:49,885 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:56:49,885 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:56:49,940 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:56:49,953 INFOCLI2 centos7: sudo -H cmp /usr/local/sbin/pbs_mom /usr/local/sbin/pbs_mom.cpuset
2016-07-18 16:56:49,990 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:56:49,999 INFO     mom centos7: reverting configuration to defaults
2016-07-18 16:56:50,000 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/epilogue
2016-07-18 16:56:50,017 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/prologue
2016-07-18 16:56:50,027 INFOCLI2 centos7: sudo -H ls /var/spool/pbs/mom_priv/config.d
2016-07-18 16:56:50,039 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbskiWmtb /var/spool/pbs/mom_priv/config
2016-07-18 16:56:50,052 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/mom_priv/config
2016-07-18 16:56:50,062 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/mom_priv/config
2016-07-18 16:56:50,072 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/mom_priv/config
2016-07-18 16:56:50,090 INFO     mom centos7: sent signal -HUP
2016-07-18 16:56:50,090 INFOCLI2 centos7: sudo -H kill -HUP 42976
2016-07-18 16:56:50,126 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:56:50,136 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 16:56:50,155 INFO     status on centos7: node centos7
2016-07-18 16:56:50,155 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:50,168 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:56:50,185 INFO     expect on server centos7: state = free node centos7 ...  OK
2016-07-18 16:56:50,185 INFO     ============================
2016-07-18 16:56:50,185 INFO     Completed CgroupsTests setUp
2016-07-18 16:56:50,186 INFO     ============================
2016-07-18 16:56:50,186 INFO     server centos7: server operating mode set to cli
2016-07-18 16:56:50,186 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:50,249 INFO     expect on server centos7: pbs_version ~ .*14.* server centos7.usa.org ...  OK
2016-07-18 16:56:50,249 INFO     manager on centos7: set server {'log_events': '2047'}
2016-07-18 16:56:50,249 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=2047
2016-07-18 16:56:50,266 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:56:50,321 INFO     expect on server centos7: log_events set 2047 server centos7.usa.org ...  OK
2016-07-18 16:56:50,322 INFO     scheduler centos7: config {'resources': 'ncpus,mem,host,vnode,vmem'}
2016-07-18 16:56:50,322 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /var/spool/pbs/sched_priv/sched_config /var/spool/pbs/sched_priv/sched_config.bak
2016-07-18 16:56:50,338 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsvkB7Ee /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:50,353 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:50,363 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:50,385 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:56:50,396 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:56:50,397 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:56:50,407 INFO     scheduler centos7 log match: searching for "Error reading line" - No match
2016-07-18 16:56:51,409 INFO     manager on centos7 as root: create resource nmics {'flag': 'nh', 'type': 'long'}
2016-07-18 16:56:51,410 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource nmics flag=nh,type=long
2016-07-18 16:56:51,443 INFO     manager on centos7 as root: create resource ngpus {'flag': 'nh', 'type': 'long'}
2016-07-18 16:56:51,443 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource ngpus flag=nh,type=long
2016-07-18 16:56:51,473 INFO     Test Summary: test_t1_l1 tests "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" cgroup hook
2016-07-18 16:56:51,474 INFO     status on centos7: hook
2016-07-18 16:56:51,474 INFO     manager on centos7: list hook
2016-07-18 16:56:51,474 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:56:51,492 INFO     status on centos7: hook
2016-07-18 16:56:51,493 INFO     manager on centos7: list hook
2016-07-18 16:56:51,493 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:56:51,523 INFO     manager on centos7: create hook t1_l1
2016-07-18 16:56:51,524 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c create hook t1_l1
2016-07-18 16:56:51,554 INFO     manager on centos7: set hook t1_l1 {'freq': 2, 'enabled': 'True', 'event': '"execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"'}
2016-07-18 16:56:51,554 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set hook t1_l1 freq=2,enabled=True,event="execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"
2016-07-18 16:56:51,582 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:56:51,609 INFO     expect on server centos7: freq set 2 || enabled set True || event set "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" hook t1_l1 ...  OK
2016-07-18 16:56:51,610 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-python', 'input-file': '/tmp/PtlPbs8k9USz', 'content-encoding': 'default'}
2016-07-18 16:56:51,610 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-python default /tmp/PtlPbs8k9USz
2016-07-18 16:56:51,676 INFO     server centos7: imported hook body
---
#  Copyright (C) 2003-2016 Altair Engineering, Inc. All rights reserved.
#
#  ALTAIR ENGINEERING INC. Proprietary and Confidential. Contains Trade Secret
#  Information. Not for use or disclosure outside ALTAIR and its licensed
#  clients. Information contained herein shall not be decompiled, disassembled,
#  duplicated or disclosed in whole or in part for any purpose. Usage of the
#  software is only as explicitly permitted in the end user software license
#  agreement.
#
#  Copyright notice does not imply publication.

# Last Updated
# Date: 07-06-2016
#
# When soft_limit is true for memory, memsw represents the hard limit.
#
# The resources value in sched_config must contain entries for mem and
# vmem if those subsystems are enabled in the hook configuration file. The
# amount of resource requested will not be avaiable to the hook if they
# are not present.

# This hook handles all of the operations necessary for PBS to support
# cgroups on linux hosts that support them (kernel 2.6.28 and higher)
#
# This hook services the following events:
# - exechost_periodic
# - exechost_startup
# - execjob_attach
# - execjob_begin
# - execjob_end
# - execjob_epilogue
# - execjob_launch

# Imports from __future__ must happen first
from __future__ import with_statement

# Additional imports
import sys
import os
import errno
import signal
import subprocess
import re
import glob
import time
import string
import platform
import traceback
import copy
from stat import S_ISBLK, S_ISCHR
import operator
import pwd
import pbs
import fnmatch
import socket

try:
    import json
except:
    import simplejson as json

CGROUP_KILL_ATTEMPTS = 12

# Flag to turn off features not in 12.2.40X branch
CRAY_12_BRANCH = False


# ============================================================================
# Derived error classes
# ============================================================================

# Base class for errors fixable only by administrative action.


class AdminError(Exception):
    pass

# Base class for errors in processing, unknown cause.


class ProcessingError(Exception):
    pass

# Base class for errors fixable by the user.


class UserError(Exception):
    pass

# Errors in PBS job resource values.


class JobValueError(UserError):
    pass

# Errors when the cgroup is busy.


class CgroupBusyError(ProcessingError):
    pass

# Errors in configuring cgroup.


class CgroupConfigError(AdminError):
    pass

# Errors in configuring cgroup.


class CgroupLimitError(AdminError):
    pass

# Errors processing cgroup.


class CgroupProcessingError(ProcessingError):
    pass

# ============================================================================
# Utility functions
# ============================================================================

#
# FUNCTION caller_name
#
# Return the name of the calling function or method.
#


def caller_name():
    return str(sys._getframe(1).f_code.co_name)

#
# FUNCTION convert_size
#
# Convert a string containing a size specification (e.g. "1m") to a
# string using different units (e.g. "1024k").
#
# This function only interprets a decimal number at the start of the string,
# stopping at any unrecognized character and ignoring the rest of the string.
#
# When down-converting (e.g. MB to KB), all calculations involve integers and
# the result returned is exact. When up-converting (e.g. KB to MB) floating
# point numbers are involved. The result is rounded up. For example:
#
# 1023MB -> GB yields 1g
# 1024MB -> GB yields 1g
# 1025MB -> GB yields 2g  <-- This value was rounded up
#
# Pattern matching or conversion may result in exceptions.
#


def convert_size(value, units='b'):
    logs = {'b': 0, 'k': 10, 'm': 20, 'g': 30,
            't': 40, 'p': 50, 'e': 60, 'z': 70, 'y': 80}
    try:
        new = units[0].lower()
        if new not in logs.keys():
            new = 'b'
        val, old = re.match('([-+]?\d+)([bkmgtpezy]?)',
                            str(value).lower()).groups()
        val = int(val)
        if val < 0:
            raise ValueError('Value may not be negative')
        if old not in logs.keys():
            old = 'b'
        factor = logs[old] - logs[new]
        val *= 2 ** factor
        slop = val - int(val)
        val = int(val)
        if slop > 0:
            val += 1
        # pbs.size() does not like units following zero
        if val <= 0:
            return '0'
        else:
            return str(val) + new
    except:
        return None

#
# FUNCTION size_as_int
#
# Convert a size string to an integer representation of size in bytes
#


def size_as_int(value):
    return int(convert_size(value).rstrip(string.ascii_lowercase))

#
# FUNCTION convert_time
#
# Converts a integer value for time into the value of the return unit
#
# A valid decimal number, with optional sign, may be followed by a character
# representing a scaling factor.  Scaling factors may be either upper or
# lower case. Examples include:
# 250ms
# 40s
# +15min
#
# Valid scaling factors are:
# ns  = 10**-9
# us  = 10**-6
# ms  = 10**-3
# s   =      1
# min =     60
# hr  =   3600
#
# Pattern matching or conversion may result in exceptions.
#


def convert_time(value, return_unit='s'):
    multipliers = {'':  1, 'ns': 10 ** -9, 'us': 10 ** -6,
                   'ms': 10 ** -3, 's': 1, 'min': 60, 'hr': 3600}
    n, factor = re.match('([-+]?\d+)(\w*[a-zA-Z])?',
                         str(value).lower()).groups(0)

    # Check to see if there was not unit of time specified
    if factor == 0:
        factor = ''

    # Check to see if the unit is valid
    if str.lower(factor) not in multipliers.keys():
        raise ValueError('Time unit not recognized.')

    # Convert the value to seconds
    value_in_sec = float(n) * float(multipliers[str.lower(factor)])
    if return_unit != 's':
        return value_in_sec / multipliers[str.lower(return_unit)]
    else:
        return float('%lf' % value_in_sec)

# simplejson hook to convert lists from unicode to utf-8


def decode_list(data):
    rv = []
    for item in data:
        if isinstance(item, unicode):
            item = item.encode('utf-8')
        elif isinstance(item, list):
            item = decode_list(item)
        elif isinstance(item, dict):
            item = decode_dict(item)
        rv.append(item)
    return rv

# simplejson hook to convert dictionaries from unicode to utf-8


def decode_dict(data):
    rv = {}
    for key, value in data.iteritems():
        if isinstance(key, unicode):
            key = key.encode('utf-8')
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        elif isinstance(value, list):
            value = decode_list(value)
        elif isinstance(value, dict):
            value = decode_dict(value)
        rv[key] = value
    return rv

# Convert CPU list format (with ranges) to a Python list


def cpus2list(s):
    # The input string is a comma separated list of digits and ranges.
    # Examples include:
    # 0-3,8-11
    # 0,2,4,6
    # 2,5-7,10
    cpus = []
    if s.strip() == '':
        return cpus
    for r in s.split(','):
        if '-' in r[1:]:
            start, end = r.split('-', 1)
            for i in range(int(start), int(end) + 1):
                cpus.append(int(i))
        else:
            cpus.append(int(r))
    return cpus


# Helper function to ignore suspended jobs when removing
# "already used" resources
def job_to_be_ignored(jobid):
    # Get job substate from small utility that calls printjob
    pbs_exec = ''
    if not CRAY_12_BRANCH:
        pbs_conf = pbs.get_pbs_conf()
        pbs_exec = pbs_conf['PBS_EXEC']
    else:
        if 'PBS_EXEC' in self.cfg:
            pbs_exec = self.cfg['PBS_EXEC']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "PBS_EXEC needs to be defined in the config file")
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Exiting the cgroups hook")
            pbs.accept()

    printjob_cmd = pbs_exec+os.sep+'bin'+os.sep+'printjob'

    cmd = [printjob_cmd, jobid]

    substate = None
    try:
        pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
        # Collect the job substate information
        process = subprocess.Popen(
                                   cmd,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        (out, err) = process.communicate()
        # Find the job substate
        substate_re = re.compile(r"substate:\s+(?P<jobstate>\S+)\s+")
        substate = substate_re.search(out)
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Unexpected error in job_to_be_ignored: %s" %
                   ' '.join([repr(sys.exc_info()[0]),
                            repr(sys.exc_info()[1])]))
        out = "Unknown: failed to run " + printjob_cmd

    if substate is not None:
        substate = substate.group().split(':')[1].strip()
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Job %s has substate %s" % (jobid, substate))

        suspended_substates = ['0x2b', '0x2d', 'unknown']
        return (substate in suspended_substates)
    else:
        return False

# ============================================================================
# Utility classes
# ============================================================================

#
# CLASS HookUtils
#


class HookUtils:

    def __init__(self, hook_events=None):
        if hook_events is not None:
            self.hook_events = hook_events
        else:
            self.hook_events = {
                # Defined in the order they appear in module_pbs_v1.c
                pbs.QUEUEJOB: {
                    'name': 'queuejob',
                    'handler': None
                },
                pbs.MODIFYJOB: {
                    'name': 'modifyjob',
                    'handler': None
                },
                pbs.RESVSUB: {
                    'name': 'resvsub',
                    'handler': None
                },
                pbs.MOVEJOB: {
                    'name': 'movejob',
                    'handler': None
                },
                pbs.RUNJOB: {
                    'name': 'runjob',
                    'handler': None
                },
                pbs.PROVISION: {
                    'name': 'provision',
                    'handler': None
                },
                pbs.EXECJOB_BEGIN: {
                    'name': 'execjob_begin',
                    'handler': self.__execjob_begin_handler
                },
                pbs.EXECJOB_PROLOGUE: {
                    'name': 'execjob_prologue',
                    'handler': None
                },
                pbs.EXECJOB_EPILOGUE: {
                    'name': 'execjob_epilogue',
                    'handler': self.__execjob_epilogue_handler
                },
                pbs.EXECJOB_PRETERM: {
                    'name': 'execjob_preterm',
                    'handler': None
                },
                pbs.EXECJOB_END: {
                    'name': 'execjob_end',
                    'handler': self.__execjob_end_handler
                },
                pbs.EXECJOB_LAUNCH: {
                    'name': 'execjob_launch',
                    'handler': self.__execjob_launch_handler
                },
                pbs.EXECHOST_PERIODIC: {
                    'name': 'exechost_periodic',
                    'handler': self.__exechost_periodic_handler
                },
                pbs.EXECHOST_STARTUP: {
                    'name': 'exechost_startup',
                    'handler': self.__exechost_startup_handler
                },
                pbs.MOM_EVENTS: {
                    'name': 'mom_events',
                    'handler': None
                },
            }

            if not CRAY_12_BRANCH:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "CRAY 12 Branch: %s" % (CRAY_12_BRANCH))
                self.hook_events[pbs.EXECJOB_ATTACH] = {
                    'name': 'execjob_attach',
                    'handler': self.__execjob_attach_handler
                }

    def __repr__(self):
        return "HookUtils(%s)" % (repr(self.hook_events))

    def event_name(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['name']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Type: %s not found" % (caller_name(), type))
            return None

    def hashandler(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['handler'] is not None
        else:
            return None

    def invoke_handler(self, event, cgroup, jobutil, *args):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: UID: real=%d, effective=%d" %
                   (caller_name(), os.getuid(), os.geteuid()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: GID: real=%d, effective=%d" %
                   (caller_name(), os.getgid(), os.getegid()))
        if self.hashandler(event.type):
            result = self.hook_events[event.type][
                'handler'](event, cgroup, jobutil, *args)
            return result
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: %s event not handled by this hook." %
                       (caller_name(), self.event_name(event.type)))

    def __execjob_begin_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Instantiate the NodeConfig class for get_memory_on_node and
        # get_vmem_on node
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Host assigned job resources: %s" %
                   (caller_name(), jobutil.host_resources))

        # Make sure the parent cgroup directories exist
        cgroup.create_paths()

        # Make sure that any old cgroups created in an earlier attempt
        # at creating or running the job are gone
        cgroup.delete(e.job.id)

        # Gather the assigned resources
        cgroup.gather_assigned_resources()
        # Create the cgroup(s) for the job
        cgroup.create_job(e.job.id, node)
        # Configure the new cgroup
        cgroup.configure_job(e.job.id, jobutil.host_resources, node)
        # Initialize resource usage for the job
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        # Write out the assigned resources
        cgroup.write_out_cgroup_host_assigned_resources(e.job.id)
        # Write out the environment variable for the host (pbs_attach)
        if 'devices_name' in cgroup.host_assigned_resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Devices: %s" %
                       cgroup.host_assigned_resources['devices_name'])
            env_list = list()
            if len(cgroup.host_assigned_resources['devices_name']) > 0:
                line = str()
                mics = list()
                gpus = list()
                for key in cgroup.host_assigned_resources['devices_name']:
                    if key.startswith('mic'):
                        mics.append(key[3:])
                    elif key.startswith('nvidia'):
                        gpus.append(key[6:])
                if len(mics) > 0:
                    env_list.append('OFFLOAD_DEVICES="%s"' %
                                    string.join(mics, ','))
                if len(gpus) > 0:
                    # don't put quotes around the values. ex "0" or "0,1".
                    # This will cause it to fail.
                    env_list.append('CUDA_VISIBLE_DEVICES=%s' %
                                    string.join(gpus, ','))

            pbs.logmsg(pbs.EVENT_DEBUG, "ENV_LIST: %s" % env_list)
            cgroup.write_out_cgroup_host_job_env_file(e.job.id, env_list)
        return True

    def __execjob_epilogue_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # The resources_used information has a base type of pbs_resource
        # Set the usage data
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        return True

    def __execjob_end_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Delete the cgroup(s) for the job
        cgroup.delete(e.job.id)
        # Remove the host_assigned_resources and job_env file
        for filename in [cgroup.hook_storage_dir+os.sep+e.job.id,
                         cgroup.host_job_env_filename % e.job.id]:
            try:
                os.remove(filename)
            except OSError:
                pbs.logmsg(pbs.EVENT_DEBUG3, "File: %s not found" % (filename))
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Error removing file: %s" % (filename))
                pass

        return True

    def __execjob_launch_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Add the parent process id to the appropriate cgroups.
        cgroup.add_pids(os.getppid(), jobutil.job.id)
        # FUTURE: Add environment variable to the job environment
        # if job requested mic or gpu
        cgroup.read_in_cgroup_host_assigned_resources(e.job.id)
        if cgroup.host_assigned_resources is not None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "host_assigned_resources: %s" %
                       (cgroup.host_assigned_resources))
            cgroup.setup_job_devices_env()
        return True

    def __exechost_periodic_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Cleanup cgroups for jobs not present on this node
        count = cgroup.cleanup_orphans(e.job_list)
        if ('periodic_resc_update' in cgroup.cfg and
                cgroup.cfg['periodic_resc_update']):
            for job in e.job_list.keys():
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: job is %s" %
                           (caller_name(), job))
                cgroup.update_job_usage(job, e.job_list[job].resources_used)
        # Online nodes that were offlined due to a cgroup not cleaning up
        if count == 0 and cgroup.cfg['online_offlined_nodes']:
            vnode = pbs.event().vnode_list[cgroup.hostname]
            if os.path.isfile(cgroup.offline_file):
                line = 'Orphan cgroup(s) have been cleaned up. '

                # Check with the pbs server to see if the node comment matches
                try:
                    tmp_comment = pbs.server().vnode(cgroup.hostname).comment
                except:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Unable to contact server for node comment")
                    tmp_comment = None
                if tmp_comment == cgroup.offline_msg:
                    line += 'Will bring the node back online'
                    vnode.state = pbs.ND_FREE
                    vnode.comment = None
                else:
                    line += 'However, the comment has changed since the node '
                    line += 'was offlined. Node will remain offline'

                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: %s" % (caller_name(), line))
                # Remove file
                os.remove(cgroup.offline_file)
        return True

    def __exechost_startup_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        cgroup.create_paths()
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))

        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        node.create_vnodes()
        if 'memory' in cgroup.subsystems:
            val = node.get_memory_on_node(cgroup.cfg)
            if val is not None:
                if 'vnode_per_numa_node' in cgroup.cfg:
                    if not cgroup.cfg['vnode_per_numa_node']:
                        hn = node.hostname
                        e.vnode_list[hn].resources_available['mem'] = \
                            pbs.size(val)
                        val_cpus = node.totalcpus
                        if val_cpus is not None:
                            e.vnode_list[hn].resources_available['ncpus'] = \
                                int(val_cpus)
                cgroup.set_node_limit('mem', val)
        if 'memsw' in cgroup.subsystems:
            val = node.get_vmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['vmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('vmem', val)
        if 'hugetlb' in cgroup.subsystems:
            val = node.get_hpmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['hpmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('hpmem', val)
        return True

    def __execjob_attach_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logjobmsg(jobutil.job.id, "%s: Attaching PID %s" %
                      (caller_name(), e.pid))
        # Add the job process id to the appropriate cgroups.
        cgroup.add_pids(e.pid, jobutil.job.id)
        return True

#
# CLASS ShallIRunUtils
#


class ShallIRunUtils:

    def __init__(self, hostname, cfg):
        self.cfg = cfg
        self.hostname = hostname

    def allow_users(self, allow_users):
        """ This still needs to be defined """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pass

    def exclude_hosts(self, exclude_hosts):
        """
        Gets the hostname for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        hostname = pbs.get_local_nodename()
        # check to see if hostname is in the exclude_hosts list
        if self.hostname in exclude_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: exclude host %s is in %s" %
                       (caller_name(), self.hostname, exclude_hosts))
            pbs.event().accept()

        # check to see if it regex syntax is used
        pass

    def exclude_vntypes(self, exclude_vntypes):
        """
        Gets the vntype for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        vntype = None

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'
        if os.path.isfile(vntype_file):
            fdata = open(vntype_file).readlines()
            vntype = fdata[0].strip()
            pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
        if vntype is None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype is set to %s" % (vntype))
        elif vntype in exclude_vntypes:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s is in %s" % (vntype, exclude_vntypes))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Exiting Hook")
            pbs.event().accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s not in global exclude: %s" %
                       (vntype, exclude_vntypes))
        pass

    def run_only_on_hosts(self, approved_hosts):
        """
        Gets the list of approved nodes to run on and checks to see if the hook
        should run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Approved hosts: %s, %s" %
                   (type(approved_hosts), approved_hosts))

        # check to see if hostname is in the approved_hosts list
        if approved_hosts == []:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "approved hosts list is empty: %s" % approved_hosts)
        elif self.hostname not in approved_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s is not in the approved list of hosts: %s" %
                       (self.hostname, approved_hosts))
            pbs.event().accept()

        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s in list of approved hosts: %s" %
                       (self.hostname, approved_hosts))

#
# CLASS JobUtils
#


class JobUtils:

    def __init__(self, job, hostname=None, resources=None):
        self.job = job
        self.host_vnodes = list()
        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if resources is not None:
            self.host_resources = resources
        else:
            self.host_resources = self.__host_assigned_resources()

    def __repr__(self):
        return "JobUtils(%s, %s, %s)" % (repr(self.job), repr(self.hostname),
                                         repr(self.host_resources))

    # Return a dictionary of resources assigned to the local node
    def __host_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Bail out if no hostname was provided
        if self.hostname is None:
            raise CgroupProcessingError('No hostname available')
        # Bail out if no job information is present
        if self.job is None:
            raise CgroupProcessingError('No job information available')
        # Determine which vnodes are on this host, if any
        if self.hostname is not None:
            vnhost_pattern = "%s\[[\d+]\]" % self.hostname
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnhost pattern: %s" %
                       (caller_name(), vnhost_pattern))
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job exec_vnode list: %s" %
                       (caller_name(), self.job.exec_vnode))
            for match in re.findall(vnhost_pattern, str(self.job.exec_vnode)):
                self.host_vnodes.append(match)
            if self.host_vnodes is not None:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Associate %s to vnodes on %s" %
                           (caller_name(), self.host_vnodes, self.hostname))

        # Collect host assigned resources
        resources = {}
        for chunk in self.job.exec_vnode.chunks:
            vnode = False
            if self.host_vnodes == [] and chunk.vnode_name != self.hostname:
                continue
            elif self.host_vnodes != [] and \
                    chunk.vnode_name not in self.host_vnodes:
                continue
            elif chunk.vnode_name in self.host_vnodes:
                vnode = True
                if 'vnodes' not in resources:
                    resources['vnodes'] = {}
                if chunk.vnode_name not in resources['vnodes']:
                    resources['vnodes'][chunk.vnode_name] = {}
                # This check is needed since not all resources are
                # required to be in each chunk of a job
                # i.e. exec_vnodes =
                # (node1[0]:ncpus=4:mem=4gb+node1[1]:mem=2gb) +
                # (node1[1]:ncpus=3+node[0]:ncpus=1:mem=4gb)
                if all(resc in resources['vnodes'][chunk.vnode_name]
                       for resc in chunk.chunk_resources.keys()):
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: resources already defined" %
                               (caller_name()))
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s,\t%s" %
                               (caller_name(),
                                chunk.chunk_resources.keys(),
                                resources['vnodes'][chunk.vnode_name].keys()))
                else:
                    # Initialize resources for each vnode
                    for resc in chunk.chunk_resources.keys():
                        # Initialize the new value
                        if isinstance(chunk.chunk_resources[resc],
                                      pbs.pbs_int):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_int(0)
                        elif isinstance(chunk.chunk_resources[resc],
                                        pbs.pbs_float):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_float(0)
                        elif isinstance(chunk.chunk_resources[resc], pbs.size):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.size('0')

            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Chunk %s" %
                       (caller_name(), chunk.vnode_name))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resources: %s" %
                       (caller_name(), resources))
            for resc in chunk.chunk_resources.keys():
                if resc not in resources.keys():
                    # Initialize the new value
                    if isinstance(chunk.chunk_resources[resc], pbs.pbs_int):
                        resources[resc] = pbs.pbs_int(0)
                    elif isinstance(chunk.chunk_resources[resc],
                                    pbs.pbs_float):
                        resources[resc] = pbs.pbs_float(0.0)
                    elif isinstance(chunk.chunk_resources[resc], pbs.size):
                        resources[resc] = pbs.size('0')
                # Add resource value to total
                if type(chunk.chunk_resources[resc]) in \
                        [pbs.pbs_int, pbs.pbs_float, pbs.size]:
                    resources[resc] += chunk.chunk_resources[resc]
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s: resources[%s][%s] is now %s" %
                               (caller_name(), self.hostname, resc,
                                resources[resc]))
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] += \
                                  chunk.chunk_resources[resc]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Setting resource %s to string %s" %
                               (caller_name(), resc,
                                str(chunk.chunk_resources[resc])))
                    resources[resc] = str(chunk.chunk_resources[resc])
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] = \
                                  str(chunk.chunk_resources[resc])
        if not resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: No resources assigned to host %s" %
                       (caller_name(), self.hostname))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources for %s: %s" %
                       (caller_name(), self.hostname, repr(resources)))

        # Return assigned resources for specified host
        pbs.logmsg(pbs.EVENT_DEBUG3, "Job Resources: %s" % (resources))
        return resources

    # Write a message to the job stdout file
    def write_to_stderr(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stderr_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

    # Write a message to the job stdout file
    def write_to_stdout(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stdout_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

#
# CLASS NodeConfig
#


class NodeConfig:

    def __init__(self, cfg, **kwargs):

        self.cfg = cfg
        hostname = None
        meminfo = None
        numa_nodes = None
        devices = None
        hyperthreading = None
        self.hyperthreads_per_core = 1
        self.totalcpus = self.__discover_cpus()

        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        self.host_mom_jobdir = pbs_home+'/mom_priv/jobs'

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'hostname':
                    hostname = val
                elif arg == 'meminfo':
                    meminfo = val
                elif arg == 'numa_nodes':
                    numa_nodes = val
                elif arg == 'devices':
                    devices = val
                elif arg == 'hyperthreading':
                    hyperthreading = val

        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if meminfo is not None:
            self.meminfo = meminfo
        else:
            self.meminfo = self.__discover_meminfo()
        if numa_nodes is not None:
            self.numa_nodes = numa_nodes
        else:
            self.numa_nodes = self.__discover_numa_nodes()
        if devices is not None:
            self.devices = devices
        else:
            self.devices = self.__discover_devices()
        if hyperthreading is not None:
            self.hyperthreading = hyperthreading
        else:
            self.hyperthreading = self.__hyperthreading_enabled()

        # Add the devices count i.e. nmics and ngpus to the numa nodes
        self.__add_devices_to_numa_node()

    def __repr__(self):
        return ("NodeConfig(%s, %s, %s, %s, %s, %s)" %
                (repr(self.cfg), repr(self.hostname), repr(self.meminfo),
                 repr(self.numa_nodes), repr(self.devices),
                 repr(self.hyperthreading)))

    def __add_devices_to_numa_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (self.devices.keys()))
        for device in self.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" % (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" %
                           (self.devices[device].keys()))
                for device_name in self.devices[device]:
                    device_socket = \
                        self.devices[device][device_name]['numa_node']
                    if device == 'mic':
                        if 'nmics' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['nmics'] = 1
                        else:
                            self.numa_nodes[device_socket]['nmics'] += 1
                    elif device == 'gpu':
                        if 'ngpus' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['ngpus'] = 1
                        else:
                            self.numa_nodes[device_socket]['ngpus'] += 1

        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (self.numa_nodes))

    # Discover what type of hardware is on this node and how is it partitioned
    def __discover_numa_nodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        numa_nodes = {}
        for dir in glob.glob(os.path.join(os.sep,
                             "sys", "devices", "system", "node", "node*")):
            id = int(dir.split(os.sep)[5][4:])
            if id not in numa_nodes.keys():
                numa_nodes[id] = {}
                numa_nodes[id]['devices'] = list()
            numa_nodes[id]['cpus'] = open(os.path.join(dir, "cpulist"),
                                          'r').readline().strip()
            with open(os.path.join(dir, "meminfo"), 'r') as fd:
                for line in fd:
                    # Each line will contain four or five fields. Examples:
                    # Node 0 MemTotal:       32995028 kB
                    # Node 0 HugePages_Total:     0
                    entries = line.split()
                    if len(entries) < 4:
                        continue
                    if entries[2] == "MemTotal:":
                        numa_nodes[id][entries[2].rstrip(':')] = \
                            convert_size(entries[3] + entries[4], 'kb')
                    elif entries[2] == "HugePages_Total:":
                        numa_nodes[id][entries[2].rstrip(':')] = entries[3]
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover Numa Nodes: %s" % numa_nodes)
        return numa_nodes

    # Identify devices and to which numa nodes they are attached
    def __discover_devices(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        devices = {}
        for file in glob.glob(os.path.join(os.sep, "sys", "class", "*", "*",
                                           "device", "numa_node")):
            # The file should contain a single integer
            numa_node = int(open(file, 'r').readline().strip())
            if numa_node < 0:
                numa_node = 0
            dirs = file.split(os.sep)
            name = dirs[3]
            instance = dirs[4]
            if name not in devices.keys():
                devices[name] = {}
            devices[name][instance] = {}
            devices[name][instance]['numa_node'] = numa_node
            if name == 'mic':
                s = os.stat('/dev/%s' % instance)
                devices[name][instance]['major'] = os.major(s.st_rdev)
                devices[name][instance]['minor'] = os.minor(s.st_rdev)
                devices[name][instance]['device_type'] = 'c'

        # Check to see if there are gpus on the node
        gpus = self.discover_gpus()
        if isinstance(gpus, dict) and len(gpus.keys()) > 0:
            devices['gpu'] = gpus['gpu']

        pbs.logmsg(pbs.EVENT_DEBUG3, "Discovered devices: %s" % (devices))
        return devices

    def discover_gpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Check to see if nvidia-smi exists and produces valid output
            cmd = ['/usr/bin/nvidia-smi', '-q', '-x']
            if 'nvidia-smi' in self.cfg:
                cmd[0] = self.cfg['nvidia-smi']

            pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
            # Collect the gpu information
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.PIPE)
            output, err = process.communicate()

            # pbs.logmsg(pbs.EVENT_DEBUG3,"output: %s" % output)
            # import the xml library and parse the output
            import xml.etree.ElementTree as ET

            # Parse the data
            name = 'gpu'
            gpu_data = dict()
            gpu_data[name] = {}
            root = ET.fromstring(output)
            pbs.logmsg(pbs.EVENT_DEBUG3, "root.tag: %s" % root.tag)
            for child in root:
                if child.tag == "attached_gpus":
                    # gpu_data['ngpus'] = int(child.text)
                    pass
                if child.tag == "gpu":
                    device_id = child.get('id')
                    number = 'nvidia%s' % child.find('minor_number').text
                    gpu_data[name][number] = dict()
                    gpu_data[name][number]['id'] = device_id

                    # Determine which socket the device is on
                    filename = '/sys/bus/pci/devices/%s/numa_node' % \
                        device_id.lower()
                    isfile = os.path.isfile(filename)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s, %s" % (filename, isfile))
                    if isfile:
                        numa_node = open(filename).read().strip()
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "numa_node: %s" % (numa_node))
                        if int(numa_node) == -1:
                            numa_node = 0
                        gpu_data[name][number]['numa_node'] = int(numa_node)
                        try:
                            dev_filename = '/dev/%s' % number
                            s = os.stat(dev_filename)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find %s" % dev_filename)
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                        gpu_data[name][number]['major'] = os.major(s.st_rdev)
                        gpu_data[name][number]['minor'] = os.minor(s.st_rdev)
                        gpu_data[name][number]['device_type'] = 'c'
            pbs.logmsg(pbs.EVENT_DEBUG, "%s" % gpu_data)
            return gpu_data
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unable to find %s" % string.join(cmd, " "))
            return {'gpu': {}}
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        return {'gpu': {}}

    # Get the memory info on this host
    def __discover_meminfo(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        meminfo = {}
        with open(os.path.join(os.sep, "proc", "meminfo"), 'r') as fd:
            for line in fd:
                entries = line.split()
                if entries[0] == "MemTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "SwapTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "Hugepagesize:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "HugePages_Total:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
                elif entries[0] == "HugePages_Rsvd:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover meminfo: %s" % meminfo)
        return meminfo

    # Determine if the cpus have hyperthreading enabled
    def __hyperthreading_enabled(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            siblings = 0
            cpu_cores = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "siblings":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    siblings = entries[1].strip()
                elif entries[0].strip() == "cpu cores":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_cores = entries[1].strip()
                elif entries[0].strip() == "flags":
                    flag_options = entries[1].split()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: flag_options: %s" %
                               (caller_name(), flag_options))
                    if "ht" in flag_options:
                        rc = True
                        break
        if rc is True:
            self.hyperthreads_per_core = int(siblings)/int(cpu_cores)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: hyperthreads/core: %d" %
                       (caller_name(), self.hyperthreads_per_core))
            if self.hyperthreads_per_core == 1:
                rc = False
        return rc

    def __discover_cpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            cpu_threads = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "processor":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_threads += 1
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: processors found: %d" %
                   (caller_name(), cpu_threads))

        return cpu_threads

    # Gather the jobs running on this node
    def gather_jobs_on_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        # Get the job list from the server
        jobs = None
        try:
            jobs = pbs.server().vnode(self.hostname).jobs
        except:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Unable to contact server to get jobs")
            return None

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs from server for %s: %s" % (self.hostname, jobs))

        if jobs is None:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "No jobs found on %s " % (self.hostname))
            return list()

        jobs_list = dict()
        for job in jobs.split(','):
            # The job list from the node has a space in front of the job id
            # This must be removed or it will not match the cgroup dir
            jobs_list[job.split('/')[0].strip()] = 0
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs on %s: %s" % (self.hostname, jobs_list.keys()))

        return jobs_list.keys()

    # Gather the jobs running on this node
    def gather_jobs_on_node_local(self):
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Method called" % (caller_name()))
        # Get the job list from the jobs directory
        jobs_list = list()
        try:
            for file in os.listdir(self.host_mom_jobdir):
                if fnmatch.fnmatch(file, "*.JB"):
                    jobs_list.append(file[:-3])
            if not jobs_list:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "No jobs found on %s " % (self.hostname))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Jobs from server for %s: %s" %
                           (self.hostname, repr(jobs_list)))

            return jobs_list
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Could not get job list from mom_priv/jobs for %s" %
                       self.hostname)

            return self.gather_jobs_on_node()

    # Get the memory resource on this mom
    def get_memory_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Calculate memory
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
            pbs.logmsg(pbs.EVENT_DEBUG3, "val: %s" % val)
        else:
            val = size_as_int(MemTotal)
        if 'percent_reserve' in config['cgroup']['memory']:
            percent_reserve = config['cgroup']['memory']['percent_reserve']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memory'].keys():
            reserve_memory = config['cgroup']['memory']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                reserve_mem = size_as_int(reserve_memory)
                if val > reserve_mem:
                    val -= reserve_mem
                else:
                    val -= size_as_int("100mb")
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Unable to reserve more memory than " +
                               "available on the node. Available %s, " +
                               "Tried to reserve %s. Reserving 100mb" %
                               (val, reserve_mem))

            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))

        pbs.logmsg(pbs.EVENT_DEBUG3, "Return val: %s" % val)
        return convert_size(str(val), 'kb')

    # Get the virtual memory resource on this mom
    def get_vmem_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Return the value for memory if no swap is configured
        if size_as_int(self.meminfo['SwapTotal']) <= 0:
            return self.get_memory_on_node(config)
        # Calculate vmem
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
        else:
            val = size_as_int(MemTotal)

        val += size_as_int(self.meminfo['SwapTotal'])
        # Determine if memory needs to be reserved
        if 'percent_reserve' in config['cgroup']['memsw']:
            percent_reserve = config['cgroup']['memsw']['reserve_memory']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memsw']:
            reserve_memory = config['cgroup']['memsw']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                val -= size_as_int(reserve_memory)
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))
        return convert_size(str(val), 'kb')

    # Get the huge page memory resource on this mom
    def get_hpmem_on_node(self, config):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        if 'percent_reserve' in config['cgroup']['hugetlb'].keys():
            percent_reserve = config['cgroup']['hugetlb']['percent_reserve']
        else:
            percent_reserve = 0
        # Calculate vmem
        val = size_as_int(self.meminfo['Hugepagesize'])
        val *= (self.meminfo['HugePages_Total'] -
                self.meminfo['HugePages_Rsvd'])
        val -= (val * percent_reserve) / 100
        return convert_size(str(val), 'kb')

    # Create individual vnodes per socket
    def create_vnodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        key = "vnode_per_numa_node"
        if key in self.cfg.keys():
            if not self.cfg[key]:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s is false" %
                           (caller_name(), key))
                return
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s not configured" %
                       (caller_name(), key))
            return

        # Create one vnode per numa node
        vnode_list = pbs.event().vnode_list
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: numa nodes: %s" %
                   (caller_name(), self.numa_nodes))
        # Define the vnodes
        vnode_name = self.hostname
        vnode_list[vnode_name] = pbs.vnode(vnode_name)
        for id in self.numa_nodes.keys():
            vnode_name = self.hostname + "[%d]" % id
            vnode_list[vnode_name] = pbs.vnode(vnode_name)
            for key, val in sorted(self.numa_nodes[id].iteritems()):
                if key == 'cpus':
                    vnode_list[vnode_name].resources_available['ncpus'] = \
                        len(cpus2list(val))/self.hyperthreads_per_core
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['ncpus'] = 0
                elif key == 'MemTotal':
                    mem = self.get_memory_on_node(self.cfg, val)
                    vnode_list[vnode_name].resources_available['mem'] = \
                        pbs.size(mem)
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['mem'] = \
                        pbs.size('0kb')
                elif key == 'HugePages_Total':
                    pass
                elif isinstance(val, list):
                    pass
                elif isinstance(val, dict):
                    pass
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s=%s" %
                               (caller_name(), key, val))
                    vnode_list[vnode_name].resources_available[key] = val

                    if isinstance(val, int):
                        vnode_list[self.hostname].resources_available[key] = 0
                    elif isinstance(val, float):
                        vnode_list[self.hostname].resources_available[key] = \
                            0.0
                    elif isinstance(val, pbs.size):
                        vnode_list[self.hostname].resources_available[key] = \
                            pbs.size('0kb')
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnode list: %s" %
                   (caller_name(), vnode_list))
        return True

#
# CLASS CgroupUtils
#


class CgroupUtils:
    def __init__(self, hostname, vnode, **kwargs):
        cfg = None
        subsystems = None
        paths = None
        vntype = None
        event = None
        self.assigned_resources = dict()
        self.existing_cgroups = dict()
        self.host_assigned_resources = None
        self.pid_lower_limit = 3

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'subsystems':
                    subsystems = val
                elif arg == 'paths':
                    paths = val
                elif arg == 'vntype':
                    vntype = val
                elif arg == 'event':
                    event = val

        try:
            self.hostname = hostname
            self.vnode = vnode
            # __check_os will raise an exception if cgroups are not present
            self.__check_os()
            # Read in the config file
            if cfg is not None:
                self.cfg = cfg
            else:
                self.cfg = self.__parse_config_file()

            # Determine if the hook should run or exit
            siru = ShallIRunUtils(self.hostname, self.cfg)
            siru.exclude_hosts(self.cfg['exclude_hosts'])
            siru.exclude_vntypes(self.cfg['exclude_vntypes'])
            siru.run_only_on_hosts(self.cfg['run_only_on_hosts'])

            # Collect the mount points
            if paths is not None:
                self.paths = paths
            else:
                self.paths = self.__set_paths()
            # Define the local vnode type
            if vntype is not None:
                self.vntype = vntype
            else:
                self.vntype = self.__get_vnode_type()
            # Determine which subsystems we care about
            if subsystems is not None:
                self.subsystems = subsystems
            else:
                self.subsystems = self.__target_subsystems()

            # location to store information for the different hook events
            pbs_home = ''
            if not CRAY_12_BRANCH:
                pbs_conf = pbs.get_pbs_conf()
                pbs_home = pbs_conf['PBS_HOME']
            else:
                if 'PBS_HOME' in self.cfg:
                    pbs_home = self.cfg['PBS_HOME']
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "PBS_HOME needs to be defined in the " +
                               "config file")
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Exiting the cgroups hook")
                    pbs.accept()

            self.hook_storage_dir = pbs_home+'/mom_priv/hooks/hook_data'
            self.host_job_env_dir = pbs_home+'/aux'
            self.host_job_env_filename = self.host_job_env_dir+os.sep+"%s.env"

            # information for offlining nodes
            self.offline_file = \
                pbs_home+os.sep+'mom_priv'+os.sep+'hooks'+os.sep
            self.offline_file += "%s.offline" % pbs.event().hook_name
            self.offline_msg = "Hook %s: " % pbs.event().hook_name
            self.offline_msg += "Unable to clean up one or more cgroups"

        except:
            raise

    def __repr__(self):
        return ("CgroupUtils(%s, %s, %s, %s, %s, %s)" %
                (repr(self.hostname), repr(self.vnode), repr(self.cfg),
                 repr(self.subsystems), repr(self.paths), repr(self.vntype)))

    # Validate the OS type and version
    def __check_os(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check to see if the platform is linux and the kernel is new enough
        if platform.system() != 'Linux':
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: OS does not support cgroups" %
                       (caller_name()))
            raise CgroupConfigError("OS type not supported")
        rel = map(int,
                  string.split(string.split(platform.release(), '-')[0], '.'))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Detected Linux kernel version %d.%d.%d" %
                   (caller_name(), rel[0], rel[1], rel[2]))
        supported = False
        if rel[0] > 2:
            supported = True
        elif rel[0] == 2:
            if rel[1] > 6:
                supported = True
            elif rel[1] == 6:
                if rel[2] >= 28:
                    supported = True
        if not supported:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Kernel needs to be >= 2.6.28: %s" %
                       (caller_name(), system_info[2]))
            raise CgroupConfigError("OS version not supported")
        return supported

    # Read the config file in json format
    def __parse_config_file(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        config = {}
        # Identify the config file and read in the data
        if 'PBS_HOOK_CONFIG_FILE' in os.environ:
            config_file = os.environ["PBS_HOOK_CONFIG_FILE"]
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Config file is %s" %
                       (caller_name(), config_file))
            try:
                config = json.load(open(config_file, 'r'),
                                   object_hook=decode_dict)
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
        else:
            raise CgroupConfigError("No configuration file present")
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Initial Cgroup Config: %s" %
                   (caller_name(), config))
        # Set some defaults if they are not present
        if 'cgroup_prefix' not in config.keys():
            config['cgroup_prefix'] = 'pbspro'
        if 'periodic_resc_update' not in config.keys():
            config['periodic_resc_update'] = False
        if 'vnode_per_numa_node' not in config.keys():
            config['vnode_per_numa_node'] = False
        if 'online_offlined_nodes' not in config.keys():
            config['online_offlined_nodes'] = False
        if 'exclude_hosts' not in config.keys():
            config['exclude_hosts'] = []
        if 'run_only_on_hosts' not in config.keys():
            config['run_only_on_hosts'] = []
        if 'exclude_vntypes' not in config.keys():
            config['exclude_vntypes'] = []
        if 'cgroup' not in config.keys():
            config['cgroup'] = {}
        else:
            for subsys in config['cgroup']:
                subsystem = config['cgroup'][subsys]
                if 'enabled' not in subsystem.keys():
                    config['cgroup'][subsys]['enabled'] = False
                if 'exclude_hosts' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_hosts'] = []
                if 'exclude_vntypes' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_vntypes'] = []

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Cgroup Config with defaults: %s" %
                   (caller_name(), config))
        return config

    # Determine which subsystems are being requested
    def __target_subsystems(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        subsystems = []
        for key in self.cfg['cgroup'].keys():
            if self.enabled(key):
                subsystems.append(key)
        if 'memory' not in subsystems:
            if 'memsw' in subsystems:
                # Remove memsw since it must be greater then
                # or equal to memory
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing memsw from enabled subsystems")
                subsystems.remove('memsw')
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Enabled subsystems: %s" %
                   (caller_name(), subsystems))
        # It is not an error for all subsystems to be disabled.
        # This host or vnode type may be in the excluded list.
        return subsystems

    # Copy a setting from the parent cgroup
    def __copy_from_parent(self, dest):
        try:
            file = os.path.basename(dest)
            dir = os.path.dirname(dest)
            parent = os.path.dirname(dir)
            source = os.path.join(parent, file)
            if not os.path.isfile(source):
                raise CgroupConfigError('Failed to read %s' % (source))
            self.write_value(dest, open(source, 'r').read().strip())
        except:
            raise

    # Create a dictionary of the cgroup subsystems and their corresponding
    # directories
    def __set_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        paths = {}
        try:
            # Loop through the mounts and collect the ones for cgroups
            with open(os.path.join(os.sep, "proc", "mounts"), 'r') as fd:
                for line in fd:
                    entries = line.split()
                    if len(entries) > 3 and entries[2] == "cgroup":
                        flags = entries[3].split(',')
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "subsysdir: %s (flags=%s)" %
                                   (entries[1], flags))
                        for subsys in flags:
                            paths[subsys] = os.path.join(
                                entries[1], self.cfg["cgroup_prefix"])
        except:
            raise
        if len(paths.keys()) < 1:
            raise CgroupConfigError("Cgroup paths not detected")
        # Both the memory and memsw cgroups share the same mount point
        if "memory" in paths.keys():
            paths["memsw"] = paths["memory"]
        return paths

    # Create the cgroup parent directories that will contain the jobs
    def create_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Create the directories that PBS will use to house the jobs
            old_umask = os.umask(0022)
            for subsys in self.subsystems:
                if subsys not in self.paths.keys():
                    raise CgroupConfigError('No path for subsystem: %s' %
                                            (subsys))
                if not os.path.exists(self.paths[subsys]):
                    os.makedirs(self.paths[subsys], 0755)
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Created directory %s" %
                               (caller_name(), self.paths[subsys]))
                    if subsys == 'memory' or subsys == 'memsw':
                        # Set file to
                        # <cgroup>/memory/pbspro/memory.use_hierarchy
                        file = os.path.join(self.paths[subsys],
                                            'memory.use_hierarchy')
                        if not os.path.isfile(file):
                            raise CgroupConfigError('Failed to configure %s' %
                                                    (file))
                        self.write_value(file, 1)
                    elif subsys == 'cpuset':
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.cpus'))
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.mems'))

        except:
            raise CgroupConfigError("Failed to create directory: %s" %
                                    (self.paths[subsys]))
        finally:
            os.umask(old_umask)

    # Return the vnode type of the local node
    def __get_vnode_type(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")

        # File to check for if it is not defined in the hook input file
        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'

        # self.vnode is None for pbs_attach events
        pbs.logmsg(pbs.EVENT_DEBUG3, "vnode: %s" % self.vnode)
        if self.vnode is not None:
            if 'vntype' in self.vnode.resources_available and \
                    self.vnode.resources_available['vntype'] is not None:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "vntype: %s" %
                           self.vnode.resources_available['vntype'])
                return self.vnode.resources_available['vntype']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3, "vntype file: %s" % vntype_file)
                if os.path.isfile(vntype_file):
                    fdata = open(vntype_file).readlines()
                    vntype = fdata[0].strip()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
                    return vntype
        # If vntype was not set then log a message. It is too expensive
        # to have all moms query the server for large jobs.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: resources_available.vntype is " % caller_name() +
                   "not set for this vnode")
        return None

    # Gather resources that are already assigned
    def gather_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        assigned_resources = {}

        # Gather the cpus and memory sockets assigned
        directories = [x[0] for x in os.walk(self.paths['cpuset'])]

        # Add the existing cgroups to a list to check if assigning
        # resources fail
        for dir in directories[1:]:
            if dir.find(pbs.event().job.id) == -1:
                self.existing_cgroups[dir] = []

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'cpuset' in self.subsystems and \
                self.cfg['cgroup']['cpuset']['enabled']:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather cpuset resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['cpuset'], tmp_dir)
                    with open(path+os.sep+'cpuset.cpus') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.cpus'] = \
                            cpus2list(tmp_val.strip())
                    with open(path+os.sep+'cpuset.mems') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.mems'] = \
                            cpus2list(tmp_val.strip())

        # Check to see if the subsystem is enabled before trying
        # to gather resources
        if 'memory' in self.subsystems and \
                self.cfg['cgroup']['memory']['enabled']:
            # Gather the memory assigned for each socket
            directories = [x[0] for x in os.walk(self.paths['memory'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather memory resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['memory'], tmp_dir)
                    with open(path+os.sep+'memory.limit_in_bytes') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memory.limit'] = \
                            tmp_val.strip()
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    if os.path.isfile(tmp_filename) is False:
                        continue
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    with open(tmp_filename) as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memsw.limit'] = \
                            tmp_val.strip()

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'devices' in self.subsystems and \
                self.cfg['cgroup']['devices']['enabled']:
            # Gather the devices assigned
            directories = [x[0] for x in os.walk(self.paths['devices'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather device resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    pbs.logmsg(pbs.EVENT_DEBUG3, "Dir: %s" % tmp_dir)
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['devices'], tmp_dir)
                    with open(path+os.sep+'devices.list') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['devices.list'] = \
                            tmp_val.strip().split('\n')
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Dir: %s\n\tValue: %s" %
                                   (path, tmp_val.replace('\n', ',')))

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Gathered assigned resources: %s" % assigned_resources)
        self.assigned_resources = assigned_resources

    # Return whether a subsystem is enabled
    def enabled(self, subsystem):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check whether the subsystem is enabled in the configuration file
        if subsystem not in self.cfg['cgroup'].keys():
            return False
        if 'enabled' not in self.cfg['cgroup'][subsystem].keys():
            return False
        if not self.cfg['cgroup'][subsystem]['enabled']:
            return False
        # Check whether the cgroup is mounted for this subsystem
        if subsystem not in self.paths.keys():
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup not mounted for %s" %
                       (caller_name(), subsystem))
            return False
        # Check whether this host is excluded
        # if self.hostname in self.cfg['exclude_hosts']:
        #    pbs.logmsg(pbs.EVENT_DEBUG, "%s: cgroup excluded on host %s" %
        #               (caller_name(), self.hostname))
        #    pbs.event().accept()
        if self.hostname in self.cfg['cgroup'][subsystem]['exclude_hosts']:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup excluded for subsystem %s on host %s" %
                       (caller_name(), subsystem, self.hostname))
            return False
        # Check whether the vnode type is excluded
        if self.vntype is not None:
            # if self.vntype in self.cfg['exclude_vntypes']:
            #    pbs.logmsg(pbs.EVENT_DEBUG,
            #               "%s: cgroup excluded on vnode type %s" %
            #               (caller_name(), self.vntype))
            #    pbs.event().accept()
            if self.vntype in self.cfg['cgroup'][subsystem]['exclude_vntypes']:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           ("%s: cgroup excluded for " +
                            "subsystem %s on vnode type %s") %
                           (caller_name(), subsystem, self.vntype))
                return False
        return True

    # Return the default value for a subsystem
    def default(self, subsystem):
        if subsystem in self.cfg['cgroup']:
            if 'default' in self.cfg['cgroup'][subsystem]:
                return self.cfg['cgroup'][subsystem]['default']
        return None

    # Check to see if the pid matches the job owners uid
    def is_pid_owned_by_job_owner(self, pid):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        try:
            proc_uid = os.stat('/proc/%d' % pid).st_uid
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       '/proc/%d uid:%d' % (pid, proc_uid))

            job_owner_uid = pwd.getpwnam(pbs.event().job.euser)[2]
            pbs.logmsg(pbs.EVENT_DEBUG3, "Job uid: %d" % job_owner_uid)

            if proc_uid == job_owner_uid:
                return True
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Proc uid: %d != Job owner:  %d" %
                           (proc_uid, job_owner_uid))
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG, "Unknown pid: %d" % pid)
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        # If we got to this point something did not match up
        return False

    # Add some number of PIDs to the cgroup tasks files for each subsystem
    def add_pids(self, pids, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # make pids a list
        if isinstance(pids, int):
            pids = [pids]

        if pbs.event().type == pbs.EXECJOB_LAUNCH:
            if 1 in pids:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "1 is not a valid pid to add")
                # Hold the job for further review
                e.reject("Hook tried to add pid 1 to the tasks file " +
                         "for job %s" % jobid)

        # check pids to make sure that they are owned by the job owner
        if not CRAY_12_BRANCH:
            pbs.logmsg(pbs.EVENT_DEBUG3, "Not in the Cray 12 Branch")
            pbs.logmsg(pbs.EVENT_DEBUG3, "check to see if execjob_attach")
            if pbs.event().type == pbs.EXECJOB_ATTACH:
                pbs.logmsg(pbs.EVENT_DEBUG3, "event type: attach or launch")
                tmp_pids = list()
                for p in pids:
                    if self.is_pid_owned_by_job_owner(p):
                        tmp_pids.append(p)
                if len(tmp_pids) == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%d is not a valid pids to add" % p)
                    return False
                else:
                    pids = tmp_pids

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: subsys = %s" %
                       (caller_name(), subsys))
            # memsw and memory use the same tasks file
            if subsys == "memsw":
                if "memory" in self.subsystems:
                    continue
            path = os.path.join(self.paths[subsys], jobid)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path = %s" %
                       (caller_name(), path))
            try:
                tasks_path = os.path.join(path, "tasks")
                for p in pids:
                    self.write_value(tasks_path, p, 'a')
            except IOError, exc:
                raise CgroupLimitError("Failed to add PIDs %s to %s (%s)" %
                                       (str(pids), tasks_path,
                                        errno.errorcode[exc.errno]))
            except:
                raise

    # Set a node limit
    def set_node_limit(self, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s = %s" %
                   (caller_name(), resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'],
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Node resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    def setup_job_devices_env(self):
        """ Setup the job environment for the devices assigned to the job for an
            execjob_launch hook
         """
        if 'devices_name' in self.host_assigned_resources:
            names = self.host_assigned_resources['devices_name']
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "devices: %s" % (names))
            offload_devices = list()
            cuda_visible_devices = list()
            for name in names:
                if name.startswith('mic'):
                    offload_devices.append(name[3:])
                elif name.startswith('nvidia'):
                    cuda_visible_devices.append(name[6:])
            if len(offload_devices) > 0:
                value = '"%s"' % string.join(offload_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['OFFLOAD_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "offload_devices: %s" % offload_devices)
            if len(cuda_visible_devices) > 0:
                value = '"%s"' % string.join(cuda_visible_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['CUDA_VISIBLE_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "cuda_visible_devices: %s" % cuda_visible_devices)
            return [offload_devices, cuda_visible_devices]
        else:
            return False

    def setup_subsys_devices(self, path, node):
        subsys = 'devices'
        # Add the devices that the user is allowed to use
        if subsys in self.cfg['cgroup']:
            devices_allowed = open(os.path.join(path,
                                   "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Initial devices.list: %s" %
                       devices_allowed)
            # Deny access to mic and gpu devices
            devices_list = list()
            devices = node.devices
            for device in devices:
                if device == 'mic' or device == 'gpu':
                    for name in devices[device]:
                        d = devices[device][name]
                        devices_list.append("%d:%d" % (d['major'], d['minor']))
            # For CentOS 7 we need to remove a *:* rwm from devices.list we
            # before can add anything to devices.allow. Otherwise our changes
            # are ignored. Check to see if a *:* rwm is in devices.list.
            # If so remove it
            if "a *:* rwm\n" in devices_allowed:
                value = "a *:* rwm"
                self.write_value(os.path.join(path, 'devices.deny'), value)
            else:
                # Verify that the following devices are not in devices.list
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing access to the following: %s" %
                           devices_list)
            for device_str in devices_list:
                value = "c %s rwm" % device_str
                self.write_value(os.path.join(path, 'devices.deny'), value)
            # Add devices back to the list
            devices_allow = list()
            if 'allow' in self.cfg['cgroup'][subsys]:
                devices_allow = self.cfg['cgroup'][subsys]['allow']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Allowing access to the following: %s" %
                           devices_allow)
                for item in devices_allow:
                    if isinstance(item, str):
                        pbs.logmsg(pbs.EVENT_DEBUG3, "string item: %s" % item)
                        value = "%s" % item
                        self.write_value(os.path.join(
                                         path, 'devices.allow'), value)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "write_value: %s" % value)
                    elif isinstance(item, list):
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "list item: %s" % item)
                        try:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Device allow: %s" % item)
                            stat_filename = '/dev/%s' % item[0]
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Stat file: %s" % stat_filename)
                            s = os.stat(stat_filename)
                            device_type = None
                            if S_ISBLK(s.st_mode):
                                device_type = "b"
                            elif S_ISCHR(s.st_mode):
                                device_type = "c"
                            if device_type is not None:
                                if len(item) == 3 and isinstance(item[2], str):
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % item[2]
                                    value += "%s" % item[1]
                                else:
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % os.minor(s.st_rdev)
                                    value += "%s" % item[1]
                                self.write_value(os.path.join(
                                                path, 'devices.allow'), value)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "write_value: %s" % value)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find /dev/%s. " % item[0] +
                                       "Not added to devices.allow!")
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                    else:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Not sure what to do with: %s" % item)
            devices_allowed = open(os.path.join(
                                   path, "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "After setup devices.list: %s" % devices_allowed)

    # Select devices to assign to the job
    def assign_devices(
                       self,
                       device_type,
                       device_list,
                       number_of_devices,
                       node):
        devices = device_list[:number_of_devices]
        device_ids = list()
        device_names = list()
        for device in devices:
            device_info = node.devices[device_type][device]
            if len(device_ids) == 0:
                if device.find("mic") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:0 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                    device_ids.append("%s %d:1 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                elif device.find("nvidia") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:255 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
            device_ids.append("%s %d:%d rwm" %
                              (device_info['device_type'],
                               device_info['major'],
                               device_info['minor']))
            device_names.append(device)
        return device_names, device_ids

    # Find the device name
    def get_device_name(self, node, available, socket, major, minor):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Get device name: major: %s, minor: %s" % (major, minor))
        avail_device = None
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Possible devices: %s" % (available[socket]['devices']))
        for avail_device in available[socket]['devices']:
            avail_major = None
            avail_minor = None
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Checking device: %s" % (avail_device))
            if avail_device.find('mic') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check mic device: %s" % (avail_device))
                avail_major = node.devices['mic'][avail_device]['major']
                avail_minor = node.devices['mic'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            elif avail_device.find('nvidia') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check gpu device: %s" % (avail_device))
                avail_major = node.devices['gpu'][avail_device]['major']
                avail_minor = node.devices['gpu'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            if int(avail_major) == int(major) and \
                    int(avail_minor) == int(minor):
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device match: name: %s, major: %s, minor: %s" %
                           (avail_device, major, minor))
                return avail_device
        pbs.logmsg(pbs.EVENT_DEBUG3, "No match found")
        return None

    # Assign resources to the job based off the vnodes assignments
    def assign_job_resources_by_vnode(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # Loop through the vnodes and assign resources
        assigned = dict()
        assigned['cpuset.cpus'] = list()
        assigned['cpuset.mems'] = list()
        room_on_socket = True

        for vnode in resources['vnodes']:
            regex = re.compile(".*\[(\d+)\].*")
            socket = int(regex.search(vnode).group(1))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current Vnode: %s" % vnode)
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current socket: %d" % socket)

            if 'ncpus' in resources['vnodes'][vnode]:
                tmp_ncpus = int(resources['vnodes'][vnode]['ncpus'])
                if int(resources['vnodes'][vnode]['ncpus']) <= \
                        len(available[socket]['cpus']):
                    assigned['cpuset.cpus'] += \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] += [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_ncpus: %s" % (tmp_ncpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "available[%d]: %s" %
                               (socket, available[socket]))
                    room_on_socket = False
            if ('nmics' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['nmics']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(mic).*")
                tmp_nmics = int(resources['vnodes'][vnode]['nmics'])
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['nmics']) > 0 and
                   int(resources['vnodes'][vnode]['nmics']) <= len(mics)):
                    tmp_names, tmp_devices = self.assign_devices(
                             'mic', mics[:tmp_nmics], tmp_nmics, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_nmics: %s" % (tmp_nmics))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "mics: %s" % (mics))
                    room_on_socket = False
            if ('ngpus' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['ngpus']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(nvidia).*")
                tmp_ngpus = int(resources['vnodes'][vnode]['ngpus'])
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['ngpus']) > 0 and
                   int(resources['vnodes'][vnode]['ngpus']) <= len(gpus)):
                    tmp_names, tmp_devices = self.assign_devices(
                        'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "tmp_ngpus: %s" % (tmp_ngpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "gpus: %s" % (gpus))
                    room_on_socket = False

        if room_on_socket:
            assigned['cpuset.cpus'].sort()
            assigned['cpuset.mems'].sort()
            if 'devices' in assigned:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids pre set and sort: %s" %
                           (assigned['devices']))
                assigned['devices'] = list(set(assigned['devices']))
                assigned['devices'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids post set and sort: %s" %
                           (assigned['devices']))
                assigned['devices_name'] = list(set(assigned['devices_name']))
                assigned['devices_name'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

    # Assign resources to the job
    def assign_job_resources(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # What would we need to do different for a large smp (UV) system?
        # See if requested resources can fit on a single socket
        assigned = dict()
        pbs.logmsg(pbs.EVENT_DEBUG3, "Requested: %s" % (resources))

        # Determine the order that we should evaluate the sockets.
        socket_list = list()
        if 'placement_type' in self.cfg:
            if self.cfg['placement_type'] == 'round_robin':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Requested round_robin placement")
                # Look at assigned_resources and determine which socket
                # to start with
                jobs_per_socket_cnt = dict()
                for socket in available.keys():
                    jobs_per_socket_cnt[socket] = 0
                for job in self.assigned_resources:
                    if 'cpuset.mems' in self.assigned_resources[job]:
                        for socket in \
                                self.assigned_resources[job]['cpuset.mems']:
                            jobs_per_socket_cnt[socket] += 1

                sorted_dict = sorted(jobs_per_socket_cnt.items(),
                                     key=operator.itemgetter(1))
                for x in sorted_dict:
                    socket_list.append(x[0])
        else:
            socket_list = available.keys()

        # Loop through the socket list and determine if the job
        # can fit on the socket
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Evaluate sockets in the following order: %s" %
                   (socket_list))
        for socket in socket_list:
            room_on_socket = True
            if 'ncpus' in resources:
                if int(resources['ncpus']) <= len(available[socket]['cpus']):
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Requested: %s, Available: %s" %
                               (resources['ncpus'], available[socket]['cpus']))
                    tmp_ncpus = int(resources['ncpus'])
                    assigned['cpuset.cpus'] = \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] = [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient ncpus on socket %d: R:%d,A:%s" %
                               (socket, resources['ncpus'],
                                available[socket]['cpus']))
                    room_on_socket = False
            # Check to see that nmics has been requested and not equal to 0
            if 'nmics' in resources and int(resources['nmics']) > 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'nmics' in resources and int(resources['nmics']) > 0 \
                        and int(resources['nmics']) <= len(mics):
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient nmics on socket %d: R:%s,A:%s" %
                               (socket, resources['nmics'], mics))
                    room_on_socket = False
            # Check to see that ngpus has been requested and not equal to 0
            if 'ngpus' in resources and int(resources['ngpus']) > 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'ngpus' in resources and int(resources['ngpus']) > 0 \
                        and int(resources['ngpus']) <= len(gpus):
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient gpus on socket %d: R:%s,A:%s" %
                               (socket, resources['ngpus'], gpus))
                    room_on_socket = False
            if 'vmem' in resources:
                if size_as_int(resources['vmem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient vmem on socket %d: R:%s,A:%s" %
                               (socket, resources['vmem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if 'mem' in resources:
                if size_as_int(resources['mem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient memory on socket %d: R:%s,A:%s" %
                               (socket, resources['mem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if room_on_socket:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
                self.host_assigned_resources = assigned
                return assigned
        # If we made it here then the requested resources did not fit
        # on a single socket
        # Future: take distance into account when selecting sockets?
        # Combine available resources into a single list
        # This should work for a dual socket machine.
        # Needs additional work for machines with more than 2 sockets
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Unable to find requested resources: %s" % resources +
                   " on a socket. Checking the entire node")
        available_copy = copy.deepcopy(available)
        available_on_node = dict()
        socket_keys = available.keys()
        socket_keys.sort()
        available_on_node['sockets'] = socket_keys
        for socket in socket_keys:
            for key in available_copy[socket].keys():
                if isinstance(available[socket][key], int):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]
                if isinstance(available_copy[socket][key], str):
                    try:
                        if key in available_on_node is False:
                            available_on_node[key] = \
                                size_as_int(available_copy[socket][key])
                        else:
                            available_on_node[key] += \
                                size_as_int(available_copy[socket][key])
                    except:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Unable to convert to int: %s" %
                                   (available_copy[socket][key]))

                if isinstance(available_copy[socket][key], list):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available on node: %s" % (available_on_node))
        assigned = dict()
        room_on_node = False
        if 'ncpus' in resources:
            if int(resources['ncpus']) <= len(available_on_node['cpus']):
                room_on_node = True
                tmp_ncpus = int(resources['ncpus'])
                assigned['cpuset.cpus'] = available_on_node['cpus'][:tmp_ncpus]
                assigned['cpuset.mems'] = available_on_node['sockets']
            if 'nmics' in resources and int(resources['nmics']) != 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['nmics']) <= len(mics) and \
                        int(resources['nmics']) > 0:
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
            if 'ngpus' in resources and int(resources['ngpus']) != 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['ngpus']) <= len(gpus) and \
                        int(resources['ngpus']) > 0:
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
        if room_on_node:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

        # Consolidate resources if needed to place the job

    # Determine which resources are available
    def available_node_resources(self, node):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))

        available = copy.deepcopy(node.numa_nodes)
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Keys: %s" % (available[0].keys()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        for socket in available.keys():
            if 'cpus' in available[socket]:
                # Find the physical cores
                if available[socket]['cpus'].find('-') != -1 and \
                        node.hyperthreading:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Splitting %s at ','" %
                               (available[socket]['cpus']))
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'].split(',')[0])
                else:
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'])
            if 'MemTotal' in available[socket]:
                # Find the memory on the socket in bites.
                # Remove the 'b' to simply math
                available[socket]['memory'] = size_as_int(
                    available[socket]['MemTotal'])

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Pre device add: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (node.devices.keys()))
        for device in node.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" %
                       (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Devices: %s" %
                           (node.devices[device].keys()))
                for device_name in node.devices[device].keys():
                    device_socket = \
                        node.devices[device][device_name]['numa_node']
                    if 'devices' not in available[device_socket]:
                        available[device_socket]['devices'] = list()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Device: %s, Socket: %s" %
                               (device, device_socket))
                    available[device_socket]['devices'].append(device_name)
            # pbs.logmsg(pbs.EVENT_DEBUG3,
            #            "Available on socket %s: %s" %
            #            (socket,available[socket]))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))

        # Remove all of the resources that are assigned to other jobs
        for job in self.assigned_resources.keys():
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            if (not job_to_be_ignored(job)):
                cpus = list()
                sockets = list()
                devices = list()
                memory = 0
                if 'cpuset.cpus' in self.assigned_resources[job]:
                    cpus = self.assigned_resources[job]['cpuset.cpus']
                if 'cpuset.mems' in self.assigned_resources[job]:
                    sockets = self.assigned_resources[job]['cpuset.mems']
                if 'devices.list' in self.assigned_resources[job]:
                    devices = self.assigned_resources[job]['devices.list']
                if 'memory.limit' in self.assigned_resources[job]:
                    memory = size_as_int(
                                self.assigned_resources[job]['memory.limit'])

                # Loop through the sockets and remove cpus that are
                # assigned to other cgroups
                for socket in sockets:
                    for cpu in cpus:
                        try:
                            available[socket]['cpus'].remove(cpu)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %d from %s" %
                                       (cpu, available[socket]['cpus']))

                if len(sockets) == 1:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Decrementing memory: %d by %d" %
                               (size_as_int(available[sockets[0]]['memory']),
                                memory))
                    available[sockets[0]]['memory'] -= memory

                # Loop throught the available sockets
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned device to %s: %s" % (job, devices))
                for socket in available.keys():
                    for device in devices:
                        try:
                            # loop through know devices and see if they match
                            if len(available[socket]['devices']) != 0:
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Check device: %s" % (device))
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Available device: %s" %
                                           (available[socket]['devices']))
                                major, minor = device.split()[1].split(':')
                                avail_device = self.get_device_name(
                                                   node, available, socket,
                                                   major, minor)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Returned device: %s" %
                                           (avail_device))
                                if avail_device is not None:
                                    pbs.logmsg(pbs.EVENT_DEBUG3,
                                               "socket: %d,\t" % socket +
                                               "devices: %s,\t" %
                                               available[socket]['devices'] +
                                               "device to remove: %s" %
                                               (avail_device))
                                    available[socket]['devices'].remove(
                                        avail_device)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %s from %s" %
                                       (device, available[socket]['devices']))
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Job %s res not removed from host " % job +
                           "available res: suspended job")
        pbs.logmsg(pbs.EVENT_DEBUG, "Available resources: %s" % (available))
        return available

    # Set a job limit
    def set_job_limit(self, jobid, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s: %s = %s" %
                   (caller_name(), jobid, resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'softmem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.soft_limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     jobid, 'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'], jobid,
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            elif resource == 'ncpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = self.select_cpus(path, value)
                    if cpus is None:
                        raise CgroupLimitError("Failed to configure cpuset.")
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
                    self.__copy_from_parent(os.path.join(self.paths['cpuset'],
                                            jobid, 'cpuset.mems'))
            elif resource == 'cpuset.cpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = value
                    if cpus is None:
                        msg = "Failed to configure cpus in cpuset."
                        raise CgroupLimitError(msg)
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
            elif resource == 'cpuset.mems':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.mems')
                    mems = value
                    if mems is None:
                        msg = "Failed to configure mems in cpuset."
                        raise CgroupLimitError(msg)
                    mems = ','.join(map(str, mems))
                    self.write_value(path, mems)
            elif resource == 'devices':
                if 'devices' in self.subsystems and \
                        self.cfg['cgroup']['devices']['enabled']:

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.allow')
                    devices = value
                    if devices is None:
                        msg = "Failed to configure device(s)."
                        raise CgroupLimitError(msg)
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Setting devices: %s for %s" % (devices, jobid))
                    for device in devices:
                        self.write_value(path, device)

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.list')
                    output = open(path, 'r').read()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "devices.list: %s" % output.replace("\n", ","))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    # Update resource usage for a job
    def update_job_usage(self, jobid, resc_used):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resc_used = %s" %
                   (caller_name(), str(resc_used)))
        # Sort the subsystems so that we consistently look at the subsystems
        # in the same order every time
        self.subsystems.sort()
        for subsys in self.subsystems:
            path = os.path.join(self.paths[subsys], jobid)
            if subsys == "memory":
                max_mem = self.__get_max_mem_usage(path)
                if max_mem is None:
                    pbs.logjobmsg(jobid, "%s: No max mem data" %
                                  (caller_name()))
                else:
                    resc_used['mem'] = pbs.size(convert_size(max_mem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: mem=%s" %
                                  (caller_name(), resc_used['mem']))
                mem_failcnt = self.__get_mem_failcnt(path)
                if mem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No mem fail count data" %
                                  (caller_name()))
                else:
                    # Check to see if the job exceeded its resource limits
                    if mem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memory limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "memsw":
                max_vmem = self.__get_max_memsw_usage(path)
                if max_vmem is None:
                    pbs.logjobmsg(jobid, "%s: No max vmem data" %
                                  (caller_name()))
                else:
                    resc_used['vmem'] = pbs.size(convert_size(max_vmem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: vmem=%s" %
                                  (caller_name(), resc_used['vmem']))

                vmem_failcnt = self.__get_memsw_failcnt(path)
                if vmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No vmem fail count data" %
                                  (caller_name()))
                else:
                    pbs.logjobmsg(jobid, "%s: vmem fail count: %d " %
                                  (caller_name(), vmem_failcnt))
                    if vmem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memsw limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "hugetlb":
                max_hpmem = self.__get_max_hugetlb_usage(path)
                if max_hpmem is None:
                    pbs.logjobmsg(jobid, "%s: No max hpmem data" %
                                  (caller_name()))
                    return
                hpmem_failcnt = self.__get_hugetlb_failcnt(path)
                if hpmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No hpmem fail count data" %
                                  (caller_name()))
                    return
                if hpmem_failcnt > 0:
                    err_msg = self.__get_error_msg(jobid)
                    pbs.logjobmsg(jobid, "Cgroup hugetlb limit exceeded: %s" %
                                  (err_msg))
                resc_used['hpmem'] = pbs.size(convert_size(max_hpmem, 'kb'))
                pbs.logjobmsg(jobid, "%s: Hugepage usage: %s" %
                              (caller_name(), resc_used['hpmem']))
            elif subsys == "cpuacct":
                cpu_usage = self.__get_cpu_usage(path)
                if cpu_usage is None:
                    pbs.logjobmsg(jobid, "%s: No CPU usage data" %
                                  (caller_name()))
                    return
                cpu_usage = convert_time(str(cpu_usage) + "ns")
                pbs.logjobmsg(jobid, "%s: CPU usage: %.3lf secs" %
                              (caller_name(), cpu_usage))
                resc_used['cput'] = pbs.duration(cpu_usage)

    # Creates the cgroup if it doesn't exists
    def create_job(self, jobid, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Iterate over the subsystems required
        for subsys in self.subsystems:
            # Create a directory for the job
            try:
                old_umask = os.umask(0022)
                path = self.paths[subsys]
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                path = os.path.join(self.paths[subsys], jobid)
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                if subsys == 'devices':
                    self.setup_subsys_devices(path, node)

            except OSError, exc:
                raise CgroupConfigError("Failed to create directory: %s (%s)" %
                                        (path, errno.errorcode[exc.errno]))
            except:
                raise
            finally:
                os.umask(old_umask)

    # Determine the cgroup limits and configure the cgroups
    def configure_job(self, jobid, hostresc, node):
        mem_enabled = 'memory' in self.subsystems
        vmem_enabled = 'memsw' in self.subsystems
        if mem_enabled or vmem_enabled:
            # Initialize mem variables
            mem_avail = node.get_memory_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "mem_avail %s" % mem_avail)
            mem_requested = None
            if 'mem' in hostresc.keys():
                mem_requested = convert_size(hostresc['mem'], 'kb')
            mem_default = None
            if mem_enabled:
                mem_default = self.default('memory')
            # Initialize vmem variables
            vmem_avail = node.get_vmem_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "vmem_avail %s" % vmem_avail)
            vmem_requested = None
            if 'vmem' in hostresc.keys():
                vmem_requested = convert_size(hostresc['vmem'], 'kb')
            vmem_default = None
            if vmem_enabled:
                vmem_default = self.default('memsw')
            # Initialize softmem variables
            if 'soft_limit' in self.cfg['cgroup']['memory'].keys():
                softmem_enabled = self.cfg['cgroup']['memory']['soft_limit']
            else:
                softmem_enabled = False
            # Sanity check
            if size_as_int(mem_avail) > size_as_int(vmem_avail):
                if size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                    raise CgroupLimitError(
                        'mem available (%s) exceeds vmem available (%s)' %
                        (mem_avail, vmem_avail))
            # Determine the mem limit
            if mem_requested is not None:
                # mem requested may not exceed available
                if size_as_int(mem_requested) > size_as_int(mem_avail):
                    raise JobValueError(
                        'mem requested (%s) exceeds mem available (%s)' %
                        (mem_requested, mem_avail))
                mem_limit = mem_requested
            else:
                # mem was not requested
                if mem_default is None:
                    mem_limit = mem_avail
                else:
                    mem_limit = mem_default
            # Determine the vmem limit
            if vmem_requested is not None:
                # vmem requested may not exceed available
                if size_as_int(vmem_requested) > size_as_int(vmem_avail):
                    raise JobValueError(
                        'vmem requested (%s) exceeds vmem available (%s)' %
                        (vmem_requested, vmem_avail))
                vmem_limit = vmem_requested
            else:
                # vmem was not requested
                if vmem_default is None:
                    vmem_limit = vmem_avail
                else:
                    vmem_limit = vmem_default
            # Ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Adjust for soft limits if enabled
            if mem_enabled and softmem_enabled:
                softmem_limit = mem_limit
                # The hard memory limit is assigned the lesser of the vmem
                # limit and available memory
                if size_as_int(vmem_limit) < size_as_int(mem_avail):
                    mem_limit = vmem_limit
                else:
                    mem_limit = mem_avail
            # Again, ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Sanity checks when both memory and memsw are enabled
            if mem_enabled and vmem_enabled:
                if vmem_requested is not None:
                    if size_as_int(vmem_limit) > size_as_int(vmem_requested):
                        # The user requested an invalid limit
                        raise JobValueError(
                            'vmem limit (%s) exceeds vmem requested (%s)' %
                            (vmem_limit, vmem_requested))
                if size_as_int(vmem_limit) > size_as_int(mem_limit):
                    # This job may utilize swap
                    if size_as_int(vmem_avail) <= size_as_int(mem_avail) and \
                      size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                        # No swap available
                        raise CgroupLimitError(
                            ('Job might utilize swap ' +
                             'and no swap space available'))
            # Assign mem and vmem
            if mem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: mem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), mem_limit))
                hostresc['mem'] = pbs.size(mem_limit)
                if softmem_enabled:
                    hostresc['softmem'] = pbs.size(softmem_limit)
            if vmem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: vmem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), vmem_limit))
                hostresc['vmem'] = pbs.size(vmem_limit)
        # Initialize hpmem variables
        hpmem_enabled = 'hugetlb' in self.subsystems
        if hpmem_enabled:
            hpmem_avail = node.get_hpmem_on_node(self.cfg)
            hpmem_limit = None
            hpmem_default = self.default('hugetlb')
            if hpmem_default is None:
                hpmem_default = hpmem_avail
            if 'hpmem' in hostresc.keys():
                hpmem_limit = convert_size(hostresc['hpmem'], 'kb')
            else:
                hpmem_limit = hpmem_default
            # Assign hpmem
            if size_as_int(hpmem_limit) > size_as_int(hpmem_avail):
                raise JobValueError('hpmem limit (%s) exceeds available (%s)' %
                                    (hpmem_limit, hpmem_avail))
            hostresc['hpmem'] = pbs.size(hpmem_limit)
        # Initialize cpuset variables
        cpuset_enabled = 'cpuset' in self.subsystems
        if cpuset_enabled:
            cpu_limit = 1
            if 'ncpus' in hostresc.keys():
                cpu_limit = hostresc['ncpus']
            if cpu_limit < 1:
                cpu_limit = 1
            hostresc['ncpus'] = pbs.pbs_int(cpu_limit)

        # Find the available resources and assign the right ones to the job

        attempt = 0
        assign_resources = False

        # Two attempts since self.cleanup_orphans may actually fix the
        # problem we see in a first attempt
        while attempt < 2:
            attempt += 1
            available_resources = self.available_node_resources(node)
            if 'vnodes' in hostresc:
                assign_resources = self.assign_job_resources_by_vnode(
                    hostresc, available_resources, node)
            else:
                assign_resources = self.assign_job_resources(
                    hostresc, available_resources, node)

            if (assign_resources is False) and (attempt < 2):
                # No resources were assigned to the job.
                # Most likely cause was that a cgroup has
                # not been cleaned up yet
                pbs.logmsg(pbs.EVENT_DEBUG, "Failed to assign resources. " +
                           "Checking the following cgroups on the node " +
                           "with the server: %s" % self.existing_cgroups)

                # Collect the jobs on the node (try reading mom_priv/jobs,
                # if not successful ask server)
                jobs_list = node.gather_jobs_on_node_local()

                if jobs_list is not None:
                    # Don't clean up the cgroup we just made for the
                    # job just yet
                    if jobid not in jobs_list:
                        jobs_list.append(jobid)

                    self.cleanup_orphans(jobs_list)

                    # Recheck what is now assigned after things were cleaned up
                    self.gather_assigned_resources()

        if assign_resources is False:
            # Cleanup cgroups for jobs not present on this node
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Assign resources failed. Attempting to cleanup all " +
                       "leftover cgroups (including event job %s)" % jobid)

            # Remove the current job from the jobs list so it will be
            # cleaned up as well.
            jobs_list.remove(jobid)

            self.cleanup_orphans(jobs_list)

            # Rerun the job and log the message
            line = 'Unable to assign resources to job. '
            line += 'Will requeue the job and try again.'
            line += 'job run_count: %d' % pbs.event().job.run_count
            pbs.event().job.rerun()
            pbs.event().reject(line)

        # Print out the assigned resources
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Assigned resources: %s" % (assign_resources))

        if cpuset_enabled:
            hostresc.pop('ncpus', None)
            for key in ['cpuset.cpus', 'cpuset.mems']:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Initialize devices variables
        devices_enabled = 'devices' in self.subsystems
        if devices_enabled:
            key = 'devices'
            if key in assign_resources:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Apply the resource limits to the cgroups
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Setting cgroup limits for: %s" %
                   (caller_name(), hostresc))

        # The vmem limit must be set after the mem limit, so sort the keys
        for resc in sorted(hostresc.keys()):
            self.set_job_limit(jobid, resc, hostresc[resc])

    # Kill any processes contained within a tasks file
    def __kill_tasks(self, tasks_file):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        with open(tasks_file, 'r') as fd:
            for line in fd:
                os.kill(int(line.strip()), signal.SIGKILL)
        fd.close()
        count = 0
        with open(tasks_file, 'r') as fd:
            for line in fd:
                count += 1
                pid = line.strip()
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Pid: %s did not clean up" %
                           (caller_name(), pid))
                if os.path.isfile('/proc/%s/status' % pid):
                    tmp = open('/proc/%s/status' % pid).readlines()
                    tmp_line = ''
                    for entry in tmp:
                        if entry.find('Name:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('State:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('Uid:') != -1:
                            tmp_line += entry.strip()
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: %s" % (caller_name(), tmp_line))

        return count

    # Perform the actual removal of the cgroup directory
    # by default, only one attempt at killing tasks in cgroup, without sleeping
    # since this method could be called many times
    # (for N directories times M jobs)
    def __remove_cgroup(self, path, tasks_kill_attempts=1):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            if os.path.exists(path):
                tasks_file = os.path.join(path, 'tasks')
                task_cnt = self.__kill_tasks(tasks_file)
                kill_attempts = 0
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Task Kill Attempts: %d" % tasks_kill_attempts)
                while (task_cnt > 0) and (kill_attempts < tasks_kill_attempts):
                    kill_attempts += 1
                    count = 0

                    with open(tasks_file, 'r') as fd:
                        for line in fd:
                            count += 1
                        task_cnt = count

                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "Attempt: %d - cgroup still has %d tasks: %s" %
                               (kill_attempts, task_cnt, path))
                    if kill_attempts < tasks_kill_attempts:
                        time.sleep(1)
                        # Added since there are race conditions where tasks are
                        # being added at the same time we get the initial
                        # task count
                        tasks_file = os.path.join(path, 'tasks')
                        task_cnt = self.__kill_tasks(tasks_file)

                if task_cnt == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Removing directory %s" %
                               (caller_name(), path))
                    os.rmdir(path)
                    if os.path.exists(path):
                        time.sleep(.25)
                        if os.path.exists(path):
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "%s: 2nd Try Removing directory %s" %
                                       (caller_name(), path))
                            os.rmdir(path)
                            time.sleep(.25)
                        if os.path.exists(path):
                            return False
                else:
                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "cgroup still has %d tasks: %s" %
                               (task_cnt, path))

                    # Check to see if the offline file is already present
                    # on the mom
                    offline_file_exists = False
                    if os.path.isfile(self.offline_file):
                        msg = "Cgroup(s) not cleaning up but the node already "
                        msg += "has the offline file."

                        pbs.logmsg(pbs.EVENT_DEBUG, msg)
                    else:
                        # Check to see that the node is not already offline.
                        try:
                            tmp_state = pbs.server().vnode(self.hostname).state
                        except:
                            msg = "Unable to contact server for node state"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            tmp_state = None
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Current Node State: %d" % tmp_state)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Offline Node State: %d" % pbs.ND_OFFLINE)

                        if tmp_state == pbs.ND_OFFLINE:
                            msg = "Cgroup(s) not cleaning up but the node is "
                            msg += "already offline."
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                        elif tmp_state is not None:
                            # Offline the node(s)
                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                            msg = "Offlining node since cgroup(s) are not "
                            msg += "cleaning up"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            vnode = pbs.event().vnode_list[self.hostname]

                            vnode.state = pbs.ND_OFFLINE

                            # Write a file locally to reduce server traffic
                            # when it comes time to online the node
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Offline file: %s" % self.offline_file)
                            open(self.offline_file, 'a').close()
                            vnode.comment = self.offline_msg

                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                        else:
                            pass

                    return False

        except OSError, exc:
            pbs.logmsg(pbs.EVENT_SYSTEM,
                       "Failed to remove cgroup path: %s (%s)" %
                       (path, errno.errorcode[exc.errno]))
        except:
            pbs.logmsg(pbs.EVENT_SYSTEM, "Failed to remove cgroup path: %s" %
                       (path))

        return True

    # Removes cgroup directories that are not associated with a local job
    def cleanup_orphans(self, local_jobs):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        orphan_cnt = 0
        for key in self.paths.keys():
            for dir in glob.glob(os.path.join(self.paths[key], '[0-9]*')):
                jobid = os.path.basename(dir)
                if jobid not in local_jobs:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Removing orphaned cgroup: %s" %
                               (caller_name(), dir))
                    # default number of attempts at killing tasks
                    status = self.__remove_cgroup(dir)
                    if not status:
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "%s: Removing orphaned cgroup %s failed " %
                                   (caller_name(), dir))
                        orphan_cnt += 1
        return orphan_cnt

    # Removes the cgroup directories for a job
    def delete(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        tasks_kill_attempted = False

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            if not tasks_kill_attempted:
                # Make multiple attempts at killing tasks in cgroups on
                # first subsystem encountered since it is "normal" for
                # a cgroup.delete to encounter processes hard to kill
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                                           jobid),
                                              tasks_kill_attempts=(
                                              CGROUP_KILL_ATTEMPTS))
                tasks_kill_attempted = True
            else:
                # We tried getting rid of job processes earlier,
                # so don't try N times with sleeps
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                              jobid), tasks_kill_attempts=1)

            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Status: %s" %
                       (caller_name(), status))
            if status is False:
                # Rerun the job and log the message
                line = 'Unable to cleanup a cgroup. '
                line += 'Will offline the node and requeue the job '
                line += 'and try again. job run_count: %d' % \
                        pbs.event().job.run_count
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Failed to remove cgroup: %s" %
                           (caller_name(), line))
                pbs.event().accept()

    # Write a value to a limit file
    def write_value(self, file, value, mode='w'):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: writing %s to %s" %
                   (caller_name(), value, file))
        try:
            with open(file, mode) as fd:
                fd.write(str(value) + '\n')
        except IOError, exc:
            if exc.errno == errno.ENOENT:
                pbs.logmsg(pbs.EVENT_SYSTEM,
                           "Trying to set limit to unknown file %s" % file)
            elif exc.errno in [errno.EACCES, errno.EPERM]:
                pbs.logmsg(pbs.EVENT_SYSTEM, "Permission denied on file %s" %
                           file)
            elif exc.errno == errno.EBUSY:
                raise CgroupBusyError("Limit rejected")
            elif exc.errno == errno.EINVAL:
                raise CgroupLimitError("Invalid limit value: %s" % (value))
            else:
                raise
        except:
            raise

    # Return memory failcount
    def __get_mem_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.failcnt"), 'r').read().strip())
        except:
            return None

    # Return vmem failcount
    def __get_memsw_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.failcnt"), 'r').read().strip())
        except:
            return None

    # Return hpmem failcount
    def __get_hugetlb_failcnt(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.failcnt"))[0], 'r').read().strip())
        except:
            return None

    # Return the max usage of memory in bytes
    def __get_max_mem_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of memsw in bytes
    def __get_max_memsw_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of hugetlb in bytes
    def __get_max_hugetlb_usage(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.max_usage_in_bytes"))[0],
                            'r').read().strip())
        except:
            return None

    # Return the cpuacct.usage in cpu seconds
    def __get_cpu_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "cpuacct.usage"), 'r').read().strip())
        except:
            return None

    # Assign CPUs to the cpuset
    def select_cpus(self, path, ncpus):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path is %s" % (caller_name(), path))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: ncpus is %s" %
                   (caller_name(), ncpus))
        if ncpus < 1:
            ncpus = 1
        try:
            # Must select from those currently available
            cpufile = os.path.basename(path)
            base = os.path.dirname(path)
            parent = os.path.dirname(base)
            avail = cpus2list(open(os.path.join(parent, cpufile),
                              'r').read().strip())
            if len(avail) < 1:
                raise CgroupProcessingError("No CPUs avaialble in cgroup.")
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Available CPUs: %s" %
                       (caller_name(), avail))
            assigned = []
            for file in glob.glob(os.path.join(parent, '[0-9]*', cpufile)):
                cpus = cpus2list(open(file, 'r').read().strip())
                for id in cpus:
                    if id in avail:
                        avail.remove(id)
            if len(avail) < ncpus:
                raise CgroupProcessingError(
                    "Insufficient CPUs avaialble in cgroup.")
            if len(avail) == ncpus:
                return avail
            # FUTURE: Try to minimize NUMA nodes based on memory requirement
            return avail[0:ncpus]
        except:
            raise

    # Return the error message in system message file
    def __get_error_msg(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        proc = subprocess.Popen(['dmesg'],
                                shell=False, stdout=subprocess.PIPE)
        stdout = proc.communicate()[0].splitlines()
        stdout.reverse()
        # Check to see if the job id is found in dmesg otherwise
        # you could get the information for another job that was killed
        # the line will look like Memory cgroup stats for /pbspro/279.centos7
        kill_line = ""

        for line in stdout:
            start = line.find('Killed process ')
            if start >= 0:
                kill_line = line[start:]
            job_start = line.find(jobid)
            if job_start >= 0:
                # pbs.logmsg(pbs.EVENT_DEBUG3,
                #           "%s: Jobid: %s, Line: %s" %
                #           (caller_name(), jobid, line))
                return kill_line
        return ""

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_job_env_file(self, jobid, env_list):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        # if not os.path.exists(self.host_job_env_dir):
        #    os.makedirs(self.host_job_env_dir, 0755)

        # Write out assigned_resources
        try:
            lines = string.join(env_list, '\n')
            filename = self.host_job_env_filename % jobid
            outfile = open(filename, "w")
            outfile.write(lines)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" % (filename))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (lines))
            return True
        except:
            return False

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        if not os.path.exists(self.hook_storage_dir):
            os.makedirs(self.hook_storage_dir, 0755)

        # Write out assigned_resources
        try:
            json_str = json.dumps(self.host_assigned_resources)
            outfile = open(self.hook_storage_dir+os.sep+jobid, "w")
            outfile.write(json_str)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" %
                       (self.hook_storage_dir+os.sep+jobid))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (json_str))
            return True
        except:
            return False

    def read_in_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        pbs.logmsg(pbs.EVENT_DEBUG3, "Host assigned resources: %s" %
                   (self.host_assigned_resources))
        if os.path.isfile(self.hook_storage_dir+os.sep+jobid):
            # Write out assigned_resources
            try:
                infile = open(self.hook_storage_dir+os.sep+jobid, 'r')
                json_data = json.load(infile, object_hook=decode_dict)
                self.host_assigned_resources = json_data
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Host assigned resources: %s" %
                           (self.host_assigned_resources))
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
            finally:
                infile.close()

        if self.host_assigned_resources is not None:
            return True
        else:
            return False

#
#
# FUNCTION main
#


def main():
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Function called" % (caller_name()))
    hostname = pbs.get_local_nodename()
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Host is %s" % (caller_name(), hostname))

    # Log the hook event type
    e = pbs.event()
    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook name is %s" %
               (caller_name(), e.hook_name))

    try:
        # Instantiate the hook utility class
        hooks = HookUtils()
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Event type is %s" %
                   (caller_name(), hooks.event_name(e.type)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(hooks)))
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: Failed to instantiate hook utility class" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        e.accept()

    if not hooks.hashandler(e.type):
        # Bail out if there is no handler for this event
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s event not handled by this hook" %
                   (caller_name(), hooks.event_name(e.type)))
        e.accept()

    try:
        # If an exception occurs, jobutil must be set
        jobutil = None
        # Instantiate the job utility class first so jobutil can be accessed
        # by the exception handlers.
        if hasattr(e, 'job'):
            jobutil = JobUtils(e.job)
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Job information class instantiated" %
                       (caller_name()))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" %
                       (caller_name(), repr(jobutil)))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Event does not include a job" %
                       (caller_name()))
        # Instantiate the cgroup utility class
        vnode = None
        if hasattr(e, 'vnode_list'):
            if hostname in e.vnode_list.keys():
                vnode = e.vnode_list[hostname]
        cgroup = CgroupUtils(hostname, vnode)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Cgroup utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(cgroup)))
        # Bail out if there is nothing to do
        if len(cgroup.subsystems) < 1:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: All cgroups disabled" %
                       (caller_name()))
            e.accept()

        # Call the appropriate handler
        if hooks.invoke_handler(e, cgroup, jobutil) is True:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned success" %
                       (caller_name()))
            e.accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned failure" %
                       (caller_name()))
            e.reject()

    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass

    except AdminError, exc:
        # Something on the system is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Admin error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except UserError, exc:
        # User must correct problem and resubmit job
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("User error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.delete()
                msg += " (deleted)"
            except:
                msg += " (delete failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except JobValueError, exc:
        # Something in PBS is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Job value error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except Exception, exc:
        # Catch all other exceptions and report them.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Unexpected error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

# line below required for unittesting
if __name__ == "__builtin__":
    start_time = time.time()
    try:
        main()
    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
    finally:
        pbs.logmsg(pbs.EVENT_DEBUG, "Elapsed time: %0.4lf" %
                   (time.time() - start_time))
---
2016-07-18 16:56:51,701 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups.json', 'content-encoding': 'default'}
2016-07-18 16:56:51,701 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups.json
2016-07-18 16:56:51,770 INFO     tmp_cfg: {
        "cgroup_prefix"         : "pbspro",
        "periodic_resc_update"  : true, 
        "exclude_hosts"         : [],
        "exclude_vntypes"       : [],
        "run_only_on_hosts"     : [],   
        "vnode_per_numa_node"   : false,
        "online_offlined_nodes" : true, 
        "cgroup":
        {
                "cpuacct":
                {
                        "enabled"               : true,
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : []
                },
                "cpuset":
                {
                        "enabled"               : true,
                        "exclude_hosts"         : ["centos7"],
                        "exclude_vntypes"       : []
                },
                "devices":
                {
                        "enabled"               : false,
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : [],
                        "allow" : ["b *:* rwm","c *:* rwm", ["mic/scif","rwm"],["nvidiactl","rwm", "*"],["nvidia-uvm","rwm"]]
                },
                "hugetlb":
                {
                        "enabled"               : false,
                        "default"               : "0MB",
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : []
                },
                "memory":
                {
                        "enabled"               : true,
                        "default"               : "256MB",
                        "reserve_memory"        : "0MB",
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : []
                },
                "memsw":
                {
                        "enabled"               : true,
                        "default"               : "256MB",
                        "reserve_memory"        : "2gb",
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : []
                }
        }
}

2016-07-18 16:56:51,770 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/tmp/t1_l1.json', 'content-encoding': 'default'}
2016-07-18 16:56:51,771 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /tmp/t1_l1.json
2016-07-18 16:56:51,794 ERROR    err: ["hook 't1_l1' contents overwritten"]
2016-07-18 16:56:51,795 INFO     job: executable set to /bin/sleep with arguments: 100
2016-07-18 16:56:51,796 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0755 /tmp/PtlPbsJobScriptb_ZLsY
2016-07-18 16:56:51,807 INFOCLI  centos7: sudo -H -u pbsuser /usr/local/bin/qsub -N test_t4c -l select=1:ncpus=1:mem=300mb /tmp/PtlPbsJobScriptb_ZLsY
2016-07-18 16:56:51,824 INFO     submit to centos7 as pbsuser: job 381.centos7 OrderedDict([('Job_Name', 'test_t4c'), ('Resource_List.select', '1:ncpus=1:mem=300mb')])
2016-07-18 16:56:51,824 INFOCLI  job script /tmp/PtlPbsJobScriptb_ZLsY
---
#!/usr/bin/env python

import os
import subprocess
from subprocess import Popen, PIPE
import time

eat_mem_c = """
#include <stdio.h>
#include <stdlib.h>
#include <string.h>


#define ONE_GB (0x40000000L)
#define ONE_MB (0x100000L)

void
eatmem(total_mb, block_mb)
{
	int i;
	char *buf;
	size_t block;

	for (i=1, buf=NULL, block=(block_mb*ONE_MB); (i*block) < (total_mb*ONE_MB); i++) {
		buf = realloc((void *)buf, (i*block));
		memset(buf+((i-1)*block), 0, block);
	}
	return;
}

int
main(int argc, char *argv[])
{
	int total_mb = 8;
	int block_mb = 1;

	if (argc > 1)
		total_mb = atoi(argv[1]);

	if (argc > 2)
		block_mb = atoi(argv[2]);

	eatmem(total_mb, block_mb);
	printf("Initialized %dMB in blocks of %dMB\\n", total_mb, block_mb);
}
"""

fname = "/tmp/pbs_pp325_eat_mem.c"
fname_exe = "/tmp/pbs_pp325_eat_mem"
fout = open(fname, "w")
fout.write(eat_mem_c)
fout.close()

print os.getppid()

p = Popen(["gcc", "-o", fname_exe, fname], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

p = Popen([fname_exe, "400", "10"], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

os.remove(fname)
os.remove(fname_exe)
time.sleep(1)

---
2016-07-18 16:56:51,824 INFOCLI  centos7: /usr/local/bin/qstat -f 381.centos7
2016-07-18 16:56:51,919 INFO     expect on server centos7: job_state = R job 381.centos7  got: job_state = Q
2016-07-18 16:56:52,420 INFOCLI  centos7: /usr/local/bin/qmgr -c set server scheduling=True
2016-07-18 16:56:52,436 INFOCLI  centos7: /usr/local/bin/qstat -f 381.centos7
2016-07-18 16:56:52,531 INFO     expect on server centos7: job_state = R job 381.centos7 attempt: 2 ...  OK
2016-07-18 16:56:52,539 INFO     mom centos7 log match: searching for ".*cgroup excluded for subsystem cpuset on host centos7.*" - using regular expression  - No match
2016-07-18 16:57:04,413 INFO     mom centos7 log match: searching for ".*cgroup excluded for subsystem cpuset on host centos7.*" - using regular expression  - attempt 2
2016-07-18 16:57:16,814 INFO     mom centos7 log match: searching for ".*cgroup excluded for subsystem cpuset on host centos7.*" - using regular expression  - attempt 3
2016-07-18 16:57:29,289 INFO     mom centos7 log match: searching for ".*cgroup excluded for subsystem cpuset on host centos7.*" - using regular expression  - attempt 4
2016-07-18 16:57:41,503 INFO     mom centos7 log match: searching for ".*cgroup excluded for subsystem cpuset on host centos7.*" - using regular expression  - attempt 5
2016-07-18 16:57:42,506 INFO     FAILED

2016-07-18 16:57:42,507 INFO     ==============================
2016-07-18 16:57:42,507 INFO      Entered CgroupsTests tearDown
2016-07-18 16:57:42,507 INFO     ==============================
2016-07-18 16:57:42,507 INFO     ===============================
2016-07-18 16:57:42,507 INFO     Completed CgroupsTests tearDown
2016-07-18 16:57:42,507 INFO     ===============================
2016-07-18 16:57:42,507 INFO     test name: test_t4d (tests.cgroups_tests.CgroupsTests)...
2016-07-18 16:57:42,508 INFO     test start time: Mon Jul 18 16:57:42 2016
2016-07-18 16:57:42,508 INFO     test docstring: 
 Test to verify that the cgroup hook only runs on nodes
            in the run_only_on_hosts
        
2016-07-18 16:57:42,508 INFO     ===========================
2016-07-18 16:57:42,508 INFO      Entered CgroupsTests setUp
2016-07-18 16:57:42,508 INFO     ===========================
2016-07-18 16:57:42,508 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:57:42,562 INFO     status on centos7: server
2016-07-18 16:57:42,563 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:57:42,622 INFO     manager on centos7: set server {'managers': (3, 'root@*')}
2016-07-18 16:57:42,623 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers-=root@*
2016-07-18 16:57:42,647 INFO     manager on centos7: set server {'managers': (2, 'root@*')}
2016-07-18 16:57:42,647 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers+=root@*
2016-07-18 16:57:42,682 INFO     server centos7: reverting configuration to defaults
2016-07-18 16:57:42,682 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:57:42,734 INFO     select on centos7: __ALL__
2016-07-18 16:57:42,734 INFOCLI  centos7: /usr/local/bin/qselect
2016-07-18 16:57:42,746 INFOCLI  centos7: /usr/local/bin/qstat -f @centos7.usa.org
2016-07-18 16:57:42,797 INFO     expect on server centos7: job_state set 0 job ...  OK
2016-07-18 16:57:42,798 INFOCLI  centos7: /usr/local/bin/pbs_rstat -f
2016-07-18 16:57:42,825 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:57:42,851 INFO     manager on centos7: delete hook t1_l1
2016-07-18 16:57:42,851 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c delete hook t1_l1
2016-07-18 16:57:42,883 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:57:42,905 INFO     expect on server centos7:  unset  hook t1_l1 ...  OK
2016-07-18 16:57:42,905 INFOCLI  centos7: /usr/local/bin/qstat -Qf @centos7.usa.org
2016-07-18 16:57:42,916 INFO     manager on centos7: delete queue workq
2016-07-18 16:57:42,916 INFOCLI  centos7: /usr/local/bin/qmgr -c delete queue workq
2016-07-18 16:57:42,927 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:57:42,937 INFO     expect on server centos7:  unset  queue workq ...  OK
2016-07-18 16:57:42,938 INFO     manager on centos7: create queue workq {'started': 'True', 'queue_type': 'Execution', 'enabled': 'True'}
2016-07-18 16:57:42,938 INFOCLI  centos7: /usr/local/bin/qmgr -c create queue workq started=True,queue_type=Execution,enabled=True
2016-07-18 16:57:42,952 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:57:42,969 INFO     expect on server centos7: started set True || queue_type set Execution || enabled set True queue workq ...  OK
2016-07-18 16:57:42,970 INFO     manager on centos7: set server {'log_events': '511', 'default_queue': 'workq'}
2016-07-18 16:57:42,970 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=511,default_queue=workq
2016-07-18 16:57:42,986 INFO     status on centos7: resource
2016-07-18 16:57:42,986 INFO     manager on centos7: list resource
2016-07-18 16:57:42,986 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource
2016-07-18 16:57:43,057 INFO     manager on centos7: delete resource nmics,ngpus
2016-07-18 16:57:43,057 INFOCLI  centos7: /usr/local/bin/qmgr -c delete resource nmics,ngpus
2016-07-18 16:57:43,128 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource nmics
2016-07-18 16:57:43,144 INFO     expect on server centos7:  unset  resource nmics ...  OK
2016-07-18 16:57:43,144 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource ngpus
2016-07-18 16:57:43,159 INFO     expect on server centos7:  unset  resource ngpus ...  OK
2016-07-18 16:57:43,159 INFOCLI  status on centos7: server license_count
2016-07-18 16:57:43,159 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:57:43,215 INFO     server: centos7.usa.org licensed
2016-07-18 16:57:43,246 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/server_priv/comm.lock
2016-07-18 16:57:43,312 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:57:43,332 INFO     scheduler centos7: reverting configuration to defaults
2016-07-18 16:57:43,332 INFO     manager on centos7: list sched
2016-07-18 16:57:43,333 INFOCLI  centos7: /usr/local/bin/qmgr -c list sched
2016-07-18 16:57:43,343 INFO     manager on centos7: unset sched ['sched_cycle_length']
2016-07-18 16:57:43,344 INFOCLI  centos7: /usr/local/bin/qmgr -c unset sched sched_cycle_length
2016-07-18 16:57:43,360 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/dedicated_time
2016-07-18 16:57:43,378 INFOCLI2 centos7: cmp /usr/local/etc/pbs_resource_group /var/spool/pbs/sched_priv/resource_group
2016-07-18 16:57:43,381 INFO     scheduler centos7: reverting holidays file to default
2016-07-18 16:57:43,381 INFOCLI2 centos7: cmp /usr/local/etc/pbs_holidays /var/spool/pbs/sched_priv/holidays
2016-07-18 16:57:43,383 INFOCLI2 centos7: cmp /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:57:43,386 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:57:43,400 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:57:43,415 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:57:43,415 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:57:43,476 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:57:43,502 INFOCLI2 centos7: sudo -H cmp /usr/local/sbin/pbs_mom /usr/local/sbin/pbs_mom.cpuset
2016-07-18 16:57:43,539 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:57:43,574 INFO     mom centos7: reverting configuration to defaults
2016-07-18 16:57:43,575 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/epilogue
2016-07-18 16:57:43,590 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/prologue
2016-07-18 16:57:43,602 INFOCLI2 centos7: sudo -H ls /var/spool/pbs/mom_priv/config.d
2016-07-18 16:57:43,615 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbs7RriUS /var/spool/pbs/mom_priv/config
2016-07-18 16:57:43,625 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/mom_priv/config
2016-07-18 16:57:43,646 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/mom_priv/config
2016-07-18 16:57:43,670 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/mom_priv/config
2016-07-18 16:57:43,701 INFO     mom centos7: sent signal -HUP
2016-07-18 16:57:43,702 INFOCLI2 centos7: sudo -H kill -HUP 42976
2016-07-18 16:57:43,744 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:57:43,770 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 16:57:43,784 INFO     status on centos7: node centos7
2016-07-18 16:57:43,784 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:57:43,805 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:57:43,822 INFO     expect on server centos7: state = free node centos7 ...  OK
2016-07-18 16:57:43,823 INFO     ============================
2016-07-18 16:57:43,823 INFO     Completed CgroupsTests setUp
2016-07-18 16:57:43,823 INFO     ============================
2016-07-18 16:57:43,823 INFO     server centos7: server operating mode set to cli
2016-07-18 16:57:43,823 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:57:43,877 INFO     expect on server centos7: pbs_version ~ .*14.* server centos7.usa.org ...  OK
2016-07-18 16:57:43,878 INFO     manager on centos7: set server {'log_events': '2047'}
2016-07-18 16:57:43,878 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=2047
2016-07-18 16:57:43,895 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:57:43,953 INFO     expect on server centos7: log_events set 2047 server centos7.usa.org ...  OK
2016-07-18 16:57:43,953 INFO     scheduler centos7: config {'resources': 'ncpus,mem,host,vnode,vmem'}
2016-07-18 16:57:43,954 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /var/spool/pbs/sched_priv/sched_config /var/spool/pbs/sched_priv/sched_config.bak
2016-07-18 16:57:43,984 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbszijuzZ /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:57:43,997 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:57:44,024 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:57:44,036 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:57:44,056 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:57:44,057 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:57:44,073 INFO     scheduler centos7 log match: searching for "Error reading line" - No match
2016-07-18 16:57:45,075 INFO     manager on centos7 as root: create resource nmics {'flag': 'nh', 'type': 'long'}
2016-07-18 16:57:45,075 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource nmics flag=nh,type=long
2016-07-18 16:57:45,104 INFO     manager on centos7 as root: create resource ngpus {'flag': 'nh', 'type': 'long'}
2016-07-18 16:57:45,104 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource ngpus flag=nh,type=long
2016-07-18 16:57:45,129 INFO     Test Summary: test_t1_l1 tests "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" cgroup hook
2016-07-18 16:57:45,129 INFO     status on centos7: hook
2016-07-18 16:57:45,129 INFO     manager on centos7: list hook
2016-07-18 16:57:45,130 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:57:45,157 INFO     status on centos7: hook
2016-07-18 16:57:45,157 INFO     manager on centos7: list hook
2016-07-18 16:57:45,158 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:57:45,189 INFO     manager on centos7: create hook t1_l1
2016-07-18 16:57:45,189 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c create hook t1_l1
2016-07-18 16:57:45,228 INFO     manager on centos7: set hook t1_l1 {'freq': 2, 'enabled': 'True', 'event': '"execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"'}
2016-07-18 16:57:45,228 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set hook t1_l1 freq=2,enabled=True,event="execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"
2016-07-18 16:57:45,257 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:57:45,301 INFO     expect on server centos7: freq set 2 || enabled set True || event set "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" hook t1_l1 ...  OK
2016-07-18 16:57:45,302 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-python', 'input-file': '/tmp/PtlPbsPhu4J_', 'content-encoding': 'default'}
2016-07-18 16:57:45,303 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-python default /tmp/PtlPbsPhu4J_
2016-07-18 16:57:45,371 INFO     server centos7: imported hook body
---
#  Copyright (C) 2003-2016 Altair Engineering, Inc. All rights reserved.
#
#  ALTAIR ENGINEERING INC. Proprietary and Confidential. Contains Trade Secret
#  Information. Not for use or disclosure outside ALTAIR and its licensed
#  clients. Information contained herein shall not be decompiled, disassembled,
#  duplicated or disclosed in whole or in part for any purpose. Usage of the
#  software is only as explicitly permitted in the end user software license
#  agreement.
#
#  Copyright notice does not imply publication.

# Last Updated
# Date: 07-06-2016
#
# When soft_limit is true for memory, memsw represents the hard limit.
#
# The resources value in sched_config must contain entries for mem and
# vmem if those subsystems are enabled in the hook configuration file. The
# amount of resource requested will not be avaiable to the hook if they
# are not present.

# This hook handles all of the operations necessary for PBS to support
# cgroups on linux hosts that support them (kernel 2.6.28 and higher)
#
# This hook services the following events:
# - exechost_periodic
# - exechost_startup
# - execjob_attach
# - execjob_begin
# - execjob_end
# - execjob_epilogue
# - execjob_launch

# Imports from __future__ must happen first
from __future__ import with_statement

# Additional imports
import sys
import os
import errno
import signal
import subprocess
import re
import glob
import time
import string
import platform
import traceback
import copy
from stat import S_ISBLK, S_ISCHR
import operator
import pwd
import pbs
import fnmatch
import socket

try:
    import json
except:
    import simplejson as json

CGROUP_KILL_ATTEMPTS = 12

# Flag to turn off features not in 12.2.40X branch
CRAY_12_BRANCH = False


# ============================================================================
# Derived error classes
# ============================================================================

# Base class for errors fixable only by administrative action.


class AdminError(Exception):
    pass

# Base class for errors in processing, unknown cause.


class ProcessingError(Exception):
    pass

# Base class for errors fixable by the user.


class UserError(Exception):
    pass

# Errors in PBS job resource values.


class JobValueError(UserError):
    pass

# Errors when the cgroup is busy.


class CgroupBusyError(ProcessingError):
    pass

# Errors in configuring cgroup.


class CgroupConfigError(AdminError):
    pass

# Errors in configuring cgroup.


class CgroupLimitError(AdminError):
    pass

# Errors processing cgroup.


class CgroupProcessingError(ProcessingError):
    pass

# ============================================================================
# Utility functions
# ============================================================================

#
# FUNCTION caller_name
#
# Return the name of the calling function or method.
#


def caller_name():
    return str(sys._getframe(1).f_code.co_name)

#
# FUNCTION convert_size
#
# Convert a string containing a size specification (e.g. "1m") to a
# string using different units (e.g. "1024k").
#
# This function only interprets a decimal number at the start of the string,
# stopping at any unrecognized character and ignoring the rest of the string.
#
# When down-converting (e.g. MB to KB), all calculations involve integers and
# the result returned is exact. When up-converting (e.g. KB to MB) floating
# point numbers are involved. The result is rounded up. For example:
#
# 1023MB -> GB yields 1g
# 1024MB -> GB yields 1g
# 1025MB -> GB yields 2g  <-- This value was rounded up
#
# Pattern matching or conversion may result in exceptions.
#


def convert_size(value, units='b'):
    logs = {'b': 0, 'k': 10, 'm': 20, 'g': 30,
            't': 40, 'p': 50, 'e': 60, 'z': 70, 'y': 80}
    try:
        new = units[0].lower()
        if new not in logs.keys():
            new = 'b'
        val, old = re.match('([-+]?\d+)([bkmgtpezy]?)',
                            str(value).lower()).groups()
        val = int(val)
        if val < 0:
            raise ValueError('Value may not be negative')
        if old not in logs.keys():
            old = 'b'
        factor = logs[old] - logs[new]
        val *= 2 ** factor
        slop = val - int(val)
        val = int(val)
        if slop > 0:
            val += 1
        # pbs.size() does not like units following zero
        if val <= 0:
            return '0'
        else:
            return str(val) + new
    except:
        return None

#
# FUNCTION size_as_int
#
# Convert a size string to an integer representation of size in bytes
#


def size_as_int(value):
    return int(convert_size(value).rstrip(string.ascii_lowercase))

#
# FUNCTION convert_time
#
# Converts a integer value for time into the value of the return unit
#
# A valid decimal number, with optional sign, may be followed by a character
# representing a scaling factor.  Scaling factors may be either upper or
# lower case. Examples include:
# 250ms
# 40s
# +15min
#
# Valid scaling factors are:
# ns  = 10**-9
# us  = 10**-6
# ms  = 10**-3
# s   =      1
# min =     60
# hr  =   3600
#
# Pattern matching or conversion may result in exceptions.
#


def convert_time(value, return_unit='s'):
    multipliers = {'':  1, 'ns': 10 ** -9, 'us': 10 ** -6,
                   'ms': 10 ** -3, 's': 1, 'min': 60, 'hr': 3600}
    n, factor = re.match('([-+]?\d+)(\w*[a-zA-Z])?',
                         str(value).lower()).groups(0)

    # Check to see if there was not unit of time specified
    if factor == 0:
        factor = ''

    # Check to see if the unit is valid
    if str.lower(factor) not in multipliers.keys():
        raise ValueError('Time unit not recognized.')

    # Convert the value to seconds
    value_in_sec = float(n) * float(multipliers[str.lower(factor)])
    if return_unit != 's':
        return value_in_sec / multipliers[str.lower(return_unit)]
    else:
        return float('%lf' % value_in_sec)

# simplejson hook to convert lists from unicode to utf-8


def decode_list(data):
    rv = []
    for item in data:
        if isinstance(item, unicode):
            item = item.encode('utf-8')
        elif isinstance(item, list):
            item = decode_list(item)
        elif isinstance(item, dict):
            item = decode_dict(item)
        rv.append(item)
    return rv

# simplejson hook to convert dictionaries from unicode to utf-8


def decode_dict(data):
    rv = {}
    for key, value in data.iteritems():
        if isinstance(key, unicode):
            key = key.encode('utf-8')
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        elif isinstance(value, list):
            value = decode_list(value)
        elif isinstance(value, dict):
            value = decode_dict(value)
        rv[key] = value
    return rv

# Convert CPU list format (with ranges) to a Python list


def cpus2list(s):
    # The input string is a comma separated list of digits and ranges.
    # Examples include:
    # 0-3,8-11
    # 0,2,4,6
    # 2,5-7,10
    cpus = []
    if s.strip() == '':
        return cpus
    for r in s.split(','):
        if '-' in r[1:]:
            start, end = r.split('-', 1)
            for i in range(int(start), int(end) + 1):
                cpus.append(int(i))
        else:
            cpus.append(int(r))
    return cpus


# Helper function to ignore suspended jobs when removing
# "already used" resources
def job_to_be_ignored(jobid):
    # Get job substate from small utility that calls printjob
    pbs_exec = ''
    if not CRAY_12_BRANCH:
        pbs_conf = pbs.get_pbs_conf()
        pbs_exec = pbs_conf['PBS_EXEC']
    else:
        if 'PBS_EXEC' in self.cfg:
            pbs_exec = self.cfg['PBS_EXEC']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "PBS_EXEC needs to be defined in the config file")
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Exiting the cgroups hook")
            pbs.accept()

    printjob_cmd = pbs_exec+os.sep+'bin'+os.sep+'printjob'

    cmd = [printjob_cmd, jobid]

    substate = None
    try:
        pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
        # Collect the job substate information
        process = subprocess.Popen(
                                   cmd,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        (out, err) = process.communicate()
        # Find the job substate
        substate_re = re.compile(r"substate:\s+(?P<jobstate>\S+)\s+")
        substate = substate_re.search(out)
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Unexpected error in job_to_be_ignored: %s" %
                   ' '.join([repr(sys.exc_info()[0]),
                            repr(sys.exc_info()[1])]))
        out = "Unknown: failed to run " + printjob_cmd

    if substate is not None:
        substate = substate.group().split(':')[1].strip()
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Job %s has substate %s" % (jobid, substate))

        suspended_substates = ['0x2b', '0x2d', 'unknown']
        return (substate in suspended_substates)
    else:
        return False

# ============================================================================
# Utility classes
# ============================================================================

#
# CLASS HookUtils
#


class HookUtils:

    def __init__(self, hook_events=None):
        if hook_events is not None:
            self.hook_events = hook_events
        else:
            self.hook_events = {
                # Defined in the order they appear in module_pbs_v1.c
                pbs.QUEUEJOB: {
                    'name': 'queuejob',
                    'handler': None
                },
                pbs.MODIFYJOB: {
                    'name': 'modifyjob',
                    'handler': None
                },
                pbs.RESVSUB: {
                    'name': 'resvsub',
                    'handler': None
                },
                pbs.MOVEJOB: {
                    'name': 'movejob',
                    'handler': None
                },
                pbs.RUNJOB: {
                    'name': 'runjob',
                    'handler': None
                },
                pbs.PROVISION: {
                    'name': 'provision',
                    'handler': None
                },
                pbs.EXECJOB_BEGIN: {
                    'name': 'execjob_begin',
                    'handler': self.__execjob_begin_handler
                },
                pbs.EXECJOB_PROLOGUE: {
                    'name': 'execjob_prologue',
                    'handler': None
                },
                pbs.EXECJOB_EPILOGUE: {
                    'name': 'execjob_epilogue',
                    'handler': self.__execjob_epilogue_handler
                },
                pbs.EXECJOB_PRETERM: {
                    'name': 'execjob_preterm',
                    'handler': None
                },
                pbs.EXECJOB_END: {
                    'name': 'execjob_end',
                    'handler': self.__execjob_end_handler
                },
                pbs.EXECJOB_LAUNCH: {
                    'name': 'execjob_launch',
                    'handler': self.__execjob_launch_handler
                },
                pbs.EXECHOST_PERIODIC: {
                    'name': 'exechost_periodic',
                    'handler': self.__exechost_periodic_handler
                },
                pbs.EXECHOST_STARTUP: {
                    'name': 'exechost_startup',
                    'handler': self.__exechost_startup_handler
                },
                pbs.MOM_EVENTS: {
                    'name': 'mom_events',
                    'handler': None
                },
            }

            if not CRAY_12_BRANCH:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "CRAY 12 Branch: %s" % (CRAY_12_BRANCH))
                self.hook_events[pbs.EXECJOB_ATTACH] = {
                    'name': 'execjob_attach',
                    'handler': self.__execjob_attach_handler
                }

    def __repr__(self):
        return "HookUtils(%s)" % (repr(self.hook_events))

    def event_name(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['name']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Type: %s not found" % (caller_name(), type))
            return None

    def hashandler(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['handler'] is not None
        else:
            return None

    def invoke_handler(self, event, cgroup, jobutil, *args):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: UID: real=%d, effective=%d" %
                   (caller_name(), os.getuid(), os.geteuid()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: GID: real=%d, effective=%d" %
                   (caller_name(), os.getgid(), os.getegid()))
        if self.hashandler(event.type):
            result = self.hook_events[event.type][
                'handler'](event, cgroup, jobutil, *args)
            return result
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: %s event not handled by this hook." %
                       (caller_name(), self.event_name(event.type)))

    def __execjob_begin_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Instantiate the NodeConfig class for get_memory_on_node and
        # get_vmem_on node
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Host assigned job resources: %s" %
                   (caller_name(), jobutil.host_resources))

        # Make sure the parent cgroup directories exist
        cgroup.create_paths()

        # Make sure that any old cgroups created in an earlier attempt
        # at creating or running the job are gone
        cgroup.delete(e.job.id)

        # Gather the assigned resources
        cgroup.gather_assigned_resources()
        # Create the cgroup(s) for the job
        cgroup.create_job(e.job.id, node)
        # Configure the new cgroup
        cgroup.configure_job(e.job.id, jobutil.host_resources, node)
        # Initialize resource usage for the job
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        # Write out the assigned resources
        cgroup.write_out_cgroup_host_assigned_resources(e.job.id)
        # Write out the environment variable for the host (pbs_attach)
        if 'devices_name' in cgroup.host_assigned_resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Devices: %s" %
                       cgroup.host_assigned_resources['devices_name'])
            env_list = list()
            if len(cgroup.host_assigned_resources['devices_name']) > 0:
                line = str()
                mics = list()
                gpus = list()
                for key in cgroup.host_assigned_resources['devices_name']:
                    if key.startswith('mic'):
                        mics.append(key[3:])
                    elif key.startswith('nvidia'):
                        gpus.append(key[6:])
                if len(mics) > 0:
                    env_list.append('OFFLOAD_DEVICES="%s"' %
                                    string.join(mics, ','))
                if len(gpus) > 0:
                    # don't put quotes around the values. ex "0" or "0,1".
                    # This will cause it to fail.
                    env_list.append('CUDA_VISIBLE_DEVICES=%s' %
                                    string.join(gpus, ','))

            pbs.logmsg(pbs.EVENT_DEBUG, "ENV_LIST: %s" % env_list)
            cgroup.write_out_cgroup_host_job_env_file(e.job.id, env_list)
        return True

    def __execjob_epilogue_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # The resources_used information has a base type of pbs_resource
        # Set the usage data
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        return True

    def __execjob_end_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Delete the cgroup(s) for the job
        cgroup.delete(e.job.id)
        # Remove the host_assigned_resources and job_env file
        for filename in [cgroup.hook_storage_dir+os.sep+e.job.id,
                         cgroup.host_job_env_filename % e.job.id]:
            try:
                os.remove(filename)
            except OSError:
                pbs.logmsg(pbs.EVENT_DEBUG3, "File: %s not found" % (filename))
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Error removing file: %s" % (filename))
                pass

        return True

    def __execjob_launch_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Add the parent process id to the appropriate cgroups.
        cgroup.add_pids(os.getppid(), jobutil.job.id)
        # FUTURE: Add environment variable to the job environment
        # if job requested mic or gpu
        cgroup.read_in_cgroup_host_assigned_resources(e.job.id)
        if cgroup.host_assigned_resources is not None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "host_assigned_resources: %s" %
                       (cgroup.host_assigned_resources))
            cgroup.setup_job_devices_env()
        return True

    def __exechost_periodic_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Cleanup cgroups for jobs not present on this node
        count = cgroup.cleanup_orphans(e.job_list)
        if ('periodic_resc_update' in cgroup.cfg and
                cgroup.cfg['periodic_resc_update']):
            for job in e.job_list.keys():
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: job is %s" %
                           (caller_name(), job))
                cgroup.update_job_usage(job, e.job_list[job].resources_used)
        # Online nodes that were offlined due to a cgroup not cleaning up
        if count == 0 and cgroup.cfg['online_offlined_nodes']:
            vnode = pbs.event().vnode_list[cgroup.hostname]
            if os.path.isfile(cgroup.offline_file):
                line = 'Orphan cgroup(s) have been cleaned up. '

                # Check with the pbs server to see if the node comment matches
                try:
                    tmp_comment = pbs.server().vnode(cgroup.hostname).comment
                except:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Unable to contact server for node comment")
                    tmp_comment = None
                if tmp_comment == cgroup.offline_msg:
                    line += 'Will bring the node back online'
                    vnode.state = pbs.ND_FREE
                    vnode.comment = None
                else:
                    line += 'However, the comment has changed since the node '
                    line += 'was offlined. Node will remain offline'

                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: %s" % (caller_name(), line))
                # Remove file
                os.remove(cgroup.offline_file)
        return True

    def __exechost_startup_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        cgroup.create_paths()
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))

        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        node.create_vnodes()
        if 'memory' in cgroup.subsystems:
            val = node.get_memory_on_node(cgroup.cfg)
            if val is not None:
                if 'vnode_per_numa_node' in cgroup.cfg:
                    if not cgroup.cfg['vnode_per_numa_node']:
                        hn = node.hostname
                        e.vnode_list[hn].resources_available['mem'] = \
                            pbs.size(val)
                        val_cpus = node.totalcpus
                        if val_cpus is not None:
                            e.vnode_list[hn].resources_available['ncpus'] = \
                                int(val_cpus)
                cgroup.set_node_limit('mem', val)
        if 'memsw' in cgroup.subsystems:
            val = node.get_vmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['vmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('vmem', val)
        if 'hugetlb' in cgroup.subsystems:
            val = node.get_hpmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['hpmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('hpmem', val)
        return True

    def __execjob_attach_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logjobmsg(jobutil.job.id, "%s: Attaching PID %s" %
                      (caller_name(), e.pid))
        # Add the job process id to the appropriate cgroups.
        cgroup.add_pids(e.pid, jobutil.job.id)
        return True

#
# CLASS ShallIRunUtils
#


class ShallIRunUtils:

    def __init__(self, hostname, cfg):
        self.cfg = cfg
        self.hostname = hostname

    def allow_users(self, allow_users):
        """ This still needs to be defined """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pass

    def exclude_hosts(self, exclude_hosts):
        """
        Gets the hostname for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        hostname = pbs.get_local_nodename()
        # check to see if hostname is in the exclude_hosts list
        if self.hostname in exclude_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: exclude host %s is in %s" %
                       (caller_name(), self.hostname, exclude_hosts))
            pbs.event().accept()

        # check to see if it regex syntax is used
        pass

    def exclude_vntypes(self, exclude_vntypes):
        """
        Gets the vntype for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        vntype = None

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'
        if os.path.isfile(vntype_file):
            fdata = open(vntype_file).readlines()
            vntype = fdata[0].strip()
            pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
        if vntype is None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype is set to %s" % (vntype))
        elif vntype in exclude_vntypes:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s is in %s" % (vntype, exclude_vntypes))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Exiting Hook")
            pbs.event().accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s not in global exclude: %s" %
                       (vntype, exclude_vntypes))
        pass

    def run_only_on_hosts(self, approved_hosts):
        """
        Gets the list of approved nodes to run on and checks to see if the hook
        should run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Approved hosts: %s, %s" %
                   (type(approved_hosts), approved_hosts))

        # check to see if hostname is in the approved_hosts list
        if approved_hosts == []:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "approved hosts list is empty: %s" % approved_hosts)
        elif self.hostname not in approved_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s is not in the approved list of hosts: %s" %
                       (self.hostname, approved_hosts))
            pbs.event().accept()

        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s in list of approved hosts: %s" %
                       (self.hostname, approved_hosts))

#
# CLASS JobUtils
#


class JobUtils:

    def __init__(self, job, hostname=None, resources=None):
        self.job = job
        self.host_vnodes = list()
        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if resources is not None:
            self.host_resources = resources
        else:
            self.host_resources = self.__host_assigned_resources()

    def __repr__(self):
        return "JobUtils(%s, %s, %s)" % (repr(self.job), repr(self.hostname),
                                         repr(self.host_resources))

    # Return a dictionary of resources assigned to the local node
    def __host_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Bail out if no hostname was provided
        if self.hostname is None:
            raise CgroupProcessingError('No hostname available')
        # Bail out if no job information is present
        if self.job is None:
            raise CgroupProcessingError('No job information available')
        # Determine which vnodes are on this host, if any
        if self.hostname is not None:
            vnhost_pattern = "%s\[[\d+]\]" % self.hostname
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnhost pattern: %s" %
                       (caller_name(), vnhost_pattern))
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job exec_vnode list: %s" %
                       (caller_name(), self.job.exec_vnode))
            for match in re.findall(vnhost_pattern, str(self.job.exec_vnode)):
                self.host_vnodes.append(match)
            if self.host_vnodes is not None:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Associate %s to vnodes on %s" %
                           (caller_name(), self.host_vnodes, self.hostname))

        # Collect host assigned resources
        resources = {}
        for chunk in self.job.exec_vnode.chunks:
            vnode = False
            if self.host_vnodes == [] and chunk.vnode_name != self.hostname:
                continue
            elif self.host_vnodes != [] and \
                    chunk.vnode_name not in self.host_vnodes:
                continue
            elif chunk.vnode_name in self.host_vnodes:
                vnode = True
                if 'vnodes' not in resources:
                    resources['vnodes'] = {}
                if chunk.vnode_name not in resources['vnodes']:
                    resources['vnodes'][chunk.vnode_name] = {}
                # This check is needed since not all resources are
                # required to be in each chunk of a job
                # i.e. exec_vnodes =
                # (node1[0]:ncpus=4:mem=4gb+node1[1]:mem=2gb) +
                # (node1[1]:ncpus=3+node[0]:ncpus=1:mem=4gb)
                if all(resc in resources['vnodes'][chunk.vnode_name]
                       for resc in chunk.chunk_resources.keys()):
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: resources already defined" %
                               (caller_name()))
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s,\t%s" %
                               (caller_name(),
                                chunk.chunk_resources.keys(),
                                resources['vnodes'][chunk.vnode_name].keys()))
                else:
                    # Initialize resources for each vnode
                    for resc in chunk.chunk_resources.keys():
                        # Initialize the new value
                        if isinstance(chunk.chunk_resources[resc],
                                      pbs.pbs_int):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_int(0)
                        elif isinstance(chunk.chunk_resources[resc],
                                        pbs.pbs_float):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_float(0)
                        elif isinstance(chunk.chunk_resources[resc], pbs.size):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.size('0')

            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Chunk %s" %
                       (caller_name(), chunk.vnode_name))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resources: %s" %
                       (caller_name(), resources))
            for resc in chunk.chunk_resources.keys():
                if resc not in resources.keys():
                    # Initialize the new value
                    if isinstance(chunk.chunk_resources[resc], pbs.pbs_int):
                        resources[resc] = pbs.pbs_int(0)
                    elif isinstance(chunk.chunk_resources[resc],
                                    pbs.pbs_float):
                        resources[resc] = pbs.pbs_float(0.0)
                    elif isinstance(chunk.chunk_resources[resc], pbs.size):
                        resources[resc] = pbs.size('0')
                # Add resource value to total
                if type(chunk.chunk_resources[resc]) in \
                        [pbs.pbs_int, pbs.pbs_float, pbs.size]:
                    resources[resc] += chunk.chunk_resources[resc]
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s: resources[%s][%s] is now %s" %
                               (caller_name(), self.hostname, resc,
                                resources[resc]))
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] += \
                                  chunk.chunk_resources[resc]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Setting resource %s to string %s" %
                               (caller_name(), resc,
                                str(chunk.chunk_resources[resc])))
                    resources[resc] = str(chunk.chunk_resources[resc])
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] = \
                                  str(chunk.chunk_resources[resc])
        if not resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: No resources assigned to host %s" %
                       (caller_name(), self.hostname))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources for %s: %s" %
                       (caller_name(), self.hostname, repr(resources)))

        # Return assigned resources for specified host
        pbs.logmsg(pbs.EVENT_DEBUG3, "Job Resources: %s" % (resources))
        return resources

    # Write a message to the job stdout file
    def write_to_stderr(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stderr_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

    # Write a message to the job stdout file
    def write_to_stdout(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stdout_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

#
# CLASS NodeConfig
#


class NodeConfig:

    def __init__(self, cfg, **kwargs):

        self.cfg = cfg
        hostname = None
        meminfo = None
        numa_nodes = None
        devices = None
        hyperthreading = None
        self.hyperthreads_per_core = 1
        self.totalcpus = self.__discover_cpus()

        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        self.host_mom_jobdir = pbs_home+'/mom_priv/jobs'

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'hostname':
                    hostname = val
                elif arg == 'meminfo':
                    meminfo = val
                elif arg == 'numa_nodes':
                    numa_nodes = val
                elif arg == 'devices':
                    devices = val
                elif arg == 'hyperthreading':
                    hyperthreading = val

        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if meminfo is not None:
            self.meminfo = meminfo
        else:
            self.meminfo = self.__discover_meminfo()
        if numa_nodes is not None:
            self.numa_nodes = numa_nodes
        else:
            self.numa_nodes = self.__discover_numa_nodes()
        if devices is not None:
            self.devices = devices
        else:
            self.devices = self.__discover_devices()
        if hyperthreading is not None:
            self.hyperthreading = hyperthreading
        else:
            self.hyperthreading = self.__hyperthreading_enabled()

        # Add the devices count i.e. nmics and ngpus to the numa nodes
        self.__add_devices_to_numa_node()

    def __repr__(self):
        return ("NodeConfig(%s, %s, %s, %s, %s, %s)" %
                (repr(self.cfg), repr(self.hostname), repr(self.meminfo),
                 repr(self.numa_nodes), repr(self.devices),
                 repr(self.hyperthreading)))

    def __add_devices_to_numa_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (self.devices.keys()))
        for device in self.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" % (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" %
                           (self.devices[device].keys()))
                for device_name in self.devices[device]:
                    device_socket = \
                        self.devices[device][device_name]['numa_node']
                    if device == 'mic':
                        if 'nmics' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['nmics'] = 1
                        else:
                            self.numa_nodes[device_socket]['nmics'] += 1
                    elif device == 'gpu':
                        if 'ngpus' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['ngpus'] = 1
                        else:
                            self.numa_nodes[device_socket]['ngpus'] += 1

        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (self.numa_nodes))

    # Discover what type of hardware is on this node and how is it partitioned
    def __discover_numa_nodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        numa_nodes = {}
        for dir in glob.glob(os.path.join(os.sep,
                             "sys", "devices", "system", "node", "node*")):
            id = int(dir.split(os.sep)[5][4:])
            if id not in numa_nodes.keys():
                numa_nodes[id] = {}
                numa_nodes[id]['devices'] = list()
            numa_nodes[id]['cpus'] = open(os.path.join(dir, "cpulist"),
                                          'r').readline().strip()
            with open(os.path.join(dir, "meminfo"), 'r') as fd:
                for line in fd:
                    # Each line will contain four or five fields. Examples:
                    # Node 0 MemTotal:       32995028 kB
                    # Node 0 HugePages_Total:     0
                    entries = line.split()
                    if len(entries) < 4:
                        continue
                    if entries[2] == "MemTotal:":
                        numa_nodes[id][entries[2].rstrip(':')] = \
                            convert_size(entries[3] + entries[4], 'kb')
                    elif entries[2] == "HugePages_Total:":
                        numa_nodes[id][entries[2].rstrip(':')] = entries[3]
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover Numa Nodes: %s" % numa_nodes)
        return numa_nodes

    # Identify devices and to which numa nodes they are attached
    def __discover_devices(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        devices = {}
        for file in glob.glob(os.path.join(os.sep, "sys", "class", "*", "*",
                                           "device", "numa_node")):
            # The file should contain a single integer
            numa_node = int(open(file, 'r').readline().strip())
            if numa_node < 0:
                numa_node = 0
            dirs = file.split(os.sep)
            name = dirs[3]
            instance = dirs[4]
            if name not in devices.keys():
                devices[name] = {}
            devices[name][instance] = {}
            devices[name][instance]['numa_node'] = numa_node
            if name == 'mic':
                s = os.stat('/dev/%s' % instance)
                devices[name][instance]['major'] = os.major(s.st_rdev)
                devices[name][instance]['minor'] = os.minor(s.st_rdev)
                devices[name][instance]['device_type'] = 'c'

        # Check to see if there are gpus on the node
        gpus = self.discover_gpus()
        if isinstance(gpus, dict) and len(gpus.keys()) > 0:
            devices['gpu'] = gpus['gpu']

        pbs.logmsg(pbs.EVENT_DEBUG3, "Discovered devices: %s" % (devices))
        return devices

    def discover_gpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Check to see if nvidia-smi exists and produces valid output
            cmd = ['/usr/bin/nvidia-smi', '-q', '-x']
            if 'nvidia-smi' in self.cfg:
                cmd[0] = self.cfg['nvidia-smi']

            pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
            # Collect the gpu information
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.PIPE)
            output, err = process.communicate()

            # pbs.logmsg(pbs.EVENT_DEBUG3,"output: %s" % output)
            # import the xml library and parse the output
            import xml.etree.ElementTree as ET

            # Parse the data
            name = 'gpu'
            gpu_data = dict()
            gpu_data[name] = {}
            root = ET.fromstring(output)
            pbs.logmsg(pbs.EVENT_DEBUG3, "root.tag: %s" % root.tag)
            for child in root:
                if child.tag == "attached_gpus":
                    # gpu_data['ngpus'] = int(child.text)
                    pass
                if child.tag == "gpu":
                    device_id = child.get('id')
                    number = 'nvidia%s' % child.find('minor_number').text
                    gpu_data[name][number] = dict()
                    gpu_data[name][number]['id'] = device_id

                    # Determine which socket the device is on
                    filename = '/sys/bus/pci/devices/%s/numa_node' % \
                        device_id.lower()
                    isfile = os.path.isfile(filename)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s, %s" % (filename, isfile))
                    if isfile:
                        numa_node = open(filename).read().strip()
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "numa_node: %s" % (numa_node))
                        if int(numa_node) == -1:
                            numa_node = 0
                        gpu_data[name][number]['numa_node'] = int(numa_node)
                        try:
                            dev_filename = '/dev/%s' % number
                            s = os.stat(dev_filename)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find %s" % dev_filename)
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                        gpu_data[name][number]['major'] = os.major(s.st_rdev)
                        gpu_data[name][number]['minor'] = os.minor(s.st_rdev)
                        gpu_data[name][number]['device_type'] = 'c'
            pbs.logmsg(pbs.EVENT_DEBUG, "%s" % gpu_data)
            return gpu_data
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unable to find %s" % string.join(cmd, " "))
            return {'gpu': {}}
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        return {'gpu': {}}

    # Get the memory info on this host
    def __discover_meminfo(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        meminfo = {}
        with open(os.path.join(os.sep, "proc", "meminfo"), 'r') as fd:
            for line in fd:
                entries = line.split()
                if entries[0] == "MemTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "SwapTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "Hugepagesize:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "HugePages_Total:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
                elif entries[0] == "HugePages_Rsvd:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover meminfo: %s" % meminfo)
        return meminfo

    # Determine if the cpus have hyperthreading enabled
    def __hyperthreading_enabled(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            siblings = 0
            cpu_cores = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "siblings":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    siblings = entries[1].strip()
                elif entries[0].strip() == "cpu cores":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_cores = entries[1].strip()
                elif entries[0].strip() == "flags":
                    flag_options = entries[1].split()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: flag_options: %s" %
                               (caller_name(), flag_options))
                    if "ht" in flag_options:
                        rc = True
                        break
        if rc is True:
            self.hyperthreads_per_core = int(siblings)/int(cpu_cores)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: hyperthreads/core: %d" %
                       (caller_name(), self.hyperthreads_per_core))
            if self.hyperthreads_per_core == 1:
                rc = False
        return rc

    def __discover_cpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            cpu_threads = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "processor":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_threads += 1
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: processors found: %d" %
                   (caller_name(), cpu_threads))

        return cpu_threads

    # Gather the jobs running on this node
    def gather_jobs_on_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        # Get the job list from the server
        jobs = None
        try:
            jobs = pbs.server().vnode(self.hostname).jobs
        except:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Unable to contact server to get jobs")
            return None

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs from server for %s: %s" % (self.hostname, jobs))

        if jobs is None:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "No jobs found on %s " % (self.hostname))
            return list()

        jobs_list = dict()
        for job in jobs.split(','):
            # The job list from the node has a space in front of the job id
            # This must be removed or it will not match the cgroup dir
            jobs_list[job.split('/')[0].strip()] = 0
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs on %s: %s" % (self.hostname, jobs_list.keys()))

        return jobs_list.keys()

    # Gather the jobs running on this node
    def gather_jobs_on_node_local(self):
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Method called" % (caller_name()))
        # Get the job list from the jobs directory
        jobs_list = list()
        try:
            for file in os.listdir(self.host_mom_jobdir):
                if fnmatch.fnmatch(file, "*.JB"):
                    jobs_list.append(file[:-3])
            if not jobs_list:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "No jobs found on %s " % (self.hostname))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Jobs from server for %s: %s" %
                           (self.hostname, repr(jobs_list)))

            return jobs_list
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Could not get job list from mom_priv/jobs for %s" %
                       self.hostname)

            return self.gather_jobs_on_node()

    # Get the memory resource on this mom
    def get_memory_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Calculate memory
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
            pbs.logmsg(pbs.EVENT_DEBUG3, "val: %s" % val)
        else:
            val = size_as_int(MemTotal)
        if 'percent_reserve' in config['cgroup']['memory']:
            percent_reserve = config['cgroup']['memory']['percent_reserve']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memory'].keys():
            reserve_memory = config['cgroup']['memory']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                reserve_mem = size_as_int(reserve_memory)
                if val > reserve_mem:
                    val -= reserve_mem
                else:
                    val -= size_as_int("100mb")
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Unable to reserve more memory than " +
                               "available on the node. Available %s, " +
                               "Tried to reserve %s. Reserving 100mb" %
                               (val, reserve_mem))

            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))

        pbs.logmsg(pbs.EVENT_DEBUG3, "Return val: %s" % val)
        return convert_size(str(val), 'kb')

    # Get the virtual memory resource on this mom
    def get_vmem_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Return the value for memory if no swap is configured
        if size_as_int(self.meminfo['SwapTotal']) <= 0:
            return self.get_memory_on_node(config)
        # Calculate vmem
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
        else:
            val = size_as_int(MemTotal)

        val += size_as_int(self.meminfo['SwapTotal'])
        # Determine if memory needs to be reserved
        if 'percent_reserve' in config['cgroup']['memsw']:
            percent_reserve = config['cgroup']['memsw']['reserve_memory']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memsw']:
            reserve_memory = config['cgroup']['memsw']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                val -= size_as_int(reserve_memory)
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))
        return convert_size(str(val), 'kb')

    # Get the huge page memory resource on this mom
    def get_hpmem_on_node(self, config):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        if 'percent_reserve' in config['cgroup']['hugetlb'].keys():
            percent_reserve = config['cgroup']['hugetlb']['percent_reserve']
        else:
            percent_reserve = 0
        # Calculate vmem
        val = size_as_int(self.meminfo['Hugepagesize'])
        val *= (self.meminfo['HugePages_Total'] -
                self.meminfo['HugePages_Rsvd'])
        val -= (val * percent_reserve) / 100
        return convert_size(str(val), 'kb')

    # Create individual vnodes per socket
    def create_vnodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        key = "vnode_per_numa_node"
        if key in self.cfg.keys():
            if not self.cfg[key]:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s is false" %
                           (caller_name(), key))
                return
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s not configured" %
                       (caller_name(), key))
            return

        # Create one vnode per numa node
        vnode_list = pbs.event().vnode_list
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: numa nodes: %s" %
                   (caller_name(), self.numa_nodes))
        # Define the vnodes
        vnode_name = self.hostname
        vnode_list[vnode_name] = pbs.vnode(vnode_name)
        for id in self.numa_nodes.keys():
            vnode_name = self.hostname + "[%d]" % id
            vnode_list[vnode_name] = pbs.vnode(vnode_name)
            for key, val in sorted(self.numa_nodes[id].iteritems()):
                if key == 'cpus':
                    vnode_list[vnode_name].resources_available['ncpus'] = \
                        len(cpus2list(val))/self.hyperthreads_per_core
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['ncpus'] = 0
                elif key == 'MemTotal':
                    mem = self.get_memory_on_node(self.cfg, val)
                    vnode_list[vnode_name].resources_available['mem'] = \
                        pbs.size(mem)
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['mem'] = \
                        pbs.size('0kb')
                elif key == 'HugePages_Total':
                    pass
                elif isinstance(val, list):
                    pass
                elif isinstance(val, dict):
                    pass
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s=%s" %
                               (caller_name(), key, val))
                    vnode_list[vnode_name].resources_available[key] = val

                    if isinstance(val, int):
                        vnode_list[self.hostname].resources_available[key] = 0
                    elif isinstance(val, float):
                        vnode_list[self.hostname].resources_available[key] = \
                            0.0
                    elif isinstance(val, pbs.size):
                        vnode_list[self.hostname].resources_available[key] = \
                            pbs.size('0kb')
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnode list: %s" %
                   (caller_name(), vnode_list))
        return True

#
# CLASS CgroupUtils
#


class CgroupUtils:
    def __init__(self, hostname, vnode, **kwargs):
        cfg = None
        subsystems = None
        paths = None
        vntype = None
        event = None
        self.assigned_resources = dict()
        self.existing_cgroups = dict()
        self.host_assigned_resources = None
        self.pid_lower_limit = 3

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'subsystems':
                    subsystems = val
                elif arg == 'paths':
                    paths = val
                elif arg == 'vntype':
                    vntype = val
                elif arg == 'event':
                    event = val

        try:
            self.hostname = hostname
            self.vnode = vnode
            # __check_os will raise an exception if cgroups are not present
            self.__check_os()
            # Read in the config file
            if cfg is not None:
                self.cfg = cfg
            else:
                self.cfg = self.__parse_config_file()

            # Determine if the hook should run or exit
            siru = ShallIRunUtils(self.hostname, self.cfg)
            siru.exclude_hosts(self.cfg['exclude_hosts'])
            siru.exclude_vntypes(self.cfg['exclude_vntypes'])
            siru.run_only_on_hosts(self.cfg['run_only_on_hosts'])

            # Collect the mount points
            if paths is not None:
                self.paths = paths
            else:
                self.paths = self.__set_paths()
            # Define the local vnode type
            if vntype is not None:
                self.vntype = vntype
            else:
                self.vntype = self.__get_vnode_type()
            # Determine which subsystems we care about
            if subsystems is not None:
                self.subsystems = subsystems
            else:
                self.subsystems = self.__target_subsystems()

            # location to store information for the different hook events
            pbs_home = ''
            if not CRAY_12_BRANCH:
                pbs_conf = pbs.get_pbs_conf()
                pbs_home = pbs_conf['PBS_HOME']
            else:
                if 'PBS_HOME' in self.cfg:
                    pbs_home = self.cfg['PBS_HOME']
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "PBS_HOME needs to be defined in the " +
                               "config file")
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Exiting the cgroups hook")
                    pbs.accept()

            self.hook_storage_dir = pbs_home+'/mom_priv/hooks/hook_data'
            self.host_job_env_dir = pbs_home+'/aux'
            self.host_job_env_filename = self.host_job_env_dir+os.sep+"%s.env"

            # information for offlining nodes
            self.offline_file = \
                pbs_home+os.sep+'mom_priv'+os.sep+'hooks'+os.sep
            self.offline_file += "%s.offline" % pbs.event().hook_name
            self.offline_msg = "Hook %s: " % pbs.event().hook_name
            self.offline_msg += "Unable to clean up one or more cgroups"

        except:
            raise

    def __repr__(self):
        return ("CgroupUtils(%s, %s, %s, %s, %s, %s)" %
                (repr(self.hostname), repr(self.vnode), repr(self.cfg),
                 repr(self.subsystems), repr(self.paths), repr(self.vntype)))

    # Validate the OS type and version
    def __check_os(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check to see if the platform is linux and the kernel is new enough
        if platform.system() != 'Linux':
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: OS does not support cgroups" %
                       (caller_name()))
            raise CgroupConfigError("OS type not supported")
        rel = map(int,
                  string.split(string.split(platform.release(), '-')[0], '.'))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Detected Linux kernel version %d.%d.%d" %
                   (caller_name(), rel[0], rel[1], rel[2]))
        supported = False
        if rel[0] > 2:
            supported = True
        elif rel[0] == 2:
            if rel[1] > 6:
                supported = True
            elif rel[1] == 6:
                if rel[2] >= 28:
                    supported = True
        if not supported:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Kernel needs to be >= 2.6.28: %s" %
                       (caller_name(), system_info[2]))
            raise CgroupConfigError("OS version not supported")
        return supported

    # Read the config file in json format
    def __parse_config_file(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        config = {}
        # Identify the config file and read in the data
        if 'PBS_HOOK_CONFIG_FILE' in os.environ:
            config_file = os.environ["PBS_HOOK_CONFIG_FILE"]
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Config file is %s" %
                       (caller_name(), config_file))
            try:
                config = json.load(open(config_file, 'r'),
                                   object_hook=decode_dict)
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
        else:
            raise CgroupConfigError("No configuration file present")
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Initial Cgroup Config: %s" %
                   (caller_name(), config))
        # Set some defaults if they are not present
        if 'cgroup_prefix' not in config.keys():
            config['cgroup_prefix'] = 'pbspro'
        if 'periodic_resc_update' not in config.keys():
            config['periodic_resc_update'] = False
        if 'vnode_per_numa_node' not in config.keys():
            config['vnode_per_numa_node'] = False
        if 'online_offlined_nodes' not in config.keys():
            config['online_offlined_nodes'] = False
        if 'exclude_hosts' not in config.keys():
            config['exclude_hosts'] = []
        if 'run_only_on_hosts' not in config.keys():
            config['run_only_on_hosts'] = []
        if 'exclude_vntypes' not in config.keys():
            config['exclude_vntypes'] = []
        if 'cgroup' not in config.keys():
            config['cgroup'] = {}
        else:
            for subsys in config['cgroup']:
                subsystem = config['cgroup'][subsys]
                if 'enabled' not in subsystem.keys():
                    config['cgroup'][subsys]['enabled'] = False
                if 'exclude_hosts' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_hosts'] = []
                if 'exclude_vntypes' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_vntypes'] = []

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Cgroup Config with defaults: %s" %
                   (caller_name(), config))
        return config

    # Determine which subsystems are being requested
    def __target_subsystems(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        subsystems = []
        for key in self.cfg['cgroup'].keys():
            if self.enabled(key):
                subsystems.append(key)
        if 'memory' not in subsystems:
            if 'memsw' in subsystems:
                # Remove memsw since it must be greater then
                # or equal to memory
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing memsw from enabled subsystems")
                subsystems.remove('memsw')
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Enabled subsystems: %s" %
                   (caller_name(), subsystems))
        # It is not an error for all subsystems to be disabled.
        # This host or vnode type may be in the excluded list.
        return subsystems

    # Copy a setting from the parent cgroup
    def __copy_from_parent(self, dest):
        try:
            file = os.path.basename(dest)
            dir = os.path.dirname(dest)
            parent = os.path.dirname(dir)
            source = os.path.join(parent, file)
            if not os.path.isfile(source):
                raise CgroupConfigError('Failed to read %s' % (source))
            self.write_value(dest, open(source, 'r').read().strip())
        except:
            raise

    # Create a dictionary of the cgroup subsystems and their corresponding
    # directories
    def __set_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        paths = {}
        try:
            # Loop through the mounts and collect the ones for cgroups
            with open(os.path.join(os.sep, "proc", "mounts"), 'r') as fd:
                for line in fd:
                    entries = line.split()
                    if len(entries) > 3 and entries[2] == "cgroup":
                        flags = entries[3].split(',')
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "subsysdir: %s (flags=%s)" %
                                   (entries[1], flags))
                        for subsys in flags:
                            paths[subsys] = os.path.join(
                                entries[1], self.cfg["cgroup_prefix"])
        except:
            raise
        if len(paths.keys()) < 1:
            raise CgroupConfigError("Cgroup paths not detected")
        # Both the memory and memsw cgroups share the same mount point
        if "memory" in paths.keys():
            paths["memsw"] = paths["memory"]
        return paths

    # Create the cgroup parent directories that will contain the jobs
    def create_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Create the directories that PBS will use to house the jobs
            old_umask = os.umask(0022)
            for subsys in self.subsystems:
                if subsys not in self.paths.keys():
                    raise CgroupConfigError('No path for subsystem: %s' %
                                            (subsys))
                if not os.path.exists(self.paths[subsys]):
                    os.makedirs(self.paths[subsys], 0755)
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Created directory %s" %
                               (caller_name(), self.paths[subsys]))
                    if subsys == 'memory' or subsys == 'memsw':
                        # Set file to
                        # <cgroup>/memory/pbspro/memory.use_hierarchy
                        file = os.path.join(self.paths[subsys],
                                            'memory.use_hierarchy')
                        if not os.path.isfile(file):
                            raise CgroupConfigError('Failed to configure %s' %
                                                    (file))
                        self.write_value(file, 1)
                    elif subsys == 'cpuset':
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.cpus'))
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.mems'))

        except:
            raise CgroupConfigError("Failed to create directory: %s" %
                                    (self.paths[subsys]))
        finally:
            os.umask(old_umask)

    # Return the vnode type of the local node
    def __get_vnode_type(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")

        # File to check for if it is not defined in the hook input file
        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'

        # self.vnode is None for pbs_attach events
        pbs.logmsg(pbs.EVENT_DEBUG3, "vnode: %s" % self.vnode)
        if self.vnode is not None:
            if 'vntype' in self.vnode.resources_available and \
                    self.vnode.resources_available['vntype'] is not None:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "vntype: %s" %
                           self.vnode.resources_available['vntype'])
                return self.vnode.resources_available['vntype']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3, "vntype file: %s" % vntype_file)
                if os.path.isfile(vntype_file):
                    fdata = open(vntype_file).readlines()
                    vntype = fdata[0].strip()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
                    return vntype
        # If vntype was not set then log a message. It is too expensive
        # to have all moms query the server for large jobs.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: resources_available.vntype is " % caller_name() +
                   "not set for this vnode")
        return None

    # Gather resources that are already assigned
    def gather_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        assigned_resources = {}

        # Gather the cpus and memory sockets assigned
        directories = [x[0] for x in os.walk(self.paths['cpuset'])]

        # Add the existing cgroups to a list to check if assigning
        # resources fail
        for dir in directories[1:]:
            if dir.find(pbs.event().job.id) == -1:
                self.existing_cgroups[dir] = []

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'cpuset' in self.subsystems and \
                self.cfg['cgroup']['cpuset']['enabled']:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather cpuset resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['cpuset'], tmp_dir)
                    with open(path+os.sep+'cpuset.cpus') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.cpus'] = \
                            cpus2list(tmp_val.strip())
                    with open(path+os.sep+'cpuset.mems') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.mems'] = \
                            cpus2list(tmp_val.strip())

        # Check to see if the subsystem is enabled before trying
        # to gather resources
        if 'memory' in self.subsystems and \
                self.cfg['cgroup']['memory']['enabled']:
            # Gather the memory assigned for each socket
            directories = [x[0] for x in os.walk(self.paths['memory'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather memory resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['memory'], tmp_dir)
                    with open(path+os.sep+'memory.limit_in_bytes') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memory.limit'] = \
                            tmp_val.strip()
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    if os.path.isfile(tmp_filename) is False:
                        continue
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    with open(tmp_filename) as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memsw.limit'] = \
                            tmp_val.strip()

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'devices' in self.subsystems and \
                self.cfg['cgroup']['devices']['enabled']:
            # Gather the devices assigned
            directories = [x[0] for x in os.walk(self.paths['devices'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather device resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    pbs.logmsg(pbs.EVENT_DEBUG3, "Dir: %s" % tmp_dir)
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['devices'], tmp_dir)
                    with open(path+os.sep+'devices.list') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['devices.list'] = \
                            tmp_val.strip().split('\n')
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Dir: %s\n\tValue: %s" %
                                   (path, tmp_val.replace('\n', ',')))

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Gathered assigned resources: %s" % assigned_resources)
        self.assigned_resources = assigned_resources

    # Return whether a subsystem is enabled
    def enabled(self, subsystem):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check whether the subsystem is enabled in the configuration file
        if subsystem not in self.cfg['cgroup'].keys():
            return False
        if 'enabled' not in self.cfg['cgroup'][subsystem].keys():
            return False
        if not self.cfg['cgroup'][subsystem]['enabled']:
            return False
        # Check whether the cgroup is mounted for this subsystem
        if subsystem not in self.paths.keys():
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup not mounted for %s" %
                       (caller_name(), subsystem))
            return False
        # Check whether this host is excluded
        # if self.hostname in self.cfg['exclude_hosts']:
        #    pbs.logmsg(pbs.EVENT_DEBUG, "%s: cgroup excluded on host %s" %
        #               (caller_name(), self.hostname))
        #    pbs.event().accept()
        if self.hostname in self.cfg['cgroup'][subsystem]['exclude_hosts']:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup excluded for subsystem %s on host %s" %
                       (caller_name(), subsystem, self.hostname))
            return False
        # Check whether the vnode type is excluded
        if self.vntype is not None:
            # if self.vntype in self.cfg['exclude_vntypes']:
            #    pbs.logmsg(pbs.EVENT_DEBUG,
            #               "%s: cgroup excluded on vnode type %s" %
            #               (caller_name(), self.vntype))
            #    pbs.event().accept()
            if self.vntype in self.cfg['cgroup'][subsystem]['exclude_vntypes']:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           ("%s: cgroup excluded for " +
                            "subsystem %s on vnode type %s") %
                           (caller_name(), subsystem, self.vntype))
                return False
        return True

    # Return the default value for a subsystem
    def default(self, subsystem):
        if subsystem in self.cfg['cgroup']:
            if 'default' in self.cfg['cgroup'][subsystem]:
                return self.cfg['cgroup'][subsystem]['default']
        return None

    # Check to see if the pid matches the job owners uid
    def is_pid_owned_by_job_owner(self, pid):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        try:
            proc_uid = os.stat('/proc/%d' % pid).st_uid
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       '/proc/%d uid:%d' % (pid, proc_uid))

            job_owner_uid = pwd.getpwnam(pbs.event().job.euser)[2]
            pbs.logmsg(pbs.EVENT_DEBUG3, "Job uid: %d" % job_owner_uid)

            if proc_uid == job_owner_uid:
                return True
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Proc uid: %d != Job owner:  %d" %
                           (proc_uid, job_owner_uid))
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG, "Unknown pid: %d" % pid)
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        # If we got to this point something did not match up
        return False

    # Add some number of PIDs to the cgroup tasks files for each subsystem
    def add_pids(self, pids, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # make pids a list
        if isinstance(pids, int):
            pids = [pids]

        if pbs.event().type == pbs.EXECJOB_LAUNCH:
            if 1 in pids:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "1 is not a valid pid to add")
                # Hold the job for further review
                e.reject("Hook tried to add pid 1 to the tasks file " +
                         "for job %s" % jobid)

        # check pids to make sure that they are owned by the job owner
        if not CRAY_12_BRANCH:
            pbs.logmsg(pbs.EVENT_DEBUG3, "Not in the Cray 12 Branch")
            pbs.logmsg(pbs.EVENT_DEBUG3, "check to see if execjob_attach")
            if pbs.event().type == pbs.EXECJOB_ATTACH:
                pbs.logmsg(pbs.EVENT_DEBUG3, "event type: attach or launch")
                tmp_pids = list()
                for p in pids:
                    if self.is_pid_owned_by_job_owner(p):
                        tmp_pids.append(p)
                if len(tmp_pids) == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%d is not a valid pids to add" % p)
                    return False
                else:
                    pids = tmp_pids

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: subsys = %s" %
                       (caller_name(), subsys))
            # memsw and memory use the same tasks file
            if subsys == "memsw":
                if "memory" in self.subsystems:
                    continue
            path = os.path.join(self.paths[subsys], jobid)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path = %s" %
                       (caller_name(), path))
            try:
                tasks_path = os.path.join(path, "tasks")
                for p in pids:
                    self.write_value(tasks_path, p, 'a')
            except IOError, exc:
                raise CgroupLimitError("Failed to add PIDs %s to %s (%s)" %
                                       (str(pids), tasks_path,
                                        errno.errorcode[exc.errno]))
            except:
                raise

    # Set a node limit
    def set_node_limit(self, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s = %s" %
                   (caller_name(), resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'],
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Node resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    def setup_job_devices_env(self):
        """ Setup the job environment for the devices assigned to the job for an
            execjob_launch hook
         """
        if 'devices_name' in self.host_assigned_resources:
            names = self.host_assigned_resources['devices_name']
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "devices: %s" % (names))
            offload_devices = list()
            cuda_visible_devices = list()
            for name in names:
                if name.startswith('mic'):
                    offload_devices.append(name[3:])
                elif name.startswith('nvidia'):
                    cuda_visible_devices.append(name[6:])
            if len(offload_devices) > 0:
                value = '"%s"' % string.join(offload_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['OFFLOAD_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "offload_devices: %s" % offload_devices)
            if len(cuda_visible_devices) > 0:
                value = '"%s"' % string.join(cuda_visible_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['CUDA_VISIBLE_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "cuda_visible_devices: %s" % cuda_visible_devices)
            return [offload_devices, cuda_visible_devices]
        else:
            return False

    def setup_subsys_devices(self, path, node):
        subsys = 'devices'
        # Add the devices that the user is allowed to use
        if subsys in self.cfg['cgroup']:
            devices_allowed = open(os.path.join(path,
                                   "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Initial devices.list: %s" %
                       devices_allowed)
            # Deny access to mic and gpu devices
            devices_list = list()
            devices = node.devices
            for device in devices:
                if device == 'mic' or device == 'gpu':
                    for name in devices[device]:
                        d = devices[device][name]
                        devices_list.append("%d:%d" % (d['major'], d['minor']))
            # For CentOS 7 we need to remove a *:* rwm from devices.list we
            # before can add anything to devices.allow. Otherwise our changes
            # are ignored. Check to see if a *:* rwm is in devices.list.
            # If so remove it
            if "a *:* rwm\n" in devices_allowed:
                value = "a *:* rwm"
                self.write_value(os.path.join(path, 'devices.deny'), value)
            else:
                # Verify that the following devices are not in devices.list
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing access to the following: %s" %
                           devices_list)
            for device_str in devices_list:
                value = "c %s rwm" % device_str
                self.write_value(os.path.join(path, 'devices.deny'), value)
            # Add devices back to the list
            devices_allow = list()
            if 'allow' in self.cfg['cgroup'][subsys]:
                devices_allow = self.cfg['cgroup'][subsys]['allow']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Allowing access to the following: %s" %
                           devices_allow)
                for item in devices_allow:
                    if isinstance(item, str):
                        pbs.logmsg(pbs.EVENT_DEBUG3, "string item: %s" % item)
                        value = "%s" % item
                        self.write_value(os.path.join(
                                         path, 'devices.allow'), value)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "write_value: %s" % value)
                    elif isinstance(item, list):
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "list item: %s" % item)
                        try:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Device allow: %s" % item)
                            stat_filename = '/dev/%s' % item[0]
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Stat file: %s" % stat_filename)
                            s = os.stat(stat_filename)
                            device_type = None
                            if S_ISBLK(s.st_mode):
                                device_type = "b"
                            elif S_ISCHR(s.st_mode):
                                device_type = "c"
                            if device_type is not None:
                                if len(item) == 3 and isinstance(item[2], str):
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % item[2]
                                    value += "%s" % item[1]
                                else:
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % os.minor(s.st_rdev)
                                    value += "%s" % item[1]
                                self.write_value(os.path.join(
                                                path, 'devices.allow'), value)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "write_value: %s" % value)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find /dev/%s. " % item[0] +
                                       "Not added to devices.allow!")
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                    else:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Not sure what to do with: %s" % item)
            devices_allowed = open(os.path.join(
                                   path, "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "After setup devices.list: %s" % devices_allowed)

    # Select devices to assign to the job
    def assign_devices(
                       self,
                       device_type,
                       device_list,
                       number_of_devices,
                       node):
        devices = device_list[:number_of_devices]
        device_ids = list()
        device_names = list()
        for device in devices:
            device_info = node.devices[device_type][device]
            if len(device_ids) == 0:
                if device.find("mic") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:0 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                    device_ids.append("%s %d:1 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                elif device.find("nvidia") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:255 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
            device_ids.append("%s %d:%d rwm" %
                              (device_info['device_type'],
                               device_info['major'],
                               device_info['minor']))
            device_names.append(device)
        return device_names, device_ids

    # Find the device name
    def get_device_name(self, node, available, socket, major, minor):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Get device name: major: %s, minor: %s" % (major, minor))
        avail_device = None
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Possible devices: %s" % (available[socket]['devices']))
        for avail_device in available[socket]['devices']:
            avail_major = None
            avail_minor = None
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Checking device: %s" % (avail_device))
            if avail_device.find('mic') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check mic device: %s" % (avail_device))
                avail_major = node.devices['mic'][avail_device]['major']
                avail_minor = node.devices['mic'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            elif avail_device.find('nvidia') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check gpu device: %s" % (avail_device))
                avail_major = node.devices['gpu'][avail_device]['major']
                avail_minor = node.devices['gpu'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            if int(avail_major) == int(major) and \
                    int(avail_minor) == int(minor):
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device match: name: %s, major: %s, minor: %s" %
                           (avail_device, major, minor))
                return avail_device
        pbs.logmsg(pbs.EVENT_DEBUG3, "No match found")
        return None

    # Assign resources to the job based off the vnodes assignments
    def assign_job_resources_by_vnode(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # Loop through the vnodes and assign resources
        assigned = dict()
        assigned['cpuset.cpus'] = list()
        assigned['cpuset.mems'] = list()
        room_on_socket = True

        for vnode in resources['vnodes']:
            regex = re.compile(".*\[(\d+)\].*")
            socket = int(regex.search(vnode).group(1))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current Vnode: %s" % vnode)
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current socket: %d" % socket)

            if 'ncpus' in resources['vnodes'][vnode]:
                tmp_ncpus = int(resources['vnodes'][vnode]['ncpus'])
                if int(resources['vnodes'][vnode]['ncpus']) <= \
                        len(available[socket]['cpus']):
                    assigned['cpuset.cpus'] += \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] += [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_ncpus: %s" % (tmp_ncpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "available[%d]: %s" %
                               (socket, available[socket]))
                    room_on_socket = False
            if ('nmics' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['nmics']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(mic).*")
                tmp_nmics = int(resources['vnodes'][vnode]['nmics'])
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['nmics']) > 0 and
                   int(resources['vnodes'][vnode]['nmics']) <= len(mics)):
                    tmp_names, tmp_devices = self.assign_devices(
                             'mic', mics[:tmp_nmics], tmp_nmics, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_nmics: %s" % (tmp_nmics))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "mics: %s" % (mics))
                    room_on_socket = False
            if ('ngpus' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['ngpus']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(nvidia).*")
                tmp_ngpus = int(resources['vnodes'][vnode]['ngpus'])
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['ngpus']) > 0 and
                   int(resources['vnodes'][vnode]['ngpus']) <= len(gpus)):
                    tmp_names, tmp_devices = self.assign_devices(
                        'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "tmp_ngpus: %s" % (tmp_ngpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "gpus: %s" % (gpus))
                    room_on_socket = False

        if room_on_socket:
            assigned['cpuset.cpus'].sort()
            assigned['cpuset.mems'].sort()
            if 'devices' in assigned:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids pre set and sort: %s" %
                           (assigned['devices']))
                assigned['devices'] = list(set(assigned['devices']))
                assigned['devices'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids post set and sort: %s" %
                           (assigned['devices']))
                assigned['devices_name'] = list(set(assigned['devices_name']))
                assigned['devices_name'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

    # Assign resources to the job
    def assign_job_resources(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # What would we need to do different for a large smp (UV) system?
        # See if requested resources can fit on a single socket
        assigned = dict()
        pbs.logmsg(pbs.EVENT_DEBUG3, "Requested: %s" % (resources))

        # Determine the order that we should evaluate the sockets.
        socket_list = list()
        if 'placement_type' in self.cfg:
            if self.cfg['placement_type'] == 'round_robin':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Requested round_robin placement")
                # Look at assigned_resources and determine which socket
                # to start with
                jobs_per_socket_cnt = dict()
                for socket in available.keys():
                    jobs_per_socket_cnt[socket] = 0
                for job in self.assigned_resources:
                    if 'cpuset.mems' in self.assigned_resources[job]:
                        for socket in \
                                self.assigned_resources[job]['cpuset.mems']:
                            jobs_per_socket_cnt[socket] += 1

                sorted_dict = sorted(jobs_per_socket_cnt.items(),
                                     key=operator.itemgetter(1))
                for x in sorted_dict:
                    socket_list.append(x[0])
        else:
            socket_list = available.keys()

        # Loop through the socket list and determine if the job
        # can fit on the socket
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Evaluate sockets in the following order: %s" %
                   (socket_list))
        for socket in socket_list:
            room_on_socket = True
            if 'ncpus' in resources:
                if int(resources['ncpus']) <= len(available[socket]['cpus']):
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Requested: %s, Available: %s" %
                               (resources['ncpus'], available[socket]['cpus']))
                    tmp_ncpus = int(resources['ncpus'])
                    assigned['cpuset.cpus'] = \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] = [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient ncpus on socket %d: R:%d,A:%s" %
                               (socket, resources['ncpus'],
                                available[socket]['cpus']))
                    room_on_socket = False
            # Check to see that nmics has been requested and not equal to 0
            if 'nmics' in resources and int(resources['nmics']) > 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'nmics' in resources and int(resources['nmics']) > 0 \
                        and int(resources['nmics']) <= len(mics):
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient nmics on socket %d: R:%s,A:%s" %
                               (socket, resources['nmics'], mics))
                    room_on_socket = False
            # Check to see that ngpus has been requested and not equal to 0
            if 'ngpus' in resources and int(resources['ngpus']) > 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'ngpus' in resources and int(resources['ngpus']) > 0 \
                        and int(resources['ngpus']) <= len(gpus):
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient gpus on socket %d: R:%s,A:%s" %
                               (socket, resources['ngpus'], gpus))
                    room_on_socket = False
            if 'vmem' in resources:
                if size_as_int(resources['vmem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient vmem on socket %d: R:%s,A:%s" %
                               (socket, resources['vmem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if 'mem' in resources:
                if size_as_int(resources['mem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient memory on socket %d: R:%s,A:%s" %
                               (socket, resources['mem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if room_on_socket:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
                self.host_assigned_resources = assigned
                return assigned
        # If we made it here then the requested resources did not fit
        # on a single socket
        # Future: take distance into account when selecting sockets?
        # Combine available resources into a single list
        # This should work for a dual socket machine.
        # Needs additional work for machines with more than 2 sockets
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Unable to find requested resources: %s" % resources +
                   " on a socket. Checking the entire node")
        available_copy = copy.deepcopy(available)
        available_on_node = dict()
        socket_keys = available.keys()
        socket_keys.sort()
        available_on_node['sockets'] = socket_keys
        for socket in socket_keys:
            for key in available_copy[socket].keys():
                if isinstance(available[socket][key], int):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]
                if isinstance(available_copy[socket][key], str):
                    try:
                        if key in available_on_node is False:
                            available_on_node[key] = \
                                size_as_int(available_copy[socket][key])
                        else:
                            available_on_node[key] += \
                                size_as_int(available_copy[socket][key])
                    except:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Unable to convert to int: %s" %
                                   (available_copy[socket][key]))

                if isinstance(available_copy[socket][key], list):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available on node: %s" % (available_on_node))
        assigned = dict()
        room_on_node = False
        if 'ncpus' in resources:
            if int(resources['ncpus']) <= len(available_on_node['cpus']):
                room_on_node = True
                tmp_ncpus = int(resources['ncpus'])
                assigned['cpuset.cpus'] = available_on_node['cpus'][:tmp_ncpus]
                assigned['cpuset.mems'] = available_on_node['sockets']
            if 'nmics' in resources and int(resources['nmics']) != 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['nmics']) <= len(mics) and \
                        int(resources['nmics']) > 0:
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
            if 'ngpus' in resources and int(resources['ngpus']) != 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['ngpus']) <= len(gpus) and \
                        int(resources['ngpus']) > 0:
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
        if room_on_node:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

        # Consolidate resources if needed to place the job

    # Determine which resources are available
    def available_node_resources(self, node):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))

        available = copy.deepcopy(node.numa_nodes)
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Keys: %s" % (available[0].keys()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        for socket in available.keys():
            if 'cpus' in available[socket]:
                # Find the physical cores
                if available[socket]['cpus'].find('-') != -1 and \
                        node.hyperthreading:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Splitting %s at ','" %
                               (available[socket]['cpus']))
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'].split(',')[0])
                else:
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'])
            if 'MemTotal' in available[socket]:
                # Find the memory on the socket in bites.
                # Remove the 'b' to simply math
                available[socket]['memory'] = size_as_int(
                    available[socket]['MemTotal'])

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Pre device add: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (node.devices.keys()))
        for device in node.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" %
                       (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Devices: %s" %
                           (node.devices[device].keys()))
                for device_name in node.devices[device].keys():
                    device_socket = \
                        node.devices[device][device_name]['numa_node']
                    if 'devices' not in available[device_socket]:
                        available[device_socket]['devices'] = list()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Device: %s, Socket: %s" %
                               (device, device_socket))
                    available[device_socket]['devices'].append(device_name)
            # pbs.logmsg(pbs.EVENT_DEBUG3,
            #            "Available on socket %s: %s" %
            #            (socket,available[socket]))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))

        # Remove all of the resources that are assigned to other jobs
        for job in self.assigned_resources.keys():
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            if (not job_to_be_ignored(job)):
                cpus = list()
                sockets = list()
                devices = list()
                memory = 0
                if 'cpuset.cpus' in self.assigned_resources[job]:
                    cpus = self.assigned_resources[job]['cpuset.cpus']
                if 'cpuset.mems' in self.assigned_resources[job]:
                    sockets = self.assigned_resources[job]['cpuset.mems']
                if 'devices.list' in self.assigned_resources[job]:
                    devices = self.assigned_resources[job]['devices.list']
                if 'memory.limit' in self.assigned_resources[job]:
                    memory = size_as_int(
                                self.assigned_resources[job]['memory.limit'])

                # Loop through the sockets and remove cpus that are
                # assigned to other cgroups
                for socket in sockets:
                    for cpu in cpus:
                        try:
                            available[socket]['cpus'].remove(cpu)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %d from %s" %
                                       (cpu, available[socket]['cpus']))

                if len(sockets) == 1:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Decrementing memory: %d by %d" %
                               (size_as_int(available[sockets[0]]['memory']),
                                memory))
                    available[sockets[0]]['memory'] -= memory

                # Loop throught the available sockets
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned device to %s: %s" % (job, devices))
                for socket in available.keys():
                    for device in devices:
                        try:
                            # loop through know devices and see if they match
                            if len(available[socket]['devices']) != 0:
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Check device: %s" % (device))
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Available device: %s" %
                                           (available[socket]['devices']))
                                major, minor = device.split()[1].split(':')
                                avail_device = self.get_device_name(
                                                   node, available, socket,
                                                   major, minor)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Returned device: %s" %
                                           (avail_device))
                                if avail_device is not None:
                                    pbs.logmsg(pbs.EVENT_DEBUG3,
                                               "socket: %d,\t" % socket +
                                               "devices: %s,\t" %
                                               available[socket]['devices'] +
                                               "device to remove: %s" %
                                               (avail_device))
                                    available[socket]['devices'].remove(
                                        avail_device)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %s from %s" %
                                       (device, available[socket]['devices']))
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Job %s res not removed from host " % job +
                           "available res: suspended job")
        pbs.logmsg(pbs.EVENT_DEBUG, "Available resources: %s" % (available))
        return available

    # Set a job limit
    def set_job_limit(self, jobid, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s: %s = %s" %
                   (caller_name(), jobid, resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'softmem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.soft_limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     jobid, 'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'], jobid,
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            elif resource == 'ncpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = self.select_cpus(path, value)
                    if cpus is None:
                        raise CgroupLimitError("Failed to configure cpuset.")
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
                    self.__copy_from_parent(os.path.join(self.paths['cpuset'],
                                            jobid, 'cpuset.mems'))
            elif resource == 'cpuset.cpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = value
                    if cpus is None:
                        msg = "Failed to configure cpus in cpuset."
                        raise CgroupLimitError(msg)
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
            elif resource == 'cpuset.mems':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.mems')
                    mems = value
                    if mems is None:
                        msg = "Failed to configure mems in cpuset."
                        raise CgroupLimitError(msg)
                    mems = ','.join(map(str, mems))
                    self.write_value(path, mems)
            elif resource == 'devices':
                if 'devices' in self.subsystems and \
                        self.cfg['cgroup']['devices']['enabled']:

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.allow')
                    devices = value
                    if devices is None:
                        msg = "Failed to configure device(s)."
                        raise CgroupLimitError(msg)
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Setting devices: %s for %s" % (devices, jobid))
                    for device in devices:
                        self.write_value(path, device)

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.list')
                    output = open(path, 'r').read()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "devices.list: %s" % output.replace("\n", ","))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    # Update resource usage for a job
    def update_job_usage(self, jobid, resc_used):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resc_used = %s" %
                   (caller_name(), str(resc_used)))
        # Sort the subsystems so that we consistently look at the subsystems
        # in the same order every time
        self.subsystems.sort()
        for subsys in self.subsystems:
            path = os.path.join(self.paths[subsys], jobid)
            if subsys == "memory":
                max_mem = self.__get_max_mem_usage(path)
                if max_mem is None:
                    pbs.logjobmsg(jobid, "%s: No max mem data" %
                                  (caller_name()))
                else:
                    resc_used['mem'] = pbs.size(convert_size(max_mem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: mem=%s" %
                                  (caller_name(), resc_used['mem']))
                mem_failcnt = self.__get_mem_failcnt(path)
                if mem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No mem fail count data" %
                                  (caller_name()))
                else:
                    # Check to see if the job exceeded its resource limits
                    if mem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memory limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "memsw":
                max_vmem = self.__get_max_memsw_usage(path)
                if max_vmem is None:
                    pbs.logjobmsg(jobid, "%s: No max vmem data" %
                                  (caller_name()))
                else:
                    resc_used['vmem'] = pbs.size(convert_size(max_vmem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: vmem=%s" %
                                  (caller_name(), resc_used['vmem']))

                vmem_failcnt = self.__get_memsw_failcnt(path)
                if vmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No vmem fail count data" %
                                  (caller_name()))
                else:
                    pbs.logjobmsg(jobid, "%s: vmem fail count: %d " %
                                  (caller_name(), vmem_failcnt))
                    if vmem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memsw limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "hugetlb":
                max_hpmem = self.__get_max_hugetlb_usage(path)
                if max_hpmem is None:
                    pbs.logjobmsg(jobid, "%s: No max hpmem data" %
                                  (caller_name()))
                    return
                hpmem_failcnt = self.__get_hugetlb_failcnt(path)
                if hpmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No hpmem fail count data" %
                                  (caller_name()))
                    return
                if hpmem_failcnt > 0:
                    err_msg = self.__get_error_msg(jobid)
                    pbs.logjobmsg(jobid, "Cgroup hugetlb limit exceeded: %s" %
                                  (err_msg))
                resc_used['hpmem'] = pbs.size(convert_size(max_hpmem, 'kb'))
                pbs.logjobmsg(jobid, "%s: Hugepage usage: %s" %
                              (caller_name(), resc_used['hpmem']))
            elif subsys == "cpuacct":
                cpu_usage = self.__get_cpu_usage(path)
                if cpu_usage is None:
                    pbs.logjobmsg(jobid, "%s: No CPU usage data" %
                                  (caller_name()))
                    return
                cpu_usage = convert_time(str(cpu_usage) + "ns")
                pbs.logjobmsg(jobid, "%s: CPU usage: %.3lf secs" %
                              (caller_name(), cpu_usage))
                resc_used['cput'] = pbs.duration(cpu_usage)

    # Creates the cgroup if it doesn't exists
    def create_job(self, jobid, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Iterate over the subsystems required
        for subsys in self.subsystems:
            # Create a directory for the job
            try:
                old_umask = os.umask(0022)
                path = self.paths[subsys]
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                path = os.path.join(self.paths[subsys], jobid)
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                if subsys == 'devices':
                    self.setup_subsys_devices(path, node)

            except OSError, exc:
                raise CgroupConfigError("Failed to create directory: %s (%s)" %
                                        (path, errno.errorcode[exc.errno]))
            except:
                raise
            finally:
                os.umask(old_umask)

    # Determine the cgroup limits and configure the cgroups
    def configure_job(self, jobid, hostresc, node):
        mem_enabled = 'memory' in self.subsystems
        vmem_enabled = 'memsw' in self.subsystems
        if mem_enabled or vmem_enabled:
            # Initialize mem variables
            mem_avail = node.get_memory_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "mem_avail %s" % mem_avail)
            mem_requested = None
            if 'mem' in hostresc.keys():
                mem_requested = convert_size(hostresc['mem'], 'kb')
            mem_default = None
            if mem_enabled:
                mem_default = self.default('memory')
            # Initialize vmem variables
            vmem_avail = node.get_vmem_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "vmem_avail %s" % vmem_avail)
            vmem_requested = None
            if 'vmem' in hostresc.keys():
                vmem_requested = convert_size(hostresc['vmem'], 'kb')
            vmem_default = None
            if vmem_enabled:
                vmem_default = self.default('memsw')
            # Initialize softmem variables
            if 'soft_limit' in self.cfg['cgroup']['memory'].keys():
                softmem_enabled = self.cfg['cgroup']['memory']['soft_limit']
            else:
                softmem_enabled = False
            # Sanity check
            if size_as_int(mem_avail) > size_as_int(vmem_avail):
                if size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                    raise CgroupLimitError(
                        'mem available (%s) exceeds vmem available (%s)' %
                        (mem_avail, vmem_avail))
            # Determine the mem limit
            if mem_requested is not None:
                # mem requested may not exceed available
                if size_as_int(mem_requested) > size_as_int(mem_avail):
                    raise JobValueError(
                        'mem requested (%s) exceeds mem available (%s)' %
                        (mem_requested, mem_avail))
                mem_limit = mem_requested
            else:
                # mem was not requested
                if mem_default is None:
                    mem_limit = mem_avail
                else:
                    mem_limit = mem_default
            # Determine the vmem limit
            if vmem_requested is not None:
                # vmem requested may not exceed available
                if size_as_int(vmem_requested) > size_as_int(vmem_avail):
                    raise JobValueError(
                        'vmem requested (%s) exceeds vmem available (%s)' %
                        (vmem_requested, vmem_avail))
                vmem_limit = vmem_requested
            else:
                # vmem was not requested
                if vmem_default is None:
                    vmem_limit = vmem_avail
                else:
                    vmem_limit = vmem_default
            # Ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Adjust for soft limits if enabled
            if mem_enabled and softmem_enabled:
                softmem_limit = mem_limit
                # The hard memory limit is assigned the lesser of the vmem
                # limit and available memory
                if size_as_int(vmem_limit) < size_as_int(mem_avail):
                    mem_limit = vmem_limit
                else:
                    mem_limit = mem_avail
            # Again, ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Sanity checks when both memory and memsw are enabled
            if mem_enabled and vmem_enabled:
                if vmem_requested is not None:
                    if size_as_int(vmem_limit) > size_as_int(vmem_requested):
                        # The user requested an invalid limit
                        raise JobValueError(
                            'vmem limit (%s) exceeds vmem requested (%s)' %
                            (vmem_limit, vmem_requested))
                if size_as_int(vmem_limit) > size_as_int(mem_limit):
                    # This job may utilize swap
                    if size_as_int(vmem_avail) <= size_as_int(mem_avail) and \
                      size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                        # No swap available
                        raise CgroupLimitError(
                            ('Job might utilize swap ' +
                             'and no swap space available'))
            # Assign mem and vmem
            if mem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: mem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), mem_limit))
                hostresc['mem'] = pbs.size(mem_limit)
                if softmem_enabled:
                    hostresc['softmem'] = pbs.size(softmem_limit)
            if vmem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: vmem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), vmem_limit))
                hostresc['vmem'] = pbs.size(vmem_limit)
        # Initialize hpmem variables
        hpmem_enabled = 'hugetlb' in self.subsystems
        if hpmem_enabled:
            hpmem_avail = node.get_hpmem_on_node(self.cfg)
            hpmem_limit = None
            hpmem_default = self.default('hugetlb')
            if hpmem_default is None:
                hpmem_default = hpmem_avail
            if 'hpmem' in hostresc.keys():
                hpmem_limit = convert_size(hostresc['hpmem'], 'kb')
            else:
                hpmem_limit = hpmem_default
            # Assign hpmem
            if size_as_int(hpmem_limit) > size_as_int(hpmem_avail):
                raise JobValueError('hpmem limit (%s) exceeds available (%s)' %
                                    (hpmem_limit, hpmem_avail))
            hostresc['hpmem'] = pbs.size(hpmem_limit)
        # Initialize cpuset variables
        cpuset_enabled = 'cpuset' in self.subsystems
        if cpuset_enabled:
            cpu_limit = 1
            if 'ncpus' in hostresc.keys():
                cpu_limit = hostresc['ncpus']
            if cpu_limit < 1:
                cpu_limit = 1
            hostresc['ncpus'] = pbs.pbs_int(cpu_limit)

        # Find the available resources and assign the right ones to the job

        attempt = 0
        assign_resources = False

        # Two attempts since self.cleanup_orphans may actually fix the
        # problem we see in a first attempt
        while attempt < 2:
            attempt += 1
            available_resources = self.available_node_resources(node)
            if 'vnodes' in hostresc:
                assign_resources = self.assign_job_resources_by_vnode(
                    hostresc, available_resources, node)
            else:
                assign_resources = self.assign_job_resources(
                    hostresc, available_resources, node)

            if (assign_resources is False) and (attempt < 2):
                # No resources were assigned to the job.
                # Most likely cause was that a cgroup has
                # not been cleaned up yet
                pbs.logmsg(pbs.EVENT_DEBUG, "Failed to assign resources. " +
                           "Checking the following cgroups on the node " +
                           "with the server: %s" % self.existing_cgroups)

                # Collect the jobs on the node (try reading mom_priv/jobs,
                # if not successful ask server)
                jobs_list = node.gather_jobs_on_node_local()

                if jobs_list is not None:
                    # Don't clean up the cgroup we just made for the
                    # job just yet
                    if jobid not in jobs_list:
                        jobs_list.append(jobid)

                    self.cleanup_orphans(jobs_list)

                    # Recheck what is now assigned after things were cleaned up
                    self.gather_assigned_resources()

        if assign_resources is False:
            # Cleanup cgroups for jobs not present on this node
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Assign resources failed. Attempting to cleanup all " +
                       "leftover cgroups (including event job %s)" % jobid)

            # Remove the current job from the jobs list so it will be
            # cleaned up as well.
            jobs_list.remove(jobid)

            self.cleanup_orphans(jobs_list)

            # Rerun the job and log the message
            line = 'Unable to assign resources to job. '
            line += 'Will requeue the job and try again.'
            line += 'job run_count: %d' % pbs.event().job.run_count
            pbs.event().job.rerun()
            pbs.event().reject(line)

        # Print out the assigned resources
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Assigned resources: %s" % (assign_resources))

        if cpuset_enabled:
            hostresc.pop('ncpus', None)
            for key in ['cpuset.cpus', 'cpuset.mems']:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Initialize devices variables
        devices_enabled = 'devices' in self.subsystems
        if devices_enabled:
            key = 'devices'
            if key in assign_resources:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Apply the resource limits to the cgroups
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Setting cgroup limits for: %s" %
                   (caller_name(), hostresc))

        # The vmem limit must be set after the mem limit, so sort the keys
        for resc in sorted(hostresc.keys()):
            self.set_job_limit(jobid, resc, hostresc[resc])

    # Kill any processes contained within a tasks file
    def __kill_tasks(self, tasks_file):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        with open(tasks_file, 'r') as fd:
            for line in fd:
                os.kill(int(line.strip()), signal.SIGKILL)
        fd.close()
        count = 0
        with open(tasks_file, 'r') as fd:
            for line in fd:
                count += 1
                pid = line.strip()
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Pid: %s did not clean up" %
                           (caller_name(), pid))
                if os.path.isfile('/proc/%s/status' % pid):
                    tmp = open('/proc/%s/status' % pid).readlines()
                    tmp_line = ''
                    for entry in tmp:
                        if entry.find('Name:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('State:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('Uid:') != -1:
                            tmp_line += entry.strip()
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: %s" % (caller_name(), tmp_line))

        return count

    # Perform the actual removal of the cgroup directory
    # by default, only one attempt at killing tasks in cgroup, without sleeping
    # since this method could be called many times
    # (for N directories times M jobs)
    def __remove_cgroup(self, path, tasks_kill_attempts=1):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            if os.path.exists(path):
                tasks_file = os.path.join(path, 'tasks')
                task_cnt = self.__kill_tasks(tasks_file)
                kill_attempts = 0
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Task Kill Attempts: %d" % tasks_kill_attempts)
                while (task_cnt > 0) and (kill_attempts < tasks_kill_attempts):
                    kill_attempts += 1
                    count = 0

                    with open(tasks_file, 'r') as fd:
                        for line in fd:
                            count += 1
                        task_cnt = count

                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "Attempt: %d - cgroup still has %d tasks: %s" %
                               (kill_attempts, task_cnt, path))
                    if kill_attempts < tasks_kill_attempts:
                        time.sleep(1)
                        # Added since there are race conditions where tasks are
                        # being added at the same time we get the initial
                        # task count
                        tasks_file = os.path.join(path, 'tasks')
                        task_cnt = self.__kill_tasks(tasks_file)

                if task_cnt == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Removing directory %s" %
                               (caller_name(), path))
                    os.rmdir(path)
                    if os.path.exists(path):
                        time.sleep(.25)
                        if os.path.exists(path):
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "%s: 2nd Try Removing directory %s" %
                                       (caller_name(), path))
                            os.rmdir(path)
                            time.sleep(.25)
                        if os.path.exists(path):
                            return False
                else:
                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "cgroup still has %d tasks: %s" %
                               (task_cnt, path))

                    # Check to see if the offline file is already present
                    # on the mom
                    offline_file_exists = False
                    if os.path.isfile(self.offline_file):
                        msg = "Cgroup(s) not cleaning up but the node already "
                        msg += "has the offline file."

                        pbs.logmsg(pbs.EVENT_DEBUG, msg)
                    else:
                        # Check to see that the node is not already offline.
                        try:
                            tmp_state = pbs.server().vnode(self.hostname).state
                        except:
                            msg = "Unable to contact server for node state"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            tmp_state = None
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Current Node State: %d" % tmp_state)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Offline Node State: %d" % pbs.ND_OFFLINE)

                        if tmp_state == pbs.ND_OFFLINE:
                            msg = "Cgroup(s) not cleaning up but the node is "
                            msg += "already offline."
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                        elif tmp_state is not None:
                            # Offline the node(s)
                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                            msg = "Offlining node since cgroup(s) are not "
                            msg += "cleaning up"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            vnode = pbs.event().vnode_list[self.hostname]

                            vnode.state = pbs.ND_OFFLINE

                            # Write a file locally to reduce server traffic
                            # when it comes time to online the node
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Offline file: %s" % self.offline_file)
                            open(self.offline_file, 'a').close()
                            vnode.comment = self.offline_msg

                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                        else:
                            pass

                    return False

        except OSError, exc:
            pbs.logmsg(pbs.EVENT_SYSTEM,
                       "Failed to remove cgroup path: %s (%s)" %
                       (path, errno.errorcode[exc.errno]))
        except:
            pbs.logmsg(pbs.EVENT_SYSTEM, "Failed to remove cgroup path: %s" %
                       (path))

        return True

    # Removes cgroup directories that are not associated with a local job
    def cleanup_orphans(self, local_jobs):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        orphan_cnt = 0
        for key in self.paths.keys():
            for dir in glob.glob(os.path.join(self.paths[key], '[0-9]*')):
                jobid = os.path.basename(dir)
                if jobid not in local_jobs:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Removing orphaned cgroup: %s" %
                               (caller_name(), dir))
                    # default number of attempts at killing tasks
                    status = self.__remove_cgroup(dir)
                    if not status:
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "%s: Removing orphaned cgroup %s failed " %
                                   (caller_name(), dir))
                        orphan_cnt += 1
        return orphan_cnt

    # Removes the cgroup directories for a job
    def delete(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        tasks_kill_attempted = False

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            if not tasks_kill_attempted:
                # Make multiple attempts at killing tasks in cgroups on
                # first subsystem encountered since it is "normal" for
                # a cgroup.delete to encounter processes hard to kill
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                                           jobid),
                                              tasks_kill_attempts=(
                                              CGROUP_KILL_ATTEMPTS))
                tasks_kill_attempted = True
            else:
                # We tried getting rid of job processes earlier,
                # so don't try N times with sleeps
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                              jobid), tasks_kill_attempts=1)

            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Status: %s" %
                       (caller_name(), status))
            if status is False:
                # Rerun the job and log the message
                line = 'Unable to cleanup a cgroup. '
                line += 'Will offline the node and requeue the job '
                line += 'and try again. job run_count: %d' % \
                        pbs.event().job.run_count
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Failed to remove cgroup: %s" %
                           (caller_name(), line))
                pbs.event().accept()

    # Write a value to a limit file
    def write_value(self, file, value, mode='w'):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: writing %s to %s" %
                   (caller_name(), value, file))
        try:
            with open(file, mode) as fd:
                fd.write(str(value) + '\n')
        except IOError, exc:
            if exc.errno == errno.ENOENT:
                pbs.logmsg(pbs.EVENT_SYSTEM,
                           "Trying to set limit to unknown file %s" % file)
            elif exc.errno in [errno.EACCES, errno.EPERM]:
                pbs.logmsg(pbs.EVENT_SYSTEM, "Permission denied on file %s" %
                           file)
            elif exc.errno == errno.EBUSY:
                raise CgroupBusyError("Limit rejected")
            elif exc.errno == errno.EINVAL:
                raise CgroupLimitError("Invalid limit value: %s" % (value))
            else:
                raise
        except:
            raise

    # Return memory failcount
    def __get_mem_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.failcnt"), 'r').read().strip())
        except:
            return None

    # Return vmem failcount
    def __get_memsw_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.failcnt"), 'r').read().strip())
        except:
            return None

    # Return hpmem failcount
    def __get_hugetlb_failcnt(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.failcnt"))[0], 'r').read().strip())
        except:
            return None

    # Return the max usage of memory in bytes
    def __get_max_mem_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of memsw in bytes
    def __get_max_memsw_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of hugetlb in bytes
    def __get_max_hugetlb_usage(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.max_usage_in_bytes"))[0],
                            'r').read().strip())
        except:
            return None

    # Return the cpuacct.usage in cpu seconds
    def __get_cpu_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "cpuacct.usage"), 'r').read().strip())
        except:
            return None

    # Assign CPUs to the cpuset
    def select_cpus(self, path, ncpus):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path is %s" % (caller_name(), path))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: ncpus is %s" %
                   (caller_name(), ncpus))
        if ncpus < 1:
            ncpus = 1
        try:
            # Must select from those currently available
            cpufile = os.path.basename(path)
            base = os.path.dirname(path)
            parent = os.path.dirname(base)
            avail = cpus2list(open(os.path.join(parent, cpufile),
                              'r').read().strip())
            if len(avail) < 1:
                raise CgroupProcessingError("No CPUs avaialble in cgroup.")
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Available CPUs: %s" %
                       (caller_name(), avail))
            assigned = []
            for file in glob.glob(os.path.join(parent, '[0-9]*', cpufile)):
                cpus = cpus2list(open(file, 'r').read().strip())
                for id in cpus:
                    if id in avail:
                        avail.remove(id)
            if len(avail) < ncpus:
                raise CgroupProcessingError(
                    "Insufficient CPUs avaialble in cgroup.")
            if len(avail) == ncpus:
                return avail
            # FUTURE: Try to minimize NUMA nodes based on memory requirement
            return avail[0:ncpus]
        except:
            raise

    # Return the error message in system message file
    def __get_error_msg(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        proc = subprocess.Popen(['dmesg'],
                                shell=False, stdout=subprocess.PIPE)
        stdout = proc.communicate()[0].splitlines()
        stdout.reverse()
        # Check to see if the job id is found in dmesg otherwise
        # you could get the information for another job that was killed
        # the line will look like Memory cgroup stats for /pbspro/279.centos7
        kill_line = ""

        for line in stdout:
            start = line.find('Killed process ')
            if start >= 0:
                kill_line = line[start:]
            job_start = line.find(jobid)
            if job_start >= 0:
                # pbs.logmsg(pbs.EVENT_DEBUG3,
                #           "%s: Jobid: %s, Line: %s" %
                #           (caller_name(), jobid, line))
                return kill_line
        return ""

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_job_env_file(self, jobid, env_list):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        # if not os.path.exists(self.host_job_env_dir):
        #    os.makedirs(self.host_job_env_dir, 0755)

        # Write out assigned_resources
        try:
            lines = string.join(env_list, '\n')
            filename = self.host_job_env_filename % jobid
            outfile = open(filename, "w")
            outfile.write(lines)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" % (filename))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (lines))
            return True
        except:
            return False

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        if not os.path.exists(self.hook_storage_dir):
            os.makedirs(self.hook_storage_dir, 0755)

        # Write out assigned_resources
        try:
            json_str = json.dumps(self.host_assigned_resources)
            outfile = open(self.hook_storage_dir+os.sep+jobid, "w")
            outfile.write(json_str)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" %
                       (self.hook_storage_dir+os.sep+jobid))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (json_str))
            return True
        except:
            return False

    def read_in_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        pbs.logmsg(pbs.EVENT_DEBUG3, "Host assigned resources: %s" %
                   (self.host_assigned_resources))
        if os.path.isfile(self.hook_storage_dir+os.sep+jobid):
            # Write out assigned_resources
            try:
                infile = open(self.hook_storage_dir+os.sep+jobid, 'r')
                json_data = json.load(infile, object_hook=decode_dict)
                self.host_assigned_resources = json_data
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Host assigned resources: %s" %
                           (self.host_assigned_resources))
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
            finally:
                infile.close()

        if self.host_assigned_resources is not None:
            return True
        else:
            return False

#
#
# FUNCTION main
#


def main():
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Function called" % (caller_name()))
    hostname = pbs.get_local_nodename()
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Host is %s" % (caller_name(), hostname))

    # Log the hook event type
    e = pbs.event()
    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook name is %s" %
               (caller_name(), e.hook_name))

    try:
        # Instantiate the hook utility class
        hooks = HookUtils()
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Event type is %s" %
                   (caller_name(), hooks.event_name(e.type)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(hooks)))
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: Failed to instantiate hook utility class" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        e.accept()

    if not hooks.hashandler(e.type):
        # Bail out if there is no handler for this event
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s event not handled by this hook" %
                   (caller_name(), hooks.event_name(e.type)))
        e.accept()

    try:
        # If an exception occurs, jobutil must be set
        jobutil = None
        # Instantiate the job utility class first so jobutil can be accessed
        # by the exception handlers.
        if hasattr(e, 'job'):
            jobutil = JobUtils(e.job)
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Job information class instantiated" %
                       (caller_name()))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" %
                       (caller_name(), repr(jobutil)))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Event does not include a job" %
                       (caller_name()))
        # Instantiate the cgroup utility class
        vnode = None
        if hasattr(e, 'vnode_list'):
            if hostname in e.vnode_list.keys():
                vnode = e.vnode_list[hostname]
        cgroup = CgroupUtils(hostname, vnode)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Cgroup utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(cgroup)))
        # Bail out if there is nothing to do
        if len(cgroup.subsystems) < 1:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: All cgroups disabled" %
                       (caller_name()))
            e.accept()

        # Call the appropriate handler
        if hooks.invoke_handler(e, cgroup, jobutil) is True:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned success" %
                       (caller_name()))
            e.accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned failure" %
                       (caller_name()))
            e.reject()

    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass

    except AdminError, exc:
        # Something on the system is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Admin error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except UserError, exc:
        # User must correct problem and resubmit job
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("User error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.delete()
                msg += " (deleted)"
            except:
                msg += " (delete failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except JobValueError, exc:
        # Something in PBS is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Job value error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except Exception, exc:
        # Catch all other exceptions and report them.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Unexpected error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

# line below required for unittesting
if __name__ == "__builtin__":
    start_time = time.time()
    try:
        main()
    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
    finally:
        pbs.logmsg(pbs.EVENT_DEBUG, "Elapsed time: %0.4lf" %
                   (time.time() - start_time))
---
2016-07-18 16:57:45,405 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups.json', 'content-encoding': 'default'}
2016-07-18 16:57:45,406 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups.json
2016-07-18 16:57:45,492 INFO     tmp_cfg: {
        "cgroup_prefix"         : "pbspro",
        "periodic_resc_update"  : true, 
        "exclude_hosts"         : [],
        "exclude_vntypes"       : [],
        "run_only_on_hosts"     : ["SomeNodeOutThere"],   
        "vnode_per_numa_node"   : false,
        "online_offlined_nodes" : true, 
        "cgroup":
        {
                "cpuacct":
                {
                        "enabled"               : true,
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : []
                },
                "cpuset":
                {
                        "enabled"               : true,
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : []
                },
                "devices":
                {
                        "enabled"               : false,
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : [],
                        "allow" : ["b *:* rwm","c *:* rwm", ["mic/scif","rwm"],["nvidiactl","rwm", "*"],["nvidia-uvm","rwm"]]
                },
                "hugetlb":
                {
                        "enabled"               : false,
                        "default"               : "0MB",
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : []
                },
                "memory":
                {
                        "enabled"               : true,
                        "default"               : "256MB",
                        "reserve_memory"        : "0MB",
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : []
                },
                "memsw":
                {
                        "enabled"               : true,
                        "default"               : "256MB",
                        "reserve_memory"        : "2gb",
                        "exclude_hosts"         : [],
                        "exclude_vntypes"       : []
                }
        }
}

2016-07-18 16:57:45,493 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/tmp/t1_l1.json', 'content-encoding': 'default'}
2016-07-18 16:57:45,493 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /tmp/t1_l1.json
2016-07-18 16:57:45,531 ERROR    err: ["hook 't1_l1' contents overwritten"]
2016-07-18 16:57:45,532 INFO     job: executable set to /bin/sleep with arguments: 100
2016-07-18 16:57:45,533 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0755 /tmp/PtlPbsJobScriptXZSuI_
2016-07-18 16:57:45,579 INFOCLI  centos7: sudo -H -u pbsuser /usr/local/bin/qsub -N test_t4d -l select=1:ncpus=1:mem=300mb /tmp/PtlPbsJobScriptXZSuI_
2016-07-18 16:57:45,597 INFO     submit to centos7 as pbsuser: job 382.centos7 OrderedDict([('Job_Name', 'test_t4d'), ('Resource_List.select', '1:ncpus=1:mem=300mb')])
2016-07-18 16:57:45,597 INFOCLI  job script /tmp/PtlPbsJobScriptXZSuI_
---
#!/usr/bin/env python

import os
import subprocess
from subprocess import Popen, PIPE
import time

eat_mem_c = """
#include <stdio.h>
#include <stdlib.h>
#include <string.h>


#define ONE_GB (0x40000000L)
#define ONE_MB (0x100000L)

void
eatmem(total_mb, block_mb)
{
	int i;
	char *buf;
	size_t block;

	for (i=1, buf=NULL, block=(block_mb*ONE_MB); (i*block) < (total_mb*ONE_MB); i++) {
		buf = realloc((void *)buf, (i*block));
		memset(buf+((i-1)*block), 0, block);
	}
	return;
}

int
main(int argc, char *argv[])
{
	int total_mb = 8;
	int block_mb = 1;

	if (argc > 1)
		total_mb = atoi(argv[1]);

	if (argc > 2)
		block_mb = atoi(argv[2]);

	eatmem(total_mb, block_mb);
	printf("Initialized %dMB in blocks of %dMB\\n", total_mb, block_mb);
}
"""

fname = "/tmp/pbs_pp325_eat_mem.c"
fname_exe = "/tmp/pbs_pp325_eat_mem"
fout = open(fname, "w")
fout.write(eat_mem_c)
fout.close()

print os.getppid()

p = Popen(["gcc", "-o", fname_exe, fname], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

p = Popen([fname_exe, "400", "10"], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

os.remove(fname)
os.remove(fname_exe)
time.sleep(1)

---
2016-07-18 16:57:45,598 INFOCLI  centos7: /usr/local/bin/qstat -f 382.centos7
2016-07-18 16:57:45,701 INFO     expect on server centos7: job_state = R job 382.centos7  got: job_state = Q
2016-07-18 16:57:46,203 INFOCLI  centos7: /usr/local/bin/qmgr -c set server scheduling=True
2016-07-18 16:57:46,219 INFOCLI  centos7: /usr/local/bin/qstat -f 382.centos7
2016-07-18 16:57:46,325 INFO     expect on server centos7: job_state = R job 382.centos7 attempt: 2 ...  OK
2016-07-18 16:57:47,328 INFO     mom centos7 log match: searching for "centos7 is not in the approved list of hosts: \['SomeNodeOutThere'\]" - using regular expression  - No match
2016-07-18 16:57:57,789 INFO     mom centos7 log match: searching for "centos7 is not in the approved list of hosts: \['SomeNodeOutThere'\]" - using regular expression ... OK
2016-07-18 16:57:57,789 INFO     ==============================
2016-07-18 16:57:57,790 INFO      Entered CgroupsTests tearDown
2016-07-18 16:57:57,790 INFO     ==============================
2016-07-18 16:57:57,790 INFO     ===============================
2016-07-18 16:57:57,790 INFO     Completed CgroupsTests tearDown
2016-07-18 16:57:57,790 INFO     ===============================
2016-07-18 16:57:57,790 INFO     ok

2016-07-18 16:57:57,790 INFO     test name: test_t5 (tests.cgroups_tests.CgroupsTests)...
2016-07-18 16:57:57,790 INFO     test start time: Mon Jul 18 16:57:57 2016
2016-07-18 16:57:57,791 INFO     test docstring: 
 Test to verify that cgroups are not enforced on nodes
            that have an exclude vntype file set
        
2016-07-18 16:57:57,791 INFO     ===========================
2016-07-18 16:57:57,791 INFO      Entered CgroupsTests setUp
2016-07-18 16:57:57,791 INFO     ===========================
2016-07-18 16:57:57,791 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:57:57,850 INFO     status on centos7: server
2016-07-18 16:57:57,850 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:57:57,905 INFO     manager on centos7: set server {'managers': (3, 'root@*')}
2016-07-18 16:57:57,905 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers-=root@*
2016-07-18 16:57:57,927 INFO     manager on centos7: set server {'managers': (2, 'root@*')}
2016-07-18 16:57:57,927 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers+=root@*
2016-07-18 16:57:57,967 INFO     server centos7: reverting configuration to defaults
2016-07-18 16:57:57,967 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:57:58,024 INFO     select on centos7: __ALL__
2016-07-18 16:57:58,024 INFOCLI  centos7: /usr/local/bin/qselect
2016-07-18 16:57:58,040 INFOCLI  centos7: /usr/local/bin/qstat -f @centos7.usa.org
2016-07-18 16:57:58,098 INFO     expect on server centos7: job_state set 0 job ...  OK
2016-07-18 16:57:58,099 INFOCLI  centos7: /usr/local/bin/pbs_rstat -f
2016-07-18 16:57:58,118 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:57:58,142 INFO     manager on centos7: delete hook t1_l1
2016-07-18 16:57:58,143 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c delete hook t1_l1
2016-07-18 16:57:58,173 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:57:58,201 INFO     expect on server centos7:  unset  hook t1_l1 ...  OK
2016-07-18 16:57:58,202 INFOCLI  centos7: /usr/local/bin/qstat -Qf @centos7.usa.org
2016-07-18 16:57:58,222 INFO     manager on centos7: delete queue workq
2016-07-18 16:57:58,222 INFOCLI  centos7: /usr/local/bin/qmgr -c delete queue workq
2016-07-18 16:57:58,238 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:57:58,253 INFO     expect on server centos7:  unset  queue workq ...  OK
2016-07-18 16:57:58,253 INFO     manager on centos7: create queue workq {'started': 'True', 'queue_type': 'Execution', 'enabled': 'True'}
2016-07-18 16:57:58,254 INFOCLI  centos7: /usr/local/bin/qmgr -c create queue workq started=True,queue_type=Execution,enabled=True
2016-07-18 16:57:58,267 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:57:58,279 INFO     expect on server centos7: started set True || queue_type set Execution || enabled set True queue workq ...  OK
2016-07-18 16:57:58,279 INFO     manager on centos7: set server {'log_events': '511', 'default_queue': 'workq'}
2016-07-18 16:57:58,279 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=511,default_queue=workq
2016-07-18 16:57:58,300 INFO     status on centos7: resource
2016-07-18 16:57:58,301 INFO     manager on centos7: list resource
2016-07-18 16:57:58,301 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource
2016-07-18 16:57:58,355 INFO     manager on centos7: delete resource nmics,ngpus
2016-07-18 16:57:58,355 INFOCLI  centos7: /usr/local/bin/qmgr -c delete resource nmics,ngpus
2016-07-18 16:57:58,417 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource nmics
2016-07-18 16:57:58,431 INFO     expect on server centos7:  unset  resource nmics ...  OK
2016-07-18 16:57:58,432 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource ngpus
2016-07-18 16:57:58,442 INFO     expect on server centos7:  unset  resource ngpus ...  OK
2016-07-18 16:57:58,443 INFOCLI  status on centos7: server license_count
2016-07-18 16:57:58,443 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:57:58,493 INFO     server: centos7.usa.org licensed
2016-07-18 16:57:58,515 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/server_priv/comm.lock
2016-07-18 16:57:58,557 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:57:58,570 INFO     scheduler centos7: reverting configuration to defaults
2016-07-18 16:57:58,571 INFO     manager on centos7: list sched
2016-07-18 16:57:58,571 INFOCLI  centos7: /usr/local/bin/qmgr -c list sched
2016-07-18 16:57:58,581 INFO     manager on centos7: unset sched ['sched_cycle_length']
2016-07-18 16:57:58,582 INFOCLI  centos7: /usr/local/bin/qmgr -c unset sched sched_cycle_length
2016-07-18 16:57:58,597 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/dedicated_time
2016-07-18 16:57:58,607 INFOCLI2 centos7: cmp /usr/local/etc/pbs_resource_group /var/spool/pbs/sched_priv/resource_group
2016-07-18 16:57:58,609 INFO     scheduler centos7: reverting holidays file to default
2016-07-18 16:57:58,610 INFOCLI2 centos7: cmp /usr/local/etc/pbs_holidays /var/spool/pbs/sched_priv/holidays
2016-07-18 16:57:58,612 INFOCLI2 centos7: cmp /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:57:58,621 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:57:58,637 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:57:58,649 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:57:58,650 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:57:58,714 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:57:58,737 INFOCLI2 centos7: sudo -H cmp /usr/local/sbin/pbs_mom /usr/local/sbin/pbs_mom.cpuset
2016-07-18 16:57:58,774 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:57:58,790 INFO     mom centos7: reverting configuration to defaults
2016-07-18 16:57:58,790 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/epilogue
2016-07-18 16:57:58,806 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/prologue
2016-07-18 16:57:58,816 INFOCLI2 centos7: sudo -H ls /var/spool/pbs/mom_priv/config.d
2016-07-18 16:57:58,829 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsXOqf9Z /var/spool/pbs/mom_priv/config
2016-07-18 16:57:58,840 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/mom_priv/config
2016-07-18 16:57:58,873 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/mom_priv/config
2016-07-18 16:57:58,885 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/mom_priv/config
2016-07-18 16:57:58,895 INFO     mom centos7: sent signal -HUP
2016-07-18 16:57:58,895 INFOCLI2 centos7: sudo -H kill -HUP 42976
2016-07-18 16:57:58,945 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:57:58,973 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 16:57:58,989 INFO     status on centos7: node centos7
2016-07-18 16:57:58,989 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:57:59,004 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:57:59,016 INFO     expect on server centos7: state = free node centos7 ...  OK
2016-07-18 16:57:59,016 INFO     ============================
2016-07-18 16:57:59,016 INFO     Completed CgroupsTests setUp
2016-07-18 16:57:59,016 INFO     ============================
2016-07-18 16:57:59,016 INFO     server centos7: server operating mode set to cli
2016-07-18 16:57:59,017 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:57:59,068 INFO     expect on server centos7: pbs_version ~ .*14.* server centos7.usa.org ...  OK
2016-07-18 16:57:59,068 INFO     manager on centos7: set server {'log_events': '2047'}
2016-07-18 16:57:59,068 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=2047
2016-07-18 16:57:59,092 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:57:59,145 INFO     expect on server centos7: log_events set 2047 server centos7.usa.org ...  OK
2016-07-18 16:57:59,146 INFO     scheduler centos7: config {'resources': 'ncpus,mem,host,vnode,vmem'}
2016-07-18 16:57:59,146 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /var/spool/pbs/sched_priv/sched_config /var/spool/pbs/sched_priv/sched_config.bak
2016-07-18 16:57:59,158 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbs4hH36D /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:57:59,183 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:57:59,194 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:57:59,204 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:57:59,228 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:57:59,228 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:57:59,245 INFO     scheduler centos7 log match: searching for "Error reading line" - No match
2016-07-18 16:58:00,247 INFO     manager on centos7 as root: create resource nmics {'flag': 'nh', 'type': 'long'}
2016-07-18 16:58:00,247 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource nmics flag=nh,type=long
2016-07-18 16:58:00,283 INFO     manager on centos7 as root: create resource ngpus {'flag': 'nh', 'type': 'long'}
2016-07-18 16:58:00,283 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource ngpus flag=nh,type=long
2016-07-18 16:58:00,314 INFO     Test Summary: test_t1_l1 tests "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" cgroup hook
2016-07-18 16:58:00,314 INFO     status on centos7: hook
2016-07-18 16:58:00,314 INFO     manager on centos7: list hook
2016-07-18 16:58:00,315 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:58:00,332 INFO     status on centos7: hook
2016-07-18 16:58:00,332 INFO     manager on centos7: list hook
2016-07-18 16:58:00,333 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:58:00,356 INFO     manager on centos7: create hook t1_l1
2016-07-18 16:58:00,356 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c create hook t1_l1
2016-07-18 16:58:00,386 INFO     manager on centos7: set hook t1_l1 {'freq': 2, 'enabled': 'True', 'event': '"execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"'}
2016-07-18 16:58:00,386 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set hook t1_l1 freq=2,enabled=True,event="execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"
2016-07-18 16:58:00,412 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:58:00,436 INFO     expect on server centos7: freq set 2 || enabled set True || event set "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" hook t1_l1 ...  OK
2016-07-18 16:58:00,436 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-python', 'input-file': '/tmp/PtlPbsz9GknV', 'content-encoding': 'default'}
2016-07-18 16:58:00,437 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-python default /tmp/PtlPbsz9GknV
2016-07-18 16:58:00,482 INFO     server centos7: imported hook body
---
#  Copyright (C) 2003-2016 Altair Engineering, Inc. All rights reserved.
#
#  ALTAIR ENGINEERING INC. Proprietary and Confidential. Contains Trade Secret
#  Information. Not for use or disclosure outside ALTAIR and its licensed
#  clients. Information contained herein shall not be decompiled, disassembled,
#  duplicated or disclosed in whole or in part for any purpose. Usage of the
#  software is only as explicitly permitted in the end user software license
#  agreement.
#
#  Copyright notice does not imply publication.

# Last Updated
# Date: 07-06-2016
#
# When soft_limit is true for memory, memsw represents the hard limit.
#
# The resources value in sched_config must contain entries for mem and
# vmem if those subsystems are enabled in the hook configuration file. The
# amount of resource requested will not be avaiable to the hook if they
# are not present.

# This hook handles all of the operations necessary for PBS to support
# cgroups on linux hosts that support them (kernel 2.6.28 and higher)
#
# This hook services the following events:
# - exechost_periodic
# - exechost_startup
# - execjob_attach
# - execjob_begin
# - execjob_end
# - execjob_epilogue
# - execjob_launch

# Imports from __future__ must happen first
from __future__ import with_statement

# Additional imports
import sys
import os
import errno
import signal
import subprocess
import re
import glob
import time
import string
import platform
import traceback
import copy
from stat import S_ISBLK, S_ISCHR
import operator
import pwd
import pbs
import fnmatch
import socket

try:
    import json
except:
    import simplejson as json

CGROUP_KILL_ATTEMPTS = 12

# Flag to turn off features not in 12.2.40X branch
CRAY_12_BRANCH = False


# ============================================================================
# Derived error classes
# ============================================================================

# Base class for errors fixable only by administrative action.


class AdminError(Exception):
    pass

# Base class for errors in processing, unknown cause.


class ProcessingError(Exception):
    pass

# Base class for errors fixable by the user.


class UserError(Exception):
    pass

# Errors in PBS job resource values.


class JobValueError(UserError):
    pass

# Errors when the cgroup is busy.


class CgroupBusyError(ProcessingError):
    pass

# Errors in configuring cgroup.


class CgroupConfigError(AdminError):
    pass

# Errors in configuring cgroup.


class CgroupLimitError(AdminError):
    pass

# Errors processing cgroup.


class CgroupProcessingError(ProcessingError):
    pass

# ============================================================================
# Utility functions
# ============================================================================

#
# FUNCTION caller_name
#
# Return the name of the calling function or method.
#


def caller_name():
    return str(sys._getframe(1).f_code.co_name)

#
# FUNCTION convert_size
#
# Convert a string containing a size specification (e.g. "1m") to a
# string using different units (e.g. "1024k").
#
# This function only interprets a decimal number at the start of the string,
# stopping at any unrecognized character and ignoring the rest of the string.
#
# When down-converting (e.g. MB to KB), all calculations involve integers and
# the result returned is exact. When up-converting (e.g. KB to MB) floating
# point numbers are involved. The result is rounded up. For example:
#
# 1023MB -> GB yields 1g
# 1024MB -> GB yields 1g
# 1025MB -> GB yields 2g  <-- This value was rounded up
#
# Pattern matching or conversion may result in exceptions.
#


def convert_size(value, units='b'):
    logs = {'b': 0, 'k': 10, 'm': 20, 'g': 30,
            't': 40, 'p': 50, 'e': 60, 'z': 70, 'y': 80}
    try:
        new = units[0].lower()
        if new not in logs.keys():
            new = 'b'
        val, old = re.match('([-+]?\d+)([bkmgtpezy]?)',
                            str(value).lower()).groups()
        val = int(val)
        if val < 0:
            raise ValueError('Value may not be negative')
        if old not in logs.keys():
            old = 'b'
        factor = logs[old] - logs[new]
        val *= 2 ** factor
        slop = val - int(val)
        val = int(val)
        if slop > 0:
            val += 1
        # pbs.size() does not like units following zero
        if val <= 0:
            return '0'
        else:
            return str(val) + new
    except:
        return None

#
# FUNCTION size_as_int
#
# Convert a size string to an integer representation of size in bytes
#


def size_as_int(value):
    return int(convert_size(value).rstrip(string.ascii_lowercase))

#
# FUNCTION convert_time
#
# Converts a integer value for time into the value of the return unit
#
# A valid decimal number, with optional sign, may be followed by a character
# representing a scaling factor.  Scaling factors may be either upper or
# lower case. Examples include:
# 250ms
# 40s
# +15min
#
# Valid scaling factors are:
# ns  = 10**-9
# us  = 10**-6
# ms  = 10**-3
# s   =      1
# min =     60
# hr  =   3600
#
# Pattern matching or conversion may result in exceptions.
#


def convert_time(value, return_unit='s'):
    multipliers = {'':  1, 'ns': 10 ** -9, 'us': 10 ** -6,
                   'ms': 10 ** -3, 's': 1, 'min': 60, 'hr': 3600}
    n, factor = re.match('([-+]?\d+)(\w*[a-zA-Z])?',
                         str(value).lower()).groups(0)

    # Check to see if there was not unit of time specified
    if factor == 0:
        factor = ''

    # Check to see if the unit is valid
    if str.lower(factor) not in multipliers.keys():
        raise ValueError('Time unit not recognized.')

    # Convert the value to seconds
    value_in_sec = float(n) * float(multipliers[str.lower(factor)])
    if return_unit != 's':
        return value_in_sec / multipliers[str.lower(return_unit)]
    else:
        return float('%lf' % value_in_sec)

# simplejson hook to convert lists from unicode to utf-8


def decode_list(data):
    rv = []
    for item in data:
        if isinstance(item, unicode):
            item = item.encode('utf-8')
        elif isinstance(item, list):
            item = decode_list(item)
        elif isinstance(item, dict):
            item = decode_dict(item)
        rv.append(item)
    return rv

# simplejson hook to convert dictionaries from unicode to utf-8


def decode_dict(data):
    rv = {}
    for key, value in data.iteritems():
        if isinstance(key, unicode):
            key = key.encode('utf-8')
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        elif isinstance(value, list):
            value = decode_list(value)
        elif isinstance(value, dict):
            value = decode_dict(value)
        rv[key] = value
    return rv

# Convert CPU list format (with ranges) to a Python list


def cpus2list(s):
    # The input string is a comma separated list of digits and ranges.
    # Examples include:
    # 0-3,8-11
    # 0,2,4,6
    # 2,5-7,10
    cpus = []
    if s.strip() == '':
        return cpus
    for r in s.split(','):
        if '-' in r[1:]:
            start, end = r.split('-', 1)
            for i in range(int(start), int(end) + 1):
                cpus.append(int(i))
        else:
            cpus.append(int(r))
    return cpus


# Helper function to ignore suspended jobs when removing
# "already used" resources
def job_to_be_ignored(jobid):
    # Get job substate from small utility that calls printjob
    pbs_exec = ''
    if not CRAY_12_BRANCH:
        pbs_conf = pbs.get_pbs_conf()
        pbs_exec = pbs_conf['PBS_EXEC']
    else:
        if 'PBS_EXEC' in self.cfg:
            pbs_exec = self.cfg['PBS_EXEC']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "PBS_EXEC needs to be defined in the config file")
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Exiting the cgroups hook")
            pbs.accept()

    printjob_cmd = pbs_exec+os.sep+'bin'+os.sep+'printjob'

    cmd = [printjob_cmd, jobid]

    substate = None
    try:
        pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
        # Collect the job substate information
        process = subprocess.Popen(
                                   cmd,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        (out, err) = process.communicate()
        # Find the job substate
        substate_re = re.compile(r"substate:\s+(?P<jobstate>\S+)\s+")
        substate = substate_re.search(out)
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Unexpected error in job_to_be_ignored: %s" %
                   ' '.join([repr(sys.exc_info()[0]),
                            repr(sys.exc_info()[1])]))
        out = "Unknown: failed to run " + printjob_cmd

    if substate is not None:
        substate = substate.group().split(':')[1].strip()
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Job %s has substate %s" % (jobid, substate))

        suspended_substates = ['0x2b', '0x2d', 'unknown']
        return (substate in suspended_substates)
    else:
        return False

# ============================================================================
# Utility classes
# ============================================================================

#
# CLASS HookUtils
#


class HookUtils:

    def __init__(self, hook_events=None):
        if hook_events is not None:
            self.hook_events = hook_events
        else:
            self.hook_events = {
                # Defined in the order they appear in module_pbs_v1.c
                pbs.QUEUEJOB: {
                    'name': 'queuejob',
                    'handler': None
                },
                pbs.MODIFYJOB: {
                    'name': 'modifyjob',
                    'handler': None
                },
                pbs.RESVSUB: {
                    'name': 'resvsub',
                    'handler': None
                },
                pbs.MOVEJOB: {
                    'name': 'movejob',
                    'handler': None
                },
                pbs.RUNJOB: {
                    'name': 'runjob',
                    'handler': None
                },
                pbs.PROVISION: {
                    'name': 'provision',
                    'handler': None
                },
                pbs.EXECJOB_BEGIN: {
                    'name': 'execjob_begin',
                    'handler': self.__execjob_begin_handler
                },
                pbs.EXECJOB_PROLOGUE: {
                    'name': 'execjob_prologue',
                    'handler': None
                },
                pbs.EXECJOB_EPILOGUE: {
                    'name': 'execjob_epilogue',
                    'handler': self.__execjob_epilogue_handler
                },
                pbs.EXECJOB_PRETERM: {
                    'name': 'execjob_preterm',
                    'handler': None
                },
                pbs.EXECJOB_END: {
                    'name': 'execjob_end',
                    'handler': self.__execjob_end_handler
                },
                pbs.EXECJOB_LAUNCH: {
                    'name': 'execjob_launch',
                    'handler': self.__execjob_launch_handler
                },
                pbs.EXECHOST_PERIODIC: {
                    'name': 'exechost_periodic',
                    'handler': self.__exechost_periodic_handler
                },
                pbs.EXECHOST_STARTUP: {
                    'name': 'exechost_startup',
                    'handler': self.__exechost_startup_handler
                },
                pbs.MOM_EVENTS: {
                    'name': 'mom_events',
                    'handler': None
                },
            }

            if not CRAY_12_BRANCH:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "CRAY 12 Branch: %s" % (CRAY_12_BRANCH))
                self.hook_events[pbs.EXECJOB_ATTACH] = {
                    'name': 'execjob_attach',
                    'handler': self.__execjob_attach_handler
                }

    def __repr__(self):
        return "HookUtils(%s)" % (repr(self.hook_events))

    def event_name(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['name']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Type: %s not found" % (caller_name(), type))
            return None

    def hashandler(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['handler'] is not None
        else:
            return None

    def invoke_handler(self, event, cgroup, jobutil, *args):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: UID: real=%d, effective=%d" %
                   (caller_name(), os.getuid(), os.geteuid()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: GID: real=%d, effective=%d" %
                   (caller_name(), os.getgid(), os.getegid()))
        if self.hashandler(event.type):
            result = self.hook_events[event.type][
                'handler'](event, cgroup, jobutil, *args)
            return result
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: %s event not handled by this hook." %
                       (caller_name(), self.event_name(event.type)))

    def __execjob_begin_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Instantiate the NodeConfig class for get_memory_on_node and
        # get_vmem_on node
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Host assigned job resources: %s" %
                   (caller_name(), jobutil.host_resources))

        # Make sure the parent cgroup directories exist
        cgroup.create_paths()

        # Make sure that any old cgroups created in an earlier attempt
        # at creating or running the job are gone
        cgroup.delete(e.job.id)

        # Gather the assigned resources
        cgroup.gather_assigned_resources()
        # Create the cgroup(s) for the job
        cgroup.create_job(e.job.id, node)
        # Configure the new cgroup
        cgroup.configure_job(e.job.id, jobutil.host_resources, node)
        # Initialize resource usage for the job
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        # Write out the assigned resources
        cgroup.write_out_cgroup_host_assigned_resources(e.job.id)
        # Write out the environment variable for the host (pbs_attach)
        if 'devices_name' in cgroup.host_assigned_resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Devices: %s" %
                       cgroup.host_assigned_resources['devices_name'])
            env_list = list()
            if len(cgroup.host_assigned_resources['devices_name']) > 0:
                line = str()
                mics = list()
                gpus = list()
                for key in cgroup.host_assigned_resources['devices_name']:
                    if key.startswith('mic'):
                        mics.append(key[3:])
                    elif key.startswith('nvidia'):
                        gpus.append(key[6:])
                if len(mics) > 0:
                    env_list.append('OFFLOAD_DEVICES="%s"' %
                                    string.join(mics, ','))
                if len(gpus) > 0:
                    # don't put quotes around the values. ex "0" or "0,1".
                    # This will cause it to fail.
                    env_list.append('CUDA_VISIBLE_DEVICES=%s' %
                                    string.join(gpus, ','))

            pbs.logmsg(pbs.EVENT_DEBUG, "ENV_LIST: %s" % env_list)
            cgroup.write_out_cgroup_host_job_env_file(e.job.id, env_list)
        return True

    def __execjob_epilogue_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # The resources_used information has a base type of pbs_resource
        # Set the usage data
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        return True

    def __execjob_end_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Delete the cgroup(s) for the job
        cgroup.delete(e.job.id)
        # Remove the host_assigned_resources and job_env file
        for filename in [cgroup.hook_storage_dir+os.sep+e.job.id,
                         cgroup.host_job_env_filename % e.job.id]:
            try:
                os.remove(filename)
            except OSError:
                pbs.logmsg(pbs.EVENT_DEBUG3, "File: %s not found" % (filename))
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Error removing file: %s" % (filename))
                pass

        return True

    def __execjob_launch_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Add the parent process id to the appropriate cgroups.
        cgroup.add_pids(os.getppid(), jobutil.job.id)
        # FUTURE: Add environment variable to the job environment
        # if job requested mic or gpu
        cgroup.read_in_cgroup_host_assigned_resources(e.job.id)
        if cgroup.host_assigned_resources is not None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "host_assigned_resources: %s" %
                       (cgroup.host_assigned_resources))
            cgroup.setup_job_devices_env()
        return True

    def __exechost_periodic_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Cleanup cgroups for jobs not present on this node
        count = cgroup.cleanup_orphans(e.job_list)
        if ('periodic_resc_update' in cgroup.cfg and
                cgroup.cfg['periodic_resc_update']):
            for job in e.job_list.keys():
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: job is %s" %
                           (caller_name(), job))
                cgroup.update_job_usage(job, e.job_list[job].resources_used)
        # Online nodes that were offlined due to a cgroup not cleaning up
        if count == 0 and cgroup.cfg['online_offlined_nodes']:
            vnode = pbs.event().vnode_list[cgroup.hostname]
            if os.path.isfile(cgroup.offline_file):
                line = 'Orphan cgroup(s) have been cleaned up. '

                # Check with the pbs server to see if the node comment matches
                try:
                    tmp_comment = pbs.server().vnode(cgroup.hostname).comment
                except:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Unable to contact server for node comment")
                    tmp_comment = None
                if tmp_comment == cgroup.offline_msg:
                    line += 'Will bring the node back online'
                    vnode.state = pbs.ND_FREE
                    vnode.comment = None
                else:
                    line += 'However, the comment has changed since the node '
                    line += 'was offlined. Node will remain offline'

                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: %s" % (caller_name(), line))
                # Remove file
                os.remove(cgroup.offline_file)
        return True

    def __exechost_startup_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        cgroup.create_paths()
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))

        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        node.create_vnodes()
        if 'memory' in cgroup.subsystems:
            val = node.get_memory_on_node(cgroup.cfg)
            if val is not None:
                if 'vnode_per_numa_node' in cgroup.cfg:
                    if not cgroup.cfg['vnode_per_numa_node']:
                        hn = node.hostname
                        e.vnode_list[hn].resources_available['mem'] = \
                            pbs.size(val)
                        val_cpus = node.totalcpus
                        if val_cpus is not None:
                            e.vnode_list[hn].resources_available['ncpus'] = \
                                int(val_cpus)
                cgroup.set_node_limit('mem', val)
        if 'memsw' in cgroup.subsystems:
            val = node.get_vmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['vmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('vmem', val)
        if 'hugetlb' in cgroup.subsystems:
            val = node.get_hpmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['hpmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('hpmem', val)
        return True

    def __execjob_attach_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logjobmsg(jobutil.job.id, "%s: Attaching PID %s" %
                      (caller_name(), e.pid))
        # Add the job process id to the appropriate cgroups.
        cgroup.add_pids(e.pid, jobutil.job.id)
        return True

#
# CLASS ShallIRunUtils
#


class ShallIRunUtils:

    def __init__(self, hostname, cfg):
        self.cfg = cfg
        self.hostname = hostname

    def allow_users(self, allow_users):
        """ This still needs to be defined """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pass

    def exclude_hosts(self, exclude_hosts):
        """
        Gets the hostname for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        hostname = pbs.get_local_nodename()
        # check to see if hostname is in the exclude_hosts list
        if self.hostname in exclude_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: exclude host %s is in %s" %
                       (caller_name(), self.hostname, exclude_hosts))
            pbs.event().accept()

        # check to see if it regex syntax is used
        pass

    def exclude_vntypes(self, exclude_vntypes):
        """
        Gets the vntype for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        vntype = None

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'
        if os.path.isfile(vntype_file):
            fdata = open(vntype_file).readlines()
            vntype = fdata[0].strip()
            pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
        if vntype is None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype is set to %s" % (vntype))
        elif vntype in exclude_vntypes:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s is in %s" % (vntype, exclude_vntypes))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Exiting Hook")
            pbs.event().accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s not in global exclude: %s" %
                       (vntype, exclude_vntypes))
        pass

    def run_only_on_hosts(self, approved_hosts):
        """
        Gets the list of approved nodes to run on and checks to see if the hook
        should run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Approved hosts: %s, %s" %
                   (type(approved_hosts), approved_hosts))

        # check to see if hostname is in the approved_hosts list
        if approved_hosts == []:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "approved hosts list is empty: %s" % approved_hosts)
        elif self.hostname not in approved_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s is not in the approved list of hosts: %s" %
                       (self.hostname, approved_hosts))
            pbs.event().accept()

        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s in list of approved hosts: %s" %
                       (self.hostname, approved_hosts))

#
# CLASS JobUtils
#


class JobUtils:

    def __init__(self, job, hostname=None, resources=None):
        self.job = job
        self.host_vnodes = list()
        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if resources is not None:
            self.host_resources = resources
        else:
            self.host_resources = self.__host_assigned_resources()

    def __repr__(self):
        return "JobUtils(%s, %s, %s)" % (repr(self.job), repr(self.hostname),
                                         repr(self.host_resources))

    # Return a dictionary of resources assigned to the local node
    def __host_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Bail out if no hostname was provided
        if self.hostname is None:
            raise CgroupProcessingError('No hostname available')
        # Bail out if no job information is present
        if self.job is None:
            raise CgroupProcessingError('No job information available')
        # Determine which vnodes are on this host, if any
        if self.hostname is not None:
            vnhost_pattern = "%s\[[\d+]\]" % self.hostname
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnhost pattern: %s" %
                       (caller_name(), vnhost_pattern))
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job exec_vnode list: %s" %
                       (caller_name(), self.job.exec_vnode))
            for match in re.findall(vnhost_pattern, str(self.job.exec_vnode)):
                self.host_vnodes.append(match)
            if self.host_vnodes is not None:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Associate %s to vnodes on %s" %
                           (caller_name(), self.host_vnodes, self.hostname))

        # Collect host assigned resources
        resources = {}
        for chunk in self.job.exec_vnode.chunks:
            vnode = False
            if self.host_vnodes == [] and chunk.vnode_name != self.hostname:
                continue
            elif self.host_vnodes != [] and \
                    chunk.vnode_name not in self.host_vnodes:
                continue
            elif chunk.vnode_name in self.host_vnodes:
                vnode = True
                if 'vnodes' not in resources:
                    resources['vnodes'] = {}
                if chunk.vnode_name not in resources['vnodes']:
                    resources['vnodes'][chunk.vnode_name] = {}
                # This check is needed since not all resources are
                # required to be in each chunk of a job
                # i.e. exec_vnodes =
                # (node1[0]:ncpus=4:mem=4gb+node1[1]:mem=2gb) +
                # (node1[1]:ncpus=3+node[0]:ncpus=1:mem=4gb)
                if all(resc in resources['vnodes'][chunk.vnode_name]
                       for resc in chunk.chunk_resources.keys()):
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: resources already defined" %
                               (caller_name()))
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s,\t%s" %
                               (caller_name(),
                                chunk.chunk_resources.keys(),
                                resources['vnodes'][chunk.vnode_name].keys()))
                else:
                    # Initialize resources for each vnode
                    for resc in chunk.chunk_resources.keys():
                        # Initialize the new value
                        if isinstance(chunk.chunk_resources[resc],
                                      pbs.pbs_int):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_int(0)
                        elif isinstance(chunk.chunk_resources[resc],
                                        pbs.pbs_float):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_float(0)
                        elif isinstance(chunk.chunk_resources[resc], pbs.size):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.size('0')

            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Chunk %s" %
                       (caller_name(), chunk.vnode_name))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resources: %s" %
                       (caller_name(), resources))
            for resc in chunk.chunk_resources.keys():
                if resc not in resources.keys():
                    # Initialize the new value
                    if isinstance(chunk.chunk_resources[resc], pbs.pbs_int):
                        resources[resc] = pbs.pbs_int(0)
                    elif isinstance(chunk.chunk_resources[resc],
                                    pbs.pbs_float):
                        resources[resc] = pbs.pbs_float(0.0)
                    elif isinstance(chunk.chunk_resources[resc], pbs.size):
                        resources[resc] = pbs.size('0')
                # Add resource value to total
                if type(chunk.chunk_resources[resc]) in \
                        [pbs.pbs_int, pbs.pbs_float, pbs.size]:
                    resources[resc] += chunk.chunk_resources[resc]
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s: resources[%s][%s] is now %s" %
                               (caller_name(), self.hostname, resc,
                                resources[resc]))
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] += \
                                  chunk.chunk_resources[resc]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Setting resource %s to string %s" %
                               (caller_name(), resc,
                                str(chunk.chunk_resources[resc])))
                    resources[resc] = str(chunk.chunk_resources[resc])
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] = \
                                  str(chunk.chunk_resources[resc])
        if not resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: No resources assigned to host %s" %
                       (caller_name(), self.hostname))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources for %s: %s" %
                       (caller_name(), self.hostname, repr(resources)))

        # Return assigned resources for specified host
        pbs.logmsg(pbs.EVENT_DEBUG3, "Job Resources: %s" % (resources))
        return resources

    # Write a message to the job stdout file
    def write_to_stderr(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stderr_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

    # Write a message to the job stdout file
    def write_to_stdout(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stdout_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

#
# CLASS NodeConfig
#


class NodeConfig:

    def __init__(self, cfg, **kwargs):

        self.cfg = cfg
        hostname = None
        meminfo = None
        numa_nodes = None
        devices = None
        hyperthreading = None
        self.hyperthreads_per_core = 1
        self.totalcpus = self.__discover_cpus()

        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        self.host_mom_jobdir = pbs_home+'/mom_priv/jobs'

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'hostname':
                    hostname = val
                elif arg == 'meminfo':
                    meminfo = val
                elif arg == 'numa_nodes':
                    numa_nodes = val
                elif arg == 'devices':
                    devices = val
                elif arg == 'hyperthreading':
                    hyperthreading = val

        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if meminfo is not None:
            self.meminfo = meminfo
        else:
            self.meminfo = self.__discover_meminfo()
        if numa_nodes is not None:
            self.numa_nodes = numa_nodes
        else:
            self.numa_nodes = self.__discover_numa_nodes()
        if devices is not None:
            self.devices = devices
        else:
            self.devices = self.__discover_devices()
        if hyperthreading is not None:
            self.hyperthreading = hyperthreading
        else:
            self.hyperthreading = self.__hyperthreading_enabled()

        # Add the devices count i.e. nmics and ngpus to the numa nodes
        self.__add_devices_to_numa_node()

    def __repr__(self):
        return ("NodeConfig(%s, %s, %s, %s, %s, %s)" %
                (repr(self.cfg), repr(self.hostname), repr(self.meminfo),
                 repr(self.numa_nodes), repr(self.devices),
                 repr(self.hyperthreading)))

    def __add_devices_to_numa_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (self.devices.keys()))
        for device in self.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" % (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" %
                           (self.devices[device].keys()))
                for device_name in self.devices[device]:
                    device_socket = \
                        self.devices[device][device_name]['numa_node']
                    if device == 'mic':
                        if 'nmics' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['nmics'] = 1
                        else:
                            self.numa_nodes[device_socket]['nmics'] += 1
                    elif device == 'gpu':
                        if 'ngpus' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['ngpus'] = 1
                        else:
                            self.numa_nodes[device_socket]['ngpus'] += 1

        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (self.numa_nodes))

    # Discover what type of hardware is on this node and how is it partitioned
    def __discover_numa_nodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        numa_nodes = {}
        for dir in glob.glob(os.path.join(os.sep,
                             "sys", "devices", "system", "node", "node*")):
            id = int(dir.split(os.sep)[5][4:])
            if id not in numa_nodes.keys():
                numa_nodes[id] = {}
                numa_nodes[id]['devices'] = list()
            numa_nodes[id]['cpus'] = open(os.path.join(dir, "cpulist"),
                                          'r').readline().strip()
            with open(os.path.join(dir, "meminfo"), 'r') as fd:
                for line in fd:
                    # Each line will contain four or five fields. Examples:
                    # Node 0 MemTotal:       32995028 kB
                    # Node 0 HugePages_Total:     0
                    entries = line.split()
                    if len(entries) < 4:
                        continue
                    if entries[2] == "MemTotal:":
                        numa_nodes[id][entries[2].rstrip(':')] = \
                            convert_size(entries[3] + entries[4], 'kb')
                    elif entries[2] == "HugePages_Total:":
                        numa_nodes[id][entries[2].rstrip(':')] = entries[3]
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover Numa Nodes: %s" % numa_nodes)
        return numa_nodes

    # Identify devices and to which numa nodes they are attached
    def __discover_devices(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        devices = {}
        for file in glob.glob(os.path.join(os.sep, "sys", "class", "*", "*",
                                           "device", "numa_node")):
            # The file should contain a single integer
            numa_node = int(open(file, 'r').readline().strip())
            if numa_node < 0:
                numa_node = 0
            dirs = file.split(os.sep)
            name = dirs[3]
            instance = dirs[4]
            if name not in devices.keys():
                devices[name] = {}
            devices[name][instance] = {}
            devices[name][instance]['numa_node'] = numa_node
            if name == 'mic':
                s = os.stat('/dev/%s' % instance)
                devices[name][instance]['major'] = os.major(s.st_rdev)
                devices[name][instance]['minor'] = os.minor(s.st_rdev)
                devices[name][instance]['device_type'] = 'c'

        # Check to see if there are gpus on the node
        gpus = self.discover_gpus()
        if isinstance(gpus, dict) and len(gpus.keys()) > 0:
            devices['gpu'] = gpus['gpu']

        pbs.logmsg(pbs.EVENT_DEBUG3, "Discovered devices: %s" % (devices))
        return devices

    def discover_gpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Check to see if nvidia-smi exists and produces valid output
            cmd = ['/usr/bin/nvidia-smi', '-q', '-x']
            if 'nvidia-smi' in self.cfg:
                cmd[0] = self.cfg['nvidia-smi']

            pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
            # Collect the gpu information
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.PIPE)
            output, err = process.communicate()

            # pbs.logmsg(pbs.EVENT_DEBUG3,"output: %s" % output)
            # import the xml library and parse the output
            import xml.etree.ElementTree as ET

            # Parse the data
            name = 'gpu'
            gpu_data = dict()
            gpu_data[name] = {}
            root = ET.fromstring(output)
            pbs.logmsg(pbs.EVENT_DEBUG3, "root.tag: %s" % root.tag)
            for child in root:
                if child.tag == "attached_gpus":
                    # gpu_data['ngpus'] = int(child.text)
                    pass
                if child.tag == "gpu":
                    device_id = child.get('id')
                    number = 'nvidia%s' % child.find('minor_number').text
                    gpu_data[name][number] = dict()
                    gpu_data[name][number]['id'] = device_id

                    # Determine which socket the device is on
                    filename = '/sys/bus/pci/devices/%s/numa_node' % \
                        device_id.lower()
                    isfile = os.path.isfile(filename)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s, %s" % (filename, isfile))
                    if isfile:
                        numa_node = open(filename).read().strip()
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "numa_node: %s" % (numa_node))
                        if int(numa_node) == -1:
                            numa_node = 0
                        gpu_data[name][number]['numa_node'] = int(numa_node)
                        try:
                            dev_filename = '/dev/%s' % number
                            s = os.stat(dev_filename)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find %s" % dev_filename)
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                        gpu_data[name][number]['major'] = os.major(s.st_rdev)
                        gpu_data[name][number]['minor'] = os.minor(s.st_rdev)
                        gpu_data[name][number]['device_type'] = 'c'
            pbs.logmsg(pbs.EVENT_DEBUG, "%s" % gpu_data)
            return gpu_data
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unable to find %s" % string.join(cmd, " "))
            return {'gpu': {}}
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        return {'gpu': {}}

    # Get the memory info on this host
    def __discover_meminfo(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        meminfo = {}
        with open(os.path.join(os.sep, "proc", "meminfo"), 'r') as fd:
            for line in fd:
                entries = line.split()
                if entries[0] == "MemTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "SwapTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "Hugepagesize:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "HugePages_Total:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
                elif entries[0] == "HugePages_Rsvd:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover meminfo: %s" % meminfo)
        return meminfo

    # Determine if the cpus have hyperthreading enabled
    def __hyperthreading_enabled(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            siblings = 0
            cpu_cores = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "siblings":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    siblings = entries[1].strip()
                elif entries[0].strip() == "cpu cores":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_cores = entries[1].strip()
                elif entries[0].strip() == "flags":
                    flag_options = entries[1].split()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: flag_options: %s" %
                               (caller_name(), flag_options))
                    if "ht" in flag_options:
                        rc = True
                        break
        if rc is True:
            self.hyperthreads_per_core = int(siblings)/int(cpu_cores)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: hyperthreads/core: %d" %
                       (caller_name(), self.hyperthreads_per_core))
            if self.hyperthreads_per_core == 1:
                rc = False
        return rc

    def __discover_cpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            cpu_threads = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "processor":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_threads += 1
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: processors found: %d" %
                   (caller_name(), cpu_threads))

        return cpu_threads

    # Gather the jobs running on this node
    def gather_jobs_on_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        # Get the job list from the server
        jobs = None
        try:
            jobs = pbs.server().vnode(self.hostname).jobs
        except:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Unable to contact server to get jobs")
            return None

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs from server for %s: %s" % (self.hostname, jobs))

        if jobs is None:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "No jobs found on %s " % (self.hostname))
            return list()

        jobs_list = dict()
        for job in jobs.split(','):
            # The job list from the node has a space in front of the job id
            # This must be removed or it will not match the cgroup dir
            jobs_list[job.split('/')[0].strip()] = 0
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs on %s: %s" % (self.hostname, jobs_list.keys()))

        return jobs_list.keys()

    # Gather the jobs running on this node
    def gather_jobs_on_node_local(self):
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Method called" % (caller_name()))
        # Get the job list from the jobs directory
        jobs_list = list()
        try:
            for file in os.listdir(self.host_mom_jobdir):
                if fnmatch.fnmatch(file, "*.JB"):
                    jobs_list.append(file[:-3])
            if not jobs_list:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "No jobs found on %s " % (self.hostname))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Jobs from server for %s: %s" %
                           (self.hostname, repr(jobs_list)))

            return jobs_list
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Could not get job list from mom_priv/jobs for %s" %
                       self.hostname)

            return self.gather_jobs_on_node()

    # Get the memory resource on this mom
    def get_memory_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Calculate memory
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
            pbs.logmsg(pbs.EVENT_DEBUG3, "val: %s" % val)
        else:
            val = size_as_int(MemTotal)
        if 'percent_reserve' in config['cgroup']['memory']:
            percent_reserve = config['cgroup']['memory']['percent_reserve']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memory'].keys():
            reserve_memory = config['cgroup']['memory']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                reserve_mem = size_as_int(reserve_memory)
                if val > reserve_mem:
                    val -= reserve_mem
                else:
                    val -= size_as_int("100mb")
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Unable to reserve more memory than " +
                               "available on the node. Available %s, " +
                               "Tried to reserve %s. Reserving 100mb" %
                               (val, reserve_mem))

            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))

        pbs.logmsg(pbs.EVENT_DEBUG3, "Return val: %s" % val)
        return convert_size(str(val), 'kb')

    # Get the virtual memory resource on this mom
    def get_vmem_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Return the value for memory if no swap is configured
        if size_as_int(self.meminfo['SwapTotal']) <= 0:
            return self.get_memory_on_node(config)
        # Calculate vmem
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
        else:
            val = size_as_int(MemTotal)

        val += size_as_int(self.meminfo['SwapTotal'])
        # Determine if memory needs to be reserved
        if 'percent_reserve' in config['cgroup']['memsw']:
            percent_reserve = config['cgroup']['memsw']['reserve_memory']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memsw']:
            reserve_memory = config['cgroup']['memsw']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                val -= size_as_int(reserve_memory)
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))
        return convert_size(str(val), 'kb')

    # Get the huge page memory resource on this mom
    def get_hpmem_on_node(self, config):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        if 'percent_reserve' in config['cgroup']['hugetlb'].keys():
            percent_reserve = config['cgroup']['hugetlb']['percent_reserve']
        else:
            percent_reserve = 0
        # Calculate vmem
        val = size_as_int(self.meminfo['Hugepagesize'])
        val *= (self.meminfo['HugePages_Total'] -
                self.meminfo['HugePages_Rsvd'])
        val -= (val * percent_reserve) / 100
        return convert_size(str(val), 'kb')

    # Create individual vnodes per socket
    def create_vnodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        key = "vnode_per_numa_node"
        if key in self.cfg.keys():
            if not self.cfg[key]:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s is false" %
                           (caller_name(), key))
                return
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s not configured" %
                       (caller_name(), key))
            return

        # Create one vnode per numa node
        vnode_list = pbs.event().vnode_list
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: numa nodes: %s" %
                   (caller_name(), self.numa_nodes))
        # Define the vnodes
        vnode_name = self.hostname
        vnode_list[vnode_name] = pbs.vnode(vnode_name)
        for id in self.numa_nodes.keys():
            vnode_name = self.hostname + "[%d]" % id
            vnode_list[vnode_name] = pbs.vnode(vnode_name)
            for key, val in sorted(self.numa_nodes[id].iteritems()):
                if key == 'cpus':
                    vnode_list[vnode_name].resources_available['ncpus'] = \
                        len(cpus2list(val))/self.hyperthreads_per_core
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['ncpus'] = 0
                elif key == 'MemTotal':
                    mem = self.get_memory_on_node(self.cfg, val)
                    vnode_list[vnode_name].resources_available['mem'] = \
                        pbs.size(mem)
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['mem'] = \
                        pbs.size('0kb')
                elif key == 'HugePages_Total':
                    pass
                elif isinstance(val, list):
                    pass
                elif isinstance(val, dict):
                    pass
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s=%s" %
                               (caller_name(), key, val))
                    vnode_list[vnode_name].resources_available[key] = val

                    if isinstance(val, int):
                        vnode_list[self.hostname].resources_available[key] = 0
                    elif isinstance(val, float):
                        vnode_list[self.hostname].resources_available[key] = \
                            0.0
                    elif isinstance(val, pbs.size):
                        vnode_list[self.hostname].resources_available[key] = \
                            pbs.size('0kb')
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnode list: %s" %
                   (caller_name(), vnode_list))
        return True

#
# CLASS CgroupUtils
#


class CgroupUtils:
    def __init__(self, hostname, vnode, **kwargs):
        cfg = None
        subsystems = None
        paths = None
        vntype = None
        event = None
        self.assigned_resources = dict()
        self.existing_cgroups = dict()
        self.host_assigned_resources = None
        self.pid_lower_limit = 3

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'subsystems':
                    subsystems = val
                elif arg == 'paths':
                    paths = val
                elif arg == 'vntype':
                    vntype = val
                elif arg == 'event':
                    event = val

        try:
            self.hostname = hostname
            self.vnode = vnode
            # __check_os will raise an exception if cgroups are not present
            self.__check_os()
            # Read in the config file
            if cfg is not None:
                self.cfg = cfg
            else:
                self.cfg = self.__parse_config_file()

            # Determine if the hook should run or exit
            siru = ShallIRunUtils(self.hostname, self.cfg)
            siru.exclude_hosts(self.cfg['exclude_hosts'])
            siru.exclude_vntypes(self.cfg['exclude_vntypes'])
            siru.run_only_on_hosts(self.cfg['run_only_on_hosts'])

            # Collect the mount points
            if paths is not None:
                self.paths = paths
            else:
                self.paths = self.__set_paths()
            # Define the local vnode type
            if vntype is not None:
                self.vntype = vntype
            else:
                self.vntype = self.__get_vnode_type()
            # Determine which subsystems we care about
            if subsystems is not None:
                self.subsystems = subsystems
            else:
                self.subsystems = self.__target_subsystems()

            # location to store information for the different hook events
            pbs_home = ''
            if not CRAY_12_BRANCH:
                pbs_conf = pbs.get_pbs_conf()
                pbs_home = pbs_conf['PBS_HOME']
            else:
                if 'PBS_HOME' in self.cfg:
                    pbs_home = self.cfg['PBS_HOME']
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "PBS_HOME needs to be defined in the " +
                               "config file")
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Exiting the cgroups hook")
                    pbs.accept()

            self.hook_storage_dir = pbs_home+'/mom_priv/hooks/hook_data'
            self.host_job_env_dir = pbs_home+'/aux'
            self.host_job_env_filename = self.host_job_env_dir+os.sep+"%s.env"

            # information for offlining nodes
            self.offline_file = \
                pbs_home+os.sep+'mom_priv'+os.sep+'hooks'+os.sep
            self.offline_file += "%s.offline" % pbs.event().hook_name
            self.offline_msg = "Hook %s: " % pbs.event().hook_name
            self.offline_msg += "Unable to clean up one or more cgroups"

        except:
            raise

    def __repr__(self):
        return ("CgroupUtils(%s, %s, %s, %s, %s, %s)" %
                (repr(self.hostname), repr(self.vnode), repr(self.cfg),
                 repr(self.subsystems), repr(self.paths), repr(self.vntype)))

    # Validate the OS type and version
    def __check_os(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check to see if the platform is linux and the kernel is new enough
        if platform.system() != 'Linux':
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: OS does not support cgroups" %
                       (caller_name()))
            raise CgroupConfigError("OS type not supported")
        rel = map(int,
                  string.split(string.split(platform.release(), '-')[0], '.'))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Detected Linux kernel version %d.%d.%d" %
                   (caller_name(), rel[0], rel[1], rel[2]))
        supported = False
        if rel[0] > 2:
            supported = True
        elif rel[0] == 2:
            if rel[1] > 6:
                supported = True
            elif rel[1] == 6:
                if rel[2] >= 28:
                    supported = True
        if not supported:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Kernel needs to be >= 2.6.28: %s" %
                       (caller_name(), system_info[2]))
            raise CgroupConfigError("OS version not supported")
        return supported

    # Read the config file in json format
    def __parse_config_file(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        config = {}
        # Identify the config file and read in the data
        if 'PBS_HOOK_CONFIG_FILE' in os.environ:
            config_file = os.environ["PBS_HOOK_CONFIG_FILE"]
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Config file is %s" %
                       (caller_name(), config_file))
            try:
                config = json.load(open(config_file, 'r'),
                                   object_hook=decode_dict)
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
        else:
            raise CgroupConfigError("No configuration file present")
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Initial Cgroup Config: %s" %
                   (caller_name(), config))
        # Set some defaults if they are not present
        if 'cgroup_prefix' not in config.keys():
            config['cgroup_prefix'] = 'pbspro'
        if 'periodic_resc_update' not in config.keys():
            config['periodic_resc_update'] = False
        if 'vnode_per_numa_node' not in config.keys():
            config['vnode_per_numa_node'] = False
        if 'online_offlined_nodes' not in config.keys():
            config['online_offlined_nodes'] = False
        if 'exclude_hosts' not in config.keys():
            config['exclude_hosts'] = []
        if 'run_only_on_hosts' not in config.keys():
            config['run_only_on_hosts'] = []
        if 'exclude_vntypes' not in config.keys():
            config['exclude_vntypes'] = []
        if 'cgroup' not in config.keys():
            config['cgroup'] = {}
        else:
            for subsys in config['cgroup']:
                subsystem = config['cgroup'][subsys]
                if 'enabled' not in subsystem.keys():
                    config['cgroup'][subsys]['enabled'] = False
                if 'exclude_hosts' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_hosts'] = []
                if 'exclude_vntypes' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_vntypes'] = []

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Cgroup Config with defaults: %s" %
                   (caller_name(), config))
        return config

    # Determine which subsystems are being requested
    def __target_subsystems(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        subsystems = []
        for key in self.cfg['cgroup'].keys():
            if self.enabled(key):
                subsystems.append(key)
        if 'memory' not in subsystems:
            if 'memsw' in subsystems:
                # Remove memsw since it must be greater then
                # or equal to memory
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing memsw from enabled subsystems")
                subsystems.remove('memsw')
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Enabled subsystems: %s" %
                   (caller_name(), subsystems))
        # It is not an error for all subsystems to be disabled.
        # This host or vnode type may be in the excluded list.
        return subsystems

    # Copy a setting from the parent cgroup
    def __copy_from_parent(self, dest):
        try:
            file = os.path.basename(dest)
            dir = os.path.dirname(dest)
            parent = os.path.dirname(dir)
            source = os.path.join(parent, file)
            if not os.path.isfile(source):
                raise CgroupConfigError('Failed to read %s' % (source))
            self.write_value(dest, open(source, 'r').read().strip())
        except:
            raise

    # Create a dictionary of the cgroup subsystems and their corresponding
    # directories
    def __set_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        paths = {}
        try:
            # Loop through the mounts and collect the ones for cgroups
            with open(os.path.join(os.sep, "proc", "mounts"), 'r') as fd:
                for line in fd:
                    entries = line.split()
                    if len(entries) > 3 and entries[2] == "cgroup":
                        flags = entries[3].split(',')
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "subsysdir: %s (flags=%s)" %
                                   (entries[1], flags))
                        for subsys in flags:
                            paths[subsys] = os.path.join(
                                entries[1], self.cfg["cgroup_prefix"])
        except:
            raise
        if len(paths.keys()) < 1:
            raise CgroupConfigError("Cgroup paths not detected")
        # Both the memory and memsw cgroups share the same mount point
        if "memory" in paths.keys():
            paths["memsw"] = paths["memory"]
        return paths

    # Create the cgroup parent directories that will contain the jobs
    def create_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Create the directories that PBS will use to house the jobs
            old_umask = os.umask(0022)
            for subsys in self.subsystems:
                if subsys not in self.paths.keys():
                    raise CgroupConfigError('No path for subsystem: %s' %
                                            (subsys))
                if not os.path.exists(self.paths[subsys]):
                    os.makedirs(self.paths[subsys], 0755)
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Created directory %s" %
                               (caller_name(), self.paths[subsys]))
                    if subsys == 'memory' or subsys == 'memsw':
                        # Set file to
                        # <cgroup>/memory/pbspro/memory.use_hierarchy
                        file = os.path.join(self.paths[subsys],
                                            'memory.use_hierarchy')
                        if not os.path.isfile(file):
                            raise CgroupConfigError('Failed to configure %s' %
                                                    (file))
                        self.write_value(file, 1)
                    elif subsys == 'cpuset':
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.cpus'))
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.mems'))

        except:
            raise CgroupConfigError("Failed to create directory: %s" %
                                    (self.paths[subsys]))
        finally:
            os.umask(old_umask)

    # Return the vnode type of the local node
    def __get_vnode_type(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")

        # File to check for if it is not defined in the hook input file
        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'

        # self.vnode is None for pbs_attach events
        pbs.logmsg(pbs.EVENT_DEBUG3, "vnode: %s" % self.vnode)
        if self.vnode is not None:
            if 'vntype' in self.vnode.resources_available and \
                    self.vnode.resources_available['vntype'] is not None:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "vntype: %s" %
                           self.vnode.resources_available['vntype'])
                return self.vnode.resources_available['vntype']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3, "vntype file: %s" % vntype_file)
                if os.path.isfile(vntype_file):
                    fdata = open(vntype_file).readlines()
                    vntype = fdata[0].strip()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
                    return vntype
        # If vntype was not set then log a message. It is too expensive
        # to have all moms query the server for large jobs.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: resources_available.vntype is " % caller_name() +
                   "not set for this vnode")
        return None

    # Gather resources that are already assigned
    def gather_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        assigned_resources = {}

        # Gather the cpus and memory sockets assigned
        directories = [x[0] for x in os.walk(self.paths['cpuset'])]

        # Add the existing cgroups to a list to check if assigning
        # resources fail
        for dir in directories[1:]:
            if dir.find(pbs.event().job.id) == -1:
                self.existing_cgroups[dir] = []

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'cpuset' in self.subsystems and \
                self.cfg['cgroup']['cpuset']['enabled']:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather cpuset resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['cpuset'], tmp_dir)
                    with open(path+os.sep+'cpuset.cpus') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.cpus'] = \
                            cpus2list(tmp_val.strip())
                    with open(path+os.sep+'cpuset.mems') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.mems'] = \
                            cpus2list(tmp_val.strip())

        # Check to see if the subsystem is enabled before trying
        # to gather resources
        if 'memory' in self.subsystems and \
                self.cfg['cgroup']['memory']['enabled']:
            # Gather the memory assigned for each socket
            directories = [x[0] for x in os.walk(self.paths['memory'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather memory resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['memory'], tmp_dir)
                    with open(path+os.sep+'memory.limit_in_bytes') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memory.limit'] = \
                            tmp_val.strip()
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    if os.path.isfile(tmp_filename) is False:
                        continue
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    with open(tmp_filename) as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memsw.limit'] = \
                            tmp_val.strip()

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'devices' in self.subsystems and \
                self.cfg['cgroup']['devices']['enabled']:
            # Gather the devices assigned
            directories = [x[0] for x in os.walk(self.paths['devices'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather device resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    pbs.logmsg(pbs.EVENT_DEBUG3, "Dir: %s" % tmp_dir)
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['devices'], tmp_dir)
                    with open(path+os.sep+'devices.list') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['devices.list'] = \
                            tmp_val.strip().split('\n')
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Dir: %s\n\tValue: %s" %
                                   (path, tmp_val.replace('\n', ',')))

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Gathered assigned resources: %s" % assigned_resources)
        self.assigned_resources = assigned_resources

    # Return whether a subsystem is enabled
    def enabled(self, subsystem):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check whether the subsystem is enabled in the configuration file
        if subsystem not in self.cfg['cgroup'].keys():
            return False
        if 'enabled' not in self.cfg['cgroup'][subsystem].keys():
            return False
        if not self.cfg['cgroup'][subsystem]['enabled']:
            return False
        # Check whether the cgroup is mounted for this subsystem
        if subsystem not in self.paths.keys():
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup not mounted for %s" %
                       (caller_name(), subsystem))
            return False
        # Check whether this host is excluded
        # if self.hostname in self.cfg['exclude_hosts']:
        #    pbs.logmsg(pbs.EVENT_DEBUG, "%s: cgroup excluded on host %s" %
        #               (caller_name(), self.hostname))
        #    pbs.event().accept()
        if self.hostname in self.cfg['cgroup'][subsystem]['exclude_hosts']:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup excluded for subsystem %s on host %s" %
                       (caller_name(), subsystem, self.hostname))
            return False
        # Check whether the vnode type is excluded
        if self.vntype is not None:
            # if self.vntype in self.cfg['exclude_vntypes']:
            #    pbs.logmsg(pbs.EVENT_DEBUG,
            #               "%s: cgroup excluded on vnode type %s" %
            #               (caller_name(), self.vntype))
            #    pbs.event().accept()
            if self.vntype in self.cfg['cgroup'][subsystem]['exclude_vntypes']:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           ("%s: cgroup excluded for " +
                            "subsystem %s on vnode type %s") %
                           (caller_name(), subsystem, self.vntype))
                return False
        return True

    # Return the default value for a subsystem
    def default(self, subsystem):
        if subsystem in self.cfg['cgroup']:
            if 'default' in self.cfg['cgroup'][subsystem]:
                return self.cfg['cgroup'][subsystem]['default']
        return None

    # Check to see if the pid matches the job owners uid
    def is_pid_owned_by_job_owner(self, pid):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        try:
            proc_uid = os.stat('/proc/%d' % pid).st_uid
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       '/proc/%d uid:%d' % (pid, proc_uid))

            job_owner_uid = pwd.getpwnam(pbs.event().job.euser)[2]
            pbs.logmsg(pbs.EVENT_DEBUG3, "Job uid: %d" % job_owner_uid)

            if proc_uid == job_owner_uid:
                return True
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Proc uid: %d != Job owner:  %d" %
                           (proc_uid, job_owner_uid))
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG, "Unknown pid: %d" % pid)
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        # If we got to this point something did not match up
        return False

    # Add some number of PIDs to the cgroup tasks files for each subsystem
    def add_pids(self, pids, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # make pids a list
        if isinstance(pids, int):
            pids = [pids]

        if pbs.event().type == pbs.EXECJOB_LAUNCH:
            if 1 in pids:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "1 is not a valid pid to add")
                # Hold the job for further review
                e.reject("Hook tried to add pid 1 to the tasks file " +
                         "for job %s" % jobid)

        # check pids to make sure that they are owned by the job owner
        if not CRAY_12_BRANCH:
            pbs.logmsg(pbs.EVENT_DEBUG3, "Not in the Cray 12 Branch")
            pbs.logmsg(pbs.EVENT_DEBUG3, "check to see if execjob_attach")
            if pbs.event().type == pbs.EXECJOB_ATTACH:
                pbs.logmsg(pbs.EVENT_DEBUG3, "event type: attach or launch")
                tmp_pids = list()
                for p in pids:
                    if self.is_pid_owned_by_job_owner(p):
                        tmp_pids.append(p)
                if len(tmp_pids) == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%d is not a valid pids to add" % p)
                    return False
                else:
                    pids = tmp_pids

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: subsys = %s" %
                       (caller_name(), subsys))
            # memsw and memory use the same tasks file
            if subsys == "memsw":
                if "memory" in self.subsystems:
                    continue
            path = os.path.join(self.paths[subsys], jobid)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path = %s" %
                       (caller_name(), path))
            try:
                tasks_path = os.path.join(path, "tasks")
                for p in pids:
                    self.write_value(tasks_path, p, 'a')
            except IOError, exc:
                raise CgroupLimitError("Failed to add PIDs %s to %s (%s)" %
                                       (str(pids), tasks_path,
                                        errno.errorcode[exc.errno]))
            except:
                raise

    # Set a node limit
    def set_node_limit(self, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s = %s" %
                   (caller_name(), resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'],
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Node resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    def setup_job_devices_env(self):
        """ Setup the job environment for the devices assigned to the job for an
            execjob_launch hook
         """
        if 'devices_name' in self.host_assigned_resources:
            names = self.host_assigned_resources['devices_name']
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "devices: %s" % (names))
            offload_devices = list()
            cuda_visible_devices = list()
            for name in names:
                if name.startswith('mic'):
                    offload_devices.append(name[3:])
                elif name.startswith('nvidia'):
                    cuda_visible_devices.append(name[6:])
            if len(offload_devices) > 0:
                value = '"%s"' % string.join(offload_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['OFFLOAD_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "offload_devices: %s" % offload_devices)
            if len(cuda_visible_devices) > 0:
                value = '"%s"' % string.join(cuda_visible_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['CUDA_VISIBLE_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "cuda_visible_devices: %s" % cuda_visible_devices)
            return [offload_devices, cuda_visible_devices]
        else:
            return False

    def setup_subsys_devices(self, path, node):
        subsys = 'devices'
        # Add the devices that the user is allowed to use
        if subsys in self.cfg['cgroup']:
            devices_allowed = open(os.path.join(path,
                                   "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Initial devices.list: %s" %
                       devices_allowed)
            # Deny access to mic and gpu devices
            devices_list = list()
            devices = node.devices
            for device in devices:
                if device == 'mic' or device == 'gpu':
                    for name in devices[device]:
                        d = devices[device][name]
                        devices_list.append("%d:%d" % (d['major'], d['minor']))
            # For CentOS 7 we need to remove a *:* rwm from devices.list we
            # before can add anything to devices.allow. Otherwise our changes
            # are ignored. Check to see if a *:* rwm is in devices.list.
            # If so remove it
            if "a *:* rwm\n" in devices_allowed:
                value = "a *:* rwm"
                self.write_value(os.path.join(path, 'devices.deny'), value)
            else:
                # Verify that the following devices are not in devices.list
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing access to the following: %s" %
                           devices_list)
            for device_str in devices_list:
                value = "c %s rwm" % device_str
                self.write_value(os.path.join(path, 'devices.deny'), value)
            # Add devices back to the list
            devices_allow = list()
            if 'allow' in self.cfg['cgroup'][subsys]:
                devices_allow = self.cfg['cgroup'][subsys]['allow']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Allowing access to the following: %s" %
                           devices_allow)
                for item in devices_allow:
                    if isinstance(item, str):
                        pbs.logmsg(pbs.EVENT_DEBUG3, "string item: %s" % item)
                        value = "%s" % item
                        self.write_value(os.path.join(
                                         path, 'devices.allow'), value)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "write_value: %s" % value)
                    elif isinstance(item, list):
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "list item: %s" % item)
                        try:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Device allow: %s" % item)
                            stat_filename = '/dev/%s' % item[0]
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Stat file: %s" % stat_filename)
                            s = os.stat(stat_filename)
                            device_type = None
                            if S_ISBLK(s.st_mode):
                                device_type = "b"
                            elif S_ISCHR(s.st_mode):
                                device_type = "c"
                            if device_type is not None:
                                if len(item) == 3 and isinstance(item[2], str):
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % item[2]
                                    value += "%s" % item[1]
                                else:
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % os.minor(s.st_rdev)
                                    value += "%s" % item[1]
                                self.write_value(os.path.join(
                                                path, 'devices.allow'), value)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "write_value: %s" % value)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find /dev/%s. " % item[0] +
                                       "Not added to devices.allow!")
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                    else:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Not sure what to do with: %s" % item)
            devices_allowed = open(os.path.join(
                                   path, "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "After setup devices.list: %s" % devices_allowed)

    # Select devices to assign to the job
    def assign_devices(
                       self,
                       device_type,
                       device_list,
                       number_of_devices,
                       node):
        devices = device_list[:number_of_devices]
        device_ids = list()
        device_names = list()
        for device in devices:
            device_info = node.devices[device_type][device]
            if len(device_ids) == 0:
                if device.find("mic") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:0 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                    device_ids.append("%s %d:1 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                elif device.find("nvidia") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:255 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
            device_ids.append("%s %d:%d rwm" %
                              (device_info['device_type'],
                               device_info['major'],
                               device_info['minor']))
            device_names.append(device)
        return device_names, device_ids

    # Find the device name
    def get_device_name(self, node, available, socket, major, minor):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Get device name: major: %s, minor: %s" % (major, minor))
        avail_device = None
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Possible devices: %s" % (available[socket]['devices']))
        for avail_device in available[socket]['devices']:
            avail_major = None
            avail_minor = None
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Checking device: %s" % (avail_device))
            if avail_device.find('mic') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check mic device: %s" % (avail_device))
                avail_major = node.devices['mic'][avail_device]['major']
                avail_minor = node.devices['mic'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            elif avail_device.find('nvidia') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check gpu device: %s" % (avail_device))
                avail_major = node.devices['gpu'][avail_device]['major']
                avail_minor = node.devices['gpu'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            if int(avail_major) == int(major) and \
                    int(avail_minor) == int(minor):
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device match: name: %s, major: %s, minor: %s" %
                           (avail_device, major, minor))
                return avail_device
        pbs.logmsg(pbs.EVENT_DEBUG3, "No match found")
        return None

    # Assign resources to the job based off the vnodes assignments
    def assign_job_resources_by_vnode(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # Loop through the vnodes and assign resources
        assigned = dict()
        assigned['cpuset.cpus'] = list()
        assigned['cpuset.mems'] = list()
        room_on_socket = True

        for vnode in resources['vnodes']:
            regex = re.compile(".*\[(\d+)\].*")
            socket = int(regex.search(vnode).group(1))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current Vnode: %s" % vnode)
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current socket: %d" % socket)

            if 'ncpus' in resources['vnodes'][vnode]:
                tmp_ncpus = int(resources['vnodes'][vnode]['ncpus'])
                if int(resources['vnodes'][vnode]['ncpus']) <= \
                        len(available[socket]['cpus']):
                    assigned['cpuset.cpus'] += \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] += [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_ncpus: %s" % (tmp_ncpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "available[%d]: %s" %
                               (socket, available[socket]))
                    room_on_socket = False
            if ('nmics' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['nmics']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(mic).*")
                tmp_nmics = int(resources['vnodes'][vnode]['nmics'])
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['nmics']) > 0 and
                   int(resources['vnodes'][vnode]['nmics']) <= len(mics)):
                    tmp_names, tmp_devices = self.assign_devices(
                             'mic', mics[:tmp_nmics], tmp_nmics, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_nmics: %s" % (tmp_nmics))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "mics: %s" % (mics))
                    room_on_socket = False
            if ('ngpus' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['ngpus']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(nvidia).*")
                tmp_ngpus = int(resources['vnodes'][vnode]['ngpus'])
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['ngpus']) > 0 and
                   int(resources['vnodes'][vnode]['ngpus']) <= len(gpus)):
                    tmp_names, tmp_devices = self.assign_devices(
                        'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "tmp_ngpus: %s" % (tmp_ngpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "gpus: %s" % (gpus))
                    room_on_socket = False

        if room_on_socket:
            assigned['cpuset.cpus'].sort()
            assigned['cpuset.mems'].sort()
            if 'devices' in assigned:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids pre set and sort: %s" %
                           (assigned['devices']))
                assigned['devices'] = list(set(assigned['devices']))
                assigned['devices'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids post set and sort: %s" %
                           (assigned['devices']))
                assigned['devices_name'] = list(set(assigned['devices_name']))
                assigned['devices_name'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

    # Assign resources to the job
    def assign_job_resources(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # What would we need to do different for a large smp (UV) system?
        # See if requested resources can fit on a single socket
        assigned = dict()
        pbs.logmsg(pbs.EVENT_DEBUG3, "Requested: %s" % (resources))

        # Determine the order that we should evaluate the sockets.
        socket_list = list()
        if 'placement_type' in self.cfg:
            if self.cfg['placement_type'] == 'round_robin':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Requested round_robin placement")
                # Look at assigned_resources and determine which socket
                # to start with
                jobs_per_socket_cnt = dict()
                for socket in available.keys():
                    jobs_per_socket_cnt[socket] = 0
                for job in self.assigned_resources:
                    if 'cpuset.mems' in self.assigned_resources[job]:
                        for socket in \
                                self.assigned_resources[job]['cpuset.mems']:
                            jobs_per_socket_cnt[socket] += 1

                sorted_dict = sorted(jobs_per_socket_cnt.items(),
                                     key=operator.itemgetter(1))
                for x in sorted_dict:
                    socket_list.append(x[0])
        else:
            socket_list = available.keys()

        # Loop through the socket list and determine if the job
        # can fit on the socket
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Evaluate sockets in the following order: %s" %
                   (socket_list))
        for socket in socket_list:
            room_on_socket = True
            if 'ncpus' in resources:
                if int(resources['ncpus']) <= len(available[socket]['cpus']):
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Requested: %s, Available: %s" %
                               (resources['ncpus'], available[socket]['cpus']))
                    tmp_ncpus = int(resources['ncpus'])
                    assigned['cpuset.cpus'] = \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] = [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient ncpus on socket %d: R:%d,A:%s" %
                               (socket, resources['ncpus'],
                                available[socket]['cpus']))
                    room_on_socket = False
            # Check to see that nmics has been requested and not equal to 0
            if 'nmics' in resources and int(resources['nmics']) > 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'nmics' in resources and int(resources['nmics']) > 0 \
                        and int(resources['nmics']) <= len(mics):
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient nmics on socket %d: R:%s,A:%s" %
                               (socket, resources['nmics'], mics))
                    room_on_socket = False
            # Check to see that ngpus has been requested and not equal to 0
            if 'ngpus' in resources and int(resources['ngpus']) > 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'ngpus' in resources and int(resources['ngpus']) > 0 \
                        and int(resources['ngpus']) <= len(gpus):
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient gpus on socket %d: R:%s,A:%s" %
                               (socket, resources['ngpus'], gpus))
                    room_on_socket = False
            if 'vmem' in resources:
                if size_as_int(resources['vmem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient vmem on socket %d: R:%s,A:%s" %
                               (socket, resources['vmem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if 'mem' in resources:
                if size_as_int(resources['mem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient memory on socket %d: R:%s,A:%s" %
                               (socket, resources['mem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if room_on_socket:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
                self.host_assigned_resources = assigned
                return assigned
        # If we made it here then the requested resources did not fit
        # on a single socket
        # Future: take distance into account when selecting sockets?
        # Combine available resources into a single list
        # This should work for a dual socket machine.
        # Needs additional work for machines with more than 2 sockets
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Unable to find requested resources: %s" % resources +
                   " on a socket. Checking the entire node")
        available_copy = copy.deepcopy(available)
        available_on_node = dict()
        socket_keys = available.keys()
        socket_keys.sort()
        available_on_node['sockets'] = socket_keys
        for socket in socket_keys:
            for key in available_copy[socket].keys():
                if isinstance(available[socket][key], int):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]
                if isinstance(available_copy[socket][key], str):
                    try:
                        if key in available_on_node is False:
                            available_on_node[key] = \
                                size_as_int(available_copy[socket][key])
                        else:
                            available_on_node[key] += \
                                size_as_int(available_copy[socket][key])
                    except:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Unable to convert to int: %s" %
                                   (available_copy[socket][key]))

                if isinstance(available_copy[socket][key], list):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available on node: %s" % (available_on_node))
        assigned = dict()
        room_on_node = False
        if 'ncpus' in resources:
            if int(resources['ncpus']) <= len(available_on_node['cpus']):
                room_on_node = True
                tmp_ncpus = int(resources['ncpus'])
                assigned['cpuset.cpus'] = available_on_node['cpus'][:tmp_ncpus]
                assigned['cpuset.mems'] = available_on_node['sockets']
            if 'nmics' in resources and int(resources['nmics']) != 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['nmics']) <= len(mics) and \
                        int(resources['nmics']) > 0:
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
            if 'ngpus' in resources and int(resources['ngpus']) != 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['ngpus']) <= len(gpus) and \
                        int(resources['ngpus']) > 0:
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
        if room_on_node:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

        # Consolidate resources if needed to place the job

    # Determine which resources are available
    def available_node_resources(self, node):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))

        available = copy.deepcopy(node.numa_nodes)
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Keys: %s" % (available[0].keys()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        for socket in available.keys():
            if 'cpus' in available[socket]:
                # Find the physical cores
                if available[socket]['cpus'].find('-') != -1 and \
                        node.hyperthreading:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Splitting %s at ','" %
                               (available[socket]['cpus']))
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'].split(',')[0])
                else:
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'])
            if 'MemTotal' in available[socket]:
                # Find the memory on the socket in bites.
                # Remove the 'b' to simply math
                available[socket]['memory'] = size_as_int(
                    available[socket]['MemTotal'])

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Pre device add: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (node.devices.keys()))
        for device in node.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" %
                       (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Devices: %s" %
                           (node.devices[device].keys()))
                for device_name in node.devices[device].keys():
                    device_socket = \
                        node.devices[device][device_name]['numa_node']
                    if 'devices' not in available[device_socket]:
                        available[device_socket]['devices'] = list()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Device: %s, Socket: %s" %
                               (device, device_socket))
                    available[device_socket]['devices'].append(device_name)
            # pbs.logmsg(pbs.EVENT_DEBUG3,
            #            "Available on socket %s: %s" %
            #            (socket,available[socket]))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))

        # Remove all of the resources that are assigned to other jobs
        for job in self.assigned_resources.keys():
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            if (not job_to_be_ignored(job)):
                cpus = list()
                sockets = list()
                devices = list()
                memory = 0
                if 'cpuset.cpus' in self.assigned_resources[job]:
                    cpus = self.assigned_resources[job]['cpuset.cpus']
                if 'cpuset.mems' in self.assigned_resources[job]:
                    sockets = self.assigned_resources[job]['cpuset.mems']
                if 'devices.list' in self.assigned_resources[job]:
                    devices = self.assigned_resources[job]['devices.list']
                if 'memory.limit' in self.assigned_resources[job]:
                    memory = size_as_int(
                                self.assigned_resources[job]['memory.limit'])

                # Loop through the sockets and remove cpus that are
                # assigned to other cgroups
                for socket in sockets:
                    for cpu in cpus:
                        try:
                            available[socket]['cpus'].remove(cpu)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %d from %s" %
                                       (cpu, available[socket]['cpus']))

                if len(sockets) == 1:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Decrementing memory: %d by %d" %
                               (size_as_int(available[sockets[0]]['memory']),
                                memory))
                    available[sockets[0]]['memory'] -= memory

                # Loop throught the available sockets
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned device to %s: %s" % (job, devices))
                for socket in available.keys():
                    for device in devices:
                        try:
                            # loop through know devices and see if they match
                            if len(available[socket]['devices']) != 0:
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Check device: %s" % (device))
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Available device: %s" %
                                           (available[socket]['devices']))
                                major, minor = device.split()[1].split(':')
                                avail_device = self.get_device_name(
                                                   node, available, socket,
                                                   major, minor)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Returned device: %s" %
                                           (avail_device))
                                if avail_device is not None:
                                    pbs.logmsg(pbs.EVENT_DEBUG3,
                                               "socket: %d,\t" % socket +
                                               "devices: %s,\t" %
                                               available[socket]['devices'] +
                                               "device to remove: %s" %
                                               (avail_device))
                                    available[socket]['devices'].remove(
                                        avail_device)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %s from %s" %
                                       (device, available[socket]['devices']))
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Job %s res not removed from host " % job +
                           "available res: suspended job")
        pbs.logmsg(pbs.EVENT_DEBUG, "Available resources: %s" % (available))
        return available

    # Set a job limit
    def set_job_limit(self, jobid, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s: %s = %s" %
                   (caller_name(), jobid, resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'softmem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.soft_limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     jobid, 'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'], jobid,
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            elif resource == 'ncpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = self.select_cpus(path, value)
                    if cpus is None:
                        raise CgroupLimitError("Failed to configure cpuset.")
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
                    self.__copy_from_parent(os.path.join(self.paths['cpuset'],
                                            jobid, 'cpuset.mems'))
            elif resource == 'cpuset.cpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = value
                    if cpus is None:
                        msg = "Failed to configure cpus in cpuset."
                        raise CgroupLimitError(msg)
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
            elif resource == 'cpuset.mems':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.mems')
                    mems = value
                    if mems is None:
                        msg = "Failed to configure mems in cpuset."
                        raise CgroupLimitError(msg)
                    mems = ','.join(map(str, mems))
                    self.write_value(path, mems)
            elif resource == 'devices':
                if 'devices' in self.subsystems and \
                        self.cfg['cgroup']['devices']['enabled']:

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.allow')
                    devices = value
                    if devices is None:
                        msg = "Failed to configure device(s)."
                        raise CgroupLimitError(msg)
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Setting devices: %s for %s" % (devices, jobid))
                    for device in devices:
                        self.write_value(path, device)

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.list')
                    output = open(path, 'r').read()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "devices.list: %s" % output.replace("\n", ","))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    # Update resource usage for a job
    def update_job_usage(self, jobid, resc_used):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resc_used = %s" %
                   (caller_name(), str(resc_used)))
        # Sort the subsystems so that we consistently look at the subsystems
        # in the same order every time
        self.subsystems.sort()
        for subsys in self.subsystems:
            path = os.path.join(self.paths[subsys], jobid)
            if subsys == "memory":
                max_mem = self.__get_max_mem_usage(path)
                if max_mem is None:
                    pbs.logjobmsg(jobid, "%s: No max mem data" %
                                  (caller_name()))
                else:
                    resc_used['mem'] = pbs.size(convert_size(max_mem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: mem=%s" %
                                  (caller_name(), resc_used['mem']))
                mem_failcnt = self.__get_mem_failcnt(path)
                if mem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No mem fail count data" %
                                  (caller_name()))
                else:
                    # Check to see if the job exceeded its resource limits
                    if mem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memory limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "memsw":
                max_vmem = self.__get_max_memsw_usage(path)
                if max_vmem is None:
                    pbs.logjobmsg(jobid, "%s: No max vmem data" %
                                  (caller_name()))
                else:
                    resc_used['vmem'] = pbs.size(convert_size(max_vmem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: vmem=%s" %
                                  (caller_name(), resc_used['vmem']))

                vmem_failcnt = self.__get_memsw_failcnt(path)
                if vmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No vmem fail count data" %
                                  (caller_name()))
                else:
                    pbs.logjobmsg(jobid, "%s: vmem fail count: %d " %
                                  (caller_name(), vmem_failcnt))
                    if vmem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memsw limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "hugetlb":
                max_hpmem = self.__get_max_hugetlb_usage(path)
                if max_hpmem is None:
                    pbs.logjobmsg(jobid, "%s: No max hpmem data" %
                                  (caller_name()))
                    return
                hpmem_failcnt = self.__get_hugetlb_failcnt(path)
                if hpmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No hpmem fail count data" %
                                  (caller_name()))
                    return
                if hpmem_failcnt > 0:
                    err_msg = self.__get_error_msg(jobid)
                    pbs.logjobmsg(jobid, "Cgroup hugetlb limit exceeded: %s" %
                                  (err_msg))
                resc_used['hpmem'] = pbs.size(convert_size(max_hpmem, 'kb'))
                pbs.logjobmsg(jobid, "%s: Hugepage usage: %s" %
                              (caller_name(), resc_used['hpmem']))
            elif subsys == "cpuacct":
                cpu_usage = self.__get_cpu_usage(path)
                if cpu_usage is None:
                    pbs.logjobmsg(jobid, "%s: No CPU usage data" %
                                  (caller_name()))
                    return
                cpu_usage = convert_time(str(cpu_usage) + "ns")
                pbs.logjobmsg(jobid, "%s: CPU usage: %.3lf secs" %
                              (caller_name(), cpu_usage))
                resc_used['cput'] = pbs.duration(cpu_usage)

    # Creates the cgroup if it doesn't exists
    def create_job(self, jobid, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Iterate over the subsystems required
        for subsys in self.subsystems:
            # Create a directory for the job
            try:
                old_umask = os.umask(0022)
                path = self.paths[subsys]
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                path = os.path.join(self.paths[subsys], jobid)
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                if subsys == 'devices':
                    self.setup_subsys_devices(path, node)

            except OSError, exc:
                raise CgroupConfigError("Failed to create directory: %s (%s)" %
                                        (path, errno.errorcode[exc.errno]))
            except:
                raise
            finally:
                os.umask(old_umask)

    # Determine the cgroup limits and configure the cgroups
    def configure_job(self, jobid, hostresc, node):
        mem_enabled = 'memory' in self.subsystems
        vmem_enabled = 'memsw' in self.subsystems
        if mem_enabled or vmem_enabled:
            # Initialize mem variables
            mem_avail = node.get_memory_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "mem_avail %s" % mem_avail)
            mem_requested = None
            if 'mem' in hostresc.keys():
                mem_requested = convert_size(hostresc['mem'], 'kb')
            mem_default = None
            if mem_enabled:
                mem_default = self.default('memory')
            # Initialize vmem variables
            vmem_avail = node.get_vmem_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "vmem_avail %s" % vmem_avail)
            vmem_requested = None
            if 'vmem' in hostresc.keys():
                vmem_requested = convert_size(hostresc['vmem'], 'kb')
            vmem_default = None
            if vmem_enabled:
                vmem_default = self.default('memsw')
            # Initialize softmem variables
            if 'soft_limit' in self.cfg['cgroup']['memory'].keys():
                softmem_enabled = self.cfg['cgroup']['memory']['soft_limit']
            else:
                softmem_enabled = False
            # Sanity check
            if size_as_int(mem_avail) > size_as_int(vmem_avail):
                if size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                    raise CgroupLimitError(
                        'mem available (%s) exceeds vmem available (%s)' %
                        (mem_avail, vmem_avail))
            # Determine the mem limit
            if mem_requested is not None:
                # mem requested may not exceed available
                if size_as_int(mem_requested) > size_as_int(mem_avail):
                    raise JobValueError(
                        'mem requested (%s) exceeds mem available (%s)' %
                        (mem_requested, mem_avail))
                mem_limit = mem_requested
            else:
                # mem was not requested
                if mem_default is None:
                    mem_limit = mem_avail
                else:
                    mem_limit = mem_default
            # Determine the vmem limit
            if vmem_requested is not None:
                # vmem requested may not exceed available
                if size_as_int(vmem_requested) > size_as_int(vmem_avail):
                    raise JobValueError(
                        'vmem requested (%s) exceeds vmem available (%s)' %
                        (vmem_requested, vmem_avail))
                vmem_limit = vmem_requested
            else:
                # vmem was not requested
                if vmem_default is None:
                    vmem_limit = vmem_avail
                else:
                    vmem_limit = vmem_default
            # Ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Adjust for soft limits if enabled
            if mem_enabled and softmem_enabled:
                softmem_limit = mem_limit
                # The hard memory limit is assigned the lesser of the vmem
                # limit and available memory
                if size_as_int(vmem_limit) < size_as_int(mem_avail):
                    mem_limit = vmem_limit
                else:
                    mem_limit = mem_avail
            # Again, ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Sanity checks when both memory and memsw are enabled
            if mem_enabled and vmem_enabled:
                if vmem_requested is not None:
                    if size_as_int(vmem_limit) > size_as_int(vmem_requested):
                        # The user requested an invalid limit
                        raise JobValueError(
                            'vmem limit (%s) exceeds vmem requested (%s)' %
                            (vmem_limit, vmem_requested))
                if size_as_int(vmem_limit) > size_as_int(mem_limit):
                    # This job may utilize swap
                    if size_as_int(vmem_avail) <= size_as_int(mem_avail) and \
                      size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                        # No swap available
                        raise CgroupLimitError(
                            ('Job might utilize swap ' +
                             'and no swap space available'))
            # Assign mem and vmem
            if mem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: mem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), mem_limit))
                hostresc['mem'] = pbs.size(mem_limit)
                if softmem_enabled:
                    hostresc['softmem'] = pbs.size(softmem_limit)
            if vmem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: vmem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), vmem_limit))
                hostresc['vmem'] = pbs.size(vmem_limit)
        # Initialize hpmem variables
        hpmem_enabled = 'hugetlb' in self.subsystems
        if hpmem_enabled:
            hpmem_avail = node.get_hpmem_on_node(self.cfg)
            hpmem_limit = None
            hpmem_default = self.default('hugetlb')
            if hpmem_default is None:
                hpmem_default = hpmem_avail
            if 'hpmem' in hostresc.keys():
                hpmem_limit = convert_size(hostresc['hpmem'], 'kb')
            else:
                hpmem_limit = hpmem_default
            # Assign hpmem
            if size_as_int(hpmem_limit) > size_as_int(hpmem_avail):
                raise JobValueError('hpmem limit (%s) exceeds available (%s)' %
                                    (hpmem_limit, hpmem_avail))
            hostresc['hpmem'] = pbs.size(hpmem_limit)
        # Initialize cpuset variables
        cpuset_enabled = 'cpuset' in self.subsystems
        if cpuset_enabled:
            cpu_limit = 1
            if 'ncpus' in hostresc.keys():
                cpu_limit = hostresc['ncpus']
            if cpu_limit < 1:
                cpu_limit = 1
            hostresc['ncpus'] = pbs.pbs_int(cpu_limit)

        # Find the available resources and assign the right ones to the job

        attempt = 0
        assign_resources = False

        # Two attempts since self.cleanup_orphans may actually fix the
        # problem we see in a first attempt
        while attempt < 2:
            attempt += 1
            available_resources = self.available_node_resources(node)
            if 'vnodes' in hostresc:
                assign_resources = self.assign_job_resources_by_vnode(
                    hostresc, available_resources, node)
            else:
                assign_resources = self.assign_job_resources(
                    hostresc, available_resources, node)

            if (assign_resources is False) and (attempt < 2):
                # No resources were assigned to the job.
                # Most likely cause was that a cgroup has
                # not been cleaned up yet
                pbs.logmsg(pbs.EVENT_DEBUG, "Failed to assign resources. " +
                           "Checking the following cgroups on the node " +
                           "with the server: %s" % self.existing_cgroups)

                # Collect the jobs on the node (try reading mom_priv/jobs,
                # if not successful ask server)
                jobs_list = node.gather_jobs_on_node_local()

                if jobs_list is not None:
                    # Don't clean up the cgroup we just made for the
                    # job just yet
                    if jobid not in jobs_list:
                        jobs_list.append(jobid)

                    self.cleanup_orphans(jobs_list)

                    # Recheck what is now assigned after things were cleaned up
                    self.gather_assigned_resources()

        if assign_resources is False:
            # Cleanup cgroups for jobs not present on this node
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Assign resources failed. Attempting to cleanup all " +
                       "leftover cgroups (including event job %s)" % jobid)

            # Remove the current job from the jobs list so it will be
            # cleaned up as well.
            jobs_list.remove(jobid)

            self.cleanup_orphans(jobs_list)

            # Rerun the job and log the message
            line = 'Unable to assign resources to job. '
            line += 'Will requeue the job and try again.'
            line += 'job run_count: %d' % pbs.event().job.run_count
            pbs.event().job.rerun()
            pbs.event().reject(line)

        # Print out the assigned resources
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Assigned resources: %s" % (assign_resources))

        if cpuset_enabled:
            hostresc.pop('ncpus', None)
            for key in ['cpuset.cpus', 'cpuset.mems']:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Initialize devices variables
        devices_enabled = 'devices' in self.subsystems
        if devices_enabled:
            key = 'devices'
            if key in assign_resources:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Apply the resource limits to the cgroups
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Setting cgroup limits for: %s" %
                   (caller_name(), hostresc))

        # The vmem limit must be set after the mem limit, so sort the keys
        for resc in sorted(hostresc.keys()):
            self.set_job_limit(jobid, resc, hostresc[resc])

    # Kill any processes contained within a tasks file
    def __kill_tasks(self, tasks_file):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        with open(tasks_file, 'r') as fd:
            for line in fd:
                os.kill(int(line.strip()), signal.SIGKILL)
        fd.close()
        count = 0
        with open(tasks_file, 'r') as fd:
            for line in fd:
                count += 1
                pid = line.strip()
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Pid: %s did not clean up" %
                           (caller_name(), pid))
                if os.path.isfile('/proc/%s/status' % pid):
                    tmp = open('/proc/%s/status' % pid).readlines()
                    tmp_line = ''
                    for entry in tmp:
                        if entry.find('Name:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('State:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('Uid:') != -1:
                            tmp_line += entry.strip()
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: %s" % (caller_name(), tmp_line))

        return count

    # Perform the actual removal of the cgroup directory
    # by default, only one attempt at killing tasks in cgroup, without sleeping
    # since this method could be called many times
    # (for N directories times M jobs)
    def __remove_cgroup(self, path, tasks_kill_attempts=1):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            if os.path.exists(path):
                tasks_file = os.path.join(path, 'tasks')
                task_cnt = self.__kill_tasks(tasks_file)
                kill_attempts = 0
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Task Kill Attempts: %d" % tasks_kill_attempts)
                while (task_cnt > 0) and (kill_attempts < tasks_kill_attempts):
                    kill_attempts += 1
                    count = 0

                    with open(tasks_file, 'r') as fd:
                        for line in fd:
                            count += 1
                        task_cnt = count

                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "Attempt: %d - cgroup still has %d tasks: %s" %
                               (kill_attempts, task_cnt, path))
                    if kill_attempts < tasks_kill_attempts:
                        time.sleep(1)
                        # Added since there are race conditions where tasks are
                        # being added at the same time we get the initial
                        # task count
                        tasks_file = os.path.join(path, 'tasks')
                        task_cnt = self.__kill_tasks(tasks_file)

                if task_cnt == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Removing directory %s" %
                               (caller_name(), path))
                    os.rmdir(path)
                    if os.path.exists(path):
                        time.sleep(.25)
                        if os.path.exists(path):
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "%s: 2nd Try Removing directory %s" %
                                       (caller_name(), path))
                            os.rmdir(path)
                            time.sleep(.25)
                        if os.path.exists(path):
                            return False
                else:
                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "cgroup still has %d tasks: %s" %
                               (task_cnt, path))

                    # Check to see if the offline file is already present
                    # on the mom
                    offline_file_exists = False
                    if os.path.isfile(self.offline_file):
                        msg = "Cgroup(s) not cleaning up but the node already "
                        msg += "has the offline file."

                        pbs.logmsg(pbs.EVENT_DEBUG, msg)
                    else:
                        # Check to see that the node is not already offline.
                        try:
                            tmp_state = pbs.server().vnode(self.hostname).state
                        except:
                            msg = "Unable to contact server for node state"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            tmp_state = None
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Current Node State: %d" % tmp_state)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Offline Node State: %d" % pbs.ND_OFFLINE)

                        if tmp_state == pbs.ND_OFFLINE:
                            msg = "Cgroup(s) not cleaning up but the node is "
                            msg += "already offline."
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                        elif tmp_state is not None:
                            # Offline the node(s)
                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                            msg = "Offlining node since cgroup(s) are not "
                            msg += "cleaning up"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            vnode = pbs.event().vnode_list[self.hostname]

                            vnode.state = pbs.ND_OFFLINE

                            # Write a file locally to reduce server traffic
                            # when it comes time to online the node
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Offline file: %s" % self.offline_file)
                            open(self.offline_file, 'a').close()
                            vnode.comment = self.offline_msg

                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                        else:
                            pass

                    return False

        except OSError, exc:
            pbs.logmsg(pbs.EVENT_SYSTEM,
                       "Failed to remove cgroup path: %s (%s)" %
                       (path, errno.errorcode[exc.errno]))
        except:
            pbs.logmsg(pbs.EVENT_SYSTEM, "Failed to remove cgroup path: %s" %
                       (path))

        return True

    # Removes cgroup directories that are not associated with a local job
    def cleanup_orphans(self, local_jobs):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        orphan_cnt = 0
        for key in self.paths.keys():
            for dir in glob.glob(os.path.join(self.paths[key], '[0-9]*')):
                jobid = os.path.basename(dir)
                if jobid not in local_jobs:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Removing orphaned cgroup: %s" %
                               (caller_name(), dir))
                    # default number of attempts at killing tasks
                    status = self.__remove_cgroup(dir)
                    if not status:
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "%s: Removing orphaned cgroup %s failed " %
                                   (caller_name(), dir))
                        orphan_cnt += 1
        return orphan_cnt

    # Removes the cgroup directories for a job
    def delete(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        tasks_kill_attempted = False

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            if not tasks_kill_attempted:
                # Make multiple attempts at killing tasks in cgroups on
                # first subsystem encountered since it is "normal" for
                # a cgroup.delete to encounter processes hard to kill
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                                           jobid),
                                              tasks_kill_attempts=(
                                              CGROUP_KILL_ATTEMPTS))
                tasks_kill_attempted = True
            else:
                # We tried getting rid of job processes earlier,
                # so don't try N times with sleeps
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                              jobid), tasks_kill_attempts=1)

            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Status: %s" %
                       (caller_name(), status))
            if status is False:
                # Rerun the job and log the message
                line = 'Unable to cleanup a cgroup. '
                line += 'Will offline the node and requeue the job '
                line += 'and try again. job run_count: %d' % \
                        pbs.event().job.run_count
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Failed to remove cgroup: %s" %
                           (caller_name(), line))
                pbs.event().accept()

    # Write a value to a limit file
    def write_value(self, file, value, mode='w'):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: writing %s to %s" %
                   (caller_name(), value, file))
        try:
            with open(file, mode) as fd:
                fd.write(str(value) + '\n')
        except IOError, exc:
            if exc.errno == errno.ENOENT:
                pbs.logmsg(pbs.EVENT_SYSTEM,
                           "Trying to set limit to unknown file %s" % file)
            elif exc.errno in [errno.EACCES, errno.EPERM]:
                pbs.logmsg(pbs.EVENT_SYSTEM, "Permission denied on file %s" %
                           file)
            elif exc.errno == errno.EBUSY:
                raise CgroupBusyError("Limit rejected")
            elif exc.errno == errno.EINVAL:
                raise CgroupLimitError("Invalid limit value: %s" % (value))
            else:
                raise
        except:
            raise

    # Return memory failcount
    def __get_mem_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.failcnt"), 'r').read().strip())
        except:
            return None

    # Return vmem failcount
    def __get_memsw_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.failcnt"), 'r').read().strip())
        except:
            return None

    # Return hpmem failcount
    def __get_hugetlb_failcnt(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.failcnt"))[0], 'r').read().strip())
        except:
            return None

    # Return the max usage of memory in bytes
    def __get_max_mem_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of memsw in bytes
    def __get_max_memsw_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of hugetlb in bytes
    def __get_max_hugetlb_usage(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.max_usage_in_bytes"))[0],
                            'r').read().strip())
        except:
            return None

    # Return the cpuacct.usage in cpu seconds
    def __get_cpu_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "cpuacct.usage"), 'r').read().strip())
        except:
            return None

    # Assign CPUs to the cpuset
    def select_cpus(self, path, ncpus):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path is %s" % (caller_name(), path))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: ncpus is %s" %
                   (caller_name(), ncpus))
        if ncpus < 1:
            ncpus = 1
        try:
            # Must select from those currently available
            cpufile = os.path.basename(path)
            base = os.path.dirname(path)
            parent = os.path.dirname(base)
            avail = cpus2list(open(os.path.join(parent, cpufile),
                              'r').read().strip())
            if len(avail) < 1:
                raise CgroupProcessingError("No CPUs avaialble in cgroup.")
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Available CPUs: %s" %
                       (caller_name(), avail))
            assigned = []
            for file in glob.glob(os.path.join(parent, '[0-9]*', cpufile)):
                cpus = cpus2list(open(file, 'r').read().strip())
                for id in cpus:
                    if id in avail:
                        avail.remove(id)
            if len(avail) < ncpus:
                raise CgroupProcessingError(
                    "Insufficient CPUs avaialble in cgroup.")
            if len(avail) == ncpus:
                return avail
            # FUTURE: Try to minimize NUMA nodes based on memory requirement
            return avail[0:ncpus]
        except:
            raise

    # Return the error message in system message file
    def __get_error_msg(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        proc = subprocess.Popen(['dmesg'],
                                shell=False, stdout=subprocess.PIPE)
        stdout = proc.communicate()[0].splitlines()
        stdout.reverse()
        # Check to see if the job id is found in dmesg otherwise
        # you could get the information for another job that was killed
        # the line will look like Memory cgroup stats for /pbspro/279.centos7
        kill_line = ""

        for line in stdout:
            start = line.find('Killed process ')
            if start >= 0:
                kill_line = line[start:]
            job_start = line.find(jobid)
            if job_start >= 0:
                # pbs.logmsg(pbs.EVENT_DEBUG3,
                #           "%s: Jobid: %s, Line: %s" %
                #           (caller_name(), jobid, line))
                return kill_line
        return ""

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_job_env_file(self, jobid, env_list):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        # if not os.path.exists(self.host_job_env_dir):
        #    os.makedirs(self.host_job_env_dir, 0755)

        # Write out assigned_resources
        try:
            lines = string.join(env_list, '\n')
            filename = self.host_job_env_filename % jobid
            outfile = open(filename, "w")
            outfile.write(lines)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" % (filename))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (lines))
            return True
        except:
            return False

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        if not os.path.exists(self.hook_storage_dir):
            os.makedirs(self.hook_storage_dir, 0755)

        # Write out assigned_resources
        try:
            json_str = json.dumps(self.host_assigned_resources)
            outfile = open(self.hook_storage_dir+os.sep+jobid, "w")
            outfile.write(json_str)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" %
                       (self.hook_storage_dir+os.sep+jobid))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (json_str))
            return True
        except:
            return False

    def read_in_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        pbs.logmsg(pbs.EVENT_DEBUG3, "Host assigned resources: %s" %
                   (self.host_assigned_resources))
        if os.path.isfile(self.hook_storage_dir+os.sep+jobid):
            # Write out assigned_resources
            try:
                infile = open(self.hook_storage_dir+os.sep+jobid, 'r')
                json_data = json.load(infile, object_hook=decode_dict)
                self.host_assigned_resources = json_data
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Host assigned resources: %s" %
                           (self.host_assigned_resources))
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
            finally:
                infile.close()

        if self.host_assigned_resources is not None:
            return True
        else:
            return False

#
#
# FUNCTION main
#


def main():
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Function called" % (caller_name()))
    hostname = pbs.get_local_nodename()
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Host is %s" % (caller_name(), hostname))

    # Log the hook event type
    e = pbs.event()
    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook name is %s" %
               (caller_name(), e.hook_name))

    try:
        # Instantiate the hook utility class
        hooks = HookUtils()
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Event type is %s" %
                   (caller_name(), hooks.event_name(e.type)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(hooks)))
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: Failed to instantiate hook utility class" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        e.accept()

    if not hooks.hashandler(e.type):
        # Bail out if there is no handler for this event
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s event not handled by this hook" %
                   (caller_name(), hooks.event_name(e.type)))
        e.accept()

    try:
        # If an exception occurs, jobutil must be set
        jobutil = None
        # Instantiate the job utility class first so jobutil can be accessed
        # by the exception handlers.
        if hasattr(e, 'job'):
            jobutil = JobUtils(e.job)
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Job information class instantiated" %
                       (caller_name()))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" %
                       (caller_name(), repr(jobutil)))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Event does not include a job" %
                       (caller_name()))
        # Instantiate the cgroup utility class
        vnode = None
        if hasattr(e, 'vnode_list'):
            if hostname in e.vnode_list.keys():
                vnode = e.vnode_list[hostname]
        cgroup = CgroupUtils(hostname, vnode)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Cgroup utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(cgroup)))
        # Bail out if there is nothing to do
        if len(cgroup.subsystems) < 1:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: All cgroups disabled" %
                       (caller_name()))
            e.accept()

        # Call the appropriate handler
        if hooks.invoke_handler(e, cgroup, jobutil) is True:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned success" %
                       (caller_name()))
            e.accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned failure" %
                       (caller_name()))
            e.reject()

    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass

    except AdminError, exc:
        # Something on the system is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Admin error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except UserError, exc:
        # User must correct problem and resubmit job
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("User error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.delete()
                msg += " (deleted)"
            except:
                msg += " (delete failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except JobValueError, exc:
        # Something in PBS is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Job value error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except Exception, exc:
        # Catch all other exceptions and report them.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Unexpected error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

# line below required for unittesting
if __name__ == "__builtin__":
    start_time = time.time()
    try:
        main()
    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
    finally:
        pbs.logmsg(pbs.EVENT_DEBUG, "Elapsed time: %0.4lf" %
                   (time.time() - start_time))
---
2016-07-18 16:58:00,550 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups.json', 'content-encoding': 'default'}
2016-07-18 16:58:00,550 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups.json
2016-07-18 16:58:00,592 INFO     job: executable set to /bin/sleep with arguments: 100
2016-07-18 16:58:00,593 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0755 /tmp/PtlPbsJobScriptzrktNo
2016-07-18 16:58:00,603 INFOCLI  centos7: sudo -H -u pbsuser /usr/local/bin/qsub -N test_t4 -l select=1:ncpus=1:mem=300mb /tmp/PtlPbsJobScriptzrktNo
2016-07-18 16:58:00,617 INFO     submit to centos7 as pbsuser: job 383.centos7 OrderedDict([('Job_Name', 'test_t4'), ('Resource_List.select', '1:ncpus=1:mem=300mb')])
2016-07-18 16:58:00,617 INFOCLI  job script /tmp/PtlPbsJobScriptzrktNo
---
#!/usr/bin/env python

import os
import subprocess
from subprocess import Popen, PIPE
import time

eat_mem_c = """
#include <stdio.h>
#include <stdlib.h>
#include <string.h>


#define ONE_GB (0x40000000L)
#define ONE_MB (0x100000L)

void
eatmem(total_mb, block_mb)
{
	int i;
	char *buf;
	size_t block;

	for (i=1, buf=NULL, block=(block_mb*ONE_MB); (i*block) < (total_mb*ONE_MB); i++) {
		buf = realloc((void *)buf, (i*block));
		memset(buf+((i-1)*block), 0, block);
	}
	return;
}

int
main(int argc, char *argv[])
{
	int total_mb = 8;
	int block_mb = 1;

	if (argc > 1)
		total_mb = atoi(argv[1]);

	if (argc > 2)
		block_mb = atoi(argv[2]);

	eatmem(total_mb, block_mb);
	printf("Initialized %dMB in blocks of %dMB\\n", total_mb, block_mb);
}
"""

fname = "/tmp/pbs_pp325_eat_mem.c"
fname_exe = "/tmp/pbs_pp325_eat_mem"
fout = open(fname, "w")
fout.write(eat_mem_c)
fout.close()

print os.getppid()

p = Popen(["gcc", "-o", fname_exe, fname], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

p = Popen([fname_exe, "400", "10"], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

os.remove(fname)
os.remove(fname_exe)
time.sleep(1)

---
2016-07-18 16:58:00,617 INFOCLI  centos7: /usr/local/bin/qstat -f 383.centos7
2016-07-18 16:58:00,714 INFO     expect on server centos7: job_state = R job 383.centos7  got: job_state = Q
2016-07-18 16:58:01,216 INFOCLI  centos7: /usr/local/bin/qmgr -c set server scheduling=True
2016-07-18 16:58:01,406 INFOCLI  centos7: /usr/local/bin/qstat -f 383.centos7
2016-07-18 16:58:01,501 INFO     expect on server centos7: job_state = R job 383.centos7 attempt: 2 ...  OK
2016-07-18 16:58:01,508 INFO     mom centos7 log match: searching for ".*cgroup excluded for subsystem memory on vnode type no_cgroups_mem.*" - using regular expression ... OK
2016-07-18 16:58:01,515 INFO     mom centos7 log match: searching for ".*383.centos7;Cgroup mem\w+ limit exceeded.*" - using regular expression  - No match
2016-07-18 16:58:13,937 INFO     mom centos7 log match: searching for ".*383.centos7;Cgroup mem\w+ limit exceeded.*" - using regular expression  - attempt 2
2016-07-18 16:58:26,761 INFO     mom centos7 log match: searching for ".*383.centos7;Cgroup mem\w+ limit exceeded.*" - using regular expression  - attempt 3
2016-07-18 16:58:39,688 INFO     mom centos7 log match: searching for ".*383.centos7;Cgroup mem\w+ limit exceeded.*" - using regular expression  - attempt 4
2016-07-18 16:58:52,347 INFO     mom centos7 log match: searching for ".*383.centos7;Cgroup mem\w+ limit exceeded.*" - using regular expression  - attempt 5
2016-07-18 16:58:53,350 INFO     ==============================
2016-07-18 16:58:53,350 INFO      Entered CgroupsTests tearDown
2016-07-18 16:58:53,350 INFO     ==============================
2016-07-18 16:58:53,350 INFO     ===============================
2016-07-18 16:58:53,350 INFO     Completed CgroupsTests tearDown
2016-07-18 16:58:53,350 INFO     ===============================
2016-07-18 16:58:53,350 INFO     ok

2016-07-18 16:58:53,351 INFO     test name: test_t6 (tests.cgroups_tests.CgroupsTests)...
2016-07-18 16:58:53,351 INFO     test start time: Mon Jul 18 16:58:53 2016
2016-07-18 16:58:53,351 INFO     test docstring: 
 Test to verify that cgroups are reporting usage for
            cput, mem 
        
2016-07-18 16:58:53,351 INFO     ===========================
2016-07-18 16:58:53,351 INFO      Entered CgroupsTests setUp
2016-07-18 16:58:53,351 INFO     ===========================
2016-07-18 16:58:53,351 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:58:53,404 INFO     status on centos7: server
2016-07-18 16:58:53,405 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:58:53,462 INFO     manager on centos7: set server {'managers': (3, 'root@*')}
2016-07-18 16:58:53,462 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers-=root@*
2016-07-18 16:58:53,484 INFO     manager on centos7: set server {'managers': (2, 'root@*')}
2016-07-18 16:58:53,484 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers+=root@*
2016-07-18 16:58:53,521 INFO     server centos7: reverting configuration to defaults
2016-07-18 16:58:53,522 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:58:53,577 INFO     select on centos7: __ALL__
2016-07-18 16:58:53,578 INFOCLI  centos7: /usr/local/bin/qselect
2016-07-18 16:58:53,600 INFOCLI  centos7: /usr/local/bin/qstat -f @centos7.usa.org
2016-07-18 16:58:53,653 INFO     expect on server centos7: job_state set 0 job ...  OK
2016-07-18 16:58:53,653 INFOCLI  centos7: /usr/local/bin/pbs_rstat -f
2016-07-18 16:58:53,675 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:58:53,698 INFO     manager on centos7: delete hook t1_l1
2016-07-18 16:58:53,698 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c delete hook t1_l1
2016-07-18 16:58:53,732 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:58:53,756 INFO     expect on server centos7:  unset  hook t1_l1 ...  OK
2016-07-18 16:58:53,756 INFOCLI  centos7: /usr/local/bin/qstat -Qf @centos7.usa.org
2016-07-18 16:58:53,768 INFO     manager on centos7: delete queue workq
2016-07-18 16:58:53,769 INFOCLI  centos7: /usr/local/bin/qmgr -c delete queue workq
2016-07-18 16:58:53,804 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:58:53,823 INFO     expect on server centos7:  unset  queue workq ...  OK
2016-07-18 16:58:53,823 INFO     manager on centos7: create queue workq {'started': 'True', 'queue_type': 'Execution', 'enabled': 'True'}
2016-07-18 16:58:53,824 INFOCLI  centos7: /usr/local/bin/qmgr -c create queue workq started=True,queue_type=Execution,enabled=True
2016-07-18 16:58:53,841 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:58:53,853 INFO     expect on server centos7: started set True || queue_type set Execution || enabled set True queue workq ...  OK
2016-07-18 16:58:53,853 INFO     manager on centos7: set server {'log_events': '511', 'default_queue': 'workq'}
2016-07-18 16:58:53,853 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=511,default_queue=workq
2016-07-18 16:58:53,867 INFO     status on centos7: resource
2016-07-18 16:58:53,867 INFO     manager on centos7: list resource
2016-07-18 16:58:53,867 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource
2016-07-18 16:58:53,922 INFO     manager on centos7: delete resource nmics,ngpus
2016-07-18 16:58:53,922 INFOCLI  centos7: /usr/local/bin/qmgr -c delete resource nmics,ngpus
2016-07-18 16:58:53,990 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource nmics
2016-07-18 16:58:54,001 INFO     expect on server centos7:  unset  resource nmics ...  OK
2016-07-18 16:58:54,001 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource ngpus
2016-07-18 16:58:54,015 INFO     expect on server centos7:  unset  resource ngpus ...  OK
2016-07-18 16:58:54,015 INFOCLI  status on centos7: server license_count
2016-07-18 16:58:54,015 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:58:54,070 INFO     server: centos7.usa.org licensed
2016-07-18 16:58:54,095 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/server_priv/comm.lock
2016-07-18 16:58:54,135 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:58:54,153 INFO     scheduler centos7: reverting configuration to defaults
2016-07-18 16:58:54,153 INFO     manager on centos7: list sched
2016-07-18 16:58:54,153 INFOCLI  centos7: /usr/local/bin/qmgr -c list sched
2016-07-18 16:58:54,164 INFO     manager on centos7: unset sched ['sched_cycle_length']
2016-07-18 16:58:54,165 INFOCLI  centos7: /usr/local/bin/qmgr -c unset sched sched_cycle_length
2016-07-18 16:58:54,177 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/dedicated_time
2016-07-18 16:58:54,199 INFOCLI2 centos7: cmp /usr/local/etc/pbs_resource_group /var/spool/pbs/sched_priv/resource_group
2016-07-18 16:58:54,206 INFO     scheduler centos7: reverting holidays file to default
2016-07-18 16:58:54,207 INFOCLI2 centos7: cmp /usr/local/etc/pbs_holidays /var/spool/pbs/sched_priv/holidays
2016-07-18 16:58:54,210 INFOCLI2 centos7: cmp /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:58:54,214 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:58:54,237 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:58:54,260 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:58:54,261 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:58:54,336 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:58:54,352 INFOCLI2 centos7: sudo -H cmp /usr/local/sbin/pbs_mom /usr/local/sbin/pbs_mom.cpuset
2016-07-18 16:58:54,388 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:58:54,399 INFO     mom centos7: reverting configuration to defaults
2016-07-18 16:58:54,399 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/epilogue
2016-07-18 16:58:54,416 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/prologue
2016-07-18 16:58:54,430 INFOCLI2 centos7: sudo -H ls /var/spool/pbs/mom_priv/config.d
2016-07-18 16:58:54,445 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsWIgv3q /var/spool/pbs/mom_priv/config
2016-07-18 16:58:54,483 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/mom_priv/config
2016-07-18 16:58:54,502 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/mom_priv/config
2016-07-18 16:58:54,516 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/mom_priv/config
2016-07-18 16:58:54,530 INFO     mom centos7: sent signal -HUP
2016-07-18 16:58:54,530 INFOCLI2 centos7: sudo -H kill -HUP 42976
2016-07-18 16:58:54,584 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:58:54,613 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 16:58:54,627 INFO     status on centos7: node centos7
2016-07-18 16:58:54,627 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:58:54,644 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:58:54,656 INFO     expect on server centos7: state = free node centos7 ...  OK
2016-07-18 16:58:54,657 INFO     ============================
2016-07-18 16:58:54,657 INFO     Completed CgroupsTests setUp
2016-07-18 16:58:54,657 INFO     ============================
2016-07-18 16:58:54,657 INFO     server centos7: server operating mode set to cli
2016-07-18 16:58:54,657 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:58:54,707 INFO     expect on server centos7: pbs_version ~ .*14.* server centos7.usa.org ...  OK
2016-07-18 16:58:54,707 INFO     manager on centos7: set server {'log_events': '2047'}
2016-07-18 16:58:54,707 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=2047
2016-07-18 16:58:54,723 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:58:54,803 INFO     expect on server centos7: log_events set 2047 server centos7.usa.org ...  OK
2016-07-18 16:58:54,803 INFO     scheduler centos7: config {'resources': 'ncpus,mem,host,vnode,vmem'}
2016-07-18 16:58:54,804 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /var/spool/pbs/sched_priv/sched_config /var/spool/pbs/sched_priv/sched_config.bak
2016-07-18 16:58:54,816 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbs3knVOZ /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:58:54,831 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:58:54,841 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:58:54,851 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:58:54,867 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:58:54,867 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:58:54,880 INFO     scheduler centos7 log match: searching for "Error reading line" - No match
2016-07-18 16:58:55,882 INFO     manager on centos7 as root: create resource nmics {'flag': 'nh', 'type': 'long'}
2016-07-18 16:58:55,882 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource nmics flag=nh,type=long
2016-07-18 16:58:55,911 INFO     manager on centos7 as root: create resource ngpus {'flag': 'nh', 'type': 'long'}
2016-07-18 16:58:55,911 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource ngpus flag=nh,type=long
2016-07-18 16:58:55,937 INFO     Test Summary: test_t1_l1 tests "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" cgroup hook
2016-07-18 16:58:55,937 INFO     status on centos7: hook
2016-07-18 16:58:55,937 INFO     manager on centos7: list hook
2016-07-18 16:58:55,937 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:58:55,958 INFO     status on centos7: hook
2016-07-18 16:58:55,958 INFO     manager on centos7: list hook
2016-07-18 16:58:55,958 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:58:55,987 INFO     manager on centos7: create hook t1_l1
2016-07-18 16:58:55,988 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c create hook t1_l1
2016-07-18 16:58:56,010 INFO     manager on centos7: set hook t1_l1 {'freq': 2, 'enabled': 'True', 'event': '"execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"'}
2016-07-18 16:58:56,010 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set hook t1_l1 freq=2,enabled=True,event="execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"
2016-07-18 16:58:56,042 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:58:56,063 INFO     expect on server centos7: freq set 2 || enabled set True || event set "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" hook t1_l1 ...  OK
2016-07-18 16:58:56,064 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-python', 'input-file': '/tmp/PtlPbsgKtvOM', 'content-encoding': 'default'}
2016-07-18 16:58:56,064 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-python default /tmp/PtlPbsgKtvOM
2016-07-18 16:58:56,136 INFO     server centos7: imported hook body
---
#  Copyright (C) 2003-2016 Altair Engineering, Inc. All rights reserved.
#
#  ALTAIR ENGINEERING INC. Proprietary and Confidential. Contains Trade Secret
#  Information. Not for use or disclosure outside ALTAIR and its licensed
#  clients. Information contained herein shall not be decompiled, disassembled,
#  duplicated or disclosed in whole or in part for any purpose. Usage of the
#  software is only as explicitly permitted in the end user software license
#  agreement.
#
#  Copyright notice does not imply publication.

# Last Updated
# Date: 07-06-2016
#
# When soft_limit is true for memory, memsw represents the hard limit.
#
# The resources value in sched_config must contain entries for mem and
# vmem if those subsystems are enabled in the hook configuration file. The
# amount of resource requested will not be avaiable to the hook if they
# are not present.

# This hook handles all of the operations necessary for PBS to support
# cgroups on linux hosts that support them (kernel 2.6.28 and higher)
#
# This hook services the following events:
# - exechost_periodic
# - exechost_startup
# - execjob_attach
# - execjob_begin
# - execjob_end
# - execjob_epilogue
# - execjob_launch

# Imports from __future__ must happen first
from __future__ import with_statement

# Additional imports
import sys
import os
import errno
import signal
import subprocess
import re
import glob
import time
import string
import platform
import traceback
import copy
from stat import S_ISBLK, S_ISCHR
import operator
import pwd
import pbs
import fnmatch
import socket

try:
    import json
except:
    import simplejson as json

CGROUP_KILL_ATTEMPTS = 12

# Flag to turn off features not in 12.2.40X branch
CRAY_12_BRANCH = False


# ============================================================================
# Derived error classes
# ============================================================================

# Base class for errors fixable only by administrative action.


class AdminError(Exception):
    pass

# Base class for errors in processing, unknown cause.


class ProcessingError(Exception):
    pass

# Base class for errors fixable by the user.


class UserError(Exception):
    pass

# Errors in PBS job resource values.


class JobValueError(UserError):
    pass

# Errors when the cgroup is busy.


class CgroupBusyError(ProcessingError):
    pass

# Errors in configuring cgroup.


class CgroupConfigError(AdminError):
    pass

# Errors in configuring cgroup.


class CgroupLimitError(AdminError):
    pass

# Errors processing cgroup.


class CgroupProcessingError(ProcessingError):
    pass

# ============================================================================
# Utility functions
# ============================================================================

#
# FUNCTION caller_name
#
# Return the name of the calling function or method.
#


def caller_name():
    return str(sys._getframe(1).f_code.co_name)

#
# FUNCTION convert_size
#
# Convert a string containing a size specification (e.g. "1m") to a
# string using different units (e.g. "1024k").
#
# This function only interprets a decimal number at the start of the string,
# stopping at any unrecognized character and ignoring the rest of the string.
#
# When down-converting (e.g. MB to KB), all calculations involve integers and
# the result returned is exact. When up-converting (e.g. KB to MB) floating
# point numbers are involved. The result is rounded up. For example:
#
# 1023MB -> GB yields 1g
# 1024MB -> GB yields 1g
# 1025MB -> GB yields 2g  <-- This value was rounded up
#
# Pattern matching or conversion may result in exceptions.
#


def convert_size(value, units='b'):
    logs = {'b': 0, 'k': 10, 'm': 20, 'g': 30,
            't': 40, 'p': 50, 'e': 60, 'z': 70, 'y': 80}
    try:
        new = units[0].lower()
        if new not in logs.keys():
            new = 'b'
        val, old = re.match('([-+]?\d+)([bkmgtpezy]?)',
                            str(value).lower()).groups()
        val = int(val)
        if val < 0:
            raise ValueError('Value may not be negative')
        if old not in logs.keys():
            old = 'b'
        factor = logs[old] - logs[new]
        val *= 2 ** factor
        slop = val - int(val)
        val = int(val)
        if slop > 0:
            val += 1
        # pbs.size() does not like units following zero
        if val <= 0:
            return '0'
        else:
            return str(val) + new
    except:
        return None

#
# FUNCTION size_as_int
#
# Convert a size string to an integer representation of size in bytes
#


def size_as_int(value):
    return int(convert_size(value).rstrip(string.ascii_lowercase))

#
# FUNCTION convert_time
#
# Converts a integer value for time into the value of the return unit
#
# A valid decimal number, with optional sign, may be followed by a character
# representing a scaling factor.  Scaling factors may be either upper or
# lower case. Examples include:
# 250ms
# 40s
# +15min
#
# Valid scaling factors are:
# ns  = 10**-9
# us  = 10**-6
# ms  = 10**-3
# s   =      1
# min =     60
# hr  =   3600
#
# Pattern matching or conversion may result in exceptions.
#


def convert_time(value, return_unit='s'):
    multipliers = {'':  1, 'ns': 10 ** -9, 'us': 10 ** -6,
                   'ms': 10 ** -3, 's': 1, 'min': 60, 'hr': 3600}
    n, factor = re.match('([-+]?\d+)(\w*[a-zA-Z])?',
                         str(value).lower()).groups(0)

    # Check to see if there was not unit of time specified
    if factor == 0:
        factor = ''

    # Check to see if the unit is valid
    if str.lower(factor) not in multipliers.keys():
        raise ValueError('Time unit not recognized.')

    # Convert the value to seconds
    value_in_sec = float(n) * float(multipliers[str.lower(factor)])
    if return_unit != 's':
        return value_in_sec / multipliers[str.lower(return_unit)]
    else:
        return float('%lf' % value_in_sec)

# simplejson hook to convert lists from unicode to utf-8


def decode_list(data):
    rv = []
    for item in data:
        if isinstance(item, unicode):
            item = item.encode('utf-8')
        elif isinstance(item, list):
            item = decode_list(item)
        elif isinstance(item, dict):
            item = decode_dict(item)
        rv.append(item)
    return rv

# simplejson hook to convert dictionaries from unicode to utf-8


def decode_dict(data):
    rv = {}
    for key, value in data.iteritems():
        if isinstance(key, unicode):
            key = key.encode('utf-8')
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        elif isinstance(value, list):
            value = decode_list(value)
        elif isinstance(value, dict):
            value = decode_dict(value)
        rv[key] = value
    return rv

# Convert CPU list format (with ranges) to a Python list


def cpus2list(s):
    # The input string is a comma separated list of digits and ranges.
    # Examples include:
    # 0-3,8-11
    # 0,2,4,6
    # 2,5-7,10
    cpus = []
    if s.strip() == '':
        return cpus
    for r in s.split(','):
        if '-' in r[1:]:
            start, end = r.split('-', 1)
            for i in range(int(start), int(end) + 1):
                cpus.append(int(i))
        else:
            cpus.append(int(r))
    return cpus


# Helper function to ignore suspended jobs when removing
# "already used" resources
def job_to_be_ignored(jobid):
    # Get job substate from small utility that calls printjob
    pbs_exec = ''
    if not CRAY_12_BRANCH:
        pbs_conf = pbs.get_pbs_conf()
        pbs_exec = pbs_conf['PBS_EXEC']
    else:
        if 'PBS_EXEC' in self.cfg:
            pbs_exec = self.cfg['PBS_EXEC']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "PBS_EXEC needs to be defined in the config file")
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Exiting the cgroups hook")
            pbs.accept()

    printjob_cmd = pbs_exec+os.sep+'bin'+os.sep+'printjob'

    cmd = [printjob_cmd, jobid]

    substate = None
    try:
        pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
        # Collect the job substate information
        process = subprocess.Popen(
                                   cmd,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        (out, err) = process.communicate()
        # Find the job substate
        substate_re = re.compile(r"substate:\s+(?P<jobstate>\S+)\s+")
        substate = substate_re.search(out)
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Unexpected error in job_to_be_ignored: %s" %
                   ' '.join([repr(sys.exc_info()[0]),
                            repr(sys.exc_info()[1])]))
        out = "Unknown: failed to run " + printjob_cmd

    if substate is not None:
        substate = substate.group().split(':')[1].strip()
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Job %s has substate %s" % (jobid, substate))

        suspended_substates = ['0x2b', '0x2d', 'unknown']
        return (substate in suspended_substates)
    else:
        return False

# ============================================================================
# Utility classes
# ============================================================================

#
# CLASS HookUtils
#


class HookUtils:

    def __init__(self, hook_events=None):
        if hook_events is not None:
            self.hook_events = hook_events
        else:
            self.hook_events = {
                # Defined in the order they appear in module_pbs_v1.c
                pbs.QUEUEJOB: {
                    'name': 'queuejob',
                    'handler': None
                },
                pbs.MODIFYJOB: {
                    'name': 'modifyjob',
                    'handler': None
                },
                pbs.RESVSUB: {
                    'name': 'resvsub',
                    'handler': None
                },
                pbs.MOVEJOB: {
                    'name': 'movejob',
                    'handler': None
                },
                pbs.RUNJOB: {
                    'name': 'runjob',
                    'handler': None
                },
                pbs.PROVISION: {
                    'name': 'provision',
                    'handler': None
                },
                pbs.EXECJOB_BEGIN: {
                    'name': 'execjob_begin',
                    'handler': self.__execjob_begin_handler
                },
                pbs.EXECJOB_PROLOGUE: {
                    'name': 'execjob_prologue',
                    'handler': None
                },
                pbs.EXECJOB_EPILOGUE: {
                    'name': 'execjob_epilogue',
                    'handler': self.__execjob_epilogue_handler
                },
                pbs.EXECJOB_PRETERM: {
                    'name': 'execjob_preterm',
                    'handler': None
                },
                pbs.EXECJOB_END: {
                    'name': 'execjob_end',
                    'handler': self.__execjob_end_handler
                },
                pbs.EXECJOB_LAUNCH: {
                    'name': 'execjob_launch',
                    'handler': self.__execjob_launch_handler
                },
                pbs.EXECHOST_PERIODIC: {
                    'name': 'exechost_periodic',
                    'handler': self.__exechost_periodic_handler
                },
                pbs.EXECHOST_STARTUP: {
                    'name': 'exechost_startup',
                    'handler': self.__exechost_startup_handler
                },
                pbs.MOM_EVENTS: {
                    'name': 'mom_events',
                    'handler': None
                },
            }

            if not CRAY_12_BRANCH:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "CRAY 12 Branch: %s" % (CRAY_12_BRANCH))
                self.hook_events[pbs.EXECJOB_ATTACH] = {
                    'name': 'execjob_attach',
                    'handler': self.__execjob_attach_handler
                }

    def __repr__(self):
        return "HookUtils(%s)" % (repr(self.hook_events))

    def event_name(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['name']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Type: %s not found" % (caller_name(), type))
            return None

    def hashandler(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['handler'] is not None
        else:
            return None

    def invoke_handler(self, event, cgroup, jobutil, *args):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: UID: real=%d, effective=%d" %
                   (caller_name(), os.getuid(), os.geteuid()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: GID: real=%d, effective=%d" %
                   (caller_name(), os.getgid(), os.getegid()))
        if self.hashandler(event.type):
            result = self.hook_events[event.type][
                'handler'](event, cgroup, jobutil, *args)
            return result
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: %s event not handled by this hook." %
                       (caller_name(), self.event_name(event.type)))

    def __execjob_begin_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Instantiate the NodeConfig class for get_memory_on_node and
        # get_vmem_on node
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Host assigned job resources: %s" %
                   (caller_name(), jobutil.host_resources))

        # Make sure the parent cgroup directories exist
        cgroup.create_paths()

        # Make sure that any old cgroups created in an earlier attempt
        # at creating or running the job are gone
        cgroup.delete(e.job.id)

        # Gather the assigned resources
        cgroup.gather_assigned_resources()
        # Create the cgroup(s) for the job
        cgroup.create_job(e.job.id, node)
        # Configure the new cgroup
        cgroup.configure_job(e.job.id, jobutil.host_resources, node)
        # Initialize resource usage for the job
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        # Write out the assigned resources
        cgroup.write_out_cgroup_host_assigned_resources(e.job.id)
        # Write out the environment variable for the host (pbs_attach)
        if 'devices_name' in cgroup.host_assigned_resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Devices: %s" %
                       cgroup.host_assigned_resources['devices_name'])
            env_list = list()
            if len(cgroup.host_assigned_resources['devices_name']) > 0:
                line = str()
                mics = list()
                gpus = list()
                for key in cgroup.host_assigned_resources['devices_name']:
                    if key.startswith('mic'):
                        mics.append(key[3:])
                    elif key.startswith('nvidia'):
                        gpus.append(key[6:])
                if len(mics) > 0:
                    env_list.append('OFFLOAD_DEVICES="%s"' %
                                    string.join(mics, ','))
                if len(gpus) > 0:
                    # don't put quotes around the values. ex "0" or "0,1".
                    # This will cause it to fail.
                    env_list.append('CUDA_VISIBLE_DEVICES=%s' %
                                    string.join(gpus, ','))

            pbs.logmsg(pbs.EVENT_DEBUG, "ENV_LIST: %s" % env_list)
            cgroup.write_out_cgroup_host_job_env_file(e.job.id, env_list)
        return True

    def __execjob_epilogue_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # The resources_used information has a base type of pbs_resource
        # Set the usage data
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        return True

    def __execjob_end_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Delete the cgroup(s) for the job
        cgroup.delete(e.job.id)
        # Remove the host_assigned_resources and job_env file
        for filename in [cgroup.hook_storage_dir+os.sep+e.job.id,
                         cgroup.host_job_env_filename % e.job.id]:
            try:
                os.remove(filename)
            except OSError:
                pbs.logmsg(pbs.EVENT_DEBUG3, "File: %s not found" % (filename))
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Error removing file: %s" % (filename))
                pass

        return True

    def __execjob_launch_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Add the parent process id to the appropriate cgroups.
        cgroup.add_pids(os.getppid(), jobutil.job.id)
        # FUTURE: Add environment variable to the job environment
        # if job requested mic or gpu
        cgroup.read_in_cgroup_host_assigned_resources(e.job.id)
        if cgroup.host_assigned_resources is not None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "host_assigned_resources: %s" %
                       (cgroup.host_assigned_resources))
            cgroup.setup_job_devices_env()
        return True

    def __exechost_periodic_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Cleanup cgroups for jobs not present on this node
        count = cgroup.cleanup_orphans(e.job_list)
        if ('periodic_resc_update' in cgroup.cfg and
                cgroup.cfg['periodic_resc_update']):
            for job in e.job_list.keys():
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: job is %s" %
                           (caller_name(), job))
                cgroup.update_job_usage(job, e.job_list[job].resources_used)
        # Online nodes that were offlined due to a cgroup not cleaning up
        if count == 0 and cgroup.cfg['online_offlined_nodes']:
            vnode = pbs.event().vnode_list[cgroup.hostname]
            if os.path.isfile(cgroup.offline_file):
                line = 'Orphan cgroup(s) have been cleaned up. '

                # Check with the pbs server to see if the node comment matches
                try:
                    tmp_comment = pbs.server().vnode(cgroup.hostname).comment
                except:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Unable to contact server for node comment")
                    tmp_comment = None
                if tmp_comment == cgroup.offline_msg:
                    line += 'Will bring the node back online'
                    vnode.state = pbs.ND_FREE
                    vnode.comment = None
                else:
                    line += 'However, the comment has changed since the node '
                    line += 'was offlined. Node will remain offline'

                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: %s" % (caller_name(), line))
                # Remove file
                os.remove(cgroup.offline_file)
        return True

    def __exechost_startup_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        cgroup.create_paths()
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))

        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        node.create_vnodes()
        if 'memory' in cgroup.subsystems:
            val = node.get_memory_on_node(cgroup.cfg)
            if val is not None:
                if 'vnode_per_numa_node' in cgroup.cfg:
                    if not cgroup.cfg['vnode_per_numa_node']:
                        hn = node.hostname
                        e.vnode_list[hn].resources_available['mem'] = \
                            pbs.size(val)
                        val_cpus = node.totalcpus
                        if val_cpus is not None:
                            e.vnode_list[hn].resources_available['ncpus'] = \
                                int(val_cpus)
                cgroup.set_node_limit('mem', val)
        if 'memsw' in cgroup.subsystems:
            val = node.get_vmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['vmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('vmem', val)
        if 'hugetlb' in cgroup.subsystems:
            val = node.get_hpmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['hpmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('hpmem', val)
        return True

    def __execjob_attach_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logjobmsg(jobutil.job.id, "%s: Attaching PID %s" %
                      (caller_name(), e.pid))
        # Add the job process id to the appropriate cgroups.
        cgroup.add_pids(e.pid, jobutil.job.id)
        return True

#
# CLASS ShallIRunUtils
#


class ShallIRunUtils:

    def __init__(self, hostname, cfg):
        self.cfg = cfg
        self.hostname = hostname

    def allow_users(self, allow_users):
        """ This still needs to be defined """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pass

    def exclude_hosts(self, exclude_hosts):
        """
        Gets the hostname for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        hostname = pbs.get_local_nodename()
        # check to see if hostname is in the exclude_hosts list
        if self.hostname in exclude_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: exclude host %s is in %s" %
                       (caller_name(), self.hostname, exclude_hosts))
            pbs.event().accept()

        # check to see if it regex syntax is used
        pass

    def exclude_vntypes(self, exclude_vntypes):
        """
        Gets the vntype for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        vntype = None

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'
        if os.path.isfile(vntype_file):
            fdata = open(vntype_file).readlines()
            vntype = fdata[0].strip()
            pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
        if vntype is None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype is set to %s" % (vntype))
        elif vntype in exclude_vntypes:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s is in %s" % (vntype, exclude_vntypes))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Exiting Hook")
            pbs.event().accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s not in global exclude: %s" %
                       (vntype, exclude_vntypes))
        pass

    def run_only_on_hosts(self, approved_hosts):
        """
        Gets the list of approved nodes to run on and checks to see if the hook
        should run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Approved hosts: %s, %s" %
                   (type(approved_hosts), approved_hosts))

        # check to see if hostname is in the approved_hosts list
        if approved_hosts == []:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "approved hosts list is empty: %s" % approved_hosts)
        elif self.hostname not in approved_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s is not in the approved list of hosts: %s" %
                       (self.hostname, approved_hosts))
            pbs.event().accept()

        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s in list of approved hosts: %s" %
                       (self.hostname, approved_hosts))

#
# CLASS JobUtils
#


class JobUtils:

    def __init__(self, job, hostname=None, resources=None):
        self.job = job
        self.host_vnodes = list()
        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if resources is not None:
            self.host_resources = resources
        else:
            self.host_resources = self.__host_assigned_resources()

    def __repr__(self):
        return "JobUtils(%s, %s, %s)" % (repr(self.job), repr(self.hostname),
                                         repr(self.host_resources))

    # Return a dictionary of resources assigned to the local node
    def __host_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Bail out if no hostname was provided
        if self.hostname is None:
            raise CgroupProcessingError('No hostname available')
        # Bail out if no job information is present
        if self.job is None:
            raise CgroupProcessingError('No job information available')
        # Determine which vnodes are on this host, if any
        if self.hostname is not None:
            vnhost_pattern = "%s\[[\d+]\]" % self.hostname
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnhost pattern: %s" %
                       (caller_name(), vnhost_pattern))
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job exec_vnode list: %s" %
                       (caller_name(), self.job.exec_vnode))
            for match in re.findall(vnhost_pattern, str(self.job.exec_vnode)):
                self.host_vnodes.append(match)
            if self.host_vnodes is not None:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Associate %s to vnodes on %s" %
                           (caller_name(), self.host_vnodes, self.hostname))

        # Collect host assigned resources
        resources = {}
        for chunk in self.job.exec_vnode.chunks:
            vnode = False
            if self.host_vnodes == [] and chunk.vnode_name != self.hostname:
                continue
            elif self.host_vnodes != [] and \
                    chunk.vnode_name not in self.host_vnodes:
                continue
            elif chunk.vnode_name in self.host_vnodes:
                vnode = True
                if 'vnodes' not in resources:
                    resources['vnodes'] = {}
                if chunk.vnode_name not in resources['vnodes']:
                    resources['vnodes'][chunk.vnode_name] = {}
                # This check is needed since not all resources are
                # required to be in each chunk of a job
                # i.e. exec_vnodes =
                # (node1[0]:ncpus=4:mem=4gb+node1[1]:mem=2gb) +
                # (node1[1]:ncpus=3+node[0]:ncpus=1:mem=4gb)
                if all(resc in resources['vnodes'][chunk.vnode_name]
                       for resc in chunk.chunk_resources.keys()):
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: resources already defined" %
                               (caller_name()))
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s,\t%s" %
                               (caller_name(),
                                chunk.chunk_resources.keys(),
                                resources['vnodes'][chunk.vnode_name].keys()))
                else:
                    # Initialize resources for each vnode
                    for resc in chunk.chunk_resources.keys():
                        # Initialize the new value
                        if isinstance(chunk.chunk_resources[resc],
                                      pbs.pbs_int):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_int(0)
                        elif isinstance(chunk.chunk_resources[resc],
                                        pbs.pbs_float):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_float(0)
                        elif isinstance(chunk.chunk_resources[resc], pbs.size):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.size('0')

            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Chunk %s" %
                       (caller_name(), chunk.vnode_name))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resources: %s" %
                       (caller_name(), resources))
            for resc in chunk.chunk_resources.keys():
                if resc not in resources.keys():
                    # Initialize the new value
                    if isinstance(chunk.chunk_resources[resc], pbs.pbs_int):
                        resources[resc] = pbs.pbs_int(0)
                    elif isinstance(chunk.chunk_resources[resc],
                                    pbs.pbs_float):
                        resources[resc] = pbs.pbs_float(0.0)
                    elif isinstance(chunk.chunk_resources[resc], pbs.size):
                        resources[resc] = pbs.size('0')
                # Add resource value to total
                if type(chunk.chunk_resources[resc]) in \
                        [pbs.pbs_int, pbs.pbs_float, pbs.size]:
                    resources[resc] += chunk.chunk_resources[resc]
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s: resources[%s][%s] is now %s" %
                               (caller_name(), self.hostname, resc,
                                resources[resc]))
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] += \
                                  chunk.chunk_resources[resc]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Setting resource %s to string %s" %
                               (caller_name(), resc,
                                str(chunk.chunk_resources[resc])))
                    resources[resc] = str(chunk.chunk_resources[resc])
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] = \
                                  str(chunk.chunk_resources[resc])
        if not resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: No resources assigned to host %s" %
                       (caller_name(), self.hostname))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources for %s: %s" %
                       (caller_name(), self.hostname, repr(resources)))

        # Return assigned resources for specified host
        pbs.logmsg(pbs.EVENT_DEBUG3, "Job Resources: %s" % (resources))
        return resources

    # Write a message to the job stdout file
    def write_to_stderr(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stderr_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

    # Write a message to the job stdout file
    def write_to_stdout(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stdout_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

#
# CLASS NodeConfig
#


class NodeConfig:

    def __init__(self, cfg, **kwargs):

        self.cfg = cfg
        hostname = None
        meminfo = None
        numa_nodes = None
        devices = None
        hyperthreading = None
        self.hyperthreads_per_core = 1
        self.totalcpus = self.__discover_cpus()

        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        self.host_mom_jobdir = pbs_home+'/mom_priv/jobs'

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'hostname':
                    hostname = val
                elif arg == 'meminfo':
                    meminfo = val
                elif arg == 'numa_nodes':
                    numa_nodes = val
                elif arg == 'devices':
                    devices = val
                elif arg == 'hyperthreading':
                    hyperthreading = val

        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if meminfo is not None:
            self.meminfo = meminfo
        else:
            self.meminfo = self.__discover_meminfo()
        if numa_nodes is not None:
            self.numa_nodes = numa_nodes
        else:
            self.numa_nodes = self.__discover_numa_nodes()
        if devices is not None:
            self.devices = devices
        else:
            self.devices = self.__discover_devices()
        if hyperthreading is not None:
            self.hyperthreading = hyperthreading
        else:
            self.hyperthreading = self.__hyperthreading_enabled()

        # Add the devices count i.e. nmics and ngpus to the numa nodes
        self.__add_devices_to_numa_node()

    def __repr__(self):
        return ("NodeConfig(%s, %s, %s, %s, %s, %s)" %
                (repr(self.cfg), repr(self.hostname), repr(self.meminfo),
                 repr(self.numa_nodes), repr(self.devices),
                 repr(self.hyperthreading)))

    def __add_devices_to_numa_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (self.devices.keys()))
        for device in self.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" % (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" %
                           (self.devices[device].keys()))
                for device_name in self.devices[device]:
                    device_socket = \
                        self.devices[device][device_name]['numa_node']
                    if device == 'mic':
                        if 'nmics' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['nmics'] = 1
                        else:
                            self.numa_nodes[device_socket]['nmics'] += 1
                    elif device == 'gpu':
                        if 'ngpus' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['ngpus'] = 1
                        else:
                            self.numa_nodes[device_socket]['ngpus'] += 1

        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (self.numa_nodes))

    # Discover what type of hardware is on this node and how is it partitioned
    def __discover_numa_nodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        numa_nodes = {}
        for dir in glob.glob(os.path.join(os.sep,
                             "sys", "devices", "system", "node", "node*")):
            id = int(dir.split(os.sep)[5][4:])
            if id not in numa_nodes.keys():
                numa_nodes[id] = {}
                numa_nodes[id]['devices'] = list()
            numa_nodes[id]['cpus'] = open(os.path.join(dir, "cpulist"),
                                          'r').readline().strip()
            with open(os.path.join(dir, "meminfo"), 'r') as fd:
                for line in fd:
                    # Each line will contain four or five fields. Examples:
                    # Node 0 MemTotal:       32995028 kB
                    # Node 0 HugePages_Total:     0
                    entries = line.split()
                    if len(entries) < 4:
                        continue
                    if entries[2] == "MemTotal:":
                        numa_nodes[id][entries[2].rstrip(':')] = \
                            convert_size(entries[3] + entries[4], 'kb')
                    elif entries[2] == "HugePages_Total:":
                        numa_nodes[id][entries[2].rstrip(':')] = entries[3]
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover Numa Nodes: %s" % numa_nodes)
        return numa_nodes

    # Identify devices and to which numa nodes they are attached
    def __discover_devices(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        devices = {}
        for file in glob.glob(os.path.join(os.sep, "sys", "class", "*", "*",
                                           "device", "numa_node")):
            # The file should contain a single integer
            numa_node = int(open(file, 'r').readline().strip())
            if numa_node < 0:
                numa_node = 0
            dirs = file.split(os.sep)
            name = dirs[3]
            instance = dirs[4]
            if name not in devices.keys():
                devices[name] = {}
            devices[name][instance] = {}
            devices[name][instance]['numa_node'] = numa_node
            if name == 'mic':
                s = os.stat('/dev/%s' % instance)
                devices[name][instance]['major'] = os.major(s.st_rdev)
                devices[name][instance]['minor'] = os.minor(s.st_rdev)
                devices[name][instance]['device_type'] = 'c'

        # Check to see if there are gpus on the node
        gpus = self.discover_gpus()
        if isinstance(gpus, dict) and len(gpus.keys()) > 0:
            devices['gpu'] = gpus['gpu']

        pbs.logmsg(pbs.EVENT_DEBUG3, "Discovered devices: %s" % (devices))
        return devices

    def discover_gpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Check to see if nvidia-smi exists and produces valid output
            cmd = ['/usr/bin/nvidia-smi', '-q', '-x']
            if 'nvidia-smi' in self.cfg:
                cmd[0] = self.cfg['nvidia-smi']

            pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
            # Collect the gpu information
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.PIPE)
            output, err = process.communicate()

            # pbs.logmsg(pbs.EVENT_DEBUG3,"output: %s" % output)
            # import the xml library and parse the output
            import xml.etree.ElementTree as ET

            # Parse the data
            name = 'gpu'
            gpu_data = dict()
            gpu_data[name] = {}
            root = ET.fromstring(output)
            pbs.logmsg(pbs.EVENT_DEBUG3, "root.tag: %s" % root.tag)
            for child in root:
                if child.tag == "attached_gpus":
                    # gpu_data['ngpus'] = int(child.text)
                    pass
                if child.tag == "gpu":
                    device_id = child.get('id')
                    number = 'nvidia%s' % child.find('minor_number').text
                    gpu_data[name][number] = dict()
                    gpu_data[name][number]['id'] = device_id

                    # Determine which socket the device is on
                    filename = '/sys/bus/pci/devices/%s/numa_node' % \
                        device_id.lower()
                    isfile = os.path.isfile(filename)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s, %s" % (filename, isfile))
                    if isfile:
                        numa_node = open(filename).read().strip()
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "numa_node: %s" % (numa_node))
                        if int(numa_node) == -1:
                            numa_node = 0
                        gpu_data[name][number]['numa_node'] = int(numa_node)
                        try:
                            dev_filename = '/dev/%s' % number
                            s = os.stat(dev_filename)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find %s" % dev_filename)
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                        gpu_data[name][number]['major'] = os.major(s.st_rdev)
                        gpu_data[name][number]['minor'] = os.minor(s.st_rdev)
                        gpu_data[name][number]['device_type'] = 'c'
            pbs.logmsg(pbs.EVENT_DEBUG, "%s" % gpu_data)
            return gpu_data
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unable to find %s" % string.join(cmd, " "))
            return {'gpu': {}}
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        return {'gpu': {}}

    # Get the memory info on this host
    def __discover_meminfo(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        meminfo = {}
        with open(os.path.join(os.sep, "proc", "meminfo"), 'r') as fd:
            for line in fd:
                entries = line.split()
                if entries[0] == "MemTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "SwapTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "Hugepagesize:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "HugePages_Total:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
                elif entries[0] == "HugePages_Rsvd:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover meminfo: %s" % meminfo)
        return meminfo

    # Determine if the cpus have hyperthreading enabled
    def __hyperthreading_enabled(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            siblings = 0
            cpu_cores = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "siblings":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    siblings = entries[1].strip()
                elif entries[0].strip() == "cpu cores":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_cores = entries[1].strip()
                elif entries[0].strip() == "flags":
                    flag_options = entries[1].split()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: flag_options: %s" %
                               (caller_name(), flag_options))
                    if "ht" in flag_options:
                        rc = True
                        break
        if rc is True:
            self.hyperthreads_per_core = int(siblings)/int(cpu_cores)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: hyperthreads/core: %d" %
                       (caller_name(), self.hyperthreads_per_core))
            if self.hyperthreads_per_core == 1:
                rc = False
        return rc

    def __discover_cpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            cpu_threads = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "processor":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_threads += 1
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: processors found: %d" %
                   (caller_name(), cpu_threads))

        return cpu_threads

    # Gather the jobs running on this node
    def gather_jobs_on_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        # Get the job list from the server
        jobs = None
        try:
            jobs = pbs.server().vnode(self.hostname).jobs
        except:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Unable to contact server to get jobs")
            return None

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs from server for %s: %s" % (self.hostname, jobs))

        if jobs is None:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "No jobs found on %s " % (self.hostname))
            return list()

        jobs_list = dict()
        for job in jobs.split(','):
            # The job list from the node has a space in front of the job id
            # This must be removed or it will not match the cgroup dir
            jobs_list[job.split('/')[0].strip()] = 0
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs on %s: %s" % (self.hostname, jobs_list.keys()))

        return jobs_list.keys()

    # Gather the jobs running on this node
    def gather_jobs_on_node_local(self):
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Method called" % (caller_name()))
        # Get the job list from the jobs directory
        jobs_list = list()
        try:
            for file in os.listdir(self.host_mom_jobdir):
                if fnmatch.fnmatch(file, "*.JB"):
                    jobs_list.append(file[:-3])
            if not jobs_list:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "No jobs found on %s " % (self.hostname))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Jobs from server for %s: %s" %
                           (self.hostname, repr(jobs_list)))

            return jobs_list
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Could not get job list from mom_priv/jobs for %s" %
                       self.hostname)

            return self.gather_jobs_on_node()

    # Get the memory resource on this mom
    def get_memory_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Calculate memory
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
            pbs.logmsg(pbs.EVENT_DEBUG3, "val: %s" % val)
        else:
            val = size_as_int(MemTotal)
        if 'percent_reserve' in config['cgroup']['memory']:
            percent_reserve = config['cgroup']['memory']['percent_reserve']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memory'].keys():
            reserve_memory = config['cgroup']['memory']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                reserve_mem = size_as_int(reserve_memory)
                if val > reserve_mem:
                    val -= reserve_mem
                else:
                    val -= size_as_int("100mb")
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Unable to reserve more memory than " +
                               "available on the node. Available %s, " +
                               "Tried to reserve %s. Reserving 100mb" %
                               (val, reserve_mem))

            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))

        pbs.logmsg(pbs.EVENT_DEBUG3, "Return val: %s" % val)
        return convert_size(str(val), 'kb')

    # Get the virtual memory resource on this mom
    def get_vmem_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Return the value for memory if no swap is configured
        if size_as_int(self.meminfo['SwapTotal']) <= 0:
            return self.get_memory_on_node(config)
        # Calculate vmem
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
        else:
            val = size_as_int(MemTotal)

        val += size_as_int(self.meminfo['SwapTotal'])
        # Determine if memory needs to be reserved
        if 'percent_reserve' in config['cgroup']['memsw']:
            percent_reserve = config['cgroup']['memsw']['reserve_memory']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memsw']:
            reserve_memory = config['cgroup']['memsw']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                val -= size_as_int(reserve_memory)
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))
        return convert_size(str(val), 'kb')

    # Get the huge page memory resource on this mom
    def get_hpmem_on_node(self, config):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        if 'percent_reserve' in config['cgroup']['hugetlb'].keys():
            percent_reserve = config['cgroup']['hugetlb']['percent_reserve']
        else:
            percent_reserve = 0
        # Calculate vmem
        val = size_as_int(self.meminfo['Hugepagesize'])
        val *= (self.meminfo['HugePages_Total'] -
                self.meminfo['HugePages_Rsvd'])
        val -= (val * percent_reserve) / 100
        return convert_size(str(val), 'kb')

    # Create individual vnodes per socket
    def create_vnodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        key = "vnode_per_numa_node"
        if key in self.cfg.keys():
            if not self.cfg[key]:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s is false" %
                           (caller_name(), key))
                return
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s not configured" %
                       (caller_name(), key))
            return

        # Create one vnode per numa node
        vnode_list = pbs.event().vnode_list
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: numa nodes: %s" %
                   (caller_name(), self.numa_nodes))
        # Define the vnodes
        vnode_name = self.hostname
        vnode_list[vnode_name] = pbs.vnode(vnode_name)
        for id in self.numa_nodes.keys():
            vnode_name = self.hostname + "[%d]" % id
            vnode_list[vnode_name] = pbs.vnode(vnode_name)
            for key, val in sorted(self.numa_nodes[id].iteritems()):
                if key == 'cpus':
                    vnode_list[vnode_name].resources_available['ncpus'] = \
                        len(cpus2list(val))/self.hyperthreads_per_core
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['ncpus'] = 0
                elif key == 'MemTotal':
                    mem = self.get_memory_on_node(self.cfg, val)
                    vnode_list[vnode_name].resources_available['mem'] = \
                        pbs.size(mem)
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['mem'] = \
                        pbs.size('0kb')
                elif key == 'HugePages_Total':
                    pass
                elif isinstance(val, list):
                    pass
                elif isinstance(val, dict):
                    pass
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s=%s" %
                               (caller_name(), key, val))
                    vnode_list[vnode_name].resources_available[key] = val

                    if isinstance(val, int):
                        vnode_list[self.hostname].resources_available[key] = 0
                    elif isinstance(val, float):
                        vnode_list[self.hostname].resources_available[key] = \
                            0.0
                    elif isinstance(val, pbs.size):
                        vnode_list[self.hostname].resources_available[key] = \
                            pbs.size('0kb')
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnode list: %s" %
                   (caller_name(), vnode_list))
        return True

#
# CLASS CgroupUtils
#


class CgroupUtils:
    def __init__(self, hostname, vnode, **kwargs):
        cfg = None
        subsystems = None
        paths = None
        vntype = None
        event = None
        self.assigned_resources = dict()
        self.existing_cgroups = dict()
        self.host_assigned_resources = None
        self.pid_lower_limit = 3

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'subsystems':
                    subsystems = val
                elif arg == 'paths':
                    paths = val
                elif arg == 'vntype':
                    vntype = val
                elif arg == 'event':
                    event = val

        try:
            self.hostname = hostname
            self.vnode = vnode
            # __check_os will raise an exception if cgroups are not present
            self.__check_os()
            # Read in the config file
            if cfg is not None:
                self.cfg = cfg
            else:
                self.cfg = self.__parse_config_file()

            # Determine if the hook should run or exit
            siru = ShallIRunUtils(self.hostname, self.cfg)
            siru.exclude_hosts(self.cfg['exclude_hosts'])
            siru.exclude_vntypes(self.cfg['exclude_vntypes'])
            siru.run_only_on_hosts(self.cfg['run_only_on_hosts'])

            # Collect the mount points
            if paths is not None:
                self.paths = paths
            else:
                self.paths = self.__set_paths()
            # Define the local vnode type
            if vntype is not None:
                self.vntype = vntype
            else:
                self.vntype = self.__get_vnode_type()
            # Determine which subsystems we care about
            if subsystems is not None:
                self.subsystems = subsystems
            else:
                self.subsystems = self.__target_subsystems()

            # location to store information for the different hook events
            pbs_home = ''
            if not CRAY_12_BRANCH:
                pbs_conf = pbs.get_pbs_conf()
                pbs_home = pbs_conf['PBS_HOME']
            else:
                if 'PBS_HOME' in self.cfg:
                    pbs_home = self.cfg['PBS_HOME']
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "PBS_HOME needs to be defined in the " +
                               "config file")
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Exiting the cgroups hook")
                    pbs.accept()

            self.hook_storage_dir = pbs_home+'/mom_priv/hooks/hook_data'
            self.host_job_env_dir = pbs_home+'/aux'
            self.host_job_env_filename = self.host_job_env_dir+os.sep+"%s.env"

            # information for offlining nodes
            self.offline_file = \
                pbs_home+os.sep+'mom_priv'+os.sep+'hooks'+os.sep
            self.offline_file += "%s.offline" % pbs.event().hook_name
            self.offline_msg = "Hook %s: " % pbs.event().hook_name
            self.offline_msg += "Unable to clean up one or more cgroups"

        except:
            raise

    def __repr__(self):
        return ("CgroupUtils(%s, %s, %s, %s, %s, %s)" %
                (repr(self.hostname), repr(self.vnode), repr(self.cfg),
                 repr(self.subsystems), repr(self.paths), repr(self.vntype)))

    # Validate the OS type and version
    def __check_os(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check to see if the platform is linux and the kernel is new enough
        if platform.system() != 'Linux':
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: OS does not support cgroups" %
                       (caller_name()))
            raise CgroupConfigError("OS type not supported")
        rel = map(int,
                  string.split(string.split(platform.release(), '-')[0], '.'))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Detected Linux kernel version %d.%d.%d" %
                   (caller_name(), rel[0], rel[1], rel[2]))
        supported = False
        if rel[0] > 2:
            supported = True
        elif rel[0] == 2:
            if rel[1] > 6:
                supported = True
            elif rel[1] == 6:
                if rel[2] >= 28:
                    supported = True
        if not supported:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Kernel needs to be >= 2.6.28: %s" %
                       (caller_name(), system_info[2]))
            raise CgroupConfigError("OS version not supported")
        return supported

    # Read the config file in json format
    def __parse_config_file(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        config = {}
        # Identify the config file and read in the data
        if 'PBS_HOOK_CONFIG_FILE' in os.environ:
            config_file = os.environ["PBS_HOOK_CONFIG_FILE"]
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Config file is %s" %
                       (caller_name(), config_file))
            try:
                config = json.load(open(config_file, 'r'),
                                   object_hook=decode_dict)
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
        else:
            raise CgroupConfigError("No configuration file present")
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Initial Cgroup Config: %s" %
                   (caller_name(), config))
        # Set some defaults if they are not present
        if 'cgroup_prefix' not in config.keys():
            config['cgroup_prefix'] = 'pbspro'
        if 'periodic_resc_update' not in config.keys():
            config['periodic_resc_update'] = False
        if 'vnode_per_numa_node' not in config.keys():
            config['vnode_per_numa_node'] = False
        if 'online_offlined_nodes' not in config.keys():
            config['online_offlined_nodes'] = False
        if 'exclude_hosts' not in config.keys():
            config['exclude_hosts'] = []
        if 'run_only_on_hosts' not in config.keys():
            config['run_only_on_hosts'] = []
        if 'exclude_vntypes' not in config.keys():
            config['exclude_vntypes'] = []
        if 'cgroup' not in config.keys():
            config['cgroup'] = {}
        else:
            for subsys in config['cgroup']:
                subsystem = config['cgroup'][subsys]
                if 'enabled' not in subsystem.keys():
                    config['cgroup'][subsys]['enabled'] = False
                if 'exclude_hosts' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_hosts'] = []
                if 'exclude_vntypes' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_vntypes'] = []

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Cgroup Config with defaults: %s" %
                   (caller_name(), config))
        return config

    # Determine which subsystems are being requested
    def __target_subsystems(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        subsystems = []
        for key in self.cfg['cgroup'].keys():
            if self.enabled(key):
                subsystems.append(key)
        if 'memory' not in subsystems:
            if 'memsw' in subsystems:
                # Remove memsw since it must be greater then
                # or equal to memory
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing memsw from enabled subsystems")
                subsystems.remove('memsw')
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Enabled subsystems: %s" %
                   (caller_name(), subsystems))
        # It is not an error for all subsystems to be disabled.
        # This host or vnode type may be in the excluded list.
        return subsystems

    # Copy a setting from the parent cgroup
    def __copy_from_parent(self, dest):
        try:
            file = os.path.basename(dest)
            dir = os.path.dirname(dest)
            parent = os.path.dirname(dir)
            source = os.path.join(parent, file)
            if not os.path.isfile(source):
                raise CgroupConfigError('Failed to read %s' % (source))
            self.write_value(dest, open(source, 'r').read().strip())
        except:
            raise

    # Create a dictionary of the cgroup subsystems and their corresponding
    # directories
    def __set_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        paths = {}
        try:
            # Loop through the mounts and collect the ones for cgroups
            with open(os.path.join(os.sep, "proc", "mounts"), 'r') as fd:
                for line in fd:
                    entries = line.split()
                    if len(entries) > 3 and entries[2] == "cgroup":
                        flags = entries[3].split(',')
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "subsysdir: %s (flags=%s)" %
                                   (entries[1], flags))
                        for subsys in flags:
                            paths[subsys] = os.path.join(
                                entries[1], self.cfg["cgroup_prefix"])
        except:
            raise
        if len(paths.keys()) < 1:
            raise CgroupConfigError("Cgroup paths not detected")
        # Both the memory and memsw cgroups share the same mount point
        if "memory" in paths.keys():
            paths["memsw"] = paths["memory"]
        return paths

    # Create the cgroup parent directories that will contain the jobs
    def create_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Create the directories that PBS will use to house the jobs
            old_umask = os.umask(0022)
            for subsys in self.subsystems:
                if subsys not in self.paths.keys():
                    raise CgroupConfigError('No path for subsystem: %s' %
                                            (subsys))
                if not os.path.exists(self.paths[subsys]):
                    os.makedirs(self.paths[subsys], 0755)
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Created directory %s" %
                               (caller_name(), self.paths[subsys]))
                    if subsys == 'memory' or subsys == 'memsw':
                        # Set file to
                        # <cgroup>/memory/pbspro/memory.use_hierarchy
                        file = os.path.join(self.paths[subsys],
                                            'memory.use_hierarchy')
                        if not os.path.isfile(file):
                            raise CgroupConfigError('Failed to configure %s' %
                                                    (file))
                        self.write_value(file, 1)
                    elif subsys == 'cpuset':
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.cpus'))
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.mems'))

        except:
            raise CgroupConfigError("Failed to create directory: %s" %
                                    (self.paths[subsys]))
        finally:
            os.umask(old_umask)

    # Return the vnode type of the local node
    def __get_vnode_type(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")

        # File to check for if it is not defined in the hook input file
        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'

        # self.vnode is None for pbs_attach events
        pbs.logmsg(pbs.EVENT_DEBUG3, "vnode: %s" % self.vnode)
        if self.vnode is not None:
            if 'vntype' in self.vnode.resources_available and \
                    self.vnode.resources_available['vntype'] is not None:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "vntype: %s" %
                           self.vnode.resources_available['vntype'])
                return self.vnode.resources_available['vntype']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3, "vntype file: %s" % vntype_file)
                if os.path.isfile(vntype_file):
                    fdata = open(vntype_file).readlines()
                    vntype = fdata[0].strip()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
                    return vntype
        # If vntype was not set then log a message. It is too expensive
        # to have all moms query the server for large jobs.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: resources_available.vntype is " % caller_name() +
                   "not set for this vnode")
        return None

    # Gather resources that are already assigned
    def gather_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        assigned_resources = {}

        # Gather the cpus and memory sockets assigned
        directories = [x[0] for x in os.walk(self.paths['cpuset'])]

        # Add the existing cgroups to a list to check if assigning
        # resources fail
        for dir in directories[1:]:
            if dir.find(pbs.event().job.id) == -1:
                self.existing_cgroups[dir] = []

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'cpuset' in self.subsystems and \
                self.cfg['cgroup']['cpuset']['enabled']:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather cpuset resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['cpuset'], tmp_dir)
                    with open(path+os.sep+'cpuset.cpus') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.cpus'] = \
                            cpus2list(tmp_val.strip())
                    with open(path+os.sep+'cpuset.mems') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.mems'] = \
                            cpus2list(tmp_val.strip())

        # Check to see if the subsystem is enabled before trying
        # to gather resources
        if 'memory' in self.subsystems and \
                self.cfg['cgroup']['memory']['enabled']:
            # Gather the memory assigned for each socket
            directories = [x[0] for x in os.walk(self.paths['memory'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather memory resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['memory'], tmp_dir)
                    with open(path+os.sep+'memory.limit_in_bytes') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memory.limit'] = \
                            tmp_val.strip()
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    if os.path.isfile(tmp_filename) is False:
                        continue
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    with open(tmp_filename) as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memsw.limit'] = \
                            tmp_val.strip()

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'devices' in self.subsystems and \
                self.cfg['cgroup']['devices']['enabled']:
            # Gather the devices assigned
            directories = [x[0] for x in os.walk(self.paths['devices'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather device resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    pbs.logmsg(pbs.EVENT_DEBUG3, "Dir: %s" % tmp_dir)
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['devices'], tmp_dir)
                    with open(path+os.sep+'devices.list') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['devices.list'] = \
                            tmp_val.strip().split('\n')
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Dir: %s\n\tValue: %s" %
                                   (path, tmp_val.replace('\n', ',')))

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Gathered assigned resources: %s" % assigned_resources)
        self.assigned_resources = assigned_resources

    # Return whether a subsystem is enabled
    def enabled(self, subsystem):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check whether the subsystem is enabled in the configuration file
        if subsystem not in self.cfg['cgroup'].keys():
            return False
        if 'enabled' not in self.cfg['cgroup'][subsystem].keys():
            return False
        if not self.cfg['cgroup'][subsystem]['enabled']:
            return False
        # Check whether the cgroup is mounted for this subsystem
        if subsystem not in self.paths.keys():
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup not mounted for %s" %
                       (caller_name(), subsystem))
            return False
        # Check whether this host is excluded
        # if self.hostname in self.cfg['exclude_hosts']:
        #    pbs.logmsg(pbs.EVENT_DEBUG, "%s: cgroup excluded on host %s" %
        #               (caller_name(), self.hostname))
        #    pbs.event().accept()
        if self.hostname in self.cfg['cgroup'][subsystem]['exclude_hosts']:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup excluded for subsystem %s on host %s" %
                       (caller_name(), subsystem, self.hostname))
            return False
        # Check whether the vnode type is excluded
        if self.vntype is not None:
            # if self.vntype in self.cfg['exclude_vntypes']:
            #    pbs.logmsg(pbs.EVENT_DEBUG,
            #               "%s: cgroup excluded on vnode type %s" %
            #               (caller_name(), self.vntype))
            #    pbs.event().accept()
            if self.vntype in self.cfg['cgroup'][subsystem]['exclude_vntypes']:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           ("%s: cgroup excluded for " +
                            "subsystem %s on vnode type %s") %
                           (caller_name(), subsystem, self.vntype))
                return False
        return True

    # Return the default value for a subsystem
    def default(self, subsystem):
        if subsystem in self.cfg['cgroup']:
            if 'default' in self.cfg['cgroup'][subsystem]:
                return self.cfg['cgroup'][subsystem]['default']
        return None

    # Check to see if the pid matches the job owners uid
    def is_pid_owned_by_job_owner(self, pid):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        try:
            proc_uid = os.stat('/proc/%d' % pid).st_uid
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       '/proc/%d uid:%d' % (pid, proc_uid))

            job_owner_uid = pwd.getpwnam(pbs.event().job.euser)[2]
            pbs.logmsg(pbs.EVENT_DEBUG3, "Job uid: %d" % job_owner_uid)

            if proc_uid == job_owner_uid:
                return True
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Proc uid: %d != Job owner:  %d" %
                           (proc_uid, job_owner_uid))
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG, "Unknown pid: %d" % pid)
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        # If we got to this point something did not match up
        return False

    # Add some number of PIDs to the cgroup tasks files for each subsystem
    def add_pids(self, pids, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # make pids a list
        if isinstance(pids, int):
            pids = [pids]

        if pbs.event().type == pbs.EXECJOB_LAUNCH:
            if 1 in pids:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "1 is not a valid pid to add")
                # Hold the job for further review
                e.reject("Hook tried to add pid 1 to the tasks file " +
                         "for job %s" % jobid)

        # check pids to make sure that they are owned by the job owner
        if not CRAY_12_BRANCH:
            pbs.logmsg(pbs.EVENT_DEBUG3, "Not in the Cray 12 Branch")
            pbs.logmsg(pbs.EVENT_DEBUG3, "check to see if execjob_attach")
            if pbs.event().type == pbs.EXECJOB_ATTACH:
                pbs.logmsg(pbs.EVENT_DEBUG3, "event type: attach or launch")
                tmp_pids = list()
                for p in pids:
                    if self.is_pid_owned_by_job_owner(p):
                        tmp_pids.append(p)
                if len(tmp_pids) == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%d is not a valid pids to add" % p)
                    return False
                else:
                    pids = tmp_pids

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: subsys = %s" %
                       (caller_name(), subsys))
            # memsw and memory use the same tasks file
            if subsys == "memsw":
                if "memory" in self.subsystems:
                    continue
            path = os.path.join(self.paths[subsys], jobid)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path = %s" %
                       (caller_name(), path))
            try:
                tasks_path = os.path.join(path, "tasks")
                for p in pids:
                    self.write_value(tasks_path, p, 'a')
            except IOError, exc:
                raise CgroupLimitError("Failed to add PIDs %s to %s (%s)" %
                                       (str(pids), tasks_path,
                                        errno.errorcode[exc.errno]))
            except:
                raise

    # Set a node limit
    def set_node_limit(self, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s = %s" %
                   (caller_name(), resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'],
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Node resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    def setup_job_devices_env(self):
        """ Setup the job environment for the devices assigned to the job for an
            execjob_launch hook
         """
        if 'devices_name' in self.host_assigned_resources:
            names = self.host_assigned_resources['devices_name']
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "devices: %s" % (names))
            offload_devices = list()
            cuda_visible_devices = list()
            for name in names:
                if name.startswith('mic'):
                    offload_devices.append(name[3:])
                elif name.startswith('nvidia'):
                    cuda_visible_devices.append(name[6:])
            if len(offload_devices) > 0:
                value = '"%s"' % string.join(offload_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['OFFLOAD_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "offload_devices: %s" % offload_devices)
            if len(cuda_visible_devices) > 0:
                value = '"%s"' % string.join(cuda_visible_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['CUDA_VISIBLE_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "cuda_visible_devices: %s" % cuda_visible_devices)
            return [offload_devices, cuda_visible_devices]
        else:
            return False

    def setup_subsys_devices(self, path, node):
        subsys = 'devices'
        # Add the devices that the user is allowed to use
        if subsys in self.cfg['cgroup']:
            devices_allowed = open(os.path.join(path,
                                   "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Initial devices.list: %s" %
                       devices_allowed)
            # Deny access to mic and gpu devices
            devices_list = list()
            devices = node.devices
            for device in devices:
                if device == 'mic' or device == 'gpu':
                    for name in devices[device]:
                        d = devices[device][name]
                        devices_list.append("%d:%d" % (d['major'], d['minor']))
            # For CentOS 7 we need to remove a *:* rwm from devices.list we
            # before can add anything to devices.allow. Otherwise our changes
            # are ignored. Check to see if a *:* rwm is in devices.list.
            # If so remove it
            if "a *:* rwm\n" in devices_allowed:
                value = "a *:* rwm"
                self.write_value(os.path.join(path, 'devices.deny'), value)
            else:
                # Verify that the following devices are not in devices.list
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing access to the following: %s" %
                           devices_list)
            for device_str in devices_list:
                value = "c %s rwm" % device_str
                self.write_value(os.path.join(path, 'devices.deny'), value)
            # Add devices back to the list
            devices_allow = list()
            if 'allow' in self.cfg['cgroup'][subsys]:
                devices_allow = self.cfg['cgroup'][subsys]['allow']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Allowing access to the following: %s" %
                           devices_allow)
                for item in devices_allow:
                    if isinstance(item, str):
                        pbs.logmsg(pbs.EVENT_DEBUG3, "string item: %s" % item)
                        value = "%s" % item
                        self.write_value(os.path.join(
                                         path, 'devices.allow'), value)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "write_value: %s" % value)
                    elif isinstance(item, list):
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "list item: %s" % item)
                        try:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Device allow: %s" % item)
                            stat_filename = '/dev/%s' % item[0]
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Stat file: %s" % stat_filename)
                            s = os.stat(stat_filename)
                            device_type = None
                            if S_ISBLK(s.st_mode):
                                device_type = "b"
                            elif S_ISCHR(s.st_mode):
                                device_type = "c"
                            if device_type is not None:
                                if len(item) == 3 and isinstance(item[2], str):
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % item[2]
                                    value += "%s" % item[1]
                                else:
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % os.minor(s.st_rdev)
                                    value += "%s" % item[1]
                                self.write_value(os.path.join(
                                                path, 'devices.allow'), value)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "write_value: %s" % value)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find /dev/%s. " % item[0] +
                                       "Not added to devices.allow!")
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                    else:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Not sure what to do with: %s" % item)
            devices_allowed = open(os.path.join(
                                   path, "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "After setup devices.list: %s" % devices_allowed)

    # Select devices to assign to the job
    def assign_devices(
                       self,
                       device_type,
                       device_list,
                       number_of_devices,
                       node):
        devices = device_list[:number_of_devices]
        device_ids = list()
        device_names = list()
        for device in devices:
            device_info = node.devices[device_type][device]
            if len(device_ids) == 0:
                if device.find("mic") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:0 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                    device_ids.append("%s %d:1 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                elif device.find("nvidia") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:255 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
            device_ids.append("%s %d:%d rwm" %
                              (device_info['device_type'],
                               device_info['major'],
                               device_info['minor']))
            device_names.append(device)
        return device_names, device_ids

    # Find the device name
    def get_device_name(self, node, available, socket, major, minor):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Get device name: major: %s, minor: %s" % (major, minor))
        avail_device = None
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Possible devices: %s" % (available[socket]['devices']))
        for avail_device in available[socket]['devices']:
            avail_major = None
            avail_minor = None
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Checking device: %s" % (avail_device))
            if avail_device.find('mic') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check mic device: %s" % (avail_device))
                avail_major = node.devices['mic'][avail_device]['major']
                avail_minor = node.devices['mic'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            elif avail_device.find('nvidia') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check gpu device: %s" % (avail_device))
                avail_major = node.devices['gpu'][avail_device]['major']
                avail_minor = node.devices['gpu'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            if int(avail_major) == int(major) and \
                    int(avail_minor) == int(minor):
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device match: name: %s, major: %s, minor: %s" %
                           (avail_device, major, minor))
                return avail_device
        pbs.logmsg(pbs.EVENT_DEBUG3, "No match found")
        return None

    # Assign resources to the job based off the vnodes assignments
    def assign_job_resources_by_vnode(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # Loop through the vnodes and assign resources
        assigned = dict()
        assigned['cpuset.cpus'] = list()
        assigned['cpuset.mems'] = list()
        room_on_socket = True

        for vnode in resources['vnodes']:
            regex = re.compile(".*\[(\d+)\].*")
            socket = int(regex.search(vnode).group(1))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current Vnode: %s" % vnode)
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current socket: %d" % socket)

            if 'ncpus' in resources['vnodes'][vnode]:
                tmp_ncpus = int(resources['vnodes'][vnode]['ncpus'])
                if int(resources['vnodes'][vnode]['ncpus']) <= \
                        len(available[socket]['cpus']):
                    assigned['cpuset.cpus'] += \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] += [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_ncpus: %s" % (tmp_ncpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "available[%d]: %s" %
                               (socket, available[socket]))
                    room_on_socket = False
            if ('nmics' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['nmics']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(mic).*")
                tmp_nmics = int(resources['vnodes'][vnode]['nmics'])
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['nmics']) > 0 and
                   int(resources['vnodes'][vnode]['nmics']) <= len(mics)):
                    tmp_names, tmp_devices = self.assign_devices(
                             'mic', mics[:tmp_nmics], tmp_nmics, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_nmics: %s" % (tmp_nmics))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "mics: %s" % (mics))
                    room_on_socket = False
            if ('ngpus' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['ngpus']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(nvidia).*")
                tmp_ngpus = int(resources['vnodes'][vnode]['ngpus'])
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['ngpus']) > 0 and
                   int(resources['vnodes'][vnode]['ngpus']) <= len(gpus)):
                    tmp_names, tmp_devices = self.assign_devices(
                        'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "tmp_ngpus: %s" % (tmp_ngpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "gpus: %s" % (gpus))
                    room_on_socket = False

        if room_on_socket:
            assigned['cpuset.cpus'].sort()
            assigned['cpuset.mems'].sort()
            if 'devices' in assigned:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids pre set and sort: %s" %
                           (assigned['devices']))
                assigned['devices'] = list(set(assigned['devices']))
                assigned['devices'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids post set and sort: %s" %
                           (assigned['devices']))
                assigned['devices_name'] = list(set(assigned['devices_name']))
                assigned['devices_name'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

    # Assign resources to the job
    def assign_job_resources(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # What would we need to do different for a large smp (UV) system?
        # See if requested resources can fit on a single socket
        assigned = dict()
        pbs.logmsg(pbs.EVENT_DEBUG3, "Requested: %s" % (resources))

        # Determine the order that we should evaluate the sockets.
        socket_list = list()
        if 'placement_type' in self.cfg:
            if self.cfg['placement_type'] == 'round_robin':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Requested round_robin placement")
                # Look at assigned_resources and determine which socket
                # to start with
                jobs_per_socket_cnt = dict()
                for socket in available.keys():
                    jobs_per_socket_cnt[socket] = 0
                for job in self.assigned_resources:
                    if 'cpuset.mems' in self.assigned_resources[job]:
                        for socket in \
                                self.assigned_resources[job]['cpuset.mems']:
                            jobs_per_socket_cnt[socket] += 1

                sorted_dict = sorted(jobs_per_socket_cnt.items(),
                                     key=operator.itemgetter(1))
                for x in sorted_dict:
                    socket_list.append(x[0])
        else:
            socket_list = available.keys()

        # Loop through the socket list and determine if the job
        # can fit on the socket
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Evaluate sockets in the following order: %s" %
                   (socket_list))
        for socket in socket_list:
            room_on_socket = True
            if 'ncpus' in resources:
                if int(resources['ncpus']) <= len(available[socket]['cpus']):
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Requested: %s, Available: %s" %
                               (resources['ncpus'], available[socket]['cpus']))
                    tmp_ncpus = int(resources['ncpus'])
                    assigned['cpuset.cpus'] = \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] = [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient ncpus on socket %d: R:%d,A:%s" %
                               (socket, resources['ncpus'],
                                available[socket]['cpus']))
                    room_on_socket = False
            # Check to see that nmics has been requested and not equal to 0
            if 'nmics' in resources and int(resources['nmics']) > 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'nmics' in resources and int(resources['nmics']) > 0 \
                        and int(resources['nmics']) <= len(mics):
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient nmics on socket %d: R:%s,A:%s" %
                               (socket, resources['nmics'], mics))
                    room_on_socket = False
            # Check to see that ngpus has been requested and not equal to 0
            if 'ngpus' in resources and int(resources['ngpus']) > 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'ngpus' in resources and int(resources['ngpus']) > 0 \
                        and int(resources['ngpus']) <= len(gpus):
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient gpus on socket %d: R:%s,A:%s" %
                               (socket, resources['ngpus'], gpus))
                    room_on_socket = False
            if 'vmem' in resources:
                if size_as_int(resources['vmem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient vmem on socket %d: R:%s,A:%s" %
                               (socket, resources['vmem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if 'mem' in resources:
                if size_as_int(resources['mem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient memory on socket %d: R:%s,A:%s" %
                               (socket, resources['mem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if room_on_socket:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
                self.host_assigned_resources = assigned
                return assigned
        # If we made it here then the requested resources did not fit
        # on a single socket
        # Future: take distance into account when selecting sockets?
        # Combine available resources into a single list
        # This should work for a dual socket machine.
        # Needs additional work for machines with more than 2 sockets
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Unable to find requested resources: %s" % resources +
                   " on a socket. Checking the entire node")
        available_copy = copy.deepcopy(available)
        available_on_node = dict()
        socket_keys = available.keys()
        socket_keys.sort()
        available_on_node['sockets'] = socket_keys
        for socket in socket_keys:
            for key in available_copy[socket].keys():
                if isinstance(available[socket][key], int):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]
                if isinstance(available_copy[socket][key], str):
                    try:
                        if key in available_on_node is False:
                            available_on_node[key] = \
                                size_as_int(available_copy[socket][key])
                        else:
                            available_on_node[key] += \
                                size_as_int(available_copy[socket][key])
                    except:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Unable to convert to int: %s" %
                                   (available_copy[socket][key]))

                if isinstance(available_copy[socket][key], list):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available on node: %s" % (available_on_node))
        assigned = dict()
        room_on_node = False
        if 'ncpus' in resources:
            if int(resources['ncpus']) <= len(available_on_node['cpus']):
                room_on_node = True
                tmp_ncpus = int(resources['ncpus'])
                assigned['cpuset.cpus'] = available_on_node['cpus'][:tmp_ncpus]
                assigned['cpuset.mems'] = available_on_node['sockets']
            if 'nmics' in resources and int(resources['nmics']) != 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['nmics']) <= len(mics) and \
                        int(resources['nmics']) > 0:
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
            if 'ngpus' in resources and int(resources['ngpus']) != 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['ngpus']) <= len(gpus) and \
                        int(resources['ngpus']) > 0:
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
        if room_on_node:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

        # Consolidate resources if needed to place the job

    # Determine which resources are available
    def available_node_resources(self, node):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))

        available = copy.deepcopy(node.numa_nodes)
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Keys: %s" % (available[0].keys()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        for socket in available.keys():
            if 'cpus' in available[socket]:
                # Find the physical cores
                if available[socket]['cpus'].find('-') != -1 and \
                        node.hyperthreading:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Splitting %s at ','" %
                               (available[socket]['cpus']))
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'].split(',')[0])
                else:
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'])
            if 'MemTotal' in available[socket]:
                # Find the memory on the socket in bites.
                # Remove the 'b' to simply math
                available[socket]['memory'] = size_as_int(
                    available[socket]['MemTotal'])

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Pre device add: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (node.devices.keys()))
        for device in node.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" %
                       (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Devices: %s" %
                           (node.devices[device].keys()))
                for device_name in node.devices[device].keys():
                    device_socket = \
                        node.devices[device][device_name]['numa_node']
                    if 'devices' not in available[device_socket]:
                        available[device_socket]['devices'] = list()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Device: %s, Socket: %s" %
                               (device, device_socket))
                    available[device_socket]['devices'].append(device_name)
            # pbs.logmsg(pbs.EVENT_DEBUG3,
            #            "Available on socket %s: %s" %
            #            (socket,available[socket]))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))

        # Remove all of the resources that are assigned to other jobs
        for job in self.assigned_resources.keys():
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            if (not job_to_be_ignored(job)):
                cpus = list()
                sockets = list()
                devices = list()
                memory = 0
                if 'cpuset.cpus' in self.assigned_resources[job]:
                    cpus = self.assigned_resources[job]['cpuset.cpus']
                if 'cpuset.mems' in self.assigned_resources[job]:
                    sockets = self.assigned_resources[job]['cpuset.mems']
                if 'devices.list' in self.assigned_resources[job]:
                    devices = self.assigned_resources[job]['devices.list']
                if 'memory.limit' in self.assigned_resources[job]:
                    memory = size_as_int(
                                self.assigned_resources[job]['memory.limit'])

                # Loop through the sockets and remove cpus that are
                # assigned to other cgroups
                for socket in sockets:
                    for cpu in cpus:
                        try:
                            available[socket]['cpus'].remove(cpu)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %d from %s" %
                                       (cpu, available[socket]['cpus']))

                if len(sockets) == 1:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Decrementing memory: %d by %d" %
                               (size_as_int(available[sockets[0]]['memory']),
                                memory))
                    available[sockets[0]]['memory'] -= memory

                # Loop throught the available sockets
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned device to %s: %s" % (job, devices))
                for socket in available.keys():
                    for device in devices:
                        try:
                            # loop through know devices and see if they match
                            if len(available[socket]['devices']) != 0:
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Check device: %s" % (device))
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Available device: %s" %
                                           (available[socket]['devices']))
                                major, minor = device.split()[1].split(':')
                                avail_device = self.get_device_name(
                                                   node, available, socket,
                                                   major, minor)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Returned device: %s" %
                                           (avail_device))
                                if avail_device is not None:
                                    pbs.logmsg(pbs.EVENT_DEBUG3,
                                               "socket: %d,\t" % socket +
                                               "devices: %s,\t" %
                                               available[socket]['devices'] +
                                               "device to remove: %s" %
                                               (avail_device))
                                    available[socket]['devices'].remove(
                                        avail_device)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %s from %s" %
                                       (device, available[socket]['devices']))
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Job %s res not removed from host " % job +
                           "available res: suspended job")
        pbs.logmsg(pbs.EVENT_DEBUG, "Available resources: %s" % (available))
        return available

    # Set a job limit
    def set_job_limit(self, jobid, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s: %s = %s" %
                   (caller_name(), jobid, resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'softmem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.soft_limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     jobid, 'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'], jobid,
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            elif resource == 'ncpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = self.select_cpus(path, value)
                    if cpus is None:
                        raise CgroupLimitError("Failed to configure cpuset.")
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
                    self.__copy_from_parent(os.path.join(self.paths['cpuset'],
                                            jobid, 'cpuset.mems'))
            elif resource == 'cpuset.cpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = value
                    if cpus is None:
                        msg = "Failed to configure cpus in cpuset."
                        raise CgroupLimitError(msg)
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
            elif resource == 'cpuset.mems':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.mems')
                    mems = value
                    if mems is None:
                        msg = "Failed to configure mems in cpuset."
                        raise CgroupLimitError(msg)
                    mems = ','.join(map(str, mems))
                    self.write_value(path, mems)
            elif resource == 'devices':
                if 'devices' in self.subsystems and \
                        self.cfg['cgroup']['devices']['enabled']:

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.allow')
                    devices = value
                    if devices is None:
                        msg = "Failed to configure device(s)."
                        raise CgroupLimitError(msg)
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Setting devices: %s for %s" % (devices, jobid))
                    for device in devices:
                        self.write_value(path, device)

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.list')
                    output = open(path, 'r').read()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "devices.list: %s" % output.replace("\n", ","))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    # Update resource usage for a job
    def update_job_usage(self, jobid, resc_used):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resc_used = %s" %
                   (caller_name(), str(resc_used)))
        # Sort the subsystems so that we consistently look at the subsystems
        # in the same order every time
        self.subsystems.sort()
        for subsys in self.subsystems:
            path = os.path.join(self.paths[subsys], jobid)
            if subsys == "memory":
                max_mem = self.__get_max_mem_usage(path)
                if max_mem is None:
                    pbs.logjobmsg(jobid, "%s: No max mem data" %
                                  (caller_name()))
                else:
                    resc_used['mem'] = pbs.size(convert_size(max_mem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: mem=%s" %
                                  (caller_name(), resc_used['mem']))
                mem_failcnt = self.__get_mem_failcnt(path)
                if mem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No mem fail count data" %
                                  (caller_name()))
                else:
                    # Check to see if the job exceeded its resource limits
                    if mem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memory limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "memsw":
                max_vmem = self.__get_max_memsw_usage(path)
                if max_vmem is None:
                    pbs.logjobmsg(jobid, "%s: No max vmem data" %
                                  (caller_name()))
                else:
                    resc_used['vmem'] = pbs.size(convert_size(max_vmem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: vmem=%s" %
                                  (caller_name(), resc_used['vmem']))

                vmem_failcnt = self.__get_memsw_failcnt(path)
                if vmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No vmem fail count data" %
                                  (caller_name()))
                else:
                    pbs.logjobmsg(jobid, "%s: vmem fail count: %d " %
                                  (caller_name(), vmem_failcnt))
                    if vmem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memsw limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "hugetlb":
                max_hpmem = self.__get_max_hugetlb_usage(path)
                if max_hpmem is None:
                    pbs.logjobmsg(jobid, "%s: No max hpmem data" %
                                  (caller_name()))
                    return
                hpmem_failcnt = self.__get_hugetlb_failcnt(path)
                if hpmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No hpmem fail count data" %
                                  (caller_name()))
                    return
                if hpmem_failcnt > 0:
                    err_msg = self.__get_error_msg(jobid)
                    pbs.logjobmsg(jobid, "Cgroup hugetlb limit exceeded: %s" %
                                  (err_msg))
                resc_used['hpmem'] = pbs.size(convert_size(max_hpmem, 'kb'))
                pbs.logjobmsg(jobid, "%s: Hugepage usage: %s" %
                              (caller_name(), resc_used['hpmem']))
            elif subsys == "cpuacct":
                cpu_usage = self.__get_cpu_usage(path)
                if cpu_usage is None:
                    pbs.logjobmsg(jobid, "%s: No CPU usage data" %
                                  (caller_name()))
                    return
                cpu_usage = convert_time(str(cpu_usage) + "ns")
                pbs.logjobmsg(jobid, "%s: CPU usage: %.3lf secs" %
                              (caller_name(), cpu_usage))
                resc_used['cput'] = pbs.duration(cpu_usage)

    # Creates the cgroup if it doesn't exists
    def create_job(self, jobid, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Iterate over the subsystems required
        for subsys in self.subsystems:
            # Create a directory for the job
            try:
                old_umask = os.umask(0022)
                path = self.paths[subsys]
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                path = os.path.join(self.paths[subsys], jobid)
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                if subsys == 'devices':
                    self.setup_subsys_devices(path, node)

            except OSError, exc:
                raise CgroupConfigError("Failed to create directory: %s (%s)" %
                                        (path, errno.errorcode[exc.errno]))
            except:
                raise
            finally:
                os.umask(old_umask)

    # Determine the cgroup limits and configure the cgroups
    def configure_job(self, jobid, hostresc, node):
        mem_enabled = 'memory' in self.subsystems
        vmem_enabled = 'memsw' in self.subsystems
        if mem_enabled or vmem_enabled:
            # Initialize mem variables
            mem_avail = node.get_memory_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "mem_avail %s" % mem_avail)
            mem_requested = None
            if 'mem' in hostresc.keys():
                mem_requested = convert_size(hostresc['mem'], 'kb')
            mem_default = None
            if mem_enabled:
                mem_default = self.default('memory')
            # Initialize vmem variables
            vmem_avail = node.get_vmem_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "vmem_avail %s" % vmem_avail)
            vmem_requested = None
            if 'vmem' in hostresc.keys():
                vmem_requested = convert_size(hostresc['vmem'], 'kb')
            vmem_default = None
            if vmem_enabled:
                vmem_default = self.default('memsw')
            # Initialize softmem variables
            if 'soft_limit' in self.cfg['cgroup']['memory'].keys():
                softmem_enabled = self.cfg['cgroup']['memory']['soft_limit']
            else:
                softmem_enabled = False
            # Sanity check
            if size_as_int(mem_avail) > size_as_int(vmem_avail):
                if size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                    raise CgroupLimitError(
                        'mem available (%s) exceeds vmem available (%s)' %
                        (mem_avail, vmem_avail))
            # Determine the mem limit
            if mem_requested is not None:
                # mem requested may not exceed available
                if size_as_int(mem_requested) > size_as_int(mem_avail):
                    raise JobValueError(
                        'mem requested (%s) exceeds mem available (%s)' %
                        (mem_requested, mem_avail))
                mem_limit = mem_requested
            else:
                # mem was not requested
                if mem_default is None:
                    mem_limit = mem_avail
                else:
                    mem_limit = mem_default
            # Determine the vmem limit
            if vmem_requested is not None:
                # vmem requested may not exceed available
                if size_as_int(vmem_requested) > size_as_int(vmem_avail):
                    raise JobValueError(
                        'vmem requested (%s) exceeds vmem available (%s)' %
                        (vmem_requested, vmem_avail))
                vmem_limit = vmem_requested
            else:
                # vmem was not requested
                if vmem_default is None:
                    vmem_limit = vmem_avail
                else:
                    vmem_limit = vmem_default
            # Ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Adjust for soft limits if enabled
            if mem_enabled and softmem_enabled:
                softmem_limit = mem_limit
                # The hard memory limit is assigned the lesser of the vmem
                # limit and available memory
                if size_as_int(vmem_limit) < size_as_int(mem_avail):
                    mem_limit = vmem_limit
                else:
                    mem_limit = mem_avail
            # Again, ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Sanity checks when both memory and memsw are enabled
            if mem_enabled and vmem_enabled:
                if vmem_requested is not None:
                    if size_as_int(vmem_limit) > size_as_int(vmem_requested):
                        # The user requested an invalid limit
                        raise JobValueError(
                            'vmem limit (%s) exceeds vmem requested (%s)' %
                            (vmem_limit, vmem_requested))
                if size_as_int(vmem_limit) > size_as_int(mem_limit):
                    # This job may utilize swap
                    if size_as_int(vmem_avail) <= size_as_int(mem_avail) and \
                      size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                        # No swap available
                        raise CgroupLimitError(
                            ('Job might utilize swap ' +
                             'and no swap space available'))
            # Assign mem and vmem
            if mem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: mem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), mem_limit))
                hostresc['mem'] = pbs.size(mem_limit)
                if softmem_enabled:
                    hostresc['softmem'] = pbs.size(softmem_limit)
            if vmem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: vmem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), vmem_limit))
                hostresc['vmem'] = pbs.size(vmem_limit)
        # Initialize hpmem variables
        hpmem_enabled = 'hugetlb' in self.subsystems
        if hpmem_enabled:
            hpmem_avail = node.get_hpmem_on_node(self.cfg)
            hpmem_limit = None
            hpmem_default = self.default('hugetlb')
            if hpmem_default is None:
                hpmem_default = hpmem_avail
            if 'hpmem' in hostresc.keys():
                hpmem_limit = convert_size(hostresc['hpmem'], 'kb')
            else:
                hpmem_limit = hpmem_default
            # Assign hpmem
            if size_as_int(hpmem_limit) > size_as_int(hpmem_avail):
                raise JobValueError('hpmem limit (%s) exceeds available (%s)' %
                                    (hpmem_limit, hpmem_avail))
            hostresc['hpmem'] = pbs.size(hpmem_limit)
        # Initialize cpuset variables
        cpuset_enabled = 'cpuset' in self.subsystems
        if cpuset_enabled:
            cpu_limit = 1
            if 'ncpus' in hostresc.keys():
                cpu_limit = hostresc['ncpus']
            if cpu_limit < 1:
                cpu_limit = 1
            hostresc['ncpus'] = pbs.pbs_int(cpu_limit)

        # Find the available resources and assign the right ones to the job

        attempt = 0
        assign_resources = False

        # Two attempts since self.cleanup_orphans may actually fix the
        # problem we see in a first attempt
        while attempt < 2:
            attempt += 1
            available_resources = self.available_node_resources(node)
            if 'vnodes' in hostresc:
                assign_resources = self.assign_job_resources_by_vnode(
                    hostresc, available_resources, node)
            else:
                assign_resources = self.assign_job_resources(
                    hostresc, available_resources, node)

            if (assign_resources is False) and (attempt < 2):
                # No resources were assigned to the job.
                # Most likely cause was that a cgroup has
                # not been cleaned up yet
                pbs.logmsg(pbs.EVENT_DEBUG, "Failed to assign resources. " +
                           "Checking the following cgroups on the node " +
                           "with the server: %s" % self.existing_cgroups)

                # Collect the jobs on the node (try reading mom_priv/jobs,
                # if not successful ask server)
                jobs_list = node.gather_jobs_on_node_local()

                if jobs_list is not None:
                    # Don't clean up the cgroup we just made for the
                    # job just yet
                    if jobid not in jobs_list:
                        jobs_list.append(jobid)

                    self.cleanup_orphans(jobs_list)

                    # Recheck what is now assigned after things were cleaned up
                    self.gather_assigned_resources()

        if assign_resources is False:
            # Cleanup cgroups for jobs not present on this node
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Assign resources failed. Attempting to cleanup all " +
                       "leftover cgroups (including event job %s)" % jobid)

            # Remove the current job from the jobs list so it will be
            # cleaned up as well.
            jobs_list.remove(jobid)

            self.cleanup_orphans(jobs_list)

            # Rerun the job and log the message
            line = 'Unable to assign resources to job. '
            line += 'Will requeue the job and try again.'
            line += 'job run_count: %d' % pbs.event().job.run_count
            pbs.event().job.rerun()
            pbs.event().reject(line)

        # Print out the assigned resources
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Assigned resources: %s" % (assign_resources))

        if cpuset_enabled:
            hostresc.pop('ncpus', None)
            for key in ['cpuset.cpus', 'cpuset.mems']:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Initialize devices variables
        devices_enabled = 'devices' in self.subsystems
        if devices_enabled:
            key = 'devices'
            if key in assign_resources:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Apply the resource limits to the cgroups
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Setting cgroup limits for: %s" %
                   (caller_name(), hostresc))

        # The vmem limit must be set after the mem limit, so sort the keys
        for resc in sorted(hostresc.keys()):
            self.set_job_limit(jobid, resc, hostresc[resc])

    # Kill any processes contained within a tasks file
    def __kill_tasks(self, tasks_file):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        with open(tasks_file, 'r') as fd:
            for line in fd:
                os.kill(int(line.strip()), signal.SIGKILL)
        fd.close()
        count = 0
        with open(tasks_file, 'r') as fd:
            for line in fd:
                count += 1
                pid = line.strip()
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Pid: %s did not clean up" %
                           (caller_name(), pid))
                if os.path.isfile('/proc/%s/status' % pid):
                    tmp = open('/proc/%s/status' % pid).readlines()
                    tmp_line = ''
                    for entry in tmp:
                        if entry.find('Name:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('State:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('Uid:') != -1:
                            tmp_line += entry.strip()
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: %s" % (caller_name(), tmp_line))

        return count

    # Perform the actual removal of the cgroup directory
    # by default, only one attempt at killing tasks in cgroup, without sleeping
    # since this method could be called many times
    # (for N directories times M jobs)
    def __remove_cgroup(self, path, tasks_kill_attempts=1):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            if os.path.exists(path):
                tasks_file = os.path.join(path, 'tasks')
                task_cnt = self.__kill_tasks(tasks_file)
                kill_attempts = 0
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Task Kill Attempts: %d" % tasks_kill_attempts)
                while (task_cnt > 0) and (kill_attempts < tasks_kill_attempts):
                    kill_attempts += 1
                    count = 0

                    with open(tasks_file, 'r') as fd:
                        for line in fd:
                            count += 1
                        task_cnt = count

                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "Attempt: %d - cgroup still has %d tasks: %s" %
                               (kill_attempts, task_cnt, path))
                    if kill_attempts < tasks_kill_attempts:
                        time.sleep(1)
                        # Added since there are race conditions where tasks are
                        # being added at the same time we get the initial
                        # task count
                        tasks_file = os.path.join(path, 'tasks')
                        task_cnt = self.__kill_tasks(tasks_file)

                if task_cnt == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Removing directory %s" %
                               (caller_name(), path))
                    os.rmdir(path)
                    if os.path.exists(path):
                        time.sleep(.25)
                        if os.path.exists(path):
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "%s: 2nd Try Removing directory %s" %
                                       (caller_name(), path))
                            os.rmdir(path)
                            time.sleep(.25)
                        if os.path.exists(path):
                            return False
                else:
                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "cgroup still has %d tasks: %s" %
                               (task_cnt, path))

                    # Check to see if the offline file is already present
                    # on the mom
                    offline_file_exists = False
                    if os.path.isfile(self.offline_file):
                        msg = "Cgroup(s) not cleaning up but the node already "
                        msg += "has the offline file."

                        pbs.logmsg(pbs.EVENT_DEBUG, msg)
                    else:
                        # Check to see that the node is not already offline.
                        try:
                            tmp_state = pbs.server().vnode(self.hostname).state
                        except:
                            msg = "Unable to contact server for node state"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            tmp_state = None
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Current Node State: %d" % tmp_state)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Offline Node State: %d" % pbs.ND_OFFLINE)

                        if tmp_state == pbs.ND_OFFLINE:
                            msg = "Cgroup(s) not cleaning up but the node is "
                            msg += "already offline."
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                        elif tmp_state is not None:
                            # Offline the node(s)
                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                            msg = "Offlining node since cgroup(s) are not "
                            msg += "cleaning up"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            vnode = pbs.event().vnode_list[self.hostname]

                            vnode.state = pbs.ND_OFFLINE

                            # Write a file locally to reduce server traffic
                            # when it comes time to online the node
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Offline file: %s" % self.offline_file)
                            open(self.offline_file, 'a').close()
                            vnode.comment = self.offline_msg

                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                        else:
                            pass

                    return False

        except OSError, exc:
            pbs.logmsg(pbs.EVENT_SYSTEM,
                       "Failed to remove cgroup path: %s (%s)" %
                       (path, errno.errorcode[exc.errno]))
        except:
            pbs.logmsg(pbs.EVENT_SYSTEM, "Failed to remove cgroup path: %s" %
                       (path))

        return True

    # Removes cgroup directories that are not associated with a local job
    def cleanup_orphans(self, local_jobs):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        orphan_cnt = 0
        for key in self.paths.keys():
            for dir in glob.glob(os.path.join(self.paths[key], '[0-9]*')):
                jobid = os.path.basename(dir)
                if jobid not in local_jobs:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Removing orphaned cgroup: %s" %
                               (caller_name(), dir))
                    # default number of attempts at killing tasks
                    status = self.__remove_cgroup(dir)
                    if not status:
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "%s: Removing orphaned cgroup %s failed " %
                                   (caller_name(), dir))
                        orphan_cnt += 1
        return orphan_cnt

    # Removes the cgroup directories for a job
    def delete(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        tasks_kill_attempted = False

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            if not tasks_kill_attempted:
                # Make multiple attempts at killing tasks in cgroups on
                # first subsystem encountered since it is "normal" for
                # a cgroup.delete to encounter processes hard to kill
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                                           jobid),
                                              tasks_kill_attempts=(
                                              CGROUP_KILL_ATTEMPTS))
                tasks_kill_attempted = True
            else:
                # We tried getting rid of job processes earlier,
                # so don't try N times with sleeps
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                              jobid), tasks_kill_attempts=1)

            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Status: %s" %
                       (caller_name(), status))
            if status is False:
                # Rerun the job and log the message
                line = 'Unable to cleanup a cgroup. '
                line += 'Will offline the node and requeue the job '
                line += 'and try again. job run_count: %d' % \
                        pbs.event().job.run_count
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Failed to remove cgroup: %s" %
                           (caller_name(), line))
                pbs.event().accept()

    # Write a value to a limit file
    def write_value(self, file, value, mode='w'):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: writing %s to %s" %
                   (caller_name(), value, file))
        try:
            with open(file, mode) as fd:
                fd.write(str(value) + '\n')
        except IOError, exc:
            if exc.errno == errno.ENOENT:
                pbs.logmsg(pbs.EVENT_SYSTEM,
                           "Trying to set limit to unknown file %s" % file)
            elif exc.errno in [errno.EACCES, errno.EPERM]:
                pbs.logmsg(pbs.EVENT_SYSTEM, "Permission denied on file %s" %
                           file)
            elif exc.errno == errno.EBUSY:
                raise CgroupBusyError("Limit rejected")
            elif exc.errno == errno.EINVAL:
                raise CgroupLimitError("Invalid limit value: %s" % (value))
            else:
                raise
        except:
            raise

    # Return memory failcount
    def __get_mem_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.failcnt"), 'r').read().strip())
        except:
            return None

    # Return vmem failcount
    def __get_memsw_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.failcnt"), 'r').read().strip())
        except:
            return None

    # Return hpmem failcount
    def __get_hugetlb_failcnt(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.failcnt"))[0], 'r').read().strip())
        except:
            return None

    # Return the max usage of memory in bytes
    def __get_max_mem_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of memsw in bytes
    def __get_max_memsw_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of hugetlb in bytes
    def __get_max_hugetlb_usage(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.max_usage_in_bytes"))[0],
                            'r').read().strip())
        except:
            return None

    # Return the cpuacct.usage in cpu seconds
    def __get_cpu_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "cpuacct.usage"), 'r').read().strip())
        except:
            return None

    # Assign CPUs to the cpuset
    def select_cpus(self, path, ncpus):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path is %s" % (caller_name(), path))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: ncpus is %s" %
                   (caller_name(), ncpus))
        if ncpus < 1:
            ncpus = 1
        try:
            # Must select from those currently available
            cpufile = os.path.basename(path)
            base = os.path.dirname(path)
            parent = os.path.dirname(base)
            avail = cpus2list(open(os.path.join(parent, cpufile),
                              'r').read().strip())
            if len(avail) < 1:
                raise CgroupProcessingError("No CPUs avaialble in cgroup.")
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Available CPUs: %s" %
                       (caller_name(), avail))
            assigned = []
            for file in glob.glob(os.path.join(parent, '[0-9]*', cpufile)):
                cpus = cpus2list(open(file, 'r').read().strip())
                for id in cpus:
                    if id in avail:
                        avail.remove(id)
            if len(avail) < ncpus:
                raise CgroupProcessingError(
                    "Insufficient CPUs avaialble in cgroup.")
            if len(avail) == ncpus:
                return avail
            # FUTURE: Try to minimize NUMA nodes based on memory requirement
            return avail[0:ncpus]
        except:
            raise

    # Return the error message in system message file
    def __get_error_msg(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        proc = subprocess.Popen(['dmesg'],
                                shell=False, stdout=subprocess.PIPE)
        stdout = proc.communicate()[0].splitlines()
        stdout.reverse()
        # Check to see if the job id is found in dmesg otherwise
        # you could get the information for another job that was killed
        # the line will look like Memory cgroup stats for /pbspro/279.centos7
        kill_line = ""

        for line in stdout:
            start = line.find('Killed process ')
            if start >= 0:
                kill_line = line[start:]
            job_start = line.find(jobid)
            if job_start >= 0:
                # pbs.logmsg(pbs.EVENT_DEBUG3,
                #           "%s: Jobid: %s, Line: %s" %
                #           (caller_name(), jobid, line))
                return kill_line
        return ""

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_job_env_file(self, jobid, env_list):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        # if not os.path.exists(self.host_job_env_dir):
        #    os.makedirs(self.host_job_env_dir, 0755)

        # Write out assigned_resources
        try:
            lines = string.join(env_list, '\n')
            filename = self.host_job_env_filename % jobid
            outfile = open(filename, "w")
            outfile.write(lines)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" % (filename))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (lines))
            return True
        except:
            return False

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        if not os.path.exists(self.hook_storage_dir):
            os.makedirs(self.hook_storage_dir, 0755)

        # Write out assigned_resources
        try:
            json_str = json.dumps(self.host_assigned_resources)
            outfile = open(self.hook_storage_dir+os.sep+jobid, "w")
            outfile.write(json_str)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" %
                       (self.hook_storage_dir+os.sep+jobid))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (json_str))
            return True
        except:
            return False

    def read_in_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        pbs.logmsg(pbs.EVENT_DEBUG3, "Host assigned resources: %s" %
                   (self.host_assigned_resources))
        if os.path.isfile(self.hook_storage_dir+os.sep+jobid):
            # Write out assigned_resources
            try:
                infile = open(self.hook_storage_dir+os.sep+jobid, 'r')
                json_data = json.load(infile, object_hook=decode_dict)
                self.host_assigned_resources = json_data
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Host assigned resources: %s" %
                           (self.host_assigned_resources))
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
            finally:
                infile.close()

        if self.host_assigned_resources is not None:
            return True
        else:
            return False

#
#
# FUNCTION main
#


def main():
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Function called" % (caller_name()))
    hostname = pbs.get_local_nodename()
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Host is %s" % (caller_name(), hostname))

    # Log the hook event type
    e = pbs.event()
    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook name is %s" %
               (caller_name(), e.hook_name))

    try:
        # Instantiate the hook utility class
        hooks = HookUtils()
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Event type is %s" %
                   (caller_name(), hooks.event_name(e.type)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(hooks)))
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: Failed to instantiate hook utility class" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        e.accept()

    if not hooks.hashandler(e.type):
        # Bail out if there is no handler for this event
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s event not handled by this hook" %
                   (caller_name(), hooks.event_name(e.type)))
        e.accept()

    try:
        # If an exception occurs, jobutil must be set
        jobutil = None
        # Instantiate the job utility class first so jobutil can be accessed
        # by the exception handlers.
        if hasattr(e, 'job'):
            jobutil = JobUtils(e.job)
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Job information class instantiated" %
                       (caller_name()))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" %
                       (caller_name(), repr(jobutil)))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Event does not include a job" %
                       (caller_name()))
        # Instantiate the cgroup utility class
        vnode = None
        if hasattr(e, 'vnode_list'):
            if hostname in e.vnode_list.keys():
                vnode = e.vnode_list[hostname]
        cgroup = CgroupUtils(hostname, vnode)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Cgroup utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(cgroup)))
        # Bail out if there is nothing to do
        if len(cgroup.subsystems) < 1:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: All cgroups disabled" %
                       (caller_name()))
            e.accept()

        # Call the appropriate handler
        if hooks.invoke_handler(e, cgroup, jobutil) is True:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned success" %
                       (caller_name()))
            e.accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned failure" %
                       (caller_name()))
            e.reject()

    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass

    except AdminError, exc:
        # Something on the system is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Admin error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except UserError, exc:
        # User must correct problem and resubmit job
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("User error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.delete()
                msg += " (deleted)"
            except:
                msg += " (delete failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except JobValueError, exc:
        # Something in PBS is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Job value error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except Exception, exc:
        # Catch all other exceptions and report them.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Unexpected error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

# line below required for unittesting
if __name__ == "__builtin__":
    start_time = time.time()
    try:
        main()
    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
    finally:
        pbs.logmsg(pbs.EVENT_DEBUG, "Elapsed time: %0.4lf" %
                   (time.time() - start_time))
---
2016-07-18 16:58:56,185 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups.json', 'content-encoding': 'default'}
2016-07-18 16:58:56,185 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups.json
2016-07-18 16:58:56,256 INFO     job: executable set to /bin/sleep with arguments: 100
2016-07-18 16:58:56,257 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0755 /tmp/PtlPbsJobScriptwjv0ih
2016-07-18 16:58:56,268 INFOCLI  centos7: sudo -H -u pbsuser /usr/local/bin/qsub -N test_t4 -l select=1:ncpus=1:mem=500mb /tmp/PtlPbsJobScriptwjv0ih
2016-07-18 16:58:56,293 INFO     submit to centos7 as pbsuser: job 384.centos7 OrderedDict([('Job_Name', 'test_t4'), ('Resource_List.select', '1:ncpus=1:mem=500mb')])
2016-07-18 16:58:56,294 INFOCLI  job script /tmp/PtlPbsJobScriptwjv0ih
---
#!/usr/bin/env python

import os
import subprocess
from subprocess import Popen, PIPE
from time import sleep

eat_mem_c = """
#include <stdio.h>
#include <stdlib.h>
#include <string.h>


#define ONE_GB (0x40000000L)
#define ONE_MB (0x100000L)

void
eatmem(total_mb, block_mb)
{
	int i;
	char *buf;
	size_t block;

	for (i=1, buf=NULL, block=(block_mb*ONE_MB); (i*block) < (total_mb*ONE_MB); i++) {
		buf = realloc((void *)buf, (i*block));
		memset(buf+((i-1)*block), 0, block);
	}
	return;
}

int
main(int argc, char *argv[])
{
	int total_mb = 8;
	int block_mb = 1;

	if (argc > 1)
		total_mb = atoi(argv[1]);

	if (argc > 2)
		block_mb = atoi(argv[2]);

	eatmem(total_mb, block_mb);
	printf("Initialized %dMB in blocks of %dMB\\n", total_mb, block_mb);
}
"""

fname = "/tmp/pbs_pp325_eat_mem.c"
fname_exe = "/tmp/pbs_pp325_eat_mem"
fout = open(fname, "w")
fout.write(eat_mem_c)
fout.close()

print os.getppid()

p = Popen(["gcc", "-o", fname_exe, fname], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error


p = Popen([fname_exe, "300", "10"], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

sleep(4)

p = Popen([fname_exe, "400", "10"], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

sleep(4)

os.remove(fname)
os.remove(fname_exe)


---
2016-07-18 16:58:56,294 INFOCLI  centos7: /usr/local/bin/qstat -f 384.centos7
2016-07-18 16:58:56,391 INFO     expect on server centos7: job_state = R job 384.centos7  got: job_state = Q
2016-07-18 16:58:56,893 INFOCLI  centos7: /usr/local/bin/qmgr -c set server scheduling=True
2016-07-18 16:58:56,911 INFOCLI  centos7: /usr/local/bin/qstat -f 384.centos7
2016-07-18 16:58:57,003 INFO     expect on server centos7: job_state = R job 384.centos7 attempt: 2 ...  OK
2016-07-18 16:58:57,010 INFO     mom centos7 log match: searching for ".*384.centos7;update_job_usage: Memory usage: vmem=3" - using regular expression  - No match
2016-07-18 16:59:10,787 INFO     mom centos7 log match: searching for ".*384.centos7;update_job_usage: Memory usage: vmem=3" - using regular expression ... OK
2016-07-18 16:59:10,793 INFO     mom centos7 log match: searching for ".*384.centos7;update_job_usage: " - using regular expression  - No match
2016-07-18 16:59:23,669 INFO     mom centos7 log match: searching for ".*384.centos7;update_job_usage: " - using regular expression ... OK
2016-07-18 16:59:28,683 INFO     mom centos7 log match: searching for ".*384.centos7;update_job_usage: Memory usage: vmem=4" - using regular expression  - No match
2016-07-18 16:59:41,497 INFO     mom centos7 log match: searching for ".*384.centos7;update_job_usage: Memory usage: vmem=4" - using regular expression ... OK
2016-07-18 16:59:41,497 INFO     ==============================
2016-07-18 16:59:41,497 INFO      Entered CgroupsTests tearDown
2016-07-18 16:59:41,497 INFO     ==============================
2016-07-18 16:59:41,497 INFO     ===============================
2016-07-18 16:59:41,497 INFO     Completed CgroupsTests tearDown
2016-07-18 16:59:41,497 INFO     ===============================
2016-07-18 16:59:41,498 INFO     ok

2016-07-18 16:59:41,498 INFO     test name: test_t6b (tests.cgroups_tests.CgroupsTests)...
2016-07-18 16:59:41,498 INFO     test start time: Mon Jul 18 16:59:41 2016
2016-07-18 16:59:41,498 INFO     test docstring: 
 Test to verify that cgroups are reporting usage for
            cput, mem, vmem in qstat 
        
2016-07-18 16:59:41,498 INFO     ===========================
2016-07-18 16:59:41,498 INFO      Entered CgroupsTests setUp
2016-07-18 16:59:41,498 INFO     ===========================
2016-07-18 16:59:41,499 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:59:41,551 INFO     status on centos7: server
2016-07-18 16:59:41,551 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:59:41,621 INFO     manager on centos7: set server {'managers': (3, 'root@*')}
2016-07-18 16:59:41,621 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers-=root@*
2016-07-18 16:59:41,652 INFO     manager on centos7: set server {'managers': (2, 'root@*')}
2016-07-18 16:59:41,652 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers+=root@*
2016-07-18 16:59:41,691 INFO     server centos7: reverting configuration to defaults
2016-07-18 16:59:41,692 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:59:41,749 INFO     select on centos7: __ALL__
2016-07-18 16:59:41,749 INFOCLI  centos7: /usr/local/bin/qselect
2016-07-18 16:59:41,761 INFOCLI  centos7: /usr/local/bin/qstat -f @centos7.usa.org
2016-07-18 16:59:41,823 INFO     expect on server centos7: job_state set 0 job ...  OK
2016-07-18 16:59:41,824 INFOCLI  centos7: /usr/local/bin/pbs_rstat -f
2016-07-18 16:59:41,838 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:59:41,861 INFO     manager on centos7: delete hook t1_l1
2016-07-18 16:59:41,862 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c delete hook t1_l1
2016-07-18 16:59:41,894 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:59:41,920 INFO     expect on server centos7:  unset  hook t1_l1 ...  OK
2016-07-18 16:59:41,920 INFOCLI  centos7: /usr/local/bin/qstat -Qf @centos7.usa.org
2016-07-18 16:59:41,961 INFO     manager on centos7: delete queue workq
2016-07-18 16:59:41,962 INFOCLI  centos7: /usr/local/bin/qmgr -c delete queue workq
2016-07-18 16:59:41,975 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:59:41,995 INFO     expect on server centos7:  unset  queue workq ...  OK
2016-07-18 16:59:41,995 INFO     manager on centos7: create queue workq {'started': 'True', 'queue_type': 'Execution', 'enabled': 'True'}
2016-07-18 16:59:41,995 INFOCLI  centos7: /usr/local/bin/qmgr -c create queue workq started=True,queue_type=Execution,enabled=True
2016-07-18 16:59:42,017 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:59:42,032 INFO     expect on server centos7: started set True || queue_type set Execution || enabled set True queue workq ...  OK
2016-07-18 16:59:42,032 INFO     manager on centos7: set server {'log_events': '511', 'default_queue': 'workq'}
2016-07-18 16:59:42,032 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=511,default_queue=workq
2016-07-18 16:59:42,058 INFO     status on centos7: resource
2016-07-18 16:59:42,058 INFO     manager on centos7: list resource
2016-07-18 16:59:42,058 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource
2016-07-18 16:59:42,114 INFO     manager on centos7: delete resource nmics,ngpus
2016-07-18 16:59:42,115 INFOCLI  centos7: /usr/local/bin/qmgr -c delete resource nmics,ngpus
2016-07-18 16:59:42,184 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource nmics
2016-07-18 16:59:42,202 INFO     expect on server centos7:  unset  resource nmics ...  OK
2016-07-18 16:59:42,203 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource ngpus
2016-07-18 16:59:42,225 INFO     expect on server centos7:  unset  resource ngpus ...  OK
2016-07-18 16:59:42,225 INFOCLI  status on centos7: server license_count
2016-07-18 16:59:42,226 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:59:42,279 INFO     server: centos7.usa.org licensed
2016-07-18 16:59:42,319 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/server_priv/comm.lock
2016-07-18 16:59:42,364 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:59:42,380 INFO     scheduler centos7: reverting configuration to defaults
2016-07-18 16:59:42,381 INFO     manager on centos7: list sched
2016-07-18 16:59:42,381 INFOCLI  centos7: /usr/local/bin/qmgr -c list sched
2016-07-18 16:59:42,400 INFO     manager on centos7: unset sched ['sched_cycle_length']
2016-07-18 16:59:42,400 INFOCLI  centos7: /usr/local/bin/qmgr -c unset sched sched_cycle_length
2016-07-18 16:59:42,416 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/dedicated_time
2016-07-18 16:59:42,427 INFOCLI2 centos7: cmp /usr/local/etc/pbs_resource_group /var/spool/pbs/sched_priv/resource_group
2016-07-18 16:59:42,430 INFO     scheduler centos7: reverting holidays file to default
2016-07-18 16:59:42,431 INFOCLI2 centos7: cmp /usr/local/etc/pbs_holidays /var/spool/pbs/sched_priv/holidays
2016-07-18 16:59:42,435 INFOCLI2 centos7: cmp /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:59:42,438 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:59:42,454 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:59:42,470 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:59:42,471 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:59:42,524 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:59:42,539 INFOCLI2 centos7: sudo -H cmp /usr/local/sbin/pbs_mom /usr/local/sbin/pbs_mom.cpuset
2016-07-18 16:59:42,589 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:59:42,599 INFO     mom centos7: reverting configuration to defaults
2016-07-18 16:59:42,599 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/epilogue
2016-07-18 16:59:42,608 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/prologue
2016-07-18 16:59:42,622 INFOCLI2 centos7: sudo -H ls /var/spool/pbs/mom_priv/config.d
2016-07-18 16:59:42,634 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsKnQIhk /var/spool/pbs/mom_priv/config
2016-07-18 16:59:42,649 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/mom_priv/config
2016-07-18 16:59:42,663 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/mom_priv/config
2016-07-18 16:59:42,673 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/mom_priv/config
2016-07-18 16:59:42,685 INFO     mom centos7: sent signal -HUP
2016-07-18 16:59:42,685 INFOCLI2 centos7: sudo -H kill -HUP 42976
2016-07-18 16:59:42,735 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:59:42,748 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 16:59:42,765 INFO     status on centos7: node centos7
2016-07-18 16:59:42,765 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:59:42,777 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:59:42,792 INFO     expect on server centos7: state = free node centos7 ...  OK
2016-07-18 16:59:42,792 INFO     ============================
2016-07-18 16:59:42,793 INFO     Completed CgroupsTests setUp
2016-07-18 16:59:42,793 INFO     ============================
2016-07-18 16:59:42,793 INFO     server centos7: server operating mode set to cli
2016-07-18 16:59:42,793 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:59:42,854 INFO     expect on server centos7: pbs_version ~ .*14.* server centos7.usa.org ...  OK
2016-07-18 16:59:42,855 INFO     manager on centos7: set server {'log_events': '2047'}
2016-07-18 16:59:42,855 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=2047
2016-07-18 16:59:42,874 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:59:42,937 INFO     expect on server centos7: log_events set 2047 server centos7.usa.org ...  OK
2016-07-18 16:59:42,938 INFO     scheduler centos7: config {'resources': 'ncpus,mem,host,vnode,vmem'}
2016-07-18 16:59:42,939 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /var/spool/pbs/sched_priv/sched_config /var/spool/pbs/sched_priv/sched_config.bak
2016-07-18 16:59:42,948 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsTDDm8h /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:59:42,964 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:59:42,985 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:59:42,995 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:59:43,008 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:59:43,008 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:59:43,026 INFO     scheduler centos7 log match: searching for "Error reading line" - No match
2016-07-18 16:59:44,028 INFO     manager on centos7 as root: create resource nmics {'flag': 'nh', 'type': 'long'}
2016-07-18 16:59:44,028 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource nmics flag=nh,type=long
2016-07-18 16:59:44,052 INFO     manager on centos7 as root: create resource ngpus {'flag': 'nh', 'type': 'long'}
2016-07-18 16:59:44,052 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource ngpus flag=nh,type=long
2016-07-18 16:59:44,091 INFO     Test Summary: test_t1_l1 tests "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" cgroup hook
2016-07-18 16:59:44,091 INFO     status on centos7: hook
2016-07-18 16:59:44,091 INFO     manager on centos7: list hook
2016-07-18 16:59:44,092 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:59:44,115 INFO     status on centos7: hook
2016-07-18 16:59:44,116 INFO     manager on centos7: list hook
2016-07-18 16:59:44,116 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:59:44,135 INFO     manager on centos7: create hook t1_l1
2016-07-18 16:59:44,135 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c create hook t1_l1
2016-07-18 16:59:44,177 INFO     manager on centos7: set hook t1_l1 {'freq': 2, 'enabled': 'True', 'event': '"execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"'}
2016-07-18 16:59:44,178 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set hook t1_l1 freq=2,enabled=True,event="execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"
2016-07-18 16:59:44,217 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:59:44,247 INFO     expect on server centos7: freq set 2 || enabled set True || event set "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" hook t1_l1 ...  OK
2016-07-18 16:59:44,248 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-python', 'input-file': '/tmp/PtlPbsMHEEN8', 'content-encoding': 'default'}
2016-07-18 16:59:44,248 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-python default /tmp/PtlPbsMHEEN8
2016-07-18 16:59:44,296 INFO     server centos7: imported hook body
---
#  Copyright (C) 2003-2016 Altair Engineering, Inc. All rights reserved.
#
#  ALTAIR ENGINEERING INC. Proprietary and Confidential. Contains Trade Secret
#  Information. Not for use or disclosure outside ALTAIR and its licensed
#  clients. Information contained herein shall not be decompiled, disassembled,
#  duplicated or disclosed in whole or in part for any purpose. Usage of the
#  software is only as explicitly permitted in the end user software license
#  agreement.
#
#  Copyright notice does not imply publication.

# Last Updated
# Date: 07-06-2016
#
# When soft_limit is true for memory, memsw represents the hard limit.
#
# The resources value in sched_config must contain entries for mem and
# vmem if those subsystems are enabled in the hook configuration file. The
# amount of resource requested will not be avaiable to the hook if they
# are not present.

# This hook handles all of the operations necessary for PBS to support
# cgroups on linux hosts that support them (kernel 2.6.28 and higher)
#
# This hook services the following events:
# - exechost_periodic
# - exechost_startup
# - execjob_attach
# - execjob_begin
# - execjob_end
# - execjob_epilogue
# - execjob_launch

# Imports from __future__ must happen first
from __future__ import with_statement

# Additional imports
import sys
import os
import errno
import signal
import subprocess
import re
import glob
import time
import string
import platform
import traceback
import copy
from stat import S_ISBLK, S_ISCHR
import operator
import pwd
import pbs
import fnmatch
import socket

try:
    import json
except:
    import simplejson as json

CGROUP_KILL_ATTEMPTS = 12

# Flag to turn off features not in 12.2.40X branch
CRAY_12_BRANCH = False


# ============================================================================
# Derived error classes
# ============================================================================

# Base class for errors fixable only by administrative action.


class AdminError(Exception):
    pass

# Base class for errors in processing, unknown cause.


class ProcessingError(Exception):
    pass

# Base class for errors fixable by the user.


class UserError(Exception):
    pass

# Errors in PBS job resource values.


class JobValueError(UserError):
    pass

# Errors when the cgroup is busy.


class CgroupBusyError(ProcessingError):
    pass

# Errors in configuring cgroup.


class CgroupConfigError(AdminError):
    pass

# Errors in configuring cgroup.


class CgroupLimitError(AdminError):
    pass

# Errors processing cgroup.


class CgroupProcessingError(ProcessingError):
    pass

# ============================================================================
# Utility functions
# ============================================================================

#
# FUNCTION caller_name
#
# Return the name of the calling function or method.
#


def caller_name():
    return str(sys._getframe(1).f_code.co_name)

#
# FUNCTION convert_size
#
# Convert a string containing a size specification (e.g. "1m") to a
# string using different units (e.g. "1024k").
#
# This function only interprets a decimal number at the start of the string,
# stopping at any unrecognized character and ignoring the rest of the string.
#
# When down-converting (e.g. MB to KB), all calculations involve integers and
# the result returned is exact. When up-converting (e.g. KB to MB) floating
# point numbers are involved. The result is rounded up. For example:
#
# 1023MB -> GB yields 1g
# 1024MB -> GB yields 1g
# 1025MB -> GB yields 2g  <-- This value was rounded up
#
# Pattern matching or conversion may result in exceptions.
#


def convert_size(value, units='b'):
    logs = {'b': 0, 'k': 10, 'm': 20, 'g': 30,
            't': 40, 'p': 50, 'e': 60, 'z': 70, 'y': 80}
    try:
        new = units[0].lower()
        if new not in logs.keys():
            new = 'b'
        val, old = re.match('([-+]?\d+)([bkmgtpezy]?)',
                            str(value).lower()).groups()
        val = int(val)
        if val < 0:
            raise ValueError('Value may not be negative')
        if old not in logs.keys():
            old = 'b'
        factor = logs[old] - logs[new]
        val *= 2 ** factor
        slop = val - int(val)
        val = int(val)
        if slop > 0:
            val += 1
        # pbs.size() does not like units following zero
        if val <= 0:
            return '0'
        else:
            return str(val) + new
    except:
        return None

#
# FUNCTION size_as_int
#
# Convert a size string to an integer representation of size in bytes
#


def size_as_int(value):
    return int(convert_size(value).rstrip(string.ascii_lowercase))

#
# FUNCTION convert_time
#
# Converts a integer value for time into the value of the return unit
#
# A valid decimal number, with optional sign, may be followed by a character
# representing a scaling factor.  Scaling factors may be either upper or
# lower case. Examples include:
# 250ms
# 40s
# +15min
#
# Valid scaling factors are:
# ns  = 10**-9
# us  = 10**-6
# ms  = 10**-3
# s   =      1
# min =     60
# hr  =   3600
#
# Pattern matching or conversion may result in exceptions.
#


def convert_time(value, return_unit='s'):
    multipliers = {'':  1, 'ns': 10 ** -9, 'us': 10 ** -6,
                   'ms': 10 ** -3, 's': 1, 'min': 60, 'hr': 3600}
    n, factor = re.match('([-+]?\d+)(\w*[a-zA-Z])?',
                         str(value).lower()).groups(0)

    # Check to see if there was not unit of time specified
    if factor == 0:
        factor = ''

    # Check to see if the unit is valid
    if str.lower(factor) not in multipliers.keys():
        raise ValueError('Time unit not recognized.')

    # Convert the value to seconds
    value_in_sec = float(n) * float(multipliers[str.lower(factor)])
    if return_unit != 's':
        return value_in_sec / multipliers[str.lower(return_unit)]
    else:
        return float('%lf' % value_in_sec)

# simplejson hook to convert lists from unicode to utf-8


def decode_list(data):
    rv = []
    for item in data:
        if isinstance(item, unicode):
            item = item.encode('utf-8')
        elif isinstance(item, list):
            item = decode_list(item)
        elif isinstance(item, dict):
            item = decode_dict(item)
        rv.append(item)
    return rv

# simplejson hook to convert dictionaries from unicode to utf-8


def decode_dict(data):
    rv = {}
    for key, value in data.iteritems():
        if isinstance(key, unicode):
            key = key.encode('utf-8')
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        elif isinstance(value, list):
            value = decode_list(value)
        elif isinstance(value, dict):
            value = decode_dict(value)
        rv[key] = value
    return rv

# Convert CPU list format (with ranges) to a Python list


def cpus2list(s):
    # The input string is a comma separated list of digits and ranges.
    # Examples include:
    # 0-3,8-11
    # 0,2,4,6
    # 2,5-7,10
    cpus = []
    if s.strip() == '':
        return cpus
    for r in s.split(','):
        if '-' in r[1:]:
            start, end = r.split('-', 1)
            for i in range(int(start), int(end) + 1):
                cpus.append(int(i))
        else:
            cpus.append(int(r))
    return cpus


# Helper function to ignore suspended jobs when removing
# "already used" resources
def job_to_be_ignored(jobid):
    # Get job substate from small utility that calls printjob
    pbs_exec = ''
    if not CRAY_12_BRANCH:
        pbs_conf = pbs.get_pbs_conf()
        pbs_exec = pbs_conf['PBS_EXEC']
    else:
        if 'PBS_EXEC' in self.cfg:
            pbs_exec = self.cfg['PBS_EXEC']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "PBS_EXEC needs to be defined in the config file")
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Exiting the cgroups hook")
            pbs.accept()

    printjob_cmd = pbs_exec+os.sep+'bin'+os.sep+'printjob'

    cmd = [printjob_cmd, jobid]

    substate = None
    try:
        pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
        # Collect the job substate information
        process = subprocess.Popen(
                                   cmd,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        (out, err) = process.communicate()
        # Find the job substate
        substate_re = re.compile(r"substate:\s+(?P<jobstate>\S+)\s+")
        substate = substate_re.search(out)
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Unexpected error in job_to_be_ignored: %s" %
                   ' '.join([repr(sys.exc_info()[0]),
                            repr(sys.exc_info()[1])]))
        out = "Unknown: failed to run " + printjob_cmd

    if substate is not None:
        substate = substate.group().split(':')[1].strip()
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Job %s has substate %s" % (jobid, substate))

        suspended_substates = ['0x2b', '0x2d', 'unknown']
        return (substate in suspended_substates)
    else:
        return False

# ============================================================================
# Utility classes
# ============================================================================

#
# CLASS HookUtils
#


class HookUtils:

    def __init__(self, hook_events=None):
        if hook_events is not None:
            self.hook_events = hook_events
        else:
            self.hook_events = {
                # Defined in the order they appear in module_pbs_v1.c
                pbs.QUEUEJOB: {
                    'name': 'queuejob',
                    'handler': None
                },
                pbs.MODIFYJOB: {
                    'name': 'modifyjob',
                    'handler': None
                },
                pbs.RESVSUB: {
                    'name': 'resvsub',
                    'handler': None
                },
                pbs.MOVEJOB: {
                    'name': 'movejob',
                    'handler': None
                },
                pbs.RUNJOB: {
                    'name': 'runjob',
                    'handler': None
                },
                pbs.PROVISION: {
                    'name': 'provision',
                    'handler': None
                },
                pbs.EXECJOB_BEGIN: {
                    'name': 'execjob_begin',
                    'handler': self.__execjob_begin_handler
                },
                pbs.EXECJOB_PROLOGUE: {
                    'name': 'execjob_prologue',
                    'handler': None
                },
                pbs.EXECJOB_EPILOGUE: {
                    'name': 'execjob_epilogue',
                    'handler': self.__execjob_epilogue_handler
                },
                pbs.EXECJOB_PRETERM: {
                    'name': 'execjob_preterm',
                    'handler': None
                },
                pbs.EXECJOB_END: {
                    'name': 'execjob_end',
                    'handler': self.__execjob_end_handler
                },
                pbs.EXECJOB_LAUNCH: {
                    'name': 'execjob_launch',
                    'handler': self.__execjob_launch_handler
                },
                pbs.EXECHOST_PERIODIC: {
                    'name': 'exechost_periodic',
                    'handler': self.__exechost_periodic_handler
                },
                pbs.EXECHOST_STARTUP: {
                    'name': 'exechost_startup',
                    'handler': self.__exechost_startup_handler
                },
                pbs.MOM_EVENTS: {
                    'name': 'mom_events',
                    'handler': None
                },
            }

            if not CRAY_12_BRANCH:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "CRAY 12 Branch: %s" % (CRAY_12_BRANCH))
                self.hook_events[pbs.EXECJOB_ATTACH] = {
                    'name': 'execjob_attach',
                    'handler': self.__execjob_attach_handler
                }

    def __repr__(self):
        return "HookUtils(%s)" % (repr(self.hook_events))

    def event_name(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['name']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Type: %s not found" % (caller_name(), type))
            return None

    def hashandler(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['handler'] is not None
        else:
            return None

    def invoke_handler(self, event, cgroup, jobutil, *args):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: UID: real=%d, effective=%d" %
                   (caller_name(), os.getuid(), os.geteuid()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: GID: real=%d, effective=%d" %
                   (caller_name(), os.getgid(), os.getegid()))
        if self.hashandler(event.type):
            result = self.hook_events[event.type][
                'handler'](event, cgroup, jobutil, *args)
            return result
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: %s event not handled by this hook." %
                       (caller_name(), self.event_name(event.type)))

    def __execjob_begin_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Instantiate the NodeConfig class for get_memory_on_node and
        # get_vmem_on node
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Host assigned job resources: %s" %
                   (caller_name(), jobutil.host_resources))

        # Make sure the parent cgroup directories exist
        cgroup.create_paths()

        # Make sure that any old cgroups created in an earlier attempt
        # at creating or running the job are gone
        cgroup.delete(e.job.id)

        # Gather the assigned resources
        cgroup.gather_assigned_resources()
        # Create the cgroup(s) for the job
        cgroup.create_job(e.job.id, node)
        # Configure the new cgroup
        cgroup.configure_job(e.job.id, jobutil.host_resources, node)
        # Initialize resource usage for the job
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        # Write out the assigned resources
        cgroup.write_out_cgroup_host_assigned_resources(e.job.id)
        # Write out the environment variable for the host (pbs_attach)
        if 'devices_name' in cgroup.host_assigned_resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Devices: %s" %
                       cgroup.host_assigned_resources['devices_name'])
            env_list = list()
            if len(cgroup.host_assigned_resources['devices_name']) > 0:
                line = str()
                mics = list()
                gpus = list()
                for key in cgroup.host_assigned_resources['devices_name']:
                    if key.startswith('mic'):
                        mics.append(key[3:])
                    elif key.startswith('nvidia'):
                        gpus.append(key[6:])
                if len(mics) > 0:
                    env_list.append('OFFLOAD_DEVICES="%s"' %
                                    string.join(mics, ','))
                if len(gpus) > 0:
                    # don't put quotes around the values. ex "0" or "0,1".
                    # This will cause it to fail.
                    env_list.append('CUDA_VISIBLE_DEVICES=%s' %
                                    string.join(gpus, ','))

            pbs.logmsg(pbs.EVENT_DEBUG, "ENV_LIST: %s" % env_list)
            cgroup.write_out_cgroup_host_job_env_file(e.job.id, env_list)
        return True

    def __execjob_epilogue_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # The resources_used information has a base type of pbs_resource
        # Set the usage data
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        return True

    def __execjob_end_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Delete the cgroup(s) for the job
        cgroup.delete(e.job.id)
        # Remove the host_assigned_resources and job_env file
        for filename in [cgroup.hook_storage_dir+os.sep+e.job.id,
                         cgroup.host_job_env_filename % e.job.id]:
            try:
                os.remove(filename)
            except OSError:
                pbs.logmsg(pbs.EVENT_DEBUG3, "File: %s not found" % (filename))
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Error removing file: %s" % (filename))
                pass

        return True

    def __execjob_launch_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Add the parent process id to the appropriate cgroups.
        cgroup.add_pids(os.getppid(), jobutil.job.id)
        # FUTURE: Add environment variable to the job environment
        # if job requested mic or gpu
        cgroup.read_in_cgroup_host_assigned_resources(e.job.id)
        if cgroup.host_assigned_resources is not None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "host_assigned_resources: %s" %
                       (cgroup.host_assigned_resources))
            cgroup.setup_job_devices_env()
        return True

    def __exechost_periodic_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Cleanup cgroups for jobs not present on this node
        count = cgroup.cleanup_orphans(e.job_list)
        if ('periodic_resc_update' in cgroup.cfg and
                cgroup.cfg['periodic_resc_update']):
            for job in e.job_list.keys():
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: job is %s" %
                           (caller_name(), job))
                cgroup.update_job_usage(job, e.job_list[job].resources_used)
        # Online nodes that were offlined due to a cgroup not cleaning up
        if count == 0 and cgroup.cfg['online_offlined_nodes']:
            vnode = pbs.event().vnode_list[cgroup.hostname]
            if os.path.isfile(cgroup.offline_file):
                line = 'Orphan cgroup(s) have been cleaned up. '

                # Check with the pbs server to see if the node comment matches
                try:
                    tmp_comment = pbs.server().vnode(cgroup.hostname).comment
                except:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Unable to contact server for node comment")
                    tmp_comment = None
                if tmp_comment == cgroup.offline_msg:
                    line += 'Will bring the node back online'
                    vnode.state = pbs.ND_FREE
                    vnode.comment = None
                else:
                    line += 'However, the comment has changed since the node '
                    line += 'was offlined. Node will remain offline'

                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: %s" % (caller_name(), line))
                # Remove file
                os.remove(cgroup.offline_file)
        return True

    def __exechost_startup_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        cgroup.create_paths()
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))

        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        node.create_vnodes()
        if 'memory' in cgroup.subsystems:
            val = node.get_memory_on_node(cgroup.cfg)
            if val is not None:
                if 'vnode_per_numa_node' in cgroup.cfg:
                    if not cgroup.cfg['vnode_per_numa_node']:
                        hn = node.hostname
                        e.vnode_list[hn].resources_available['mem'] = \
                            pbs.size(val)
                        val_cpus = node.totalcpus
                        if val_cpus is not None:
                            e.vnode_list[hn].resources_available['ncpus'] = \
                                int(val_cpus)
                cgroup.set_node_limit('mem', val)
        if 'memsw' in cgroup.subsystems:
            val = node.get_vmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['vmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('vmem', val)
        if 'hugetlb' in cgroup.subsystems:
            val = node.get_hpmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['hpmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('hpmem', val)
        return True

    def __execjob_attach_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logjobmsg(jobutil.job.id, "%s: Attaching PID %s" %
                      (caller_name(), e.pid))
        # Add the job process id to the appropriate cgroups.
        cgroup.add_pids(e.pid, jobutil.job.id)
        return True

#
# CLASS ShallIRunUtils
#


class ShallIRunUtils:

    def __init__(self, hostname, cfg):
        self.cfg = cfg
        self.hostname = hostname

    def allow_users(self, allow_users):
        """ This still needs to be defined """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pass

    def exclude_hosts(self, exclude_hosts):
        """
        Gets the hostname for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        hostname = pbs.get_local_nodename()
        # check to see if hostname is in the exclude_hosts list
        if self.hostname in exclude_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: exclude host %s is in %s" %
                       (caller_name(), self.hostname, exclude_hosts))
            pbs.event().accept()

        # check to see if it regex syntax is used
        pass

    def exclude_vntypes(self, exclude_vntypes):
        """
        Gets the vntype for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        vntype = None

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'
        if os.path.isfile(vntype_file):
            fdata = open(vntype_file).readlines()
            vntype = fdata[0].strip()
            pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
        if vntype is None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype is set to %s" % (vntype))
        elif vntype in exclude_vntypes:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s is in %s" % (vntype, exclude_vntypes))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Exiting Hook")
            pbs.event().accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s not in global exclude: %s" %
                       (vntype, exclude_vntypes))
        pass

    def run_only_on_hosts(self, approved_hosts):
        """
        Gets the list of approved nodes to run on and checks to see if the hook
        should run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Approved hosts: %s, %s" %
                   (type(approved_hosts), approved_hosts))

        # check to see if hostname is in the approved_hosts list
        if approved_hosts == []:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "approved hosts list is empty: %s" % approved_hosts)
        elif self.hostname not in approved_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s is not in the approved list of hosts: %s" %
                       (self.hostname, approved_hosts))
            pbs.event().accept()

        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s in list of approved hosts: %s" %
                       (self.hostname, approved_hosts))

#
# CLASS JobUtils
#


class JobUtils:

    def __init__(self, job, hostname=None, resources=None):
        self.job = job
        self.host_vnodes = list()
        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if resources is not None:
            self.host_resources = resources
        else:
            self.host_resources = self.__host_assigned_resources()

    def __repr__(self):
        return "JobUtils(%s, %s, %s)" % (repr(self.job), repr(self.hostname),
                                         repr(self.host_resources))

    # Return a dictionary of resources assigned to the local node
    def __host_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Bail out if no hostname was provided
        if self.hostname is None:
            raise CgroupProcessingError('No hostname available')
        # Bail out if no job information is present
        if self.job is None:
            raise CgroupProcessingError('No job information available')
        # Determine which vnodes are on this host, if any
        if self.hostname is not None:
            vnhost_pattern = "%s\[[\d+]\]" % self.hostname
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnhost pattern: %s" %
                       (caller_name(), vnhost_pattern))
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job exec_vnode list: %s" %
                       (caller_name(), self.job.exec_vnode))
            for match in re.findall(vnhost_pattern, str(self.job.exec_vnode)):
                self.host_vnodes.append(match)
            if self.host_vnodes is not None:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Associate %s to vnodes on %s" %
                           (caller_name(), self.host_vnodes, self.hostname))

        # Collect host assigned resources
        resources = {}
        for chunk in self.job.exec_vnode.chunks:
            vnode = False
            if self.host_vnodes == [] and chunk.vnode_name != self.hostname:
                continue
            elif self.host_vnodes != [] and \
                    chunk.vnode_name not in self.host_vnodes:
                continue
            elif chunk.vnode_name in self.host_vnodes:
                vnode = True
                if 'vnodes' not in resources:
                    resources['vnodes'] = {}
                if chunk.vnode_name not in resources['vnodes']:
                    resources['vnodes'][chunk.vnode_name] = {}
                # This check is needed since not all resources are
                # required to be in each chunk of a job
                # i.e. exec_vnodes =
                # (node1[0]:ncpus=4:mem=4gb+node1[1]:mem=2gb) +
                # (node1[1]:ncpus=3+node[0]:ncpus=1:mem=4gb)
                if all(resc in resources['vnodes'][chunk.vnode_name]
                       for resc in chunk.chunk_resources.keys()):
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: resources already defined" %
                               (caller_name()))
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s,\t%s" %
                               (caller_name(),
                                chunk.chunk_resources.keys(),
                                resources['vnodes'][chunk.vnode_name].keys()))
                else:
                    # Initialize resources for each vnode
                    for resc in chunk.chunk_resources.keys():
                        # Initialize the new value
                        if isinstance(chunk.chunk_resources[resc],
                                      pbs.pbs_int):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_int(0)
                        elif isinstance(chunk.chunk_resources[resc],
                                        pbs.pbs_float):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_float(0)
                        elif isinstance(chunk.chunk_resources[resc], pbs.size):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.size('0')

            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Chunk %s" %
                       (caller_name(), chunk.vnode_name))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resources: %s" %
                       (caller_name(), resources))
            for resc in chunk.chunk_resources.keys():
                if resc not in resources.keys():
                    # Initialize the new value
                    if isinstance(chunk.chunk_resources[resc], pbs.pbs_int):
                        resources[resc] = pbs.pbs_int(0)
                    elif isinstance(chunk.chunk_resources[resc],
                                    pbs.pbs_float):
                        resources[resc] = pbs.pbs_float(0.0)
                    elif isinstance(chunk.chunk_resources[resc], pbs.size):
                        resources[resc] = pbs.size('0')
                # Add resource value to total
                if type(chunk.chunk_resources[resc]) in \
                        [pbs.pbs_int, pbs.pbs_float, pbs.size]:
                    resources[resc] += chunk.chunk_resources[resc]
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s: resources[%s][%s] is now %s" %
                               (caller_name(), self.hostname, resc,
                                resources[resc]))
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] += \
                                  chunk.chunk_resources[resc]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Setting resource %s to string %s" %
                               (caller_name(), resc,
                                str(chunk.chunk_resources[resc])))
                    resources[resc] = str(chunk.chunk_resources[resc])
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] = \
                                  str(chunk.chunk_resources[resc])
        if not resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: No resources assigned to host %s" %
                       (caller_name(), self.hostname))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources for %s: %s" %
                       (caller_name(), self.hostname, repr(resources)))

        # Return assigned resources for specified host
        pbs.logmsg(pbs.EVENT_DEBUG3, "Job Resources: %s" % (resources))
        return resources

    # Write a message to the job stdout file
    def write_to_stderr(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stderr_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

    # Write a message to the job stdout file
    def write_to_stdout(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stdout_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

#
# CLASS NodeConfig
#


class NodeConfig:

    def __init__(self, cfg, **kwargs):

        self.cfg = cfg
        hostname = None
        meminfo = None
        numa_nodes = None
        devices = None
        hyperthreading = None
        self.hyperthreads_per_core = 1
        self.totalcpus = self.__discover_cpus()

        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        self.host_mom_jobdir = pbs_home+'/mom_priv/jobs'

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'hostname':
                    hostname = val
                elif arg == 'meminfo':
                    meminfo = val
                elif arg == 'numa_nodes':
                    numa_nodes = val
                elif arg == 'devices':
                    devices = val
                elif arg == 'hyperthreading':
                    hyperthreading = val

        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if meminfo is not None:
            self.meminfo = meminfo
        else:
            self.meminfo = self.__discover_meminfo()
        if numa_nodes is not None:
            self.numa_nodes = numa_nodes
        else:
            self.numa_nodes = self.__discover_numa_nodes()
        if devices is not None:
            self.devices = devices
        else:
            self.devices = self.__discover_devices()
        if hyperthreading is not None:
            self.hyperthreading = hyperthreading
        else:
            self.hyperthreading = self.__hyperthreading_enabled()

        # Add the devices count i.e. nmics and ngpus to the numa nodes
        self.__add_devices_to_numa_node()

    def __repr__(self):
        return ("NodeConfig(%s, %s, %s, %s, %s, %s)" %
                (repr(self.cfg), repr(self.hostname), repr(self.meminfo),
                 repr(self.numa_nodes), repr(self.devices),
                 repr(self.hyperthreading)))

    def __add_devices_to_numa_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (self.devices.keys()))
        for device in self.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" % (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" %
                           (self.devices[device].keys()))
                for device_name in self.devices[device]:
                    device_socket = \
                        self.devices[device][device_name]['numa_node']
                    if device == 'mic':
                        if 'nmics' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['nmics'] = 1
                        else:
                            self.numa_nodes[device_socket]['nmics'] += 1
                    elif device == 'gpu':
                        if 'ngpus' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['ngpus'] = 1
                        else:
                            self.numa_nodes[device_socket]['ngpus'] += 1

        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (self.numa_nodes))

    # Discover what type of hardware is on this node and how is it partitioned
    def __discover_numa_nodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        numa_nodes = {}
        for dir in glob.glob(os.path.join(os.sep,
                             "sys", "devices", "system", "node", "node*")):
            id = int(dir.split(os.sep)[5][4:])
            if id not in numa_nodes.keys():
                numa_nodes[id] = {}
                numa_nodes[id]['devices'] = list()
            numa_nodes[id]['cpus'] = open(os.path.join(dir, "cpulist"),
                                          'r').readline().strip()
            with open(os.path.join(dir, "meminfo"), 'r') as fd:
                for line in fd:
                    # Each line will contain four or five fields. Examples:
                    # Node 0 MemTotal:       32995028 kB
                    # Node 0 HugePages_Total:     0
                    entries = line.split()
                    if len(entries) < 4:
                        continue
                    if entries[2] == "MemTotal:":
                        numa_nodes[id][entries[2].rstrip(':')] = \
                            convert_size(entries[3] + entries[4], 'kb')
                    elif entries[2] == "HugePages_Total:":
                        numa_nodes[id][entries[2].rstrip(':')] = entries[3]
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover Numa Nodes: %s" % numa_nodes)
        return numa_nodes

    # Identify devices and to which numa nodes they are attached
    def __discover_devices(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        devices = {}
        for file in glob.glob(os.path.join(os.sep, "sys", "class", "*", "*",
                                           "device", "numa_node")):
            # The file should contain a single integer
            numa_node = int(open(file, 'r').readline().strip())
            if numa_node < 0:
                numa_node = 0
            dirs = file.split(os.sep)
            name = dirs[3]
            instance = dirs[4]
            if name not in devices.keys():
                devices[name] = {}
            devices[name][instance] = {}
            devices[name][instance]['numa_node'] = numa_node
            if name == 'mic':
                s = os.stat('/dev/%s' % instance)
                devices[name][instance]['major'] = os.major(s.st_rdev)
                devices[name][instance]['minor'] = os.minor(s.st_rdev)
                devices[name][instance]['device_type'] = 'c'

        # Check to see if there are gpus on the node
        gpus = self.discover_gpus()
        if isinstance(gpus, dict) and len(gpus.keys()) > 0:
            devices['gpu'] = gpus['gpu']

        pbs.logmsg(pbs.EVENT_DEBUG3, "Discovered devices: %s" % (devices))
        return devices

    def discover_gpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Check to see if nvidia-smi exists and produces valid output
            cmd = ['/usr/bin/nvidia-smi', '-q', '-x']
            if 'nvidia-smi' in self.cfg:
                cmd[0] = self.cfg['nvidia-smi']

            pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
            # Collect the gpu information
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.PIPE)
            output, err = process.communicate()

            # pbs.logmsg(pbs.EVENT_DEBUG3,"output: %s" % output)
            # import the xml library and parse the output
            import xml.etree.ElementTree as ET

            # Parse the data
            name = 'gpu'
            gpu_data = dict()
            gpu_data[name] = {}
            root = ET.fromstring(output)
            pbs.logmsg(pbs.EVENT_DEBUG3, "root.tag: %s" % root.tag)
            for child in root:
                if child.tag == "attached_gpus":
                    # gpu_data['ngpus'] = int(child.text)
                    pass
                if child.tag == "gpu":
                    device_id = child.get('id')
                    number = 'nvidia%s' % child.find('minor_number').text
                    gpu_data[name][number] = dict()
                    gpu_data[name][number]['id'] = device_id

                    # Determine which socket the device is on
                    filename = '/sys/bus/pci/devices/%s/numa_node' % \
                        device_id.lower()
                    isfile = os.path.isfile(filename)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s, %s" % (filename, isfile))
                    if isfile:
                        numa_node = open(filename).read().strip()
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "numa_node: %s" % (numa_node))
                        if int(numa_node) == -1:
                            numa_node = 0
                        gpu_data[name][number]['numa_node'] = int(numa_node)
                        try:
                            dev_filename = '/dev/%s' % number
                            s = os.stat(dev_filename)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find %s" % dev_filename)
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                        gpu_data[name][number]['major'] = os.major(s.st_rdev)
                        gpu_data[name][number]['minor'] = os.minor(s.st_rdev)
                        gpu_data[name][number]['device_type'] = 'c'
            pbs.logmsg(pbs.EVENT_DEBUG, "%s" % gpu_data)
            return gpu_data
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unable to find %s" % string.join(cmd, " "))
            return {'gpu': {}}
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        return {'gpu': {}}

    # Get the memory info on this host
    def __discover_meminfo(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        meminfo = {}
        with open(os.path.join(os.sep, "proc", "meminfo"), 'r') as fd:
            for line in fd:
                entries = line.split()
                if entries[0] == "MemTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "SwapTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "Hugepagesize:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "HugePages_Total:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
                elif entries[0] == "HugePages_Rsvd:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover meminfo: %s" % meminfo)
        return meminfo

    # Determine if the cpus have hyperthreading enabled
    def __hyperthreading_enabled(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            siblings = 0
            cpu_cores = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "siblings":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    siblings = entries[1].strip()
                elif entries[0].strip() == "cpu cores":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_cores = entries[1].strip()
                elif entries[0].strip() == "flags":
                    flag_options = entries[1].split()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: flag_options: %s" %
                               (caller_name(), flag_options))
                    if "ht" in flag_options:
                        rc = True
                        break
        if rc is True:
            self.hyperthreads_per_core = int(siblings)/int(cpu_cores)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: hyperthreads/core: %d" %
                       (caller_name(), self.hyperthreads_per_core))
            if self.hyperthreads_per_core == 1:
                rc = False
        return rc

    def __discover_cpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            cpu_threads = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "processor":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_threads += 1
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: processors found: %d" %
                   (caller_name(), cpu_threads))

        return cpu_threads

    # Gather the jobs running on this node
    def gather_jobs_on_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        # Get the job list from the server
        jobs = None
        try:
            jobs = pbs.server().vnode(self.hostname).jobs
        except:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Unable to contact server to get jobs")
            return None

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs from server for %s: %s" % (self.hostname, jobs))

        if jobs is None:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "No jobs found on %s " % (self.hostname))
            return list()

        jobs_list = dict()
        for job in jobs.split(','):
            # The job list from the node has a space in front of the job id
            # This must be removed or it will not match the cgroup dir
            jobs_list[job.split('/')[0].strip()] = 0
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs on %s: %s" % (self.hostname, jobs_list.keys()))

        return jobs_list.keys()

    # Gather the jobs running on this node
    def gather_jobs_on_node_local(self):
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Method called" % (caller_name()))
        # Get the job list from the jobs directory
        jobs_list = list()
        try:
            for file in os.listdir(self.host_mom_jobdir):
                if fnmatch.fnmatch(file, "*.JB"):
                    jobs_list.append(file[:-3])
            if not jobs_list:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "No jobs found on %s " % (self.hostname))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Jobs from server for %s: %s" %
                           (self.hostname, repr(jobs_list)))

            return jobs_list
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Could not get job list from mom_priv/jobs for %s" %
                       self.hostname)

            return self.gather_jobs_on_node()

    # Get the memory resource on this mom
    def get_memory_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Calculate memory
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
            pbs.logmsg(pbs.EVENT_DEBUG3, "val: %s" % val)
        else:
            val = size_as_int(MemTotal)
        if 'percent_reserve' in config['cgroup']['memory']:
            percent_reserve = config['cgroup']['memory']['percent_reserve']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memory'].keys():
            reserve_memory = config['cgroup']['memory']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                reserve_mem = size_as_int(reserve_memory)
                if val > reserve_mem:
                    val -= reserve_mem
                else:
                    val -= size_as_int("100mb")
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Unable to reserve more memory than " +
                               "available on the node. Available %s, " +
                               "Tried to reserve %s. Reserving 100mb" %
                               (val, reserve_mem))

            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))

        pbs.logmsg(pbs.EVENT_DEBUG3, "Return val: %s" % val)
        return convert_size(str(val), 'kb')

    # Get the virtual memory resource on this mom
    def get_vmem_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Return the value for memory if no swap is configured
        if size_as_int(self.meminfo['SwapTotal']) <= 0:
            return self.get_memory_on_node(config)
        # Calculate vmem
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
        else:
            val = size_as_int(MemTotal)

        val += size_as_int(self.meminfo['SwapTotal'])
        # Determine if memory needs to be reserved
        if 'percent_reserve' in config['cgroup']['memsw']:
            percent_reserve = config['cgroup']['memsw']['reserve_memory']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memsw']:
            reserve_memory = config['cgroup']['memsw']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                val -= size_as_int(reserve_memory)
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))
        return convert_size(str(val), 'kb')

    # Get the huge page memory resource on this mom
    def get_hpmem_on_node(self, config):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        if 'percent_reserve' in config['cgroup']['hugetlb'].keys():
            percent_reserve = config['cgroup']['hugetlb']['percent_reserve']
        else:
            percent_reserve = 0
        # Calculate vmem
        val = size_as_int(self.meminfo['Hugepagesize'])
        val *= (self.meminfo['HugePages_Total'] -
                self.meminfo['HugePages_Rsvd'])
        val -= (val * percent_reserve) / 100
        return convert_size(str(val), 'kb')

    # Create individual vnodes per socket
    def create_vnodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        key = "vnode_per_numa_node"
        if key in self.cfg.keys():
            if not self.cfg[key]:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s is false" %
                           (caller_name(), key))
                return
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s not configured" %
                       (caller_name(), key))
            return

        # Create one vnode per numa node
        vnode_list = pbs.event().vnode_list
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: numa nodes: %s" %
                   (caller_name(), self.numa_nodes))
        # Define the vnodes
        vnode_name = self.hostname
        vnode_list[vnode_name] = pbs.vnode(vnode_name)
        for id in self.numa_nodes.keys():
            vnode_name = self.hostname + "[%d]" % id
            vnode_list[vnode_name] = pbs.vnode(vnode_name)
            for key, val in sorted(self.numa_nodes[id].iteritems()):
                if key == 'cpus':
                    vnode_list[vnode_name].resources_available['ncpus'] = \
                        len(cpus2list(val))/self.hyperthreads_per_core
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['ncpus'] = 0
                elif key == 'MemTotal':
                    mem = self.get_memory_on_node(self.cfg, val)
                    vnode_list[vnode_name].resources_available['mem'] = \
                        pbs.size(mem)
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['mem'] = \
                        pbs.size('0kb')
                elif key == 'HugePages_Total':
                    pass
                elif isinstance(val, list):
                    pass
                elif isinstance(val, dict):
                    pass
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s=%s" %
                               (caller_name(), key, val))
                    vnode_list[vnode_name].resources_available[key] = val

                    if isinstance(val, int):
                        vnode_list[self.hostname].resources_available[key] = 0
                    elif isinstance(val, float):
                        vnode_list[self.hostname].resources_available[key] = \
                            0.0
                    elif isinstance(val, pbs.size):
                        vnode_list[self.hostname].resources_available[key] = \
                            pbs.size('0kb')
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnode list: %s" %
                   (caller_name(), vnode_list))
        return True

#
# CLASS CgroupUtils
#


class CgroupUtils:
    def __init__(self, hostname, vnode, **kwargs):
        cfg = None
        subsystems = None
        paths = None
        vntype = None
        event = None
        self.assigned_resources = dict()
        self.existing_cgroups = dict()
        self.host_assigned_resources = None
        self.pid_lower_limit = 3

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'subsystems':
                    subsystems = val
                elif arg == 'paths':
                    paths = val
                elif arg == 'vntype':
                    vntype = val
                elif arg == 'event':
                    event = val

        try:
            self.hostname = hostname
            self.vnode = vnode
            # __check_os will raise an exception if cgroups are not present
            self.__check_os()
            # Read in the config file
            if cfg is not None:
                self.cfg = cfg
            else:
                self.cfg = self.__parse_config_file()

            # Determine if the hook should run or exit
            siru = ShallIRunUtils(self.hostname, self.cfg)
            siru.exclude_hosts(self.cfg['exclude_hosts'])
            siru.exclude_vntypes(self.cfg['exclude_vntypes'])
            siru.run_only_on_hosts(self.cfg['run_only_on_hosts'])

            # Collect the mount points
            if paths is not None:
                self.paths = paths
            else:
                self.paths = self.__set_paths()
            # Define the local vnode type
            if vntype is not None:
                self.vntype = vntype
            else:
                self.vntype = self.__get_vnode_type()
            # Determine which subsystems we care about
            if subsystems is not None:
                self.subsystems = subsystems
            else:
                self.subsystems = self.__target_subsystems()

            # location to store information for the different hook events
            pbs_home = ''
            if not CRAY_12_BRANCH:
                pbs_conf = pbs.get_pbs_conf()
                pbs_home = pbs_conf['PBS_HOME']
            else:
                if 'PBS_HOME' in self.cfg:
                    pbs_home = self.cfg['PBS_HOME']
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "PBS_HOME needs to be defined in the " +
                               "config file")
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Exiting the cgroups hook")
                    pbs.accept()

            self.hook_storage_dir = pbs_home+'/mom_priv/hooks/hook_data'
            self.host_job_env_dir = pbs_home+'/aux'
            self.host_job_env_filename = self.host_job_env_dir+os.sep+"%s.env"

            # information for offlining nodes
            self.offline_file = \
                pbs_home+os.sep+'mom_priv'+os.sep+'hooks'+os.sep
            self.offline_file += "%s.offline" % pbs.event().hook_name
            self.offline_msg = "Hook %s: " % pbs.event().hook_name
            self.offline_msg += "Unable to clean up one or more cgroups"

        except:
            raise

    def __repr__(self):
        return ("CgroupUtils(%s, %s, %s, %s, %s, %s)" %
                (repr(self.hostname), repr(self.vnode), repr(self.cfg),
                 repr(self.subsystems), repr(self.paths), repr(self.vntype)))

    # Validate the OS type and version
    def __check_os(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check to see if the platform is linux and the kernel is new enough
        if platform.system() != 'Linux':
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: OS does not support cgroups" %
                       (caller_name()))
            raise CgroupConfigError("OS type not supported")
        rel = map(int,
                  string.split(string.split(platform.release(), '-')[0], '.'))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Detected Linux kernel version %d.%d.%d" %
                   (caller_name(), rel[0], rel[1], rel[2]))
        supported = False
        if rel[0] > 2:
            supported = True
        elif rel[0] == 2:
            if rel[1] > 6:
                supported = True
            elif rel[1] == 6:
                if rel[2] >= 28:
                    supported = True
        if not supported:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Kernel needs to be >= 2.6.28: %s" %
                       (caller_name(), system_info[2]))
            raise CgroupConfigError("OS version not supported")
        return supported

    # Read the config file in json format
    def __parse_config_file(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        config = {}
        # Identify the config file and read in the data
        if 'PBS_HOOK_CONFIG_FILE' in os.environ:
            config_file = os.environ["PBS_HOOK_CONFIG_FILE"]
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Config file is %s" %
                       (caller_name(), config_file))
            try:
                config = json.load(open(config_file, 'r'),
                                   object_hook=decode_dict)
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
        else:
            raise CgroupConfigError("No configuration file present")
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Initial Cgroup Config: %s" %
                   (caller_name(), config))
        # Set some defaults if they are not present
        if 'cgroup_prefix' not in config.keys():
            config['cgroup_prefix'] = 'pbspro'
        if 'periodic_resc_update' not in config.keys():
            config['periodic_resc_update'] = False
        if 'vnode_per_numa_node' not in config.keys():
            config['vnode_per_numa_node'] = False
        if 'online_offlined_nodes' not in config.keys():
            config['online_offlined_nodes'] = False
        if 'exclude_hosts' not in config.keys():
            config['exclude_hosts'] = []
        if 'run_only_on_hosts' not in config.keys():
            config['run_only_on_hosts'] = []
        if 'exclude_vntypes' not in config.keys():
            config['exclude_vntypes'] = []
        if 'cgroup' not in config.keys():
            config['cgroup'] = {}
        else:
            for subsys in config['cgroup']:
                subsystem = config['cgroup'][subsys]
                if 'enabled' not in subsystem.keys():
                    config['cgroup'][subsys]['enabled'] = False
                if 'exclude_hosts' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_hosts'] = []
                if 'exclude_vntypes' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_vntypes'] = []

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Cgroup Config with defaults: %s" %
                   (caller_name(), config))
        return config

    # Determine which subsystems are being requested
    def __target_subsystems(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        subsystems = []
        for key in self.cfg['cgroup'].keys():
            if self.enabled(key):
                subsystems.append(key)
        if 'memory' not in subsystems:
            if 'memsw' in subsystems:
                # Remove memsw since it must be greater then
                # or equal to memory
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing memsw from enabled subsystems")
                subsystems.remove('memsw')
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Enabled subsystems: %s" %
                   (caller_name(), subsystems))
        # It is not an error for all subsystems to be disabled.
        # This host or vnode type may be in the excluded list.
        return subsystems

    # Copy a setting from the parent cgroup
    def __copy_from_parent(self, dest):
        try:
            file = os.path.basename(dest)
            dir = os.path.dirname(dest)
            parent = os.path.dirname(dir)
            source = os.path.join(parent, file)
            if not os.path.isfile(source):
                raise CgroupConfigError('Failed to read %s' % (source))
            self.write_value(dest, open(source, 'r').read().strip())
        except:
            raise

    # Create a dictionary of the cgroup subsystems and their corresponding
    # directories
    def __set_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        paths = {}
        try:
            # Loop through the mounts and collect the ones for cgroups
            with open(os.path.join(os.sep, "proc", "mounts"), 'r') as fd:
                for line in fd:
                    entries = line.split()
                    if len(entries) > 3 and entries[2] == "cgroup":
                        flags = entries[3].split(',')
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "subsysdir: %s (flags=%s)" %
                                   (entries[1], flags))
                        for subsys in flags:
                            paths[subsys] = os.path.join(
                                entries[1], self.cfg["cgroup_prefix"])
        except:
            raise
        if len(paths.keys()) < 1:
            raise CgroupConfigError("Cgroup paths not detected")
        # Both the memory and memsw cgroups share the same mount point
        if "memory" in paths.keys():
            paths["memsw"] = paths["memory"]
        return paths

    # Create the cgroup parent directories that will contain the jobs
    def create_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Create the directories that PBS will use to house the jobs
            old_umask = os.umask(0022)
            for subsys in self.subsystems:
                if subsys not in self.paths.keys():
                    raise CgroupConfigError('No path for subsystem: %s' %
                                            (subsys))
                if not os.path.exists(self.paths[subsys]):
                    os.makedirs(self.paths[subsys], 0755)
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Created directory %s" %
                               (caller_name(), self.paths[subsys]))
                    if subsys == 'memory' or subsys == 'memsw':
                        # Set file to
                        # <cgroup>/memory/pbspro/memory.use_hierarchy
                        file = os.path.join(self.paths[subsys],
                                            'memory.use_hierarchy')
                        if not os.path.isfile(file):
                            raise CgroupConfigError('Failed to configure %s' %
                                                    (file))
                        self.write_value(file, 1)
                    elif subsys == 'cpuset':
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.cpus'))
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.mems'))

        except:
            raise CgroupConfigError("Failed to create directory: %s" %
                                    (self.paths[subsys]))
        finally:
            os.umask(old_umask)

    # Return the vnode type of the local node
    def __get_vnode_type(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")

        # File to check for if it is not defined in the hook input file
        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'

        # self.vnode is None for pbs_attach events
        pbs.logmsg(pbs.EVENT_DEBUG3, "vnode: %s" % self.vnode)
        if self.vnode is not None:
            if 'vntype' in self.vnode.resources_available and \
                    self.vnode.resources_available['vntype'] is not None:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "vntype: %s" %
                           self.vnode.resources_available['vntype'])
                return self.vnode.resources_available['vntype']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3, "vntype file: %s" % vntype_file)
                if os.path.isfile(vntype_file):
                    fdata = open(vntype_file).readlines()
                    vntype = fdata[0].strip()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
                    return vntype
        # If vntype was not set then log a message. It is too expensive
        # to have all moms query the server for large jobs.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: resources_available.vntype is " % caller_name() +
                   "not set for this vnode")
        return None

    # Gather resources that are already assigned
    def gather_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        assigned_resources = {}

        # Gather the cpus and memory sockets assigned
        directories = [x[0] for x in os.walk(self.paths['cpuset'])]

        # Add the existing cgroups to a list to check if assigning
        # resources fail
        for dir in directories[1:]:
            if dir.find(pbs.event().job.id) == -1:
                self.existing_cgroups[dir] = []

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'cpuset' in self.subsystems and \
                self.cfg['cgroup']['cpuset']['enabled']:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather cpuset resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['cpuset'], tmp_dir)
                    with open(path+os.sep+'cpuset.cpus') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.cpus'] = \
                            cpus2list(tmp_val.strip())
                    with open(path+os.sep+'cpuset.mems') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.mems'] = \
                            cpus2list(tmp_val.strip())

        # Check to see if the subsystem is enabled before trying
        # to gather resources
        if 'memory' in self.subsystems and \
                self.cfg['cgroup']['memory']['enabled']:
            # Gather the memory assigned for each socket
            directories = [x[0] for x in os.walk(self.paths['memory'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather memory resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['memory'], tmp_dir)
                    with open(path+os.sep+'memory.limit_in_bytes') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memory.limit'] = \
                            tmp_val.strip()
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    if os.path.isfile(tmp_filename) is False:
                        continue
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    with open(tmp_filename) as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memsw.limit'] = \
                            tmp_val.strip()

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'devices' in self.subsystems and \
                self.cfg['cgroup']['devices']['enabled']:
            # Gather the devices assigned
            directories = [x[0] for x in os.walk(self.paths['devices'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather device resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    pbs.logmsg(pbs.EVENT_DEBUG3, "Dir: %s" % tmp_dir)
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['devices'], tmp_dir)
                    with open(path+os.sep+'devices.list') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['devices.list'] = \
                            tmp_val.strip().split('\n')
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Dir: %s\n\tValue: %s" %
                                   (path, tmp_val.replace('\n', ',')))

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Gathered assigned resources: %s" % assigned_resources)
        self.assigned_resources = assigned_resources

    # Return whether a subsystem is enabled
    def enabled(self, subsystem):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check whether the subsystem is enabled in the configuration file
        if subsystem not in self.cfg['cgroup'].keys():
            return False
        if 'enabled' not in self.cfg['cgroup'][subsystem].keys():
            return False
        if not self.cfg['cgroup'][subsystem]['enabled']:
            return False
        # Check whether the cgroup is mounted for this subsystem
        if subsystem not in self.paths.keys():
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup not mounted for %s" %
                       (caller_name(), subsystem))
            return False
        # Check whether this host is excluded
        # if self.hostname in self.cfg['exclude_hosts']:
        #    pbs.logmsg(pbs.EVENT_DEBUG, "%s: cgroup excluded on host %s" %
        #               (caller_name(), self.hostname))
        #    pbs.event().accept()
        if self.hostname in self.cfg['cgroup'][subsystem]['exclude_hosts']:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup excluded for subsystem %s on host %s" %
                       (caller_name(), subsystem, self.hostname))
            return False
        # Check whether the vnode type is excluded
        if self.vntype is not None:
            # if self.vntype in self.cfg['exclude_vntypes']:
            #    pbs.logmsg(pbs.EVENT_DEBUG,
            #               "%s: cgroup excluded on vnode type %s" %
            #               (caller_name(), self.vntype))
            #    pbs.event().accept()
            if self.vntype in self.cfg['cgroup'][subsystem]['exclude_vntypes']:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           ("%s: cgroup excluded for " +
                            "subsystem %s on vnode type %s") %
                           (caller_name(), subsystem, self.vntype))
                return False
        return True

    # Return the default value for a subsystem
    def default(self, subsystem):
        if subsystem in self.cfg['cgroup']:
            if 'default' in self.cfg['cgroup'][subsystem]:
                return self.cfg['cgroup'][subsystem]['default']
        return None

    # Check to see if the pid matches the job owners uid
    def is_pid_owned_by_job_owner(self, pid):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        try:
            proc_uid = os.stat('/proc/%d' % pid).st_uid
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       '/proc/%d uid:%d' % (pid, proc_uid))

            job_owner_uid = pwd.getpwnam(pbs.event().job.euser)[2]
            pbs.logmsg(pbs.EVENT_DEBUG3, "Job uid: %d" % job_owner_uid)

            if proc_uid == job_owner_uid:
                return True
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Proc uid: %d != Job owner:  %d" %
                           (proc_uid, job_owner_uid))
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG, "Unknown pid: %d" % pid)
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        # If we got to this point something did not match up
        return False

    # Add some number of PIDs to the cgroup tasks files for each subsystem
    def add_pids(self, pids, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # make pids a list
        if isinstance(pids, int):
            pids = [pids]

        if pbs.event().type == pbs.EXECJOB_LAUNCH:
            if 1 in pids:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "1 is not a valid pid to add")
                # Hold the job for further review
                e.reject("Hook tried to add pid 1 to the tasks file " +
                         "for job %s" % jobid)

        # check pids to make sure that they are owned by the job owner
        if not CRAY_12_BRANCH:
            pbs.logmsg(pbs.EVENT_DEBUG3, "Not in the Cray 12 Branch")
            pbs.logmsg(pbs.EVENT_DEBUG3, "check to see if execjob_attach")
            if pbs.event().type == pbs.EXECJOB_ATTACH:
                pbs.logmsg(pbs.EVENT_DEBUG3, "event type: attach or launch")
                tmp_pids = list()
                for p in pids:
                    if self.is_pid_owned_by_job_owner(p):
                        tmp_pids.append(p)
                if len(tmp_pids) == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%d is not a valid pids to add" % p)
                    return False
                else:
                    pids = tmp_pids

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: subsys = %s" %
                       (caller_name(), subsys))
            # memsw and memory use the same tasks file
            if subsys == "memsw":
                if "memory" in self.subsystems:
                    continue
            path = os.path.join(self.paths[subsys], jobid)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path = %s" %
                       (caller_name(), path))
            try:
                tasks_path = os.path.join(path, "tasks")
                for p in pids:
                    self.write_value(tasks_path, p, 'a')
            except IOError, exc:
                raise CgroupLimitError("Failed to add PIDs %s to %s (%s)" %
                                       (str(pids), tasks_path,
                                        errno.errorcode[exc.errno]))
            except:
                raise

    # Set a node limit
    def set_node_limit(self, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s = %s" %
                   (caller_name(), resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'],
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Node resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    def setup_job_devices_env(self):
        """ Setup the job environment for the devices assigned to the job for an
            execjob_launch hook
         """
        if 'devices_name' in self.host_assigned_resources:
            names = self.host_assigned_resources['devices_name']
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "devices: %s" % (names))
            offload_devices = list()
            cuda_visible_devices = list()
            for name in names:
                if name.startswith('mic'):
                    offload_devices.append(name[3:])
                elif name.startswith('nvidia'):
                    cuda_visible_devices.append(name[6:])
            if len(offload_devices) > 0:
                value = '"%s"' % string.join(offload_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['OFFLOAD_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "offload_devices: %s" % offload_devices)
            if len(cuda_visible_devices) > 0:
                value = '"%s"' % string.join(cuda_visible_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['CUDA_VISIBLE_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "cuda_visible_devices: %s" % cuda_visible_devices)
            return [offload_devices, cuda_visible_devices]
        else:
            return False

    def setup_subsys_devices(self, path, node):
        subsys = 'devices'
        # Add the devices that the user is allowed to use
        if subsys in self.cfg['cgroup']:
            devices_allowed = open(os.path.join(path,
                                   "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Initial devices.list: %s" %
                       devices_allowed)
            # Deny access to mic and gpu devices
            devices_list = list()
            devices = node.devices
            for device in devices:
                if device == 'mic' or device == 'gpu':
                    for name in devices[device]:
                        d = devices[device][name]
                        devices_list.append("%d:%d" % (d['major'], d['minor']))
            # For CentOS 7 we need to remove a *:* rwm from devices.list we
            # before can add anything to devices.allow. Otherwise our changes
            # are ignored. Check to see if a *:* rwm is in devices.list.
            # If so remove it
            if "a *:* rwm\n" in devices_allowed:
                value = "a *:* rwm"
                self.write_value(os.path.join(path, 'devices.deny'), value)
            else:
                # Verify that the following devices are not in devices.list
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing access to the following: %s" %
                           devices_list)
            for device_str in devices_list:
                value = "c %s rwm" % device_str
                self.write_value(os.path.join(path, 'devices.deny'), value)
            # Add devices back to the list
            devices_allow = list()
            if 'allow' in self.cfg['cgroup'][subsys]:
                devices_allow = self.cfg['cgroup'][subsys]['allow']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Allowing access to the following: %s" %
                           devices_allow)
                for item in devices_allow:
                    if isinstance(item, str):
                        pbs.logmsg(pbs.EVENT_DEBUG3, "string item: %s" % item)
                        value = "%s" % item
                        self.write_value(os.path.join(
                                         path, 'devices.allow'), value)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "write_value: %s" % value)
                    elif isinstance(item, list):
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "list item: %s" % item)
                        try:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Device allow: %s" % item)
                            stat_filename = '/dev/%s' % item[0]
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Stat file: %s" % stat_filename)
                            s = os.stat(stat_filename)
                            device_type = None
                            if S_ISBLK(s.st_mode):
                                device_type = "b"
                            elif S_ISCHR(s.st_mode):
                                device_type = "c"
                            if device_type is not None:
                                if len(item) == 3 and isinstance(item[2], str):
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % item[2]
                                    value += "%s" % item[1]
                                else:
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % os.minor(s.st_rdev)
                                    value += "%s" % item[1]
                                self.write_value(os.path.join(
                                                path, 'devices.allow'), value)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "write_value: %s" % value)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find /dev/%s. " % item[0] +
                                       "Not added to devices.allow!")
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                    else:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Not sure what to do with: %s" % item)
            devices_allowed = open(os.path.join(
                                   path, "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "After setup devices.list: %s" % devices_allowed)

    # Select devices to assign to the job
    def assign_devices(
                       self,
                       device_type,
                       device_list,
                       number_of_devices,
                       node):
        devices = device_list[:number_of_devices]
        device_ids = list()
        device_names = list()
        for device in devices:
            device_info = node.devices[device_type][device]
            if len(device_ids) == 0:
                if device.find("mic") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:0 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                    device_ids.append("%s %d:1 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                elif device.find("nvidia") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:255 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
            device_ids.append("%s %d:%d rwm" %
                              (device_info['device_type'],
                               device_info['major'],
                               device_info['minor']))
            device_names.append(device)
        return device_names, device_ids

    # Find the device name
    def get_device_name(self, node, available, socket, major, minor):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Get device name: major: %s, minor: %s" % (major, minor))
        avail_device = None
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Possible devices: %s" % (available[socket]['devices']))
        for avail_device in available[socket]['devices']:
            avail_major = None
            avail_minor = None
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Checking device: %s" % (avail_device))
            if avail_device.find('mic') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check mic device: %s" % (avail_device))
                avail_major = node.devices['mic'][avail_device]['major']
                avail_minor = node.devices['mic'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            elif avail_device.find('nvidia') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check gpu device: %s" % (avail_device))
                avail_major = node.devices['gpu'][avail_device]['major']
                avail_minor = node.devices['gpu'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            if int(avail_major) == int(major) and \
                    int(avail_minor) == int(minor):
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device match: name: %s, major: %s, minor: %s" %
                           (avail_device, major, minor))
                return avail_device
        pbs.logmsg(pbs.EVENT_DEBUG3, "No match found")
        return None

    # Assign resources to the job based off the vnodes assignments
    def assign_job_resources_by_vnode(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # Loop through the vnodes and assign resources
        assigned = dict()
        assigned['cpuset.cpus'] = list()
        assigned['cpuset.mems'] = list()
        room_on_socket = True

        for vnode in resources['vnodes']:
            regex = re.compile(".*\[(\d+)\].*")
            socket = int(regex.search(vnode).group(1))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current Vnode: %s" % vnode)
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current socket: %d" % socket)

            if 'ncpus' in resources['vnodes'][vnode]:
                tmp_ncpus = int(resources['vnodes'][vnode]['ncpus'])
                if int(resources['vnodes'][vnode]['ncpus']) <= \
                        len(available[socket]['cpus']):
                    assigned['cpuset.cpus'] += \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] += [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_ncpus: %s" % (tmp_ncpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "available[%d]: %s" %
                               (socket, available[socket]))
                    room_on_socket = False
            if ('nmics' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['nmics']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(mic).*")
                tmp_nmics = int(resources['vnodes'][vnode]['nmics'])
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['nmics']) > 0 and
                   int(resources['vnodes'][vnode]['nmics']) <= len(mics)):
                    tmp_names, tmp_devices = self.assign_devices(
                             'mic', mics[:tmp_nmics], tmp_nmics, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_nmics: %s" % (tmp_nmics))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "mics: %s" % (mics))
                    room_on_socket = False
            if ('ngpus' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['ngpus']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(nvidia).*")
                tmp_ngpus = int(resources['vnodes'][vnode]['ngpus'])
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['ngpus']) > 0 and
                   int(resources['vnodes'][vnode]['ngpus']) <= len(gpus)):
                    tmp_names, tmp_devices = self.assign_devices(
                        'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "tmp_ngpus: %s" % (tmp_ngpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "gpus: %s" % (gpus))
                    room_on_socket = False

        if room_on_socket:
            assigned['cpuset.cpus'].sort()
            assigned['cpuset.mems'].sort()
            if 'devices' in assigned:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids pre set and sort: %s" %
                           (assigned['devices']))
                assigned['devices'] = list(set(assigned['devices']))
                assigned['devices'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids post set and sort: %s" %
                           (assigned['devices']))
                assigned['devices_name'] = list(set(assigned['devices_name']))
                assigned['devices_name'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

    # Assign resources to the job
    def assign_job_resources(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # What would we need to do different for a large smp (UV) system?
        # See if requested resources can fit on a single socket
        assigned = dict()
        pbs.logmsg(pbs.EVENT_DEBUG3, "Requested: %s" % (resources))

        # Determine the order that we should evaluate the sockets.
        socket_list = list()
        if 'placement_type' in self.cfg:
            if self.cfg['placement_type'] == 'round_robin':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Requested round_robin placement")
                # Look at assigned_resources and determine which socket
                # to start with
                jobs_per_socket_cnt = dict()
                for socket in available.keys():
                    jobs_per_socket_cnt[socket] = 0
                for job in self.assigned_resources:
                    if 'cpuset.mems' in self.assigned_resources[job]:
                        for socket in \
                                self.assigned_resources[job]['cpuset.mems']:
                            jobs_per_socket_cnt[socket] += 1

                sorted_dict = sorted(jobs_per_socket_cnt.items(),
                                     key=operator.itemgetter(1))
                for x in sorted_dict:
                    socket_list.append(x[0])
        else:
            socket_list = available.keys()

        # Loop through the socket list and determine if the job
        # can fit on the socket
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Evaluate sockets in the following order: %s" %
                   (socket_list))
        for socket in socket_list:
            room_on_socket = True
            if 'ncpus' in resources:
                if int(resources['ncpus']) <= len(available[socket]['cpus']):
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Requested: %s, Available: %s" %
                               (resources['ncpus'], available[socket]['cpus']))
                    tmp_ncpus = int(resources['ncpus'])
                    assigned['cpuset.cpus'] = \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] = [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient ncpus on socket %d: R:%d,A:%s" %
                               (socket, resources['ncpus'],
                                available[socket]['cpus']))
                    room_on_socket = False
            # Check to see that nmics has been requested and not equal to 0
            if 'nmics' in resources and int(resources['nmics']) > 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'nmics' in resources and int(resources['nmics']) > 0 \
                        and int(resources['nmics']) <= len(mics):
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient nmics on socket %d: R:%s,A:%s" %
                               (socket, resources['nmics'], mics))
                    room_on_socket = False
            # Check to see that ngpus has been requested and not equal to 0
            if 'ngpus' in resources and int(resources['ngpus']) > 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'ngpus' in resources and int(resources['ngpus']) > 0 \
                        and int(resources['ngpus']) <= len(gpus):
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient gpus on socket %d: R:%s,A:%s" %
                               (socket, resources['ngpus'], gpus))
                    room_on_socket = False
            if 'vmem' in resources:
                if size_as_int(resources['vmem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient vmem on socket %d: R:%s,A:%s" %
                               (socket, resources['vmem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if 'mem' in resources:
                if size_as_int(resources['mem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient memory on socket %d: R:%s,A:%s" %
                               (socket, resources['mem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if room_on_socket:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
                self.host_assigned_resources = assigned
                return assigned
        # If we made it here then the requested resources did not fit
        # on a single socket
        # Future: take distance into account when selecting sockets?
        # Combine available resources into a single list
        # This should work for a dual socket machine.
        # Needs additional work for machines with more than 2 sockets
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Unable to find requested resources: %s" % resources +
                   " on a socket. Checking the entire node")
        available_copy = copy.deepcopy(available)
        available_on_node = dict()
        socket_keys = available.keys()
        socket_keys.sort()
        available_on_node['sockets'] = socket_keys
        for socket in socket_keys:
            for key in available_copy[socket].keys():
                if isinstance(available[socket][key], int):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]
                if isinstance(available_copy[socket][key], str):
                    try:
                        if key in available_on_node is False:
                            available_on_node[key] = \
                                size_as_int(available_copy[socket][key])
                        else:
                            available_on_node[key] += \
                                size_as_int(available_copy[socket][key])
                    except:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Unable to convert to int: %s" %
                                   (available_copy[socket][key]))

                if isinstance(available_copy[socket][key], list):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available on node: %s" % (available_on_node))
        assigned = dict()
        room_on_node = False
        if 'ncpus' in resources:
            if int(resources['ncpus']) <= len(available_on_node['cpus']):
                room_on_node = True
                tmp_ncpus = int(resources['ncpus'])
                assigned['cpuset.cpus'] = available_on_node['cpus'][:tmp_ncpus]
                assigned['cpuset.mems'] = available_on_node['sockets']
            if 'nmics' in resources and int(resources['nmics']) != 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['nmics']) <= len(mics) and \
                        int(resources['nmics']) > 0:
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
            if 'ngpus' in resources and int(resources['ngpus']) != 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['ngpus']) <= len(gpus) and \
                        int(resources['ngpus']) > 0:
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
        if room_on_node:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

        # Consolidate resources if needed to place the job

    # Determine which resources are available
    def available_node_resources(self, node):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))

        available = copy.deepcopy(node.numa_nodes)
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Keys: %s" % (available[0].keys()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        for socket in available.keys():
            if 'cpus' in available[socket]:
                # Find the physical cores
                if available[socket]['cpus'].find('-') != -1 and \
                        node.hyperthreading:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Splitting %s at ','" %
                               (available[socket]['cpus']))
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'].split(',')[0])
                else:
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'])
            if 'MemTotal' in available[socket]:
                # Find the memory on the socket in bites.
                # Remove the 'b' to simply math
                available[socket]['memory'] = size_as_int(
                    available[socket]['MemTotal'])

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Pre device add: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (node.devices.keys()))
        for device in node.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" %
                       (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Devices: %s" %
                           (node.devices[device].keys()))
                for device_name in node.devices[device].keys():
                    device_socket = \
                        node.devices[device][device_name]['numa_node']
                    if 'devices' not in available[device_socket]:
                        available[device_socket]['devices'] = list()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Device: %s, Socket: %s" %
                               (device, device_socket))
                    available[device_socket]['devices'].append(device_name)
            # pbs.logmsg(pbs.EVENT_DEBUG3,
            #            "Available on socket %s: %s" %
            #            (socket,available[socket]))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))

        # Remove all of the resources that are assigned to other jobs
        for job in self.assigned_resources.keys():
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            if (not job_to_be_ignored(job)):
                cpus = list()
                sockets = list()
                devices = list()
                memory = 0
                if 'cpuset.cpus' in self.assigned_resources[job]:
                    cpus = self.assigned_resources[job]['cpuset.cpus']
                if 'cpuset.mems' in self.assigned_resources[job]:
                    sockets = self.assigned_resources[job]['cpuset.mems']
                if 'devices.list' in self.assigned_resources[job]:
                    devices = self.assigned_resources[job]['devices.list']
                if 'memory.limit' in self.assigned_resources[job]:
                    memory = size_as_int(
                                self.assigned_resources[job]['memory.limit'])

                # Loop through the sockets and remove cpus that are
                # assigned to other cgroups
                for socket in sockets:
                    for cpu in cpus:
                        try:
                            available[socket]['cpus'].remove(cpu)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %d from %s" %
                                       (cpu, available[socket]['cpus']))

                if len(sockets) == 1:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Decrementing memory: %d by %d" %
                               (size_as_int(available[sockets[0]]['memory']),
                                memory))
                    available[sockets[0]]['memory'] -= memory

                # Loop throught the available sockets
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned device to %s: %s" % (job, devices))
                for socket in available.keys():
                    for device in devices:
                        try:
                            # loop through know devices and see if they match
                            if len(available[socket]['devices']) != 0:
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Check device: %s" % (device))
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Available device: %s" %
                                           (available[socket]['devices']))
                                major, minor = device.split()[1].split(':')
                                avail_device = self.get_device_name(
                                                   node, available, socket,
                                                   major, minor)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Returned device: %s" %
                                           (avail_device))
                                if avail_device is not None:
                                    pbs.logmsg(pbs.EVENT_DEBUG3,
                                               "socket: %d,\t" % socket +
                                               "devices: %s,\t" %
                                               available[socket]['devices'] +
                                               "device to remove: %s" %
                                               (avail_device))
                                    available[socket]['devices'].remove(
                                        avail_device)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %s from %s" %
                                       (device, available[socket]['devices']))
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Job %s res not removed from host " % job +
                           "available res: suspended job")
        pbs.logmsg(pbs.EVENT_DEBUG, "Available resources: %s" % (available))
        return available

    # Set a job limit
    def set_job_limit(self, jobid, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s: %s = %s" %
                   (caller_name(), jobid, resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'softmem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.soft_limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     jobid, 'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'], jobid,
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            elif resource == 'ncpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = self.select_cpus(path, value)
                    if cpus is None:
                        raise CgroupLimitError("Failed to configure cpuset.")
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
                    self.__copy_from_parent(os.path.join(self.paths['cpuset'],
                                            jobid, 'cpuset.mems'))
            elif resource == 'cpuset.cpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = value
                    if cpus is None:
                        msg = "Failed to configure cpus in cpuset."
                        raise CgroupLimitError(msg)
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
            elif resource == 'cpuset.mems':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.mems')
                    mems = value
                    if mems is None:
                        msg = "Failed to configure mems in cpuset."
                        raise CgroupLimitError(msg)
                    mems = ','.join(map(str, mems))
                    self.write_value(path, mems)
            elif resource == 'devices':
                if 'devices' in self.subsystems and \
                        self.cfg['cgroup']['devices']['enabled']:

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.allow')
                    devices = value
                    if devices is None:
                        msg = "Failed to configure device(s)."
                        raise CgroupLimitError(msg)
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Setting devices: %s for %s" % (devices, jobid))
                    for device in devices:
                        self.write_value(path, device)

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.list')
                    output = open(path, 'r').read()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "devices.list: %s" % output.replace("\n", ","))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    # Update resource usage for a job
    def update_job_usage(self, jobid, resc_used):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resc_used = %s" %
                   (caller_name(), str(resc_used)))
        # Sort the subsystems so that we consistently look at the subsystems
        # in the same order every time
        self.subsystems.sort()
        for subsys in self.subsystems:
            path = os.path.join(self.paths[subsys], jobid)
            if subsys == "memory":
                max_mem = self.__get_max_mem_usage(path)
                if max_mem is None:
                    pbs.logjobmsg(jobid, "%s: No max mem data" %
                                  (caller_name()))
                else:
                    resc_used['mem'] = pbs.size(convert_size(max_mem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: mem=%s" %
                                  (caller_name(), resc_used['mem']))
                mem_failcnt = self.__get_mem_failcnt(path)
                if mem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No mem fail count data" %
                                  (caller_name()))
                else:
                    # Check to see if the job exceeded its resource limits
                    if mem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memory limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "memsw":
                max_vmem = self.__get_max_memsw_usage(path)
                if max_vmem is None:
                    pbs.logjobmsg(jobid, "%s: No max vmem data" %
                                  (caller_name()))
                else:
                    resc_used['vmem'] = pbs.size(convert_size(max_vmem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: vmem=%s" %
                                  (caller_name(), resc_used['vmem']))

                vmem_failcnt = self.__get_memsw_failcnt(path)
                if vmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No vmem fail count data" %
                                  (caller_name()))
                else:
                    pbs.logjobmsg(jobid, "%s: vmem fail count: %d " %
                                  (caller_name(), vmem_failcnt))
                    if vmem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memsw limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "hugetlb":
                max_hpmem = self.__get_max_hugetlb_usage(path)
                if max_hpmem is None:
                    pbs.logjobmsg(jobid, "%s: No max hpmem data" %
                                  (caller_name()))
                    return
                hpmem_failcnt = self.__get_hugetlb_failcnt(path)
                if hpmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No hpmem fail count data" %
                                  (caller_name()))
                    return
                if hpmem_failcnt > 0:
                    err_msg = self.__get_error_msg(jobid)
                    pbs.logjobmsg(jobid, "Cgroup hugetlb limit exceeded: %s" %
                                  (err_msg))
                resc_used['hpmem'] = pbs.size(convert_size(max_hpmem, 'kb'))
                pbs.logjobmsg(jobid, "%s: Hugepage usage: %s" %
                              (caller_name(), resc_used['hpmem']))
            elif subsys == "cpuacct":
                cpu_usage = self.__get_cpu_usage(path)
                if cpu_usage is None:
                    pbs.logjobmsg(jobid, "%s: No CPU usage data" %
                                  (caller_name()))
                    return
                cpu_usage = convert_time(str(cpu_usage) + "ns")
                pbs.logjobmsg(jobid, "%s: CPU usage: %.3lf secs" %
                              (caller_name(), cpu_usage))
                resc_used['cput'] = pbs.duration(cpu_usage)

    # Creates the cgroup if it doesn't exists
    def create_job(self, jobid, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Iterate over the subsystems required
        for subsys in self.subsystems:
            # Create a directory for the job
            try:
                old_umask = os.umask(0022)
                path = self.paths[subsys]
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                path = os.path.join(self.paths[subsys], jobid)
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                if subsys == 'devices':
                    self.setup_subsys_devices(path, node)

            except OSError, exc:
                raise CgroupConfigError("Failed to create directory: %s (%s)" %
                                        (path, errno.errorcode[exc.errno]))
            except:
                raise
            finally:
                os.umask(old_umask)

    # Determine the cgroup limits and configure the cgroups
    def configure_job(self, jobid, hostresc, node):
        mem_enabled = 'memory' in self.subsystems
        vmem_enabled = 'memsw' in self.subsystems
        if mem_enabled or vmem_enabled:
            # Initialize mem variables
            mem_avail = node.get_memory_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "mem_avail %s" % mem_avail)
            mem_requested = None
            if 'mem' in hostresc.keys():
                mem_requested = convert_size(hostresc['mem'], 'kb')
            mem_default = None
            if mem_enabled:
                mem_default = self.default('memory')
            # Initialize vmem variables
            vmem_avail = node.get_vmem_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "vmem_avail %s" % vmem_avail)
            vmem_requested = None
            if 'vmem' in hostresc.keys():
                vmem_requested = convert_size(hostresc['vmem'], 'kb')
            vmem_default = None
            if vmem_enabled:
                vmem_default = self.default('memsw')
            # Initialize softmem variables
            if 'soft_limit' in self.cfg['cgroup']['memory'].keys():
                softmem_enabled = self.cfg['cgroup']['memory']['soft_limit']
            else:
                softmem_enabled = False
            # Sanity check
            if size_as_int(mem_avail) > size_as_int(vmem_avail):
                if size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                    raise CgroupLimitError(
                        'mem available (%s) exceeds vmem available (%s)' %
                        (mem_avail, vmem_avail))
            # Determine the mem limit
            if mem_requested is not None:
                # mem requested may not exceed available
                if size_as_int(mem_requested) > size_as_int(mem_avail):
                    raise JobValueError(
                        'mem requested (%s) exceeds mem available (%s)' %
                        (mem_requested, mem_avail))
                mem_limit = mem_requested
            else:
                # mem was not requested
                if mem_default is None:
                    mem_limit = mem_avail
                else:
                    mem_limit = mem_default
            # Determine the vmem limit
            if vmem_requested is not None:
                # vmem requested may not exceed available
                if size_as_int(vmem_requested) > size_as_int(vmem_avail):
                    raise JobValueError(
                        'vmem requested (%s) exceeds vmem available (%s)' %
                        (vmem_requested, vmem_avail))
                vmem_limit = vmem_requested
            else:
                # vmem was not requested
                if vmem_default is None:
                    vmem_limit = vmem_avail
                else:
                    vmem_limit = vmem_default
            # Ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Adjust for soft limits if enabled
            if mem_enabled and softmem_enabled:
                softmem_limit = mem_limit
                # The hard memory limit is assigned the lesser of the vmem
                # limit and available memory
                if size_as_int(vmem_limit) < size_as_int(mem_avail):
                    mem_limit = vmem_limit
                else:
                    mem_limit = mem_avail
            # Again, ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Sanity checks when both memory and memsw are enabled
            if mem_enabled and vmem_enabled:
                if vmem_requested is not None:
                    if size_as_int(vmem_limit) > size_as_int(vmem_requested):
                        # The user requested an invalid limit
                        raise JobValueError(
                            'vmem limit (%s) exceeds vmem requested (%s)' %
                            (vmem_limit, vmem_requested))
                if size_as_int(vmem_limit) > size_as_int(mem_limit):
                    # This job may utilize swap
                    if size_as_int(vmem_avail) <= size_as_int(mem_avail) and \
                      size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                        # No swap available
                        raise CgroupLimitError(
                            ('Job might utilize swap ' +
                             'and no swap space available'))
            # Assign mem and vmem
            if mem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: mem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), mem_limit))
                hostresc['mem'] = pbs.size(mem_limit)
                if softmem_enabled:
                    hostresc['softmem'] = pbs.size(softmem_limit)
            if vmem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: vmem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), vmem_limit))
                hostresc['vmem'] = pbs.size(vmem_limit)
        # Initialize hpmem variables
        hpmem_enabled = 'hugetlb' in self.subsystems
        if hpmem_enabled:
            hpmem_avail = node.get_hpmem_on_node(self.cfg)
            hpmem_limit = None
            hpmem_default = self.default('hugetlb')
            if hpmem_default is None:
                hpmem_default = hpmem_avail
            if 'hpmem' in hostresc.keys():
                hpmem_limit = convert_size(hostresc['hpmem'], 'kb')
            else:
                hpmem_limit = hpmem_default
            # Assign hpmem
            if size_as_int(hpmem_limit) > size_as_int(hpmem_avail):
                raise JobValueError('hpmem limit (%s) exceeds available (%s)' %
                                    (hpmem_limit, hpmem_avail))
            hostresc['hpmem'] = pbs.size(hpmem_limit)
        # Initialize cpuset variables
        cpuset_enabled = 'cpuset' in self.subsystems
        if cpuset_enabled:
            cpu_limit = 1
            if 'ncpus' in hostresc.keys():
                cpu_limit = hostresc['ncpus']
            if cpu_limit < 1:
                cpu_limit = 1
            hostresc['ncpus'] = pbs.pbs_int(cpu_limit)

        # Find the available resources and assign the right ones to the job

        attempt = 0
        assign_resources = False

        # Two attempts since self.cleanup_orphans may actually fix the
        # problem we see in a first attempt
        while attempt < 2:
            attempt += 1
            available_resources = self.available_node_resources(node)
            if 'vnodes' in hostresc:
                assign_resources = self.assign_job_resources_by_vnode(
                    hostresc, available_resources, node)
            else:
                assign_resources = self.assign_job_resources(
                    hostresc, available_resources, node)

            if (assign_resources is False) and (attempt < 2):
                # No resources were assigned to the job.
                # Most likely cause was that a cgroup has
                # not been cleaned up yet
                pbs.logmsg(pbs.EVENT_DEBUG, "Failed to assign resources. " +
                           "Checking the following cgroups on the node " +
                           "with the server: %s" % self.existing_cgroups)

                # Collect the jobs on the node (try reading mom_priv/jobs,
                # if not successful ask server)
                jobs_list = node.gather_jobs_on_node_local()

                if jobs_list is not None:
                    # Don't clean up the cgroup we just made for the
                    # job just yet
                    if jobid not in jobs_list:
                        jobs_list.append(jobid)

                    self.cleanup_orphans(jobs_list)

                    # Recheck what is now assigned after things were cleaned up
                    self.gather_assigned_resources()

        if assign_resources is False:
            # Cleanup cgroups for jobs not present on this node
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Assign resources failed. Attempting to cleanup all " +
                       "leftover cgroups (including event job %s)" % jobid)

            # Remove the current job from the jobs list so it will be
            # cleaned up as well.
            jobs_list.remove(jobid)

            self.cleanup_orphans(jobs_list)

            # Rerun the job and log the message
            line = 'Unable to assign resources to job. '
            line += 'Will requeue the job and try again.'
            line += 'job run_count: %d' % pbs.event().job.run_count
            pbs.event().job.rerun()
            pbs.event().reject(line)

        # Print out the assigned resources
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Assigned resources: %s" % (assign_resources))

        if cpuset_enabled:
            hostresc.pop('ncpus', None)
            for key in ['cpuset.cpus', 'cpuset.mems']:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Initialize devices variables
        devices_enabled = 'devices' in self.subsystems
        if devices_enabled:
            key = 'devices'
            if key in assign_resources:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Apply the resource limits to the cgroups
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Setting cgroup limits for: %s" %
                   (caller_name(), hostresc))

        # The vmem limit must be set after the mem limit, so sort the keys
        for resc in sorted(hostresc.keys()):
            self.set_job_limit(jobid, resc, hostresc[resc])

    # Kill any processes contained within a tasks file
    def __kill_tasks(self, tasks_file):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        with open(tasks_file, 'r') as fd:
            for line in fd:
                os.kill(int(line.strip()), signal.SIGKILL)
        fd.close()
        count = 0
        with open(tasks_file, 'r') as fd:
            for line in fd:
                count += 1
                pid = line.strip()
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Pid: %s did not clean up" %
                           (caller_name(), pid))
                if os.path.isfile('/proc/%s/status' % pid):
                    tmp = open('/proc/%s/status' % pid).readlines()
                    tmp_line = ''
                    for entry in tmp:
                        if entry.find('Name:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('State:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('Uid:') != -1:
                            tmp_line += entry.strip()
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: %s" % (caller_name(), tmp_line))

        return count

    # Perform the actual removal of the cgroup directory
    # by default, only one attempt at killing tasks in cgroup, without sleeping
    # since this method could be called many times
    # (for N directories times M jobs)
    def __remove_cgroup(self, path, tasks_kill_attempts=1):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            if os.path.exists(path):
                tasks_file = os.path.join(path, 'tasks')
                task_cnt = self.__kill_tasks(tasks_file)
                kill_attempts = 0
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Task Kill Attempts: %d" % tasks_kill_attempts)
                while (task_cnt > 0) and (kill_attempts < tasks_kill_attempts):
                    kill_attempts += 1
                    count = 0

                    with open(tasks_file, 'r') as fd:
                        for line in fd:
                            count += 1
                        task_cnt = count

                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "Attempt: %d - cgroup still has %d tasks: %s" %
                               (kill_attempts, task_cnt, path))
                    if kill_attempts < tasks_kill_attempts:
                        time.sleep(1)
                        # Added since there are race conditions where tasks are
                        # being added at the same time we get the initial
                        # task count
                        tasks_file = os.path.join(path, 'tasks')
                        task_cnt = self.__kill_tasks(tasks_file)

                if task_cnt == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Removing directory %s" %
                               (caller_name(), path))
                    os.rmdir(path)
                    if os.path.exists(path):
                        time.sleep(.25)
                        if os.path.exists(path):
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "%s: 2nd Try Removing directory %s" %
                                       (caller_name(), path))
                            os.rmdir(path)
                            time.sleep(.25)
                        if os.path.exists(path):
                            return False
                else:
                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "cgroup still has %d tasks: %s" %
                               (task_cnt, path))

                    # Check to see if the offline file is already present
                    # on the mom
                    offline_file_exists = False
                    if os.path.isfile(self.offline_file):
                        msg = "Cgroup(s) not cleaning up but the node already "
                        msg += "has the offline file."

                        pbs.logmsg(pbs.EVENT_DEBUG, msg)
                    else:
                        # Check to see that the node is not already offline.
                        try:
                            tmp_state = pbs.server().vnode(self.hostname).state
                        except:
                            msg = "Unable to contact server for node state"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            tmp_state = None
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Current Node State: %d" % tmp_state)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Offline Node State: %d" % pbs.ND_OFFLINE)

                        if tmp_state == pbs.ND_OFFLINE:
                            msg = "Cgroup(s) not cleaning up but the node is "
                            msg += "already offline."
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                        elif tmp_state is not None:
                            # Offline the node(s)
                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                            msg = "Offlining node since cgroup(s) are not "
                            msg += "cleaning up"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            vnode = pbs.event().vnode_list[self.hostname]

                            vnode.state = pbs.ND_OFFLINE

                            # Write a file locally to reduce server traffic
                            # when it comes time to online the node
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Offline file: %s" % self.offline_file)
                            open(self.offline_file, 'a').close()
                            vnode.comment = self.offline_msg

                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                        else:
                            pass

                    return False

        except OSError, exc:
            pbs.logmsg(pbs.EVENT_SYSTEM,
                       "Failed to remove cgroup path: %s (%s)" %
                       (path, errno.errorcode[exc.errno]))
        except:
            pbs.logmsg(pbs.EVENT_SYSTEM, "Failed to remove cgroup path: %s" %
                       (path))

        return True

    # Removes cgroup directories that are not associated with a local job
    def cleanup_orphans(self, local_jobs):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        orphan_cnt = 0
        for key in self.paths.keys():
            for dir in glob.glob(os.path.join(self.paths[key], '[0-9]*')):
                jobid = os.path.basename(dir)
                if jobid not in local_jobs:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Removing orphaned cgroup: %s" %
                               (caller_name(), dir))
                    # default number of attempts at killing tasks
                    status = self.__remove_cgroup(dir)
                    if not status:
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "%s: Removing orphaned cgroup %s failed " %
                                   (caller_name(), dir))
                        orphan_cnt += 1
        return orphan_cnt

    # Removes the cgroup directories for a job
    def delete(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        tasks_kill_attempted = False

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            if not tasks_kill_attempted:
                # Make multiple attempts at killing tasks in cgroups on
                # first subsystem encountered since it is "normal" for
                # a cgroup.delete to encounter processes hard to kill
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                                           jobid),
                                              tasks_kill_attempts=(
                                              CGROUP_KILL_ATTEMPTS))
                tasks_kill_attempted = True
            else:
                # We tried getting rid of job processes earlier,
                # so don't try N times with sleeps
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                              jobid), tasks_kill_attempts=1)

            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Status: %s" %
                       (caller_name(), status))
            if status is False:
                # Rerun the job and log the message
                line = 'Unable to cleanup a cgroup. '
                line += 'Will offline the node and requeue the job '
                line += 'and try again. job run_count: %d' % \
                        pbs.event().job.run_count
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Failed to remove cgroup: %s" %
                           (caller_name(), line))
                pbs.event().accept()

    # Write a value to a limit file
    def write_value(self, file, value, mode='w'):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: writing %s to %s" %
                   (caller_name(), value, file))
        try:
            with open(file, mode) as fd:
                fd.write(str(value) + '\n')
        except IOError, exc:
            if exc.errno == errno.ENOENT:
                pbs.logmsg(pbs.EVENT_SYSTEM,
                           "Trying to set limit to unknown file %s" % file)
            elif exc.errno in [errno.EACCES, errno.EPERM]:
                pbs.logmsg(pbs.EVENT_SYSTEM, "Permission denied on file %s" %
                           file)
            elif exc.errno == errno.EBUSY:
                raise CgroupBusyError("Limit rejected")
            elif exc.errno == errno.EINVAL:
                raise CgroupLimitError("Invalid limit value: %s" % (value))
            else:
                raise
        except:
            raise

    # Return memory failcount
    def __get_mem_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.failcnt"), 'r').read().strip())
        except:
            return None

    # Return vmem failcount
    def __get_memsw_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.failcnt"), 'r').read().strip())
        except:
            return None

    # Return hpmem failcount
    def __get_hugetlb_failcnt(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.failcnt"))[0], 'r').read().strip())
        except:
            return None

    # Return the max usage of memory in bytes
    def __get_max_mem_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of memsw in bytes
    def __get_max_memsw_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of hugetlb in bytes
    def __get_max_hugetlb_usage(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.max_usage_in_bytes"))[0],
                            'r').read().strip())
        except:
            return None

    # Return the cpuacct.usage in cpu seconds
    def __get_cpu_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "cpuacct.usage"), 'r').read().strip())
        except:
            return None

    # Assign CPUs to the cpuset
    def select_cpus(self, path, ncpus):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path is %s" % (caller_name(), path))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: ncpus is %s" %
                   (caller_name(), ncpus))
        if ncpus < 1:
            ncpus = 1
        try:
            # Must select from those currently available
            cpufile = os.path.basename(path)
            base = os.path.dirname(path)
            parent = os.path.dirname(base)
            avail = cpus2list(open(os.path.join(parent, cpufile),
                              'r').read().strip())
            if len(avail) < 1:
                raise CgroupProcessingError("No CPUs avaialble in cgroup.")
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Available CPUs: %s" %
                       (caller_name(), avail))
            assigned = []
            for file in glob.glob(os.path.join(parent, '[0-9]*', cpufile)):
                cpus = cpus2list(open(file, 'r').read().strip())
                for id in cpus:
                    if id in avail:
                        avail.remove(id)
            if len(avail) < ncpus:
                raise CgroupProcessingError(
                    "Insufficient CPUs avaialble in cgroup.")
            if len(avail) == ncpus:
                return avail
            # FUTURE: Try to minimize NUMA nodes based on memory requirement
            return avail[0:ncpus]
        except:
            raise

    # Return the error message in system message file
    def __get_error_msg(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        proc = subprocess.Popen(['dmesg'],
                                shell=False, stdout=subprocess.PIPE)
        stdout = proc.communicate()[0].splitlines()
        stdout.reverse()
        # Check to see if the job id is found in dmesg otherwise
        # you could get the information for another job that was killed
        # the line will look like Memory cgroup stats for /pbspro/279.centos7
        kill_line = ""

        for line in stdout:
            start = line.find('Killed process ')
            if start >= 0:
                kill_line = line[start:]
            job_start = line.find(jobid)
            if job_start >= 0:
                # pbs.logmsg(pbs.EVENT_DEBUG3,
                #           "%s: Jobid: %s, Line: %s" %
                #           (caller_name(), jobid, line))
                return kill_line
        return ""

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_job_env_file(self, jobid, env_list):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        # if not os.path.exists(self.host_job_env_dir):
        #    os.makedirs(self.host_job_env_dir, 0755)

        # Write out assigned_resources
        try:
            lines = string.join(env_list, '\n')
            filename = self.host_job_env_filename % jobid
            outfile = open(filename, "w")
            outfile.write(lines)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" % (filename))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (lines))
            return True
        except:
            return False

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        if not os.path.exists(self.hook_storage_dir):
            os.makedirs(self.hook_storage_dir, 0755)

        # Write out assigned_resources
        try:
            json_str = json.dumps(self.host_assigned_resources)
            outfile = open(self.hook_storage_dir+os.sep+jobid, "w")
            outfile.write(json_str)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" %
                       (self.hook_storage_dir+os.sep+jobid))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (json_str))
            return True
        except:
            return False

    def read_in_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        pbs.logmsg(pbs.EVENT_DEBUG3, "Host assigned resources: %s" %
                   (self.host_assigned_resources))
        if os.path.isfile(self.hook_storage_dir+os.sep+jobid):
            # Write out assigned_resources
            try:
                infile = open(self.hook_storage_dir+os.sep+jobid, 'r')
                json_data = json.load(infile, object_hook=decode_dict)
                self.host_assigned_resources = json_data
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Host assigned resources: %s" %
                           (self.host_assigned_resources))
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
            finally:
                infile.close()

        if self.host_assigned_resources is not None:
            return True
        else:
            return False

#
#
# FUNCTION main
#


def main():
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Function called" % (caller_name()))
    hostname = pbs.get_local_nodename()
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Host is %s" % (caller_name(), hostname))

    # Log the hook event type
    e = pbs.event()
    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook name is %s" %
               (caller_name(), e.hook_name))

    try:
        # Instantiate the hook utility class
        hooks = HookUtils()
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Event type is %s" %
                   (caller_name(), hooks.event_name(e.type)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(hooks)))
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: Failed to instantiate hook utility class" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        e.accept()

    if not hooks.hashandler(e.type):
        # Bail out if there is no handler for this event
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s event not handled by this hook" %
                   (caller_name(), hooks.event_name(e.type)))
        e.accept()

    try:
        # If an exception occurs, jobutil must be set
        jobutil = None
        # Instantiate the job utility class first so jobutil can be accessed
        # by the exception handlers.
        if hasattr(e, 'job'):
            jobutil = JobUtils(e.job)
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Job information class instantiated" %
                       (caller_name()))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" %
                       (caller_name(), repr(jobutil)))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Event does not include a job" %
                       (caller_name()))
        # Instantiate the cgroup utility class
        vnode = None
        if hasattr(e, 'vnode_list'):
            if hostname in e.vnode_list.keys():
                vnode = e.vnode_list[hostname]
        cgroup = CgroupUtils(hostname, vnode)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Cgroup utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(cgroup)))
        # Bail out if there is nothing to do
        if len(cgroup.subsystems) < 1:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: All cgroups disabled" %
                       (caller_name()))
            e.accept()

        # Call the appropriate handler
        if hooks.invoke_handler(e, cgroup, jobutil) is True:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned success" %
                       (caller_name()))
            e.accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned failure" %
                       (caller_name()))
            e.reject()

    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass

    except AdminError, exc:
        # Something on the system is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Admin error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except UserError, exc:
        # User must correct problem and resubmit job
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("User error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.delete()
                msg += " (deleted)"
            except:
                msg += " (delete failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except JobValueError, exc:
        # Something in PBS is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Job value error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except Exception, exc:
        # Catch all other exceptions and report them.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Unexpected error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

# line below required for unittesting
if __name__ == "__builtin__":
    start_time = time.time()
    try:
        main()
    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
    finally:
        pbs.logmsg(pbs.EVENT_DEBUG, "Elapsed time: %0.4lf" %
                   (time.time() - start_time))
---
2016-07-18 16:59:44,333 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups.json', 'content-encoding': 'default'}
2016-07-18 16:59:44,334 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups.json
2016-07-18 16:59:44,387 INFO     job: executable set to /bin/sleep with arguments: 100
2016-07-18 16:59:44,388 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0755 /tmp/PtlPbsJobScriptkReMOL
2016-07-18 16:59:44,408 INFOCLI  centos7: sudo -H -u pbsuser /usr/local/bin/qsub -N test_t4 -l select=1:ncpus=1:mem=500mb /tmp/PtlPbsJobScriptkReMOL
2016-07-18 16:59:44,439 INFO     submit to centos7 as pbsuser: job 385.centos7 OrderedDict([('Job_Name', 'test_t4'), ('Resource_List.select', '1:ncpus=1:mem=500mb')])
2016-07-18 16:59:44,440 INFOCLI  job script /tmp/PtlPbsJobScriptkReMOL
---
#!/usr/bin/env python

import os
import subprocess
from subprocess import Popen, PIPE
from time import sleep

eat_mem_c = """
#include <stdio.h>
#include <stdlib.h>
#include <string.h>


#define ONE_GB (0x40000000L)
#define ONE_MB (0x100000L)

void
eatmem(total_mb, block_mb)
{
	int i;
	char *buf;
	size_t block;

	for (i=1, buf=NULL, block=(block_mb*ONE_MB); (i*block) < (total_mb*ONE_MB); i++) {
		buf = realloc((void *)buf, (i*block));
		memset(buf+((i-1)*block), 0, block);
	}
	return;
}

int
main(int argc, char *argv[])
{
	int total_mb = 8;
	int block_mb = 1;

	if (argc > 1)
		total_mb = atoi(argv[1]);

	if (argc > 2)
		block_mb = atoi(argv[2]);

	eatmem(total_mb, block_mb);
	printf("Initialized %dMB in blocks of %dMB\\n", total_mb, block_mb);
}
"""

fname = "/tmp/pbs_pp325_eat_mem.c"
fname_exe = "/tmp/pbs_pp325_eat_mem"
fout = open(fname, "w")
fout.write(eat_mem_c)
fout.close()

print os.getppid()

p = Popen(["gcc", "-o", fname_exe, fname], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

p = Popen([fname_exe, "300", "10"], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

sleep(5)

p = Popen([fname_exe, "400", "10"], stdout=PIPE, stderr=PIPE)
(output, error) = p.communicate()
print output
print error

sleep(10)

os.remove(fname)
os.remove(fname_exe)


---
2016-07-18 16:59:44,440 INFOCLI  centos7: /usr/local/bin/qstat -f 385.centos7
2016-07-18 16:59:44,532 INFO     expect on server centos7: job_state = R job 385.centos7  got: job_state = Q
2016-07-18 16:59:45,033 INFOCLI  centos7: /usr/local/bin/qmgr -c set server scheduling=True
2016-07-18 16:59:45,047 INFOCLI  centos7: /usr/local/bin/qstat -f 385.centos7
2016-07-18 16:59:45,138 INFO     expect on server centos7: job_state = R job 385.centos7 attempt: 2 ...  OK
2016-07-18 16:59:51,145 INFO     status on centos7: job 385.centos7 resources_used.vmem
2016-07-18 16:59:51,145 INFOCLI  centos7: /usr/local/bin/qstat -f 385.centos7
2016-07-18 16:59:58,239 INFO     status on centos7: job 385.centos7 resources_used.vmem
2016-07-18 16:59:58,239 INFOCLI  centos7: /usr/local/bin/qstat -f 385.centos7
2016-07-18 16:59:58,338 INFO     Q: {'id': '385.centos7', 'resources_used.vmem': '300268kb'}
2016-07-18 16:59:58,338 INFO     Mem-1: 0b
2016-07-18 16:59:58,338 INFO     Mem-2: 300268kb
2016-07-18 16:59:58,338 INFO     Mem-1: 0.0b
2016-07-18 16:59:58,338 INFO     Mem-2: 293.2mb
2016-07-18 16:59:58,338 INFO     ==============================
2016-07-18 16:59:58,338 INFO      Entered CgroupsTests tearDown
2016-07-18 16:59:58,339 INFO     ==============================
2016-07-18 16:59:58,339 INFO     ===============================
2016-07-18 16:59:58,339 INFO     Completed CgroupsTests tearDown
2016-07-18 16:59:58,339 INFO     ===============================
2016-07-18 16:59:58,339 INFO     ok

2016-07-18 16:59:58,339 INFO     test name: test_t7 (tests.cgroups_tests.CgroupsTests)...
2016-07-18 16:59:58,340 INFO     test start time: Mon Jul 18 16:59:58 2016
2016-07-18 16:59:58,340 INFO     test docstring: 
 Test to verify that the mom reserve memory for OS
            when there is a reserve mem request in the config
         
2016-07-18 16:59:58,340 INFO     ===========================
2016-07-18 16:59:58,340 INFO      Entered CgroupsTests setUp
2016-07-18 16:59:58,340 INFO     ===========================
2016-07-18 16:59:58,340 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:59:58,402 INFO     status on centos7: server
2016-07-18 16:59:58,402 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:59:58,459 INFO     manager on centos7: set server {'managers': (3, 'root@*')}
2016-07-18 16:59:58,459 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers-=root@*
2016-07-18 16:59:58,480 INFO     manager on centos7: set server {'managers': (2, 'root@*')}
2016-07-18 16:59:58,481 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set server managers+=root@*
2016-07-18 16:59:58,519 INFO     server centos7: reverting configuration to defaults
2016-07-18 16:59:58,520 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:59:58,574 INFO     select on centos7: __ALL__
2016-07-18 16:59:58,574 INFOCLI  centos7: /usr/local/bin/qselect
2016-07-18 16:59:58,589 INFO     delete job on centos7: 385.centos7
2016-07-18 16:59:58,589 INFOCLI  centos7: /usr/local/bin/qdel -W force 385.centos7
2016-07-18 16:59:58,680 INFOCLI  centos7: /usr/local/bin/qstat -f 385.centos7
2016-07-18 16:59:58,781 INFOCLI  centos7: /usr/local/bin/qstat -f @centos7.usa.org
2016-07-18 16:59:58,863 INFO     expect on server centos7: job_state set 0 job ...  OK
2016-07-18 16:59:58,863 INFOCLI  centos7: /usr/local/bin/pbs_rstat -f
2016-07-18 16:59:58,874 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 16:59:58,901 INFO     manager on centos7: delete hook t1_l1
2016-07-18 16:59:58,901 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c delete hook t1_l1
2016-07-18 16:59:58,923 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 16:59:58,947 INFO     expect on server centos7:  unset  hook t1_l1 ...  OK
2016-07-18 16:59:58,947 INFOCLI  centos7: /usr/local/bin/qstat -Qf @centos7.usa.org
2016-07-18 16:59:58,957 INFO     manager on centos7: delete queue workq
2016-07-18 16:59:58,957 INFOCLI  centos7: /usr/local/bin/qmgr -c delete queue workq
2016-07-18 16:59:58,969 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:59:58,981 INFO     expect on server centos7:  unset  queue workq ...  OK
2016-07-18 16:59:58,982 INFO     manager on centos7: create queue workq {'started': 'True', 'queue_type': 'Execution', 'enabled': 'True'}
2016-07-18 16:59:58,982 INFOCLI  centos7: /usr/local/bin/qmgr -c create queue workq started=True,queue_type=Execution,enabled=True
2016-07-18 16:59:58,996 INFOCLI  centos7: /usr/local/bin/qstat -Qf workq@centos7.usa.org
2016-07-18 16:59:59,016 INFO     expect on server centos7: started set True || queue_type set Execution || enabled set True queue workq ...  OK
2016-07-18 16:59:59,017 INFO     manager on centos7: set server {'log_events': '511', 'default_queue': 'workq'}
2016-07-18 16:59:59,017 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=511,default_queue=workq
2016-07-18 16:59:59,040 INFO     status on centos7: resource
2016-07-18 16:59:59,041 INFO     manager on centos7: list resource
2016-07-18 16:59:59,041 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource
2016-07-18 16:59:59,093 INFO     manager on centos7: delete resource nmics,ngpus
2016-07-18 16:59:59,094 INFOCLI  centos7: /usr/local/bin/qmgr -c delete resource nmics,ngpus
2016-07-18 16:59:59,167 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource nmics
2016-07-18 16:59:59,177 INFO     expect on server centos7:  unset  resource nmics ...  OK
2016-07-18 16:59:59,178 INFOCLI  centos7: /usr/local/bin/qmgr -c list resource ngpus
2016-07-18 16:59:59,192 INFO     expect on server centos7:  unset  resource ngpus ...  OK
2016-07-18 16:59:59,193 INFOCLI  status on centos7: server license_count
2016-07-18 16:59:59,193 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:59:59,246 INFO     server: centos7.usa.org licensed
2016-07-18 16:59:59,273 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/server_priv/comm.lock
2016-07-18 16:59:59,324 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:59:59,334 INFO     scheduler centos7: reverting configuration to defaults
2016-07-18 16:59:59,334 INFO     manager on centos7: list sched
2016-07-18 16:59:59,334 INFOCLI  centos7: /usr/local/bin/qmgr -c list sched
2016-07-18 16:59:59,346 INFO     manager on centos7: unset sched ['sched_cycle_length']
2016-07-18 16:59:59,346 INFOCLI  centos7: /usr/local/bin/qmgr -c unset sched sched_cycle_length
2016-07-18 16:59:59,367 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/dedicated_time
2016-07-18 16:59:59,380 INFOCLI2 centos7: cmp /usr/local/etc/pbs_resource_group /var/spool/pbs/sched_priv/resource_group
2016-07-18 16:59:59,384 INFO     scheduler centos7: reverting holidays file to default
2016-07-18 16:59:59,384 INFOCLI2 centos7: cmp /usr/local/etc/pbs_holidays /var/spool/pbs/sched_priv/holidays
2016-07-18 16:59:59,387 INFOCLI2 centos7: cmp /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:59:59,390 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /usr/local/etc/pbs_sched_config /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:59:59,400 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:59:59,411 INFO     scheduler centos7: sent signal -HUP
2016-07-18 16:59:59,411 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 16:59:59,467 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/sched_priv/sched.lock
2016-07-18 16:59:59,493 INFOCLI2 centos7: sudo -H cmp /usr/local/sbin/pbs_mom /usr/local/sbin/pbs_mom.cpuset
2016-07-18 16:59:59,549 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:59:59,568 INFO     mom centos7: reverting configuration to defaults
2016-07-18 16:59:59,568 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/epilogue
2016-07-18 16:59:59,577 INFOCLI2 centos7: sudo -H /usr/bin/rm -f /var/spool/pbs/mom_priv/prologue
2016-07-18 16:59:59,596 INFOCLI2 centos7: sudo -H ls /var/spool/pbs/mom_priv/config.d
2016-07-18 16:59:59,610 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbspd6Ybw /var/spool/pbs/mom_priv/config
2016-07-18 16:59:59,631 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/mom_priv/config
2016-07-18 16:59:59,646 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/mom_priv/config
2016-07-18 16:59:59,659 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/mom_priv/config
2016-07-18 16:59:59,672 INFO     mom centos7: sent signal -HUP
2016-07-18 16:59:59,673 INFOCLI2 centos7: sudo -H kill -HUP 42976
2016-07-18 16:59:59,713 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 16:59:59,732 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 16:59:59,753 INFO     status on centos7: node centos7
2016-07-18 16:59:59,754 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:59:59,769 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v centos7
2016-07-18 16:59:59,783 INFO     expect on server centos7: state = free node centos7 ...  OK
2016-07-18 16:59:59,783 INFO     ============================
2016-07-18 16:59:59,783 INFO     Completed CgroupsTests setUp
2016-07-18 16:59:59,783 INFO     ============================
2016-07-18 16:59:59,784 INFO     server centos7: server operating mode set to cli
2016-07-18 16:59:59,784 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:59:59,858 INFO     expect on server centos7: pbs_version ~ .*14.* server centos7.usa.org ...  OK
2016-07-18 16:59:59,858 INFO     manager on centos7: set server {'log_events': '2047'}
2016-07-18 16:59:59,858 INFOCLI  centos7: /usr/local/bin/qmgr -c set server log_events=2047
2016-07-18 16:59:59,879 INFOCLI  centos7: /usr/local/bin/qstat -Bf centos7.usa.org
2016-07-18 16:59:59,934 INFO     expect on server centos7: log_events set 2047 server centos7.usa.org ...  OK
2016-07-18 16:59:59,935 INFO     scheduler centos7: config {'resources': 'ncpus,mem,host,vnode,vmem'}
2016-07-18 16:59:59,936 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /var/spool/pbs/sched_priv/sched_config /var/spool/pbs/sched_priv/sched_config.bak
2016-07-18 16:59:59,947 INFOCLI2 centos7: sudo -H /usr/bin/cp -p /tmp/PtlPbsGBbmEq /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:59:59,969 INFOCLI2 centos7: sudo -H /usr/bin/chmod 0644 /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:59:59,980 INFOCLI2 centos7: sudo -H /usr/bin/chown root /var/spool/pbs/sched_priv/sched_config
2016-07-18 16:59:59,996 INFOCLI2 centos7: sudo -H /usr/bin/chgrp root /var/spool/pbs/sched_priv/sched_config
2016-07-18 17:00:00,011 INFO     scheduler centos7: sent signal -HUP
2016-07-18 17:00:00,011 INFOCLI2 centos7: sudo -H kill -HUP 32631
2016-07-18 17:00:00,031 INFO     scheduler centos7 log match: searching for "Error reading line" - No match
2016-07-18 17:00:01,032 INFO     manager on centos7 as root: create resource nmics {'flag': 'nh', 'type': 'long'}
2016-07-18 17:00:01,033 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource nmics flag=nh,type=long
2016-07-18 17:00:01,060 INFO     manager on centos7 as root: create resource ngpus {'flag': 'nh', 'type': 'long'}
2016-07-18 17:00:01,061 INFOCLI  centos7: /usr/local/bin/qmgr -c create resource ngpus flag=nh,type=long
2016-07-18 17:00:01,087 INFO     Test Summary: test_t1_l1 tests "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" cgroup hook
2016-07-18 17:00:01,088 INFO     status on centos7: hook
2016-07-18 17:00:01,088 INFO     manager on centos7: list hook
2016-07-18 17:00:01,088 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 17:00:01,117 INFO     status on centos7: hook
2016-07-18 17:00:01,117 INFO     manager on centos7: list hook
2016-07-18 17:00:01,117 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook
2016-07-18 17:00:01,147 INFO     manager on centos7: create hook t1_l1
2016-07-18 17:00:01,147 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c create hook t1_l1
2016-07-18 17:00:01,180 INFO     manager on centos7: set hook t1_l1 {'freq': 2, 'enabled': 'True', 'event': '"execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"'}
2016-07-18 17:00:01,181 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c set hook t1_l1 freq=2,enabled=True,event="execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic"
2016-07-18 17:00:01,230 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c list hook t1_l1
2016-07-18 17:00:01,278 INFO     expect on server centos7: freq set 2 || enabled set True || event set "execjob_begin,execjob_launch,execjob_attach,execjob_epilogue,execjob_end,exechost_startup,exechost_periodic" hook t1_l1 ...  OK
2016-07-18 17:00:01,279 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-python', 'input-file': '/tmp/PtlPbs3y4uID', 'content-encoding': 'default'}
2016-07-18 17:00:01,279 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-python default /tmp/PtlPbs3y4uID
2016-07-18 17:00:01,399 INFO     server centos7: imported hook body
---
#  Copyright (C) 2003-2016 Altair Engineering, Inc. All rights reserved.
#
#  ALTAIR ENGINEERING INC. Proprietary and Confidential. Contains Trade Secret
#  Information. Not for use or disclosure outside ALTAIR and its licensed
#  clients. Information contained herein shall not be decompiled, disassembled,
#  duplicated or disclosed in whole or in part for any purpose. Usage of the
#  software is only as explicitly permitted in the end user software license
#  agreement.
#
#  Copyright notice does not imply publication.

# Last Updated
# Date: 07-06-2016
#
# When soft_limit is true for memory, memsw represents the hard limit.
#
# The resources value in sched_config must contain entries for mem and
# vmem if those subsystems are enabled in the hook configuration file. The
# amount of resource requested will not be avaiable to the hook if they
# are not present.

# This hook handles all of the operations necessary for PBS to support
# cgroups on linux hosts that support them (kernel 2.6.28 and higher)
#
# This hook services the following events:
# - exechost_periodic
# - exechost_startup
# - execjob_attach
# - execjob_begin
# - execjob_end
# - execjob_epilogue
# - execjob_launch

# Imports from __future__ must happen first
from __future__ import with_statement

# Additional imports
import sys
import os
import errno
import signal
import subprocess
import re
import glob
import time
import string
import platform
import traceback
import copy
from stat import S_ISBLK, S_ISCHR
import operator
import pwd
import pbs
import fnmatch
import socket

try:
    import json
except:
    import simplejson as json

CGROUP_KILL_ATTEMPTS = 12

# Flag to turn off features not in 12.2.40X branch
CRAY_12_BRANCH = False


# ============================================================================
# Derived error classes
# ============================================================================

# Base class for errors fixable only by administrative action.


class AdminError(Exception):
    pass

# Base class for errors in processing, unknown cause.


class ProcessingError(Exception):
    pass

# Base class for errors fixable by the user.


class UserError(Exception):
    pass

# Errors in PBS job resource values.


class JobValueError(UserError):
    pass

# Errors when the cgroup is busy.


class CgroupBusyError(ProcessingError):
    pass

# Errors in configuring cgroup.


class CgroupConfigError(AdminError):
    pass

# Errors in configuring cgroup.


class CgroupLimitError(AdminError):
    pass

# Errors processing cgroup.


class CgroupProcessingError(ProcessingError):
    pass

# ============================================================================
# Utility functions
# ============================================================================

#
# FUNCTION caller_name
#
# Return the name of the calling function or method.
#


def caller_name():
    return str(sys._getframe(1).f_code.co_name)

#
# FUNCTION convert_size
#
# Convert a string containing a size specification (e.g. "1m") to a
# string using different units (e.g. "1024k").
#
# This function only interprets a decimal number at the start of the string,
# stopping at any unrecognized character and ignoring the rest of the string.
#
# When down-converting (e.g. MB to KB), all calculations involve integers and
# the result returned is exact. When up-converting (e.g. KB to MB) floating
# point numbers are involved. The result is rounded up. For example:
#
# 1023MB -> GB yields 1g
# 1024MB -> GB yields 1g
# 1025MB -> GB yields 2g  <-- This value was rounded up
#
# Pattern matching or conversion may result in exceptions.
#


def convert_size(value, units='b'):
    logs = {'b': 0, 'k': 10, 'm': 20, 'g': 30,
            't': 40, 'p': 50, 'e': 60, 'z': 70, 'y': 80}
    try:
        new = units[0].lower()
        if new not in logs.keys():
            new = 'b'
        val, old = re.match('([-+]?\d+)([bkmgtpezy]?)',
                            str(value).lower()).groups()
        val = int(val)
        if val < 0:
            raise ValueError('Value may not be negative')
        if old not in logs.keys():
            old = 'b'
        factor = logs[old] - logs[new]
        val *= 2 ** factor
        slop = val - int(val)
        val = int(val)
        if slop > 0:
            val += 1
        # pbs.size() does not like units following zero
        if val <= 0:
            return '0'
        else:
            return str(val) + new
    except:
        return None

#
# FUNCTION size_as_int
#
# Convert a size string to an integer representation of size in bytes
#


def size_as_int(value):
    return int(convert_size(value).rstrip(string.ascii_lowercase))

#
# FUNCTION convert_time
#
# Converts a integer value for time into the value of the return unit
#
# A valid decimal number, with optional sign, may be followed by a character
# representing a scaling factor.  Scaling factors may be either upper or
# lower case. Examples include:
# 250ms
# 40s
# +15min
#
# Valid scaling factors are:
# ns  = 10**-9
# us  = 10**-6
# ms  = 10**-3
# s   =      1
# min =     60
# hr  =   3600
#
# Pattern matching or conversion may result in exceptions.
#


def convert_time(value, return_unit='s'):
    multipliers = {'':  1, 'ns': 10 ** -9, 'us': 10 ** -6,
                   'ms': 10 ** -3, 's': 1, 'min': 60, 'hr': 3600}
    n, factor = re.match('([-+]?\d+)(\w*[a-zA-Z])?',
                         str(value).lower()).groups(0)

    # Check to see if there was not unit of time specified
    if factor == 0:
        factor = ''

    # Check to see if the unit is valid
    if str.lower(factor) not in multipliers.keys():
        raise ValueError('Time unit not recognized.')

    # Convert the value to seconds
    value_in_sec = float(n) * float(multipliers[str.lower(factor)])
    if return_unit != 's':
        return value_in_sec / multipliers[str.lower(return_unit)]
    else:
        return float('%lf' % value_in_sec)

# simplejson hook to convert lists from unicode to utf-8


def decode_list(data):
    rv = []
    for item in data:
        if isinstance(item, unicode):
            item = item.encode('utf-8')
        elif isinstance(item, list):
            item = decode_list(item)
        elif isinstance(item, dict):
            item = decode_dict(item)
        rv.append(item)
    return rv

# simplejson hook to convert dictionaries from unicode to utf-8


def decode_dict(data):
    rv = {}
    for key, value in data.iteritems():
        if isinstance(key, unicode):
            key = key.encode('utf-8')
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        elif isinstance(value, list):
            value = decode_list(value)
        elif isinstance(value, dict):
            value = decode_dict(value)
        rv[key] = value
    return rv

# Convert CPU list format (with ranges) to a Python list


def cpus2list(s):
    # The input string is a comma separated list of digits and ranges.
    # Examples include:
    # 0-3,8-11
    # 0,2,4,6
    # 2,5-7,10
    cpus = []
    if s.strip() == '':
        return cpus
    for r in s.split(','):
        if '-' in r[1:]:
            start, end = r.split('-', 1)
            for i in range(int(start), int(end) + 1):
                cpus.append(int(i))
        else:
            cpus.append(int(r))
    return cpus


# Helper function to ignore suspended jobs when removing
# "already used" resources
def job_to_be_ignored(jobid):
    # Get job substate from small utility that calls printjob
    pbs_exec = ''
    if not CRAY_12_BRANCH:
        pbs_conf = pbs.get_pbs_conf()
        pbs_exec = pbs_conf['PBS_EXEC']
    else:
        if 'PBS_EXEC' in self.cfg:
            pbs_exec = self.cfg['PBS_EXEC']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "PBS_EXEC needs to be defined in the config file")
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Exiting the cgroups hook")
            pbs.accept()

    printjob_cmd = pbs_exec+os.sep+'bin'+os.sep+'printjob'

    cmd = [printjob_cmd, jobid]

    substate = None
    try:
        pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
        # Collect the job substate information
        process = subprocess.Popen(
                                   cmd,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        (out, err) = process.communicate()
        # Find the job substate
        substate_re = re.compile(r"substate:\s+(?P<jobstate>\S+)\s+")
        substate = substate_re.search(out)
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Unexpected error in job_to_be_ignored: %s" %
                   ' '.join([repr(sys.exc_info()[0]),
                            repr(sys.exc_info()[1])]))
        out = "Unknown: failed to run " + printjob_cmd

    if substate is not None:
        substate = substate.group().split(':')[1].strip()
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Job %s has substate %s" % (jobid, substate))

        suspended_substates = ['0x2b', '0x2d', 'unknown']
        return (substate in suspended_substates)
    else:
        return False

# ============================================================================
# Utility classes
# ============================================================================

#
# CLASS HookUtils
#


class HookUtils:

    def __init__(self, hook_events=None):
        if hook_events is not None:
            self.hook_events = hook_events
        else:
            self.hook_events = {
                # Defined in the order they appear in module_pbs_v1.c
                pbs.QUEUEJOB: {
                    'name': 'queuejob',
                    'handler': None
                },
                pbs.MODIFYJOB: {
                    'name': 'modifyjob',
                    'handler': None
                },
                pbs.RESVSUB: {
                    'name': 'resvsub',
                    'handler': None
                },
                pbs.MOVEJOB: {
                    'name': 'movejob',
                    'handler': None
                },
                pbs.RUNJOB: {
                    'name': 'runjob',
                    'handler': None
                },
                pbs.PROVISION: {
                    'name': 'provision',
                    'handler': None
                },
                pbs.EXECJOB_BEGIN: {
                    'name': 'execjob_begin',
                    'handler': self.__execjob_begin_handler
                },
                pbs.EXECJOB_PROLOGUE: {
                    'name': 'execjob_prologue',
                    'handler': None
                },
                pbs.EXECJOB_EPILOGUE: {
                    'name': 'execjob_epilogue',
                    'handler': self.__execjob_epilogue_handler
                },
                pbs.EXECJOB_PRETERM: {
                    'name': 'execjob_preterm',
                    'handler': None
                },
                pbs.EXECJOB_END: {
                    'name': 'execjob_end',
                    'handler': self.__execjob_end_handler
                },
                pbs.EXECJOB_LAUNCH: {
                    'name': 'execjob_launch',
                    'handler': self.__execjob_launch_handler
                },
                pbs.EXECHOST_PERIODIC: {
                    'name': 'exechost_periodic',
                    'handler': self.__exechost_periodic_handler
                },
                pbs.EXECHOST_STARTUP: {
                    'name': 'exechost_startup',
                    'handler': self.__exechost_startup_handler
                },
                pbs.MOM_EVENTS: {
                    'name': 'mom_events',
                    'handler': None
                },
            }

            if not CRAY_12_BRANCH:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "CRAY 12 Branch: %s" % (CRAY_12_BRANCH))
                self.hook_events[pbs.EXECJOB_ATTACH] = {
                    'name': 'execjob_attach',
                    'handler': self.__execjob_attach_handler
                }

    def __repr__(self):
        return "HookUtils(%s)" % (repr(self.hook_events))

    def event_name(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['name']
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Type: %s not found" % (caller_name(), type))
            return None

    def hashandler(self, type):
        if type in self.hook_events:
            return self.hook_events[type]['handler'] is not None
        else:
            return None

    def invoke_handler(self, event, cgroup, jobutil, *args):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: UID: real=%d, effective=%d" %
                   (caller_name(), os.getuid(), os.geteuid()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: GID: real=%d, effective=%d" %
                   (caller_name(), os.getgid(), os.getegid()))
        if self.hashandler(event.type):
            result = self.hook_events[event.type][
                'handler'](event, cgroup, jobutil, *args)
            return result
        else:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: %s event not handled by this hook." %
                       (caller_name(), self.event_name(event.type)))

    def __execjob_begin_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Instantiate the NodeConfig class for get_memory_on_node and
        # get_vmem_on node
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Host assigned job resources: %s" %
                   (caller_name(), jobutil.host_resources))

        # Make sure the parent cgroup directories exist
        cgroup.create_paths()

        # Make sure that any old cgroups created in an earlier attempt
        # at creating or running the job are gone
        cgroup.delete(e.job.id)

        # Gather the assigned resources
        cgroup.gather_assigned_resources()
        # Create the cgroup(s) for the job
        cgroup.create_job(e.job.id, node)
        # Configure the new cgroup
        cgroup.configure_job(e.job.id, jobutil.host_resources, node)
        # Initialize resource usage for the job
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        # Write out the assigned resources
        cgroup.write_out_cgroup_host_assigned_resources(e.job.id)
        # Write out the environment variable for the host (pbs_attach)
        if 'devices_name' in cgroup.host_assigned_resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Devices: %s" %
                       cgroup.host_assigned_resources['devices_name'])
            env_list = list()
            if len(cgroup.host_assigned_resources['devices_name']) > 0:
                line = str()
                mics = list()
                gpus = list()
                for key in cgroup.host_assigned_resources['devices_name']:
                    if key.startswith('mic'):
                        mics.append(key[3:])
                    elif key.startswith('nvidia'):
                        gpus.append(key[6:])
                if len(mics) > 0:
                    env_list.append('OFFLOAD_DEVICES="%s"' %
                                    string.join(mics, ','))
                if len(gpus) > 0:
                    # don't put quotes around the values. ex "0" or "0,1".
                    # This will cause it to fail.
                    env_list.append('CUDA_VISIBLE_DEVICES=%s' %
                                    string.join(gpus, ','))

            pbs.logmsg(pbs.EVENT_DEBUG, "ENV_LIST: %s" % env_list)
            cgroup.write_out_cgroup_host_job_env_file(e.job.id, env_list)
        return True

    def __execjob_epilogue_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # The resources_used information has a base type of pbs_resource
        # Set the usage data
        cgroup.update_job_usage(e.job.id, e.job.resources_used)
        return True

    def __execjob_end_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Delete the cgroup(s) for the job
        cgroup.delete(e.job.id)
        # Remove the host_assigned_resources and job_env file
        for filename in [cgroup.hook_storage_dir+os.sep+e.job.id,
                         cgroup.host_job_env_filename % e.job.id]:
            try:
                os.remove(filename)
            except OSError:
                pbs.logmsg(pbs.EVENT_DEBUG3, "File: %s not found" % (filename))
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Error removing file: %s" % (filename))
                pass

        return True

    def __execjob_launch_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Add the parent process id to the appropriate cgroups.
        cgroup.add_pids(os.getppid(), jobutil.job.id)
        # FUTURE: Add environment variable to the job environment
        # if job requested mic or gpu
        cgroup.read_in_cgroup_host_assigned_resources(e.job.id)
        if cgroup.host_assigned_resources is not None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "host_assigned_resources: %s" %
                       (cgroup.host_assigned_resources))
            cgroup.setup_job_devices_env()
        return True

    def __exechost_periodic_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Cleanup cgroups for jobs not present on this node
        count = cgroup.cleanup_orphans(e.job_list)
        if ('periodic_resc_update' in cgroup.cfg and
                cgroup.cfg['periodic_resc_update']):
            for job in e.job_list.keys():
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: job is %s" %
                           (caller_name(), job))
                cgroup.update_job_usage(job, e.job_list[job].resources_used)
        # Online nodes that were offlined due to a cgroup not cleaning up
        if count == 0 and cgroup.cfg['online_offlined_nodes']:
            vnode = pbs.event().vnode_list[cgroup.hostname]
            if os.path.isfile(cgroup.offline_file):
                line = 'Orphan cgroup(s) have been cleaned up. '

                # Check with the pbs server to see if the node comment matches
                try:
                    tmp_comment = pbs.server().vnode(cgroup.hostname).comment
                except:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Unable to contact server for node comment")
                    tmp_comment = None
                if tmp_comment == cgroup.offline_msg:
                    line += 'Will bring the node back online'
                    vnode.state = pbs.ND_FREE
                    vnode.comment = None
                else:
                    line += 'However, the comment has changed since the node '
                    line += 'was offlined. Node will remain offline'

                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: %s" % (caller_name(), line))
                # Remove file
                os.remove(cgroup.offline_file)
        return True

    def __exechost_startup_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        cgroup.create_paths()
        node = NodeConfig(cgroup.cfg)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: NodeConfig class instantiated" %
                   (caller_name()))

        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(node)))
        node.create_vnodes()
        if 'memory' in cgroup.subsystems:
            val = node.get_memory_on_node(cgroup.cfg)
            if val is not None:
                if 'vnode_per_numa_node' in cgroup.cfg:
                    if not cgroup.cfg['vnode_per_numa_node']:
                        hn = node.hostname
                        e.vnode_list[hn].resources_available['mem'] = \
                            pbs.size(val)
                        val_cpus = node.totalcpus
                        if val_cpus is not None:
                            e.vnode_list[hn].resources_available['ncpus'] = \
                                int(val_cpus)
                cgroup.set_node_limit('mem', val)
        if 'memsw' in cgroup.subsystems:
            val = node.get_vmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['vmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('vmem', val)
        if 'hugetlb' in cgroup.subsystems:
            val = node.get_hpmem_on_node(cgroup.cfg)
            if val is not None:
                e.vnode_list[node.hostname].resources_available['hpmem'] = \
                    pbs.size(val)
                cgroup.set_node_limit('hpmem', val)
        return True

    def __execjob_attach_handler(self, e, cgroup, jobutil):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logjobmsg(jobutil.job.id, "%s: Attaching PID %s" %
                      (caller_name(), e.pid))
        # Add the job process id to the appropriate cgroups.
        cgroup.add_pids(e.pid, jobutil.job.id)
        return True

#
# CLASS ShallIRunUtils
#


class ShallIRunUtils:

    def __init__(self, hostname, cfg):
        self.cfg = cfg
        self.hostname = hostname

    def allow_users(self, allow_users):
        """ This still needs to be defined """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pass

    def exclude_hosts(self, exclude_hosts):
        """
        Gets the hostname for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        hostname = pbs.get_local_nodename()
        # check to see if hostname is in the exclude_hosts list
        if self.hostname in exclude_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: exclude host %s is in %s" %
                       (caller_name(), self.hostname, exclude_hosts))
            pbs.event().accept()

        # check to see if it regex syntax is used
        pass

    def exclude_vntypes(self, exclude_vntypes):
        """
        Gets the vntype for the node and checks to see if the hook should
        run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        vntype = None

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'
        if os.path.isfile(vntype_file):
            fdata = open(vntype_file).readlines()
            vntype = fdata[0].strip()
            pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
        if vntype is None:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype is set to %s" % (vntype))
        elif vntype in exclude_vntypes:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s is in %s" % (vntype, exclude_vntypes))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Exiting Hook")
            pbs.event().accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "vntype: %s not in global exclude: %s" %
                       (vntype, exclude_vntypes))
        pass

    def run_only_on_hosts(self, approved_hosts):
        """
        Gets the list of approved nodes to run on and checks to see if the hook
        should run on this node
        """
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Approved hosts: %s, %s" %
                   (type(approved_hosts), approved_hosts))

        # check to see if hostname is in the approved_hosts list
        if approved_hosts == []:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "approved hosts list is empty: %s" % approved_hosts)
        elif self.hostname not in approved_hosts:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s is not in the approved list of hosts: %s" %
                       (self.hostname, approved_hosts))
            pbs.event().accept()

        else:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s in list of approved hosts: %s" %
                       (self.hostname, approved_hosts))

#
# CLASS JobUtils
#


class JobUtils:

    def __init__(self, job, hostname=None, resources=None):
        self.job = job
        self.host_vnodes = list()
        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if resources is not None:
            self.host_resources = resources
        else:
            self.host_resources = self.__host_assigned_resources()

    def __repr__(self):
        return "JobUtils(%s, %s, %s)" % (repr(self.job), repr(self.hostname),
                                         repr(self.host_resources))

    # Return a dictionary of resources assigned to the local node
    def __host_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Bail out if no hostname was provided
        if self.hostname is None:
            raise CgroupProcessingError('No hostname available')
        # Bail out if no job information is present
        if self.job is None:
            raise CgroupProcessingError('No job information available')
        # Determine which vnodes are on this host, if any
        if self.hostname is not None:
            vnhost_pattern = "%s\[[\d+]\]" % self.hostname
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnhost pattern: %s" %
                       (caller_name(), vnhost_pattern))
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job exec_vnode list: %s" %
                       (caller_name(), self.job.exec_vnode))
            for match in re.findall(vnhost_pattern, str(self.job.exec_vnode)):
                self.host_vnodes.append(match)
            if self.host_vnodes is not None:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Associate %s to vnodes on %s" %
                           (caller_name(), self.host_vnodes, self.hostname))

        # Collect host assigned resources
        resources = {}
        for chunk in self.job.exec_vnode.chunks:
            vnode = False
            if self.host_vnodes == [] and chunk.vnode_name != self.hostname:
                continue
            elif self.host_vnodes != [] and \
                    chunk.vnode_name not in self.host_vnodes:
                continue
            elif chunk.vnode_name in self.host_vnodes:
                vnode = True
                if 'vnodes' not in resources:
                    resources['vnodes'] = {}
                if chunk.vnode_name not in resources['vnodes']:
                    resources['vnodes'][chunk.vnode_name] = {}
                # This check is needed since not all resources are
                # required to be in each chunk of a job
                # i.e. exec_vnodes =
                # (node1[0]:ncpus=4:mem=4gb+node1[1]:mem=2gb) +
                # (node1[1]:ncpus=3+node[0]:ncpus=1:mem=4gb)
                if all(resc in resources['vnodes'][chunk.vnode_name]
                       for resc in chunk.chunk_resources.keys()):
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: resources already defined" %
                               (caller_name()))
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s,\t%s" %
                               (caller_name(),
                                chunk.chunk_resources.keys(),
                                resources['vnodes'][chunk.vnode_name].keys()))
                else:
                    # Initialize resources for each vnode
                    for resc in chunk.chunk_resources.keys():
                        # Initialize the new value
                        if isinstance(chunk.chunk_resources[resc],
                                      pbs.pbs_int):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_int(0)
                        elif isinstance(chunk.chunk_resources[resc],
                                        pbs.pbs_float):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.pbs_float(0)
                        elif isinstance(chunk.chunk_resources[resc], pbs.size):
                            resources['vnodes'][chunk.vnode_name][resc] = \
                                      pbs.size('0')

            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Chunk %s" %
                       (caller_name(), chunk.vnode_name))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resources: %s" %
                       (caller_name(), resources))
            for resc in chunk.chunk_resources.keys():
                if resc not in resources.keys():
                    # Initialize the new value
                    if isinstance(chunk.chunk_resources[resc], pbs.pbs_int):
                        resources[resc] = pbs.pbs_int(0)
                    elif isinstance(chunk.chunk_resources[resc],
                                    pbs.pbs_float):
                        resources[resc] = pbs.pbs_float(0.0)
                    elif isinstance(chunk.chunk_resources[resc], pbs.size):
                        resources[resc] = pbs.size('0')
                # Add resource value to total
                if type(chunk.chunk_resources[resc]) in \
                        [pbs.pbs_int, pbs.pbs_float, pbs.size]:
                    resources[resc] += chunk.chunk_resources[resc]
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s: resources[%s][%s] is now %s" %
                               (caller_name(), self.hostname, resc,
                                resources[resc]))
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] += \
                                  chunk.chunk_resources[resc]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Setting resource %s to string %s" %
                               (caller_name(), resc,
                                str(chunk.chunk_resources[resc])))
                    resources[resc] = str(chunk.chunk_resources[resc])
                    if vnode is True:
                        resources['vnodes'][chunk.vnode_name][resc] = \
                                  str(chunk.chunk_resources[resc])
        if not resources:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: No resources assigned to host %s" %
                       (caller_name(), self.hostname))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources for %s: %s" %
                       (caller_name(), self.hostname, repr(resources)))

        # Return assigned resources for specified host
        pbs.logmsg(pbs.EVENT_DEBUG3, "Job Resources: %s" % (resources))
        return resources

    # Write a message to the job stdout file
    def write_to_stderr(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stderr_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

    # Write a message to the job stdout file
    def write_to_stdout(self, job, msg):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            filename = job.stdout_file()
            if filename is None:
                return
            fd = open(filename, 'a')
            fd.write(msg)
        except Exception, exc:
            pass

#
# CLASS NodeConfig
#


class NodeConfig:

    def __init__(self, cfg, **kwargs):

        self.cfg = cfg
        hostname = None
        meminfo = None
        numa_nodes = None
        devices = None
        hyperthreading = None
        self.hyperthreads_per_core = 1
        self.totalcpus = self.__discover_cpus()

        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")
                pbs.accept()

        self.host_mom_jobdir = pbs_home+'/mom_priv/jobs'

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'hostname':
                    hostname = val
                elif arg == 'meminfo':
                    meminfo = val
                elif arg == 'numa_nodes':
                    numa_nodes = val
                elif arg == 'devices':
                    devices = val
                elif arg == 'hyperthreading':
                    hyperthreading = val

        if hostname is not None:
            self.hostname = hostname
        else:
            self.hostname = pbs.get_local_nodename()
        if meminfo is not None:
            self.meminfo = meminfo
        else:
            self.meminfo = self.__discover_meminfo()
        if numa_nodes is not None:
            self.numa_nodes = numa_nodes
        else:
            self.numa_nodes = self.__discover_numa_nodes()
        if devices is not None:
            self.devices = devices
        else:
            self.devices = self.__discover_devices()
        if hyperthreading is not None:
            self.hyperthreading = hyperthreading
        else:
            self.hyperthreading = self.__hyperthreading_enabled()

        # Add the devices count i.e. nmics and ngpus to the numa nodes
        self.__add_devices_to_numa_node()

    def __repr__(self):
        return ("NodeConfig(%s, %s, %s, %s, %s, %s)" %
                (repr(self.cfg), repr(self.hostname), repr(self.meminfo),
                 repr(self.numa_nodes), repr(self.devices),
                 repr(self.hyperthreading)))

    def __add_devices_to_numa_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (self.devices.keys()))
        for device in self.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" % (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" %
                           (self.devices[device].keys()))
                for device_name in self.devices[device]:
                    device_socket = \
                        self.devices[device][device_name]['numa_node']
                    if device == 'mic':
                        if 'nmics' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['nmics'] = 1
                        else:
                            self.numa_nodes[device_socket]['nmics'] += 1
                    elif device == 'gpu':
                        if 'ngpus' not in self.numa_nodes[device_socket]:
                            self.numa_nodes[device_socket]['ngpus'] = 1
                        else:
                            self.numa_nodes[device_socket]['ngpus'] += 1

        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (self.numa_nodes))

    # Discover what type of hardware is on this node and how is it partitioned
    def __discover_numa_nodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        numa_nodes = {}
        for dir in glob.glob(os.path.join(os.sep,
                             "sys", "devices", "system", "node", "node*")):
            id = int(dir.split(os.sep)[5][4:])
            if id not in numa_nodes.keys():
                numa_nodes[id] = {}
                numa_nodes[id]['devices'] = list()
            numa_nodes[id]['cpus'] = open(os.path.join(dir, "cpulist"),
                                          'r').readline().strip()
            with open(os.path.join(dir, "meminfo"), 'r') as fd:
                for line in fd:
                    # Each line will contain four or five fields. Examples:
                    # Node 0 MemTotal:       32995028 kB
                    # Node 0 HugePages_Total:     0
                    entries = line.split()
                    if len(entries) < 4:
                        continue
                    if entries[2] == "MemTotal:":
                        numa_nodes[id][entries[2].rstrip(':')] = \
                            convert_size(entries[3] + entries[4], 'kb')
                    elif entries[2] == "HugePages_Total:":
                        numa_nodes[id][entries[2].rstrip(':')] = entries[3]
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover Numa Nodes: %s" % numa_nodes)
        return numa_nodes

    # Identify devices and to which numa nodes they are attached
    def __discover_devices(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        devices = {}
        for file in glob.glob(os.path.join(os.sep, "sys", "class", "*", "*",
                                           "device", "numa_node")):
            # The file should contain a single integer
            numa_node = int(open(file, 'r').readline().strip())
            if numa_node < 0:
                numa_node = 0
            dirs = file.split(os.sep)
            name = dirs[3]
            instance = dirs[4]
            if name not in devices.keys():
                devices[name] = {}
            devices[name][instance] = {}
            devices[name][instance]['numa_node'] = numa_node
            if name == 'mic':
                s = os.stat('/dev/%s' % instance)
                devices[name][instance]['major'] = os.major(s.st_rdev)
                devices[name][instance]['minor'] = os.minor(s.st_rdev)
                devices[name][instance]['device_type'] = 'c'

        # Check to see if there are gpus on the node
        gpus = self.discover_gpus()
        if isinstance(gpus, dict) and len(gpus.keys()) > 0:
            devices['gpu'] = gpus['gpu']

        pbs.logmsg(pbs.EVENT_DEBUG3, "Discovered devices: %s" % (devices))
        return devices

    def discover_gpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Check to see if nvidia-smi exists and produces valid output
            cmd = ['/usr/bin/nvidia-smi', '-q', '-x']
            if 'nvidia-smi' in self.cfg:
                cmd[0] = self.cfg['nvidia-smi']

            pbs.logmsg(pbs.EVENT_DEBUG3, "cmd: %s" % cmd)
            # Collect the gpu information
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.PIPE)
            output, err = process.communicate()

            # pbs.logmsg(pbs.EVENT_DEBUG3,"output: %s" % output)
            # import the xml library and parse the output
            import xml.etree.ElementTree as ET

            # Parse the data
            name = 'gpu'
            gpu_data = dict()
            gpu_data[name] = {}
            root = ET.fromstring(output)
            pbs.logmsg(pbs.EVENT_DEBUG3, "root.tag: %s" % root.tag)
            for child in root:
                if child.tag == "attached_gpus":
                    # gpu_data['ngpus'] = int(child.text)
                    pass
                if child.tag == "gpu":
                    device_id = child.get('id')
                    number = 'nvidia%s' % child.find('minor_number').text
                    gpu_data[name][number] = dict()
                    gpu_data[name][number]['id'] = device_id

                    # Determine which socket the device is on
                    filename = '/sys/bus/pci/devices/%s/numa_node' % \
                        device_id.lower()
                    isfile = os.path.isfile(filename)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "%s, %s" % (filename, isfile))
                    if isfile:
                        numa_node = open(filename).read().strip()
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "numa_node: %s" % (numa_node))
                        if int(numa_node) == -1:
                            numa_node = 0
                        gpu_data[name][number]['numa_node'] = int(numa_node)
                        try:
                            dev_filename = '/dev/%s' % number
                            s = os.stat(dev_filename)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find %s" % dev_filename)
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                        gpu_data[name][number]['major'] = os.major(s.st_rdev)
                        gpu_data[name][number]['minor'] = os.minor(s.st_rdev)
                        gpu_data[name][number]['device_type'] = 'c'
            pbs.logmsg(pbs.EVENT_DEBUG, "%s" % gpu_data)
            return gpu_data
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unable to find %s" % string.join(cmd, " "))
            return {'gpu': {}}
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        return {'gpu': {}}

    # Get the memory info on this host
    def __discover_meminfo(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        meminfo = {}
        with open(os.path.join(os.sep, "proc", "meminfo"), 'r') as fd:
            for line in fd:
                entries = line.split()
                if entries[0] == "MemTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "SwapTotal:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "Hugepagesize:":
                    meminfo[entries[0].rstrip(':')] = \
                        convert_size(entries[1] + entries[2], 'kb')
                elif entries[0] == "HugePages_Total:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
                elif entries[0] == "HugePages_Rsvd:":
                    meminfo[entries[0].rstrip(':')] = int(entries[1])
        pbs.logmsg(pbs.EVENT_DEBUG3, "Discover meminfo: %s" % meminfo)
        return meminfo

    # Determine if the cpus have hyperthreading enabled
    def __hyperthreading_enabled(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            siblings = 0
            cpu_cores = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "siblings":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    siblings = entries[1].strip()
                elif entries[0].strip() == "cpu cores":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_cores = entries[1].strip()
                elif entries[0].strip() == "flags":
                    flag_options = entries[1].split()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: flag_options: %s" %
                               (caller_name(), flag_options))
                    if "ht" in flag_options:
                        rc = True
                        break
        if rc is True:
            self.hyperthreads_per_core = int(siblings)/int(cpu_cores)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: hyperthreads/core: %d" %
                       (caller_name(), self.hyperthreads_per_core))
            if self.hyperthreads_per_core == 1:
                rc = False
        return rc

    def __discover_cpus(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        rc = False
        with open(os.path.join(os.sep, "proc", "cpuinfo"), 'r') as fd:
            cpu_threads = 0
            for line in fd:
                entries = line.strip().split(":")
                if len(entries) < 1:
                    continue
                elif entries[0].strip() == "processor":
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: line: %s" %
                               (caller_name(), entries))
                    cpu_threads += 1
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: processors found: %d" %
                   (caller_name(), cpu_threads))

        return cpu_threads

    # Gather the jobs running on this node
    def gather_jobs_on_node(self):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        # Get the job list from the server
        jobs = None
        try:
            jobs = pbs.server().vnode(self.hostname).jobs
        except:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Unable to contact server to get jobs")
            return None

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs from server for %s: %s" % (self.hostname, jobs))

        if jobs is None:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "No jobs found on %s " % (self.hostname))
            return list()

        jobs_list = dict()
        for job in jobs.split(','):
            # The job list from the node has a space in front of the job id
            # This must be removed or it will not match the cgroup dir
            jobs_list[job.split('/')[0].strip()] = 0
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Jobs on %s: %s" % (self.hostname, jobs_list.keys()))

        return jobs_list.keys()

    # Gather the jobs running on this node
    def gather_jobs_on_node_local(self):
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Method called" % (caller_name()))
        # Get the job list from the jobs directory
        jobs_list = list()
        try:
            for file in os.listdir(self.host_mom_jobdir):
                if fnmatch.fnmatch(file, "*.JB"):
                    jobs_list.append(file[:-3])
            if not jobs_list:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "No jobs found on %s " % (self.hostname))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Jobs from server for %s: %s" %
                           (self.hostname, repr(jobs_list)))

            return jobs_list
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Could not get job list from mom_priv/jobs for %s" %
                       self.hostname)

            return self.gather_jobs_on_node()

    # Get the memory resource on this mom
    def get_memory_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Calculate memory
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
            pbs.logmsg(pbs.EVENT_DEBUG3, "val: %s" % val)
        else:
            val = size_as_int(MemTotal)
        if 'percent_reserve' in config['cgroup']['memory']:
            percent_reserve = config['cgroup']['memory']['percent_reserve']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memory'].keys():
            reserve_memory = config['cgroup']['memory']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                reserve_mem = size_as_int(reserve_memory)
                if val > reserve_mem:
                    val -= reserve_mem
                else:
                    val -= size_as_int("100mb")
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Unable to reserve more memory than " +
                               "available on the node. Available %s, " +
                               "Tried to reserve %s. Reserving 100mb" %
                               (val, reserve_mem))

            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))

        pbs.logmsg(pbs.EVENT_DEBUG3, "Return val: %s" % val)
        return convert_size(str(val), 'kb')

    # Get the virtual memory resource on this mom
    def get_vmem_on_node(self, config, MemTotal=None):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Return the value for memory if no swap is configured
        if size_as_int(self.meminfo['SwapTotal']) <= 0:
            return self.get_memory_on_node(config)
        # Calculate vmem
        val = 0
        if MemTotal is None:
            val = size_as_int(self.meminfo['MemTotal'])
        else:
            val = size_as_int(MemTotal)

        val += size_as_int(self.meminfo['SwapTotal'])
        # Determine if memory needs to be reserved
        if 'percent_reserve' in config['cgroup']['memsw']:
            percent_reserve = config['cgroup']['memsw']['reserve_memory']
            val -= (val * percent_reserve) / 100
        elif 'reserve_memory' in config['cgroup']['memsw']:
            reserve_memory = config['cgroup']['memsw']['reserve_memory']
            try:
                pbs.size(reserve_memory)
                val -= size_as_int(reserve_memory)
            except:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "%s: Invalid memory reserve value: %s" %
                           (caller_name(), reserve_memory))
        return convert_size(str(val), 'kb')

    # Get the huge page memory resource on this mom
    def get_hpmem_on_node(self, config):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        if 'percent_reserve' in config['cgroup']['hugetlb'].keys():
            percent_reserve = config['cgroup']['hugetlb']['percent_reserve']
        else:
            percent_reserve = 0
        # Calculate vmem
        val = size_as_int(self.meminfo['Hugepagesize'])
        val *= (self.meminfo['HugePages_Total'] -
                self.meminfo['HugePages_Rsvd'])
        val -= (val * percent_reserve) / 100
        return convert_size(str(val), 'kb')

    # Create individual vnodes per socket
    def create_vnodes(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        key = "vnode_per_numa_node"
        if key in self.cfg.keys():
            if not self.cfg[key]:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s is false" %
                           (caller_name(), key))
                return
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s not configured" %
                       (caller_name(), key))
            return

        # Create one vnode per numa node
        vnode_list = pbs.event().vnode_list
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: numa nodes: %s" %
                   (caller_name(), self.numa_nodes))
        # Define the vnodes
        vnode_name = self.hostname
        vnode_list[vnode_name] = pbs.vnode(vnode_name)
        for id in self.numa_nodes.keys():
            vnode_name = self.hostname + "[%d]" % id
            vnode_list[vnode_name] = pbs.vnode(vnode_name)
            for key, val in sorted(self.numa_nodes[id].iteritems()):
                if key == 'cpus':
                    vnode_list[vnode_name].resources_available['ncpus'] = \
                        len(cpus2list(val))/self.hyperthreads_per_core
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['ncpus'] = 0
                elif key == 'MemTotal':
                    mem = self.get_memory_on_node(self.cfg, val)
                    vnode_list[vnode_name].resources_available['mem'] = \
                        pbs.size(mem)
                    # set the value on the host to 0
                    vnode_list[self.hostname].resources_available['mem'] = \
                        pbs.size('0kb')
                elif key == 'HugePages_Total':
                    pass
                elif isinstance(val, list):
                    pass
                elif isinstance(val, dict):
                    pass
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s=%s" %
                               (caller_name(), key, val))
                    vnode_list[vnode_name].resources_available[key] = val

                    if isinstance(val, int):
                        vnode_list[self.hostname].resources_available[key] = 0
                    elif isinstance(val, float):
                        vnode_list[self.hostname].resources_available[key] = \
                            0.0
                    elif isinstance(val, pbs.size):
                        vnode_list[self.hostname].resources_available[key] = \
                            pbs.size('0kb')
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: vnode list: %s" %
                   (caller_name(), vnode_list))
        return True

#
# CLASS CgroupUtils
#


class CgroupUtils:
    def __init__(self, hostname, vnode, **kwargs):
        cfg = None
        subsystems = None
        paths = None
        vntype = None
        event = None
        self.assigned_resources = dict()
        self.existing_cgroups = dict()
        self.host_assigned_resources = None
        self.pid_lower_limit = 3

        if kwargs:
            for arg, val in kwargs.items():
                if arg == 'cfg':
                    cfg = val
                elif arg == 'subsystems':
                    subsystems = val
                elif arg == 'paths':
                    paths = val
                elif arg == 'vntype':
                    vntype = val
                elif arg == 'event':
                    event = val

        try:
            self.hostname = hostname
            self.vnode = vnode
            # __check_os will raise an exception if cgroups are not present
            self.__check_os()
            # Read in the config file
            if cfg is not None:
                self.cfg = cfg
            else:
                self.cfg = self.__parse_config_file()

            # Determine if the hook should run or exit
            siru = ShallIRunUtils(self.hostname, self.cfg)
            siru.exclude_hosts(self.cfg['exclude_hosts'])
            siru.exclude_vntypes(self.cfg['exclude_vntypes'])
            siru.run_only_on_hosts(self.cfg['run_only_on_hosts'])

            # Collect the mount points
            if paths is not None:
                self.paths = paths
            else:
                self.paths = self.__set_paths()
            # Define the local vnode type
            if vntype is not None:
                self.vntype = vntype
            else:
                self.vntype = self.__get_vnode_type()
            # Determine which subsystems we care about
            if subsystems is not None:
                self.subsystems = subsystems
            else:
                self.subsystems = self.__target_subsystems()

            # location to store information for the different hook events
            pbs_home = ''
            if not CRAY_12_BRANCH:
                pbs_conf = pbs.get_pbs_conf()
                pbs_home = pbs_conf['PBS_HOME']
            else:
                if 'PBS_HOME' in self.cfg:
                    pbs_home = self.cfg['PBS_HOME']
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "PBS_HOME needs to be defined in the " +
                               "config file")
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Exiting the cgroups hook")
                    pbs.accept()

            self.hook_storage_dir = pbs_home+'/mom_priv/hooks/hook_data'
            self.host_job_env_dir = pbs_home+'/aux'
            self.host_job_env_filename = self.host_job_env_dir+os.sep+"%s.env"

            # information for offlining nodes
            self.offline_file = \
                pbs_home+os.sep+'mom_priv'+os.sep+'hooks'+os.sep
            self.offline_file += "%s.offline" % pbs.event().hook_name
            self.offline_msg = "Hook %s: " % pbs.event().hook_name
            self.offline_msg += "Unable to clean up one or more cgroups"

        except:
            raise

    def __repr__(self):
        return ("CgroupUtils(%s, %s, %s, %s, %s, %s)" %
                (repr(self.hostname), repr(self.vnode), repr(self.cfg),
                 repr(self.subsystems), repr(self.paths), repr(self.vntype)))

    # Validate the OS type and version
    def __check_os(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check to see if the platform is linux and the kernel is new enough
        if platform.system() != 'Linux':
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: OS does not support cgroups" %
                       (caller_name()))
            raise CgroupConfigError("OS type not supported")
        rel = map(int,
                  string.split(string.split(platform.release(), '-')[0], '.'))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Detected Linux kernel version %d.%d.%d" %
                   (caller_name(), rel[0], rel[1], rel[2]))
        supported = False
        if rel[0] > 2:
            supported = True
        elif rel[0] == 2:
            if rel[1] > 6:
                supported = True
            elif rel[1] == 6:
                if rel[2] >= 28:
                    supported = True
        if not supported:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Kernel needs to be >= 2.6.28: %s" %
                       (caller_name(), system_info[2]))
            raise CgroupConfigError("OS version not supported")
        return supported

    # Read the config file in json format
    def __parse_config_file(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        config = {}
        # Identify the config file and read in the data
        if 'PBS_HOOK_CONFIG_FILE' in os.environ:
            config_file = os.environ["PBS_HOOK_CONFIG_FILE"]
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Config file is %s" %
                       (caller_name(), config_file))
            try:
                config = json.load(open(config_file, 'r'),
                                   object_hook=decode_dict)
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
        else:
            raise CgroupConfigError("No configuration file present")
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Initial Cgroup Config: %s" %
                   (caller_name(), config))
        # Set some defaults if they are not present
        if 'cgroup_prefix' not in config.keys():
            config['cgroup_prefix'] = 'pbspro'
        if 'periodic_resc_update' not in config.keys():
            config['periodic_resc_update'] = False
        if 'vnode_per_numa_node' not in config.keys():
            config['vnode_per_numa_node'] = False
        if 'online_offlined_nodes' not in config.keys():
            config['online_offlined_nodes'] = False
        if 'exclude_hosts' not in config.keys():
            config['exclude_hosts'] = []
        if 'run_only_on_hosts' not in config.keys():
            config['run_only_on_hosts'] = []
        if 'exclude_vntypes' not in config.keys():
            config['exclude_vntypes'] = []
        if 'cgroup' not in config.keys():
            config['cgroup'] = {}
        else:
            for subsys in config['cgroup']:
                subsystem = config['cgroup'][subsys]
                if 'enabled' not in subsystem.keys():
                    config['cgroup'][subsys]['enabled'] = False
                if 'exclude_hosts' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_hosts'] = []
                if 'exclude_vntypes' not in subsystem.keys():
                    config['cgroup'][subsys]['exclude_vntypes'] = []

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Cgroup Config with defaults: %s" %
                   (caller_name(), config))
        return config

    # Determine which subsystems are being requested
    def __target_subsystems(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        subsystems = []
        for key in self.cfg['cgroup'].keys():
            if self.enabled(key):
                subsystems.append(key)
        if 'memory' not in subsystems:
            if 'memsw' in subsystems:
                # Remove memsw since it must be greater then
                # or equal to memory
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing memsw from enabled subsystems")
                subsystems.remove('memsw')
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Enabled subsystems: %s" %
                   (caller_name(), subsystems))
        # It is not an error for all subsystems to be disabled.
        # This host or vnode type may be in the excluded list.
        return subsystems

    # Copy a setting from the parent cgroup
    def __copy_from_parent(self, dest):
        try:
            file = os.path.basename(dest)
            dir = os.path.dirname(dest)
            parent = os.path.dirname(dir)
            source = os.path.join(parent, file)
            if not os.path.isfile(source):
                raise CgroupConfigError('Failed to read %s' % (source))
            self.write_value(dest, open(source, 'r').read().strip())
        except:
            raise

    # Create a dictionary of the cgroup subsystems and their corresponding
    # directories
    def __set_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        paths = {}
        try:
            # Loop through the mounts and collect the ones for cgroups
            with open(os.path.join(os.sep, "proc", "mounts"), 'r') as fd:
                for line in fd:
                    entries = line.split()
                    if len(entries) > 3 and entries[2] == "cgroup":
                        flags = entries[3].split(',')
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "subsysdir: %s (flags=%s)" %
                                   (entries[1], flags))
                        for subsys in flags:
                            paths[subsys] = os.path.join(
                                entries[1], self.cfg["cgroup_prefix"])
        except:
            raise
        if len(paths.keys()) < 1:
            raise CgroupConfigError("Cgroup paths not detected")
        # Both the memory and memsw cgroups share the same mount point
        if "memory" in paths.keys():
            paths["memsw"] = paths["memory"]
        return paths

    # Create the cgroup parent directories that will contain the jobs
    def create_paths(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            # Create the directories that PBS will use to house the jobs
            old_umask = os.umask(0022)
            for subsys in self.subsystems:
                if subsys not in self.paths.keys():
                    raise CgroupConfigError('No path for subsystem: %s' %
                                            (subsys))
                if not os.path.exists(self.paths[subsys]):
                    os.makedirs(self.paths[subsys], 0755)
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Created directory %s" %
                               (caller_name(), self.paths[subsys]))
                    if subsys == 'memory' or subsys == 'memsw':
                        # Set file to
                        # <cgroup>/memory/pbspro/memory.use_hierarchy
                        file = os.path.join(self.paths[subsys],
                                            'memory.use_hierarchy')
                        if not os.path.isfile(file):
                            raise CgroupConfigError('Failed to configure %s' %
                                                    (file))
                        self.write_value(file, 1)
                    elif subsys == 'cpuset':
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.cpus'))
                        self.__copy_from_parent(os.path.join(
                                                self.paths['cpuset'],
                                                'cpuset.mems'))

        except:
            raise CgroupConfigError("Failed to create directory: %s" %
                                    (self.paths[subsys]))
        finally:
            os.umask(old_umask)

    # Return the vnode type of the local node
    def __get_vnode_type(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # look for on file on the mom until we can pass the vntype to the hook
        pbs_home = ''
        if not CRAY_12_BRANCH:
            pbs_conf = pbs.get_pbs_conf()
            pbs_home = pbs_conf['PBS_HOME']
        else:
            if 'PBS_HOME' in self.cfg:
                pbs_home = self.cfg['PBS_HOME']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "PBS_HOME needs to be defined in the config file")
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "Exiting the cgroups hook")

        # File to check for if it is not defined in the hook input file
        vntype_file = pbs_home+os.sep+'mom_priv'+os.sep+'vntype'

        # self.vnode is None for pbs_attach events
        pbs.logmsg(pbs.EVENT_DEBUG3, "vnode: %s" % self.vnode)
        if self.vnode is not None:
            if 'vntype' in self.vnode.resources_available and \
                    self.vnode.resources_available['vntype'] is not None:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "vntype: %s" %
                           self.vnode.resources_available['vntype'])
                return self.vnode.resources_available['vntype']
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3, "vntype file: %s" % vntype_file)
                if os.path.isfile(vntype_file):
                    fdata = open(vntype_file).readlines()
                    vntype = fdata[0].strip()
                    pbs.logmsg(pbs.EVENT_DEBUG3, "vntype: %s" % vntype)
                    return vntype
        # If vntype was not set then log a message. It is too expensive
        # to have all moms query the server for large jobs.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: resources_available.vntype is " % caller_name() +
                   "not set for this vnode")
        return None

    # Gather resources that are already assigned
    def gather_assigned_resources(self):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        assigned_resources = {}

        # Gather the cpus and memory sockets assigned
        directories = [x[0] for x in os.walk(self.paths['cpuset'])]

        # Add the existing cgroups to a list to check if assigning
        # resources fail
        for dir in directories[1:]:
            if dir.find(pbs.event().job.id) == -1:
                self.existing_cgroups[dir] = []

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'cpuset' in self.subsystems and \
                self.cfg['cgroup']['cpuset']['enabled']:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather cpuset resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['cpuset'], tmp_dir)
                    with open(path+os.sep+'cpuset.cpus') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.cpus'] = \
                            cpus2list(tmp_val.strip())
                    with open(path+os.sep+'cpuset.mems') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['cpuset.mems'] = \
                            cpus2list(tmp_val.strip())

        # Check to see if the subsystem is enabled before trying
        # to gather resources
        if 'memory' in self.subsystems and \
                self.cfg['cgroup']['memory']['enabled']:
            # Gather the memory assigned for each socket
            directories = [x[0] for x in os.walk(self.paths['memory'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather memory resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['memory'], tmp_dir)
                    with open(path+os.sep+'memory.limit_in_bytes') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memory.limit'] = \
                            tmp_val.strip()
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    if os.path.isfile(tmp_filename) is False:
                        continue
                    tmp_filename = path+os.sep+'memory.memsw.limit_in_bytes'
                    with open(tmp_filename) as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['memsw.limit'] = \
                            tmp_val.strip()

        # Check to see if the subsystem is enabled before trying to
        # gather resources
        if 'devices' in self.subsystems and \
                self.cfg['cgroup']['devices']['enabled']:
            # Gather the devices assigned
            directories = [x[0] for x in os.walk(self.paths['devices'])]

            # Add the existing cgroups to a list to check if assigning
            # resources fail
            for dir in directories[1:]:
                self.existing_cgroups[dir] = []

            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Gather device resources from: %s" % directories)
            if len(directories) > 1:
                for dir in directories[1:]:
                    tmp_dir = os.path.split(dir)[-1]
                    pbs.logmsg(pbs.EVENT_DEBUG3, "Dir: %s" % tmp_dir)
                    if tmp_dir not in assigned_resources:
                        assigned_resources[tmp_dir] = {}
                    path = os.path.join(self.paths['devices'], tmp_dir)
                    with open(path+os.sep+'devices.list') as fd:
                        tmp_val = fd.read()
                        assigned_resources[tmp_dir]['devices.list'] = \
                            tmp_val.strip().split('\n')
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Dir: %s\n\tValue: %s" %
                                   (path, tmp_val.replace('\n', ',')))

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Gathered assigned resources: %s" % assigned_resources)
        self.assigned_resources = assigned_resources

    # Return whether a subsystem is enabled
    def enabled(self, subsystem):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Check whether the subsystem is enabled in the configuration file
        if subsystem not in self.cfg['cgroup'].keys():
            return False
        if 'enabled' not in self.cfg['cgroup'][subsystem].keys():
            return False
        if not self.cfg['cgroup'][subsystem]['enabled']:
            return False
        # Check whether the cgroup is mounted for this subsystem
        if subsystem not in self.paths.keys():
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup not mounted for %s" %
                       (caller_name(), subsystem))
            return False
        # Check whether this host is excluded
        # if self.hostname in self.cfg['exclude_hosts']:
        #    pbs.logmsg(pbs.EVENT_DEBUG, "%s: cgroup excluded on host %s" %
        #               (caller_name(), self.hostname))
        #    pbs.event().accept()
        if self.hostname in self.cfg['cgroup'][subsystem]['exclude_hosts']:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: cgroup excluded for subsystem %s on host %s" %
                       (caller_name(), subsystem, self.hostname))
            return False
        # Check whether the vnode type is excluded
        if self.vntype is not None:
            # if self.vntype in self.cfg['exclude_vntypes']:
            #    pbs.logmsg(pbs.EVENT_DEBUG,
            #               "%s: cgroup excluded on vnode type %s" %
            #               (caller_name(), self.vntype))
            #    pbs.event().accept()
            if self.vntype in self.cfg['cgroup'][subsystem]['exclude_vntypes']:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           ("%s: cgroup excluded for " +
                            "subsystem %s on vnode type %s") %
                           (caller_name(), subsystem, self.vntype))
                return False
        return True

    # Return the default value for a subsystem
    def default(self, subsystem):
        if subsystem in self.cfg['cgroup']:
            if 'default' in self.cfg['cgroup'][subsystem]:
                return self.cfg['cgroup'][subsystem]['default']
        return None

    # Check to see if the pid matches the job owners uid
    def is_pid_owned_by_job_owner(self, pid):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))
        try:
            proc_uid = os.stat('/proc/%d' % pid).st_uid
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       '/proc/%d uid:%d' % (pid, proc_uid))

            job_owner_uid = pwd.getpwnam(pbs.event().job.euser)[2]
            pbs.logmsg(pbs.EVENT_DEBUG3, "Job uid: %d" % job_owner_uid)

            if proc_uid == job_owner_uid:
                return True
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Proc uid: %d != Job owner:  %d" %
                           (proc_uid, job_owner_uid))
        except OSError:
            pbs.logmsg(pbs.EVENT_DEBUG, "Unknown pid: %d" % pid)
        except:
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Unexpected error: %s" % sys.exc_info()[0])

        # If we got to this point something did not match up
        return False

    # Add some number of PIDs to the cgroup tasks files for each subsystem
    def add_pids(self, pids, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        # make pids a list
        if isinstance(pids, int):
            pids = [pids]

        if pbs.event().type == pbs.EXECJOB_LAUNCH:
            if 1 in pids:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "1 is not a valid pid to add")
                # Hold the job for further review
                e.reject("Hook tried to add pid 1 to the tasks file " +
                         "for job %s" % jobid)

        # check pids to make sure that they are owned by the job owner
        if not CRAY_12_BRANCH:
            pbs.logmsg(pbs.EVENT_DEBUG3, "Not in the Cray 12 Branch")
            pbs.logmsg(pbs.EVENT_DEBUG3, "check to see if execjob_attach")
            if pbs.event().type == pbs.EXECJOB_ATTACH:
                pbs.logmsg(pbs.EVENT_DEBUG3, "event type: attach or launch")
                tmp_pids = list()
                for p in pids:
                    if self.is_pid_owned_by_job_owner(p):
                        tmp_pids.append(p)
                if len(tmp_pids) == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%d is not a valid pids to add" % p)
                    return False
                else:
                    pids = tmp_pids

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: subsys = %s" %
                       (caller_name(), subsys))
            # memsw and memory use the same tasks file
            if subsys == "memsw":
                if "memory" in self.subsystems:
                    continue
            path = os.path.join(self.paths[subsys], jobid)
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path = %s" %
                       (caller_name(), path))
            try:
                tasks_path = os.path.join(path, "tasks")
                for p in pids:
                    self.write_value(tasks_path, p, 'a')
            except IOError, exc:
                raise CgroupLimitError("Failed to add PIDs %s to %s (%s)" %
                                       (str(pids), tasks_path,
                                        errno.errorcode[exc.errno]))
            except:
                raise

    # Set a node limit
    def set_node_limit(self, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s = %s" %
                   (caller_name(), resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'],
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Node resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    def setup_job_devices_env(self):
        """ Setup the job environment for the devices assigned to the job for an
            execjob_launch hook
         """
        if 'devices_name' in self.host_assigned_resources:
            names = self.host_assigned_resources['devices_name']
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "devices: %s" % (names))
            offload_devices = list()
            cuda_visible_devices = list()
            for name in names:
                if name.startswith('mic'):
                    offload_devices.append(name[3:])
                elif name.startswith('nvidia'):
                    cuda_visible_devices.append(name[6:])
            if len(offload_devices) > 0:
                value = '"%s"' % string.join(offload_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['OFFLOAD_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "offload_devices: %s" % offload_devices)
            if len(cuda_visible_devices) > 0:
                value = '"%s"' % string.join(cuda_visible_devices, ",")
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Environment: %s" % pbs.event().env)
                pbs.event().env['CUDA_VISIBLE_DEVICES'] = value
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Post Add: %s" % pbs.event().env)
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "cuda_visible_devices: %s" % cuda_visible_devices)
            return [offload_devices, cuda_visible_devices]
        else:
            return False

    def setup_subsys_devices(self, path, node):
        subsys = 'devices'
        # Add the devices that the user is allowed to use
        if subsys in self.cfg['cgroup']:
            devices_allowed = open(os.path.join(path,
                                   "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Initial devices.list: %s" %
                       devices_allowed)
            # Deny access to mic and gpu devices
            devices_list = list()
            devices = node.devices
            for device in devices:
                if device == 'mic' or device == 'gpu':
                    for name in devices[device]:
                        d = devices[device][name]
                        devices_list.append("%d:%d" % (d['major'], d['minor']))
            # For CentOS 7 we need to remove a *:* rwm from devices.list we
            # before can add anything to devices.allow. Otherwise our changes
            # are ignored. Check to see if a *:* rwm is in devices.list.
            # If so remove it
            if "a *:* rwm\n" in devices_allowed:
                value = "a *:* rwm"
                self.write_value(os.path.join(path, 'devices.deny'), value)
            else:
                # Verify that the following devices are not in devices.list
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Removing access to the following: %s" %
                           devices_list)
            for device_str in devices_list:
                value = "c %s rwm" % device_str
                self.write_value(os.path.join(path, 'devices.deny'), value)
            # Add devices back to the list
            devices_allow = list()
            if 'allow' in self.cfg['cgroup'][subsys]:
                devices_allow = self.cfg['cgroup'][subsys]['allow']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Allowing access to the following: %s" %
                           devices_allow)
                for item in devices_allow:
                    if isinstance(item, str):
                        pbs.logmsg(pbs.EVENT_DEBUG3, "string item: %s" % item)
                        value = "%s" % item
                        self.write_value(os.path.join(
                                         path, 'devices.allow'), value)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "write_value: %s" % value)
                    elif isinstance(item, list):
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "list item: %s" % item)
                        try:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Device allow: %s" % item)
                            stat_filename = '/dev/%s' % item[0]
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Stat file: %s" % stat_filename)
                            s = os.stat(stat_filename)
                            device_type = None
                            if S_ISBLK(s.st_mode):
                                device_type = "b"
                            elif S_ISCHR(s.st_mode):
                                device_type = "c"
                            if device_type is not None:
                                if len(item) == 3 and isinstance(item[2], str):
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % item[2]
                                    value += "%s" % item[1]
                                else:
                                    value = "%s " % device_type
                                    value += "%s:" % os.major(s.st_rdev)
                                    value += "%s " % os.minor(s.st_rdev)
                                    value += "%s" % item[1]
                                self.write_value(os.path.join(
                                                path, 'devices.allow'), value)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "write_value: %s" % value)
                        except OSError:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unable to find /dev/%s. " % item[0] +
                                       "Not added to devices.allow!")
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                    else:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Not sure what to do with: %s" % item)
            devices_allowed = open(os.path.join(
                                   path, "devices.list")).readlines()
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "After setup devices.list: %s" % devices_allowed)

    # Select devices to assign to the job
    def assign_devices(
                       self,
                       device_type,
                       device_list,
                       number_of_devices,
                       node):
        devices = device_list[:number_of_devices]
        device_ids = list()
        device_names = list()
        for device in devices:
            device_info = node.devices[device_type][device]
            if len(device_ids) == 0:
                if device.find("mic") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:0 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                    device_ids.append("%s %d:1 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
                elif device.find("nvidia") != -1:
                    # Requires the ctrl (0) and the scif (1) to be added
                    device_ids.append("%s %d:255 rwm" %
                                      (device_info['device_type'],
                                       device_info['major']))
            device_ids.append("%s %d:%d rwm" %
                              (device_info['device_type'],
                               device_info['major'],
                               device_info['minor']))
            device_names.append(device)
        return device_names, device_ids

    # Find the device name
    def get_device_name(self, node, available, socket, major, minor):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Get device name: major: %s, minor: %s" % (major, minor))
        avail_device = None
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Possible devices: %s" % (available[socket]['devices']))
        for avail_device in available[socket]['devices']:
            avail_major = None
            avail_minor = None
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Checking device: %s" % (avail_device))
            if avail_device.find('mic') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check mic device: %s" % (avail_device))
                avail_major = node.devices['mic'][avail_device]['major']
                avail_minor = node.devices['mic'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            elif avail_device.find('nvidia') != -1:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Check gpu device: %s" % (avail_device))
                avail_major = node.devices['gpu'][avail_device]['major']
                avail_minor = node.devices['gpu'][avail_device]['minor']
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device major: %s, minor: %s" % (major, minor))
            if int(avail_major) == int(major) and \
                    int(avail_minor) == int(minor):
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Device match: name: %s, major: %s, minor: %s" %
                           (avail_device, major, minor))
                return avail_device
        pbs.logmsg(pbs.EVENT_DEBUG3, "No match found")
        return None

    # Assign resources to the job based off the vnodes assignments
    def assign_job_resources_by_vnode(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # Loop through the vnodes and assign resources
        assigned = dict()
        assigned['cpuset.cpus'] = list()
        assigned['cpuset.mems'] = list()
        room_on_socket = True

        for vnode in resources['vnodes']:
            regex = re.compile(".*\[(\d+)\].*")
            socket = int(regex.search(vnode).group(1))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current Vnode: %s" % vnode)
            pbs.logmsg(pbs.EVENT_DEBUG3, "Current socket: %d" % socket)

            if 'ncpus' in resources['vnodes'][vnode]:
                tmp_ncpus = int(resources['vnodes'][vnode]['ncpus'])
                if int(resources['vnodes'][vnode]['ncpus']) <= \
                        len(available[socket]['cpus']):
                    assigned['cpuset.cpus'] += \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] += [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_ncpus: %s" % (tmp_ncpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "available[%d]: %s" %
                               (socket, available[socket]))
                    room_on_socket = False
            if ('nmics' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['nmics']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(mic).*")
                tmp_nmics = int(resources['vnodes'][vnode]['nmics'])
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['nmics']) > 0 and
                   int(resources['vnodes'][vnode]['nmics']) <= len(mics)):
                    tmp_names, tmp_devices = self.assign_devices(
                             'mic', mics[:tmp_nmics], tmp_nmics, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3, "tmp_nmics: %s" % (tmp_nmics))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "mics: %s" % (mics))
                    room_on_socket = False
            if ('ngpus' in resources['vnodes'][vnode] and
               int(resources['vnodes'][vnode]['ngpus']) > 0):
                if 'devices_name' not in assigned:
                    assigned['devices_name'] = list()
                    assigned['devices'] = list()
                regex = re.compile(".*(nvidia).*")
                tmp_ngpus = int(resources['vnodes'][vnode]['ngpus'])
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if (int(resources['vnodes'][vnode]['ngpus']) > 0 and
                   int(resources['vnodes'][vnode]['ngpus']) <= len(gpus)):
                    tmp_names, tmp_devices = self.assign_devices(
                        'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                    assigned['devices_name'] += tmp_names
                    assigned['devices'] += tmp_devices
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficent room on socket: %d" % socket)
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "tmp_ngpus: %s" % (tmp_ngpus))
                    pbs.logmsg(pbs.EVENT_DEBUG3, "gpus: %s" % (gpus))
                    room_on_socket = False

        if room_on_socket:
            assigned['cpuset.cpus'].sort()
            assigned['cpuset.mems'].sort()
            if 'devices' in assigned:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids pre set and sort: %s" %
                           (assigned['devices']))
                assigned['devices'] = list(set(assigned['devices']))
                assigned['devices'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "device_ids post set and sort: %s" %
                           (assigned['devices']))
                assigned['devices_name'] = list(set(assigned['devices_name']))
                assigned['devices_name'].sort()
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

    # Assign resources to the job
    def assign_job_resources(self, resources, available, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Resources: %s" %
                   (caller_name(), resources))

        # Determine where the resources should be placed
        pbs.logmsg(pbs.EVENT_DEBUG3, "Resources: %s" % (resources))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Numa Nodes: %s" % (node.numa_nodes))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Devices: %s" % (node.devices))

        # What would we need to do different for a large smp (UV) system?
        # See if requested resources can fit on a single socket
        assigned = dict()
        pbs.logmsg(pbs.EVENT_DEBUG3, "Requested: %s" % (resources))

        # Determine the order that we should evaluate the sockets.
        socket_list = list()
        if 'placement_type' in self.cfg:
            if self.cfg['placement_type'] == 'round_robin':
                pbs.logmsg(pbs.EVENT_DEBUG3, "Requested round_robin placement")
                # Look at assigned_resources and determine which socket
                # to start with
                jobs_per_socket_cnt = dict()
                for socket in available.keys():
                    jobs_per_socket_cnt[socket] = 0
                for job in self.assigned_resources:
                    if 'cpuset.mems' in self.assigned_resources[job]:
                        for socket in \
                                self.assigned_resources[job]['cpuset.mems']:
                            jobs_per_socket_cnt[socket] += 1

                sorted_dict = sorted(jobs_per_socket_cnt.items(),
                                     key=operator.itemgetter(1))
                for x in sorted_dict:
                    socket_list.append(x[0])
        else:
            socket_list = available.keys()

        # Loop through the socket list and determine if the job
        # can fit on the socket
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Evaluate sockets in the following order: %s" %
                   (socket_list))
        for socket in socket_list:
            room_on_socket = True
            if 'ncpus' in resources:
                if int(resources['ncpus']) <= len(available[socket]['cpus']):
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Requested: %s, Available: %s" %
                               (resources['ncpus'], available[socket]['cpus']))
                    tmp_ncpus = int(resources['ncpus'])
                    assigned['cpuset.cpus'] = \
                        available[socket]['cpus'][:tmp_ncpus]
                    assigned['cpuset.mems'] = [socket]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient ncpus on socket %d: R:%d,A:%s" %
                               (socket, resources['ncpus'],
                                available[socket]['cpus']))
                    room_on_socket = False
            # Check to see that nmics has been requested and not equal to 0
            if 'nmics' in resources and int(resources['nmics']) > 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'nmics' in resources and int(resources['nmics']) > 0 \
                        and int(resources['nmics']) <= len(mics):
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient nmics on socket %d: R:%s,A:%s" %
                               (socket, resources['nmics'], mics))
                    room_on_socket = False
            # Check to see that ngpus has been requested and not equal to 0
            if 'ngpus' in resources and int(resources['ngpus']) > 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available[socket]['devices']
                        for m in [regex.search(l)] if m]
                if 'ngpus' in resources and int(resources['ngpus']) > 0 \
                        and int(resources['ngpus']) <= len(gpus):
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient gpus on socket %d: R:%s,A:%s" %
                               (socket, resources['ngpus'], gpus))
                    room_on_socket = False
            if 'vmem' in resources:
                if size_as_int(resources['vmem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient vmem on socket %d: R:%s,A:%s" %
                               (socket, resources['vmem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if 'mem' in resources:
                if size_as_int(resources['mem']) >= \
                        available[socket]['memory']:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Insufficient memory on socket %d: R:%s,A:%s" %
                               (socket, resources['mem'],
                                available[socket]['memory']))
                    room_on_socket = False
            if room_on_socket:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned Resources: %s" % (assigned))
                self.host_assigned_resources = assigned
                return assigned
        # If we made it here then the requested resources did not fit
        # on a single socket
        # Future: take distance into account when selecting sockets?
        # Combine available resources into a single list
        # This should work for a dual socket machine.
        # Needs additional work for machines with more than 2 sockets
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Unable to find requested resources: %s" % resources +
                   " on a socket. Checking the entire node")
        available_copy = copy.deepcopy(available)
        available_on_node = dict()
        socket_keys = available.keys()
        socket_keys.sort()
        available_on_node['sockets'] = socket_keys
        for socket in socket_keys:
            for key in available_copy[socket].keys():
                if isinstance(available[socket][key], int):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]
                if isinstance(available_copy[socket][key], str):
                    try:
                        if key in available_on_node is False:
                            available_on_node[key] = \
                                size_as_int(available_copy[socket][key])
                        else:
                            available_on_node[key] += \
                                size_as_int(available_copy[socket][key])
                    except:
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Unable to convert to int: %s" %
                                   (available_copy[socket][key]))

                if isinstance(available_copy[socket][key], list):
                    if key in available_on_node is False:
                        available_on_node[key] = available_copy[socket][key]
                    else:
                        available_on_node[key] += available_copy[socket][key]

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available on node: %s" % (available_on_node))
        assigned = dict()
        room_on_node = False
        if 'ncpus' in resources:
            if int(resources['ncpus']) <= len(available_on_node['cpus']):
                room_on_node = True
                tmp_ncpus = int(resources['ncpus'])
                assigned['cpuset.cpus'] = available_on_node['cpus'][:tmp_ncpus]
                assigned['cpuset.mems'] = available_on_node['sockets']
            if 'nmics' in resources and int(resources['nmics']) != 0:
                regex = re.compile(".*(mic).*")
                mics = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['nmics']) <= len(mics) and \
                        int(resources['nmics']) > 0:
                    tmp_nmics = int(resources['nmics'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'mic', mics[:tmp_nmics], tmp_nmics, node)
            if 'ngpus' in resources and int(resources['ngpus']) != 0:
                regex = re.compile(".*(nvidia).*")
                gpus = [m.group(0)
                        for l in available_on_node['devices']
                        for m in [regex.search(l)] if m]
                if int(resources['ngpus']) <= len(gpus) and \
                        int(resources['ngpus']) > 0:
                    tmp_ngpus = int(resources['ngpus'])
                    assigned['devices_name'], assigned['devices'] = \
                        self.assign_devices(
                            'gpu', gpus[:tmp_ngpus], tmp_ngpus, node)
        if room_on_node:
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "Assigned Resources: %s" % (assigned))
            self.host_assigned_resources = assigned
            return assigned
        return False

        # Consolidate resources if needed to place the job

    # Determine which resources are available
    def available_node_resources(self, node):
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "%s: Method called" % (caller_name()))

        available = copy.deepcopy(node.numa_nodes)
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Keys: %s" % (available[0].keys()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))
        for socket in available.keys():
            if 'cpus' in available[socket]:
                # Find the physical cores
                if available[socket]['cpus'].find('-') != -1 and \
                        node.hyperthreading:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Splitting %s at ','" %
                               (available[socket]['cpus']))
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'].split(',')[0])
                else:
                    available[socket]['cpus'] = cpus2list(
                        available[socket]['cpus'])
            if 'MemTotal' in available[socket]:
                # Find the memory on the socket in bites.
                # Remove the 'b' to simply math
                available[socket]['memory'] = size_as_int(
                    available[socket]['MemTotal'])

        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Available Pre device add: %s" % (available))
        pbs.logmsg(pbs.EVENT_DEBUG3,
                   "Node Devices: %s" % (node.devices.keys()))
        for device in node.devices.keys():
            pbs.logmsg(pbs.EVENT_DEBUG3,
                       "%s: Device Names: %s" %
                       (caller_name(), device))
            if device == 'mic' or device == 'gpu':
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Devices: %s" %
                           (node.devices[device].keys()))
                for device_name in node.devices[device].keys():
                    device_socket = \
                        node.devices[device][device_name]['numa_node']
                    if 'devices' not in available[device_socket]:
                        available[device_socket]['devices'] = list()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Device: %s, Socket: %s" %
                               (device, device_socket))
                    available[device_socket]['devices'].append(device_name)
            # pbs.logmsg(pbs.EVENT_DEBUG3,
            #            "Available on socket %s: %s" %
            #            (socket,available[socket]))
        pbs.logmsg(pbs.EVENT_DEBUG3, "Available: %s" % (available))

        # Remove all of the resources that are assigned to other jobs
        for job in self.assigned_resources.keys():
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            if (not job_to_be_ignored(job)):
                cpus = list()
                sockets = list()
                devices = list()
                memory = 0
                if 'cpuset.cpus' in self.assigned_resources[job]:
                    cpus = self.assigned_resources[job]['cpuset.cpus']
                if 'cpuset.mems' in self.assigned_resources[job]:
                    sockets = self.assigned_resources[job]['cpuset.mems']
                if 'devices.list' in self.assigned_resources[job]:
                    devices = self.assigned_resources[job]['devices.list']
                if 'memory.limit' in self.assigned_resources[job]:
                    memory = size_as_int(
                                self.assigned_resources[job]['memory.limit'])

                # Loop through the sockets and remove cpus that are
                # assigned to other cgroups
                for socket in sockets:
                    for cpu in cpus:
                        try:
                            available[socket]['cpus'].remove(cpu)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %d from %s" %
                                       (cpu, available[socket]['cpus']))

                if len(sockets) == 1:
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "Decrementing memory: %d by %d" %
                               (size_as_int(available[sockets[0]]['memory']),
                                memory))
                    available[sockets[0]]['memory'] -= memory

                # Loop throught the available sockets
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Assigned device to %s: %s" % (job, devices))
                for socket in available.keys():
                    for device in devices:
                        try:
                            # loop through know devices and see if they match
                            if len(available[socket]['devices']) != 0:
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Check device: %s" % (device))
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Available device: %s" %
                                           (available[socket]['devices']))
                                major, minor = device.split()[1].split(':')
                                avail_device = self.get_device_name(
                                                   node, available, socket,
                                                   major, minor)
                                pbs.logmsg(pbs.EVENT_DEBUG3,
                                           "Returned device: %s" %
                                           (avail_device))
                                if avail_device is not None:
                                    pbs.logmsg(pbs.EVENT_DEBUG3,
                                               "socket: %d,\t" % socket +
                                               "devices: %s,\t" %
                                               available[socket]['devices'] +
                                               "device to remove: %s" %
                                               (avail_device))
                                    available[socket]['devices'].remove(
                                        avail_device)
                        except ValueError:
                            pass
                        except:
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Unexpected error: %s" %
                                       sys.exc_info()[0])
                            pbs.logmsg(pbs.EVENT_DEBUG3,
                                       "Error removing %s from %s" %
                                       (device, available[socket]['devices']))
            # Added by Alexis Cousein on 24th of Jan 2016 to support
            # suspended jobs on nodes
            else:
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Job %s res not removed from host " % job +
                           "available res: suspended job")
        pbs.logmsg(pbs.EVENT_DEBUG, "Available resources: %s" % (available))
        return available

    # Set a job limit
    def set_job_limit(self, jobid, resource, value):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s: %s = %s" %
                   (caller_name(), jobid, resource, value))
        try:
            if resource == 'mem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'softmem':
                if 'memory' in self.subsystems and \
                        self.cfg['cgroup']['memory']['enabled']:
                    self.write_value(os.path.join(self.paths['memory'],
                                     jobid, 'memory.soft_limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'vmem':
                if 'memsw' in self.subsystems and \
                        self.cfg['cgroup']['memsw']['enabled']:
                    self.write_value(os.path.join(self.paths['memsw'],
                                     jobid, 'memory.memsw.limit_in_bytes'),
                                     size_as_int(value))
            elif resource == 'hpmem':
                if 'hugetlb' in self.subsystems and \
                        self.cfg['cgroup']['hugetlb']['enabled']:
                    self.write_value(glob.glob(os.path.join(
                                     self.paths['hugetlb'], jobid,
                                     'hugetlb.*MB.limit_in_bytes'))[0],
                                     size_as_int(value))
            elif resource == 'ncpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = self.select_cpus(path, value)
                    if cpus is None:
                        raise CgroupLimitError("Failed to configure cpuset.")
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
                    self.__copy_from_parent(os.path.join(self.paths['cpuset'],
                                            jobid, 'cpuset.mems'))
            elif resource == 'cpuset.cpus':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.cpus')
                    cpus = value
                    if cpus is None:
                        msg = "Failed to configure cpus in cpuset."
                        raise CgroupLimitError(msg)
                    cpus = ','.join(map(str, cpus))
                    self.write_value(path, cpus)
            elif resource == 'cpuset.mems':
                if 'cpuset' in self.subsystems and \
                        self.cfg['cgroup']['cpuset']['enabled']:
                    path = os.path.join(self.paths['cpuset'],
                                        jobid, 'cpuset.mems')
                    mems = value
                    if mems is None:
                        msg = "Failed to configure mems in cpuset."
                        raise CgroupLimitError(msg)
                    mems = ','.join(map(str, mems))
                    self.write_value(path, mems)
            elif resource == 'devices':
                if 'devices' in self.subsystems and \
                        self.cfg['cgroup']['devices']['enabled']:

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.allow')
                    devices = value
                    if devices is None:
                        msg = "Failed to configure device(s)."
                        raise CgroupLimitError(msg)
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Setting devices: %s for %s" % (devices, jobid))
                    for device in devices:
                        self.write_value(path, device)

                    path = os.path.join(self.paths['devices'],
                                        jobid, 'devices.list')
                    output = open(path, 'r').read()
                    pbs.logmsg(pbs.EVENT_DEBUG3,
                               "devices.list: %s" % output.replace("\n", ","))
            else:
                pbs.logmsg(pbs.EVENT_DEBUG, "%s: Job resource %s not handled" %
                           (caller_name(), resource))
        except:
            raise

    # Update resource usage for a job
    def update_job_usage(self, jobid, resc_used):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: resc_used = %s" %
                   (caller_name(), str(resc_used)))
        # Sort the subsystems so that we consistently look at the subsystems
        # in the same order every time
        self.subsystems.sort()
        for subsys in self.subsystems:
            path = os.path.join(self.paths[subsys], jobid)
            if subsys == "memory":
                max_mem = self.__get_max_mem_usage(path)
                if max_mem is None:
                    pbs.logjobmsg(jobid, "%s: No max mem data" %
                                  (caller_name()))
                else:
                    resc_used['mem'] = pbs.size(convert_size(max_mem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: mem=%s" %
                                  (caller_name(), resc_used['mem']))
                mem_failcnt = self.__get_mem_failcnt(path)
                if mem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No mem fail count data" %
                                  (caller_name()))
                else:
                    # Check to see if the job exceeded its resource limits
                    if mem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memory limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "memsw":
                max_vmem = self.__get_max_memsw_usage(path)
                if max_vmem is None:
                    pbs.logjobmsg(jobid, "%s: No max vmem data" %
                                  (caller_name()))
                else:
                    resc_used['vmem'] = pbs.size(convert_size(max_vmem, 'kb'))
                    pbs.logjobmsg(jobid,
                                  "%s: Memory usage: vmem=%s" %
                                  (caller_name(), resc_used['vmem']))

                vmem_failcnt = self.__get_memsw_failcnt(path)
                if vmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No vmem fail count data" %
                                  (caller_name()))
                else:
                    pbs.logjobmsg(jobid, "%s: vmem fail count: %d " %
                                  (caller_name(), vmem_failcnt))
                    if vmem_failcnt > 0:
                        err_msg = self.__get_error_msg(jobid)
                        pbs.logjobmsg(jobid,
                                      "Cgroup memsw limit exceeded: %s" %
                                      (err_msg))
            elif subsys == "hugetlb":
                max_hpmem = self.__get_max_hugetlb_usage(path)
                if max_hpmem is None:
                    pbs.logjobmsg(jobid, "%s: No max hpmem data" %
                                  (caller_name()))
                    return
                hpmem_failcnt = self.__get_hugetlb_failcnt(path)
                if hpmem_failcnt is None:
                    pbs.logjobmsg(jobid, "%s: No hpmem fail count data" %
                                  (caller_name()))
                    return
                if hpmem_failcnt > 0:
                    err_msg = self.__get_error_msg(jobid)
                    pbs.logjobmsg(jobid, "Cgroup hugetlb limit exceeded: %s" %
                                  (err_msg))
                resc_used['hpmem'] = pbs.size(convert_size(max_hpmem, 'kb'))
                pbs.logjobmsg(jobid, "%s: Hugepage usage: %s" %
                              (caller_name(), resc_used['hpmem']))
            elif subsys == "cpuacct":
                cpu_usage = self.__get_cpu_usage(path)
                if cpu_usage is None:
                    pbs.logjobmsg(jobid, "%s: No CPU usage data" %
                                  (caller_name()))
                    return
                cpu_usage = convert_time(str(cpu_usage) + "ns")
                pbs.logjobmsg(jobid, "%s: CPU usage: %.3lf secs" %
                              (caller_name(), cpu_usage))
                resc_used['cput'] = pbs.duration(cpu_usage)

    # Creates the cgroup if it doesn't exists
    def create_job(self, jobid, node):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        # Iterate over the subsystems required
        for subsys in self.subsystems:
            # Create a directory for the job
            try:
                old_umask = os.umask(0022)
                path = self.paths[subsys]
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                path = os.path.join(self.paths[subsys], jobid)
                if not os.path.exists(path):
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Creating directory %s" %
                               (caller_name(), path))
                    os.makedirs(path, 0755)
                if subsys == 'devices':
                    self.setup_subsys_devices(path, node)

            except OSError, exc:
                raise CgroupConfigError("Failed to create directory: %s (%s)" %
                                        (path, errno.errorcode[exc.errno]))
            except:
                raise
            finally:
                os.umask(old_umask)

    # Determine the cgroup limits and configure the cgroups
    def configure_job(self, jobid, hostresc, node):
        mem_enabled = 'memory' in self.subsystems
        vmem_enabled = 'memsw' in self.subsystems
        if mem_enabled or vmem_enabled:
            # Initialize mem variables
            mem_avail = node.get_memory_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "mem_avail %s" % mem_avail)
            mem_requested = None
            if 'mem' in hostresc.keys():
                mem_requested = convert_size(hostresc['mem'], 'kb')
            mem_default = None
            if mem_enabled:
                mem_default = self.default('memory')
            # Initialize vmem variables
            vmem_avail = node.get_vmem_on_node(self.cfg)
            pbs.logmsg(pbs.EVENT_DEBUG, "vmem_avail %s" % vmem_avail)
            vmem_requested = None
            if 'vmem' in hostresc.keys():
                vmem_requested = convert_size(hostresc['vmem'], 'kb')
            vmem_default = None
            if vmem_enabled:
                vmem_default = self.default('memsw')
            # Initialize softmem variables
            if 'soft_limit' in self.cfg['cgroup']['memory'].keys():
                softmem_enabled = self.cfg['cgroup']['memory']['soft_limit']
            else:
                softmem_enabled = False
            # Sanity check
            if size_as_int(mem_avail) > size_as_int(vmem_avail):
                if size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                    raise CgroupLimitError(
                        'mem available (%s) exceeds vmem available (%s)' %
                        (mem_avail, vmem_avail))
            # Determine the mem limit
            if mem_requested is not None:
                # mem requested may not exceed available
                if size_as_int(mem_requested) > size_as_int(mem_avail):
                    raise JobValueError(
                        'mem requested (%s) exceeds mem available (%s)' %
                        (mem_requested, mem_avail))
                mem_limit = mem_requested
            else:
                # mem was not requested
                if mem_default is None:
                    mem_limit = mem_avail
                else:
                    mem_limit = mem_default
            # Determine the vmem limit
            if vmem_requested is not None:
                # vmem requested may not exceed available
                if size_as_int(vmem_requested) > size_as_int(vmem_avail):
                    raise JobValueError(
                        'vmem requested (%s) exceeds vmem available (%s)' %
                        (vmem_requested, vmem_avail))
                vmem_limit = vmem_requested
            else:
                # vmem was not requested
                if vmem_default is None:
                    vmem_limit = vmem_avail
                else:
                    vmem_limit = vmem_default
            # Ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Adjust for soft limits if enabled
            if mem_enabled and softmem_enabled:
                softmem_limit = mem_limit
                # The hard memory limit is assigned the lesser of the vmem
                # limit and available memory
                if size_as_int(vmem_limit) < size_as_int(mem_avail):
                    mem_limit = vmem_limit
                else:
                    mem_limit = mem_avail
            # Again, ensure vmem is at least as large as mem
            if size_as_int(vmem_limit) < size_as_int(mem_limit):
                vmem_limit = mem_limit
            # Sanity checks when both memory and memsw are enabled
            if mem_enabled and vmem_enabled:
                if vmem_requested is not None:
                    if size_as_int(vmem_limit) > size_as_int(vmem_requested):
                        # The user requested an invalid limit
                        raise JobValueError(
                            'vmem limit (%s) exceeds vmem requested (%s)' %
                            (vmem_limit, vmem_requested))
                if size_as_int(vmem_limit) > size_as_int(mem_limit):
                    # This job may utilize swap
                    if size_as_int(vmem_avail) <= size_as_int(mem_avail) and \
                      size_as_int(mem_avail) - size_as_int(vmem_avail) > 10240:
                        # No swap available
                        raise CgroupLimitError(
                            ('Job might utilize swap ' +
                             'and no swap space available'))
            # Assign mem and vmem
            if mem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: mem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), mem_limit))
                hostresc['mem'] = pbs.size(mem_limit)
                if softmem_enabled:
                    hostresc['softmem'] = pbs.size(softmem_limit)
            if vmem_enabled:
                if mem_requested is None:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               ("%s: vmem not requested, " +
                                "assigning %s to cgroup") %
                               (caller_name(), vmem_limit))
                hostresc['vmem'] = pbs.size(vmem_limit)
        # Initialize hpmem variables
        hpmem_enabled = 'hugetlb' in self.subsystems
        if hpmem_enabled:
            hpmem_avail = node.get_hpmem_on_node(self.cfg)
            hpmem_limit = None
            hpmem_default = self.default('hugetlb')
            if hpmem_default is None:
                hpmem_default = hpmem_avail
            if 'hpmem' in hostresc.keys():
                hpmem_limit = convert_size(hostresc['hpmem'], 'kb')
            else:
                hpmem_limit = hpmem_default
            # Assign hpmem
            if size_as_int(hpmem_limit) > size_as_int(hpmem_avail):
                raise JobValueError('hpmem limit (%s) exceeds available (%s)' %
                                    (hpmem_limit, hpmem_avail))
            hostresc['hpmem'] = pbs.size(hpmem_limit)
        # Initialize cpuset variables
        cpuset_enabled = 'cpuset' in self.subsystems
        if cpuset_enabled:
            cpu_limit = 1
            if 'ncpus' in hostresc.keys():
                cpu_limit = hostresc['ncpus']
            if cpu_limit < 1:
                cpu_limit = 1
            hostresc['ncpus'] = pbs.pbs_int(cpu_limit)

        # Find the available resources and assign the right ones to the job

        attempt = 0
        assign_resources = False

        # Two attempts since self.cleanup_orphans may actually fix the
        # problem we see in a first attempt
        while attempt < 2:
            attempt += 1
            available_resources = self.available_node_resources(node)
            if 'vnodes' in hostresc:
                assign_resources = self.assign_job_resources_by_vnode(
                    hostresc, available_resources, node)
            else:
                assign_resources = self.assign_job_resources(
                    hostresc, available_resources, node)

            if (assign_resources is False) and (attempt < 2):
                # No resources were assigned to the job.
                # Most likely cause was that a cgroup has
                # not been cleaned up yet
                pbs.logmsg(pbs.EVENT_DEBUG, "Failed to assign resources. " +
                           "Checking the following cgroups on the node " +
                           "with the server: %s" % self.existing_cgroups)

                # Collect the jobs on the node (try reading mom_priv/jobs,
                # if not successful ask server)
                jobs_list = node.gather_jobs_on_node_local()

                if jobs_list is not None:
                    # Don't clean up the cgroup we just made for the
                    # job just yet
                    if jobid not in jobs_list:
                        jobs_list.append(jobid)

                    self.cleanup_orphans(jobs_list)

                    # Recheck what is now assigned after things were cleaned up
                    self.gather_assigned_resources()

        if assign_resources is False:
            # Cleanup cgroups for jobs not present on this node
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "Assign resources failed. Attempting to cleanup all " +
                       "leftover cgroups (including event job %s)" % jobid)

            # Remove the current job from the jobs list so it will be
            # cleaned up as well.
            jobs_list.remove(jobid)

            self.cleanup_orphans(jobs_list)

            # Rerun the job and log the message
            line = 'Unable to assign resources to job. '
            line += 'Will requeue the job and try again.'
            line += 'job run_count: %d' % pbs.event().job.run_count
            pbs.event().job.rerun()
            pbs.event().reject(line)

        # Print out the assigned resources
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "Assigned resources: %s" % (assign_resources))

        if cpuset_enabled:
            hostresc.pop('ncpus', None)
            for key in ['cpuset.cpus', 'cpuset.mems']:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Initialize devices variables
        devices_enabled = 'devices' in self.subsystems
        if devices_enabled:
            key = 'devices'
            if key in assign_resources:
                if key in assign_resources:
                    hostresc[key] = assign_resources[key]
                else:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "Key: %s not found in assign_resources" % key)

        # Apply the resource limits to the cgroups
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Setting cgroup limits for: %s" %
                   (caller_name(), hostresc))

        # The vmem limit must be set after the mem limit, so sort the keys
        for resc in sorted(hostresc.keys()):
            self.set_job_limit(jobid, resc, hostresc[resc])

    # Kill any processes contained within a tasks file
    def __kill_tasks(self, tasks_file):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        with open(tasks_file, 'r') as fd:
            for line in fd:
                os.kill(int(line.strip()), signal.SIGKILL)
        fd.close()
        count = 0
        with open(tasks_file, 'r') as fd:
            for line in fd:
                count += 1
                pid = line.strip()
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Pid: %s did not clean up" %
                           (caller_name(), pid))
                if os.path.isfile('/proc/%s/status' % pid):
                    tmp = open('/proc/%s/status' % pid).readlines()
                    tmp_line = ''
                    for entry in tmp:
                        if entry.find('Name:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('State:') != -1:
                            tmp_line += entry.strip() + ', '
                        if entry.find('Uid:') != -1:
                            tmp_line += entry.strip()
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: %s" % (caller_name(), tmp_line))

        return count

    # Perform the actual removal of the cgroup directory
    # by default, only one attempt at killing tasks in cgroup, without sleeping
    # since this method could be called many times
    # (for N directories times M jobs)
    def __remove_cgroup(self, path, tasks_kill_attempts=1):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        try:
            if os.path.exists(path):
                tasks_file = os.path.join(path, 'tasks')
                task_cnt = self.__kill_tasks(tasks_file)
                kill_attempts = 0
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Task Kill Attempts: %d" % tasks_kill_attempts)
                while (task_cnt > 0) and (kill_attempts < tasks_kill_attempts):
                    kill_attempts += 1
                    count = 0

                    with open(tasks_file, 'r') as fd:
                        for line in fd:
                            count += 1
                        task_cnt = count

                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "Attempt: %d - cgroup still has %d tasks: %s" %
                               (kill_attempts, task_cnt, path))
                    if kill_attempts < tasks_kill_attempts:
                        time.sleep(1)
                        # Added since there are race conditions where tasks are
                        # being added at the same time we get the initial
                        # task count
                        tasks_file = os.path.join(path, 'tasks')
                        task_cnt = self.__kill_tasks(tasks_file)

                if task_cnt == 0:
                    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Removing directory %s" %
                               (caller_name(), path))
                    os.rmdir(path)
                    if os.path.exists(path):
                        time.sleep(.25)
                        if os.path.exists(path):
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "%s: 2nd Try Removing directory %s" %
                                       (caller_name(), path))
                            os.rmdir(path)
                            time.sleep(.25)
                        if os.path.exists(path):
                            return False
                else:
                    pbs.logmsg(pbs.EVENT_SYSTEM,
                               "cgroup still has %d tasks: %s" %
                               (task_cnt, path))

                    # Check to see if the offline file is already present
                    # on the mom
                    offline_file_exists = False
                    if os.path.isfile(self.offline_file):
                        msg = "Cgroup(s) not cleaning up but the node already "
                        msg += "has the offline file."

                        pbs.logmsg(pbs.EVENT_DEBUG, msg)
                    else:
                        # Check to see that the node is not already offline.
                        try:
                            tmp_state = pbs.server().vnode(self.hostname).state
                        except:
                            msg = "Unable to contact server for node state"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            tmp_state = None
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Current Node State: %d" % tmp_state)
                        pbs.logmsg(pbs.EVENT_DEBUG3,
                                   "Offline Node State: %d" % pbs.ND_OFFLINE)

                        if tmp_state == pbs.ND_OFFLINE:
                            msg = "Cgroup(s) not cleaning up but the node is "
                            msg += "already offline."
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                        elif tmp_state is not None:
                            # Offline the node(s)
                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                            msg = "Offlining node since cgroup(s) are not "
                            msg += "cleaning up"
                            pbs.logmsg(pbs.EVENT_DEBUG, msg)
                            vnode = pbs.event().vnode_list[self.hostname]

                            vnode.state = pbs.ND_OFFLINE

                            # Write a file locally to reduce server traffic
                            # when it comes time to online the node
                            pbs.logmsg(pbs.EVENT_DEBUG,
                                       "Offline file: %s" % self.offline_file)
                            open(self.offline_file, 'a').close()
                            vnode.comment = self.offline_msg

                            pbs.logmsg(pbs.EVENT_DEBUG, self.offline_msg)
                        else:
                            pass

                    return False

        except OSError, exc:
            pbs.logmsg(pbs.EVENT_SYSTEM,
                       "Failed to remove cgroup path: %s (%s)" %
                       (path, errno.errorcode[exc.errno]))
        except:
            pbs.logmsg(pbs.EVENT_SYSTEM, "Failed to remove cgroup path: %s" %
                       (path))

        return True

    # Removes cgroup directories that are not associated with a local job
    def cleanup_orphans(self, local_jobs):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        orphan_cnt = 0
        for key in self.paths.keys():
            for dir in glob.glob(os.path.join(self.paths[key], '[0-9]*')):
                jobid = os.path.basename(dir)
                if jobid not in local_jobs:
                    pbs.logmsg(pbs.EVENT_DEBUG,
                               "%s: Removing orphaned cgroup: %s" %
                               (caller_name(), dir))
                    # default number of attempts at killing tasks
                    status = self.__remove_cgroup(dir)
                    if not status:
                        pbs.logmsg(pbs.EVENT_DEBUG,
                                   "%s: Removing orphaned cgroup %s failed " %
                                   (caller_name(), dir))
                        orphan_cnt += 1
        return orphan_cnt

    # Removes the cgroup directories for a job
    def delete(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))

        tasks_kill_attempted = False

        # Determine which subsystems will be used
        for subsys in self.subsystems:
            if not tasks_kill_attempted:
                # Make multiple attempts at killing tasks in cgroups on
                # first subsystem encountered since it is "normal" for
                # a cgroup.delete to encounter processes hard to kill
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                                           jobid),
                                              tasks_kill_attempts=(
                                              CGROUP_KILL_ATTEMPTS))
                tasks_kill_attempted = True
            else:
                # We tried getting rid of job processes earlier,
                # so don't try N times with sleeps
                status = self.__remove_cgroup(os.path.join(self.paths[subsys],
                                              jobid), tasks_kill_attempts=1)

            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Status: %s" %
                       (caller_name(), status))
            if status is False:
                # Rerun the job and log the message
                line = 'Unable to cleanup a cgroup. '
                line += 'Will offline the node and requeue the job '
                line += 'and try again. job run_count: %d' % \
                        pbs.event().job.run_count
                pbs.logmsg(pbs.EVENT_DEBUG,
                           "%s: Failed to remove cgroup: %s" %
                           (caller_name(), line))
                pbs.event().accept()

    # Write a value to a limit file
    def write_value(self, file, value, mode='w'):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: writing %s to %s" %
                   (caller_name(), value, file))
        try:
            with open(file, mode) as fd:
                fd.write(str(value) + '\n')
        except IOError, exc:
            if exc.errno == errno.ENOENT:
                pbs.logmsg(pbs.EVENT_SYSTEM,
                           "Trying to set limit to unknown file %s" % file)
            elif exc.errno in [errno.EACCES, errno.EPERM]:
                pbs.logmsg(pbs.EVENT_SYSTEM, "Permission denied on file %s" %
                           file)
            elif exc.errno == errno.EBUSY:
                raise CgroupBusyError("Limit rejected")
            elif exc.errno == errno.EINVAL:
                raise CgroupLimitError("Invalid limit value: %s" % (value))
            else:
                raise
        except:
            raise

    # Return memory failcount
    def __get_mem_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.failcnt"), 'r').read().strip())
        except:
            return None

    # Return vmem failcount
    def __get_memsw_failcnt(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.failcnt"), 'r').read().strip())
        except:
            return None

    # Return hpmem failcount
    def __get_hugetlb_failcnt(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.failcnt"))[0], 'r').read().strip())
        except:
            return None

    # Return the max usage of memory in bytes
    def __get_max_mem_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of memsw in bytes
    def __get_max_memsw_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "memory.memsw.max_usage_in_bytes"), 'r').read().strip())
        except:
            return None

    # Return the max usage of hugetlb in bytes
    def __get_max_hugetlb_usage(self, path):
        try:
            return int(open(glob.glob(os.path.join(path,
                       "hugetlb.*MB.max_usage_in_bytes"))[0],
                            'r').read().strip())
        except:
            return None

    # Return the cpuacct.usage in cpu seconds
    def __get_cpu_usage(self, path):
        try:
            return int(open(os.path.join(path,
                       "cpuacct.usage"), 'r').read().strip())
        except:
            return None

    # Assign CPUs to the cpuset
    def select_cpus(self, path, ncpus):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: path is %s" % (caller_name(), path))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: ncpus is %s" %
                   (caller_name(), ncpus))
        if ncpus < 1:
            ncpus = 1
        try:
            # Must select from those currently available
            cpufile = os.path.basename(path)
            base = os.path.dirname(path)
            parent = os.path.dirname(base)
            avail = cpus2list(open(os.path.join(parent, cpufile),
                              'r').read().strip())
            if len(avail) < 1:
                raise CgroupProcessingError("No CPUs avaialble in cgroup.")
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Available CPUs: %s" %
                       (caller_name(), avail))
            assigned = []
            for file in glob.glob(os.path.join(parent, '[0-9]*', cpufile)):
                cpus = cpus2list(open(file, 'r').read().strip())
                for id in cpus:
                    if id in avail:
                        avail.remove(id)
            if len(avail) < ncpus:
                raise CgroupProcessingError(
                    "Insufficient CPUs avaialble in cgroup.")
            if len(avail) == ncpus:
                return avail
            # FUTURE: Try to minimize NUMA nodes based on memory requirement
            return avail[0:ncpus]
        except:
            raise

    # Return the error message in system message file
    def __get_error_msg(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        proc = subprocess.Popen(['dmesg'],
                                shell=False, stdout=subprocess.PIPE)
        stdout = proc.communicate()[0].splitlines()
        stdout.reverse()
        # Check to see if the job id is found in dmesg otherwise
        # you could get the information for another job that was killed
        # the line will look like Memory cgroup stats for /pbspro/279.centos7
        kill_line = ""

        for line in stdout:
            start = line.find('Killed process ')
            if start >= 0:
                kill_line = line[start:]
            job_start = line.find(jobid)
            if job_start >= 0:
                # pbs.logmsg(pbs.EVENT_DEBUG3,
                #           "%s: Jobid: %s, Line: %s" %
                #           (caller_name(), jobid, line))
                return kill_line
        return ""

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_job_env_file(self, jobid, env_list):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        # if not os.path.exists(self.host_job_env_dir):
        #    os.makedirs(self.host_job_env_dir, 0755)

        # Write out assigned_resources
        try:
            lines = string.join(env_list, '\n')
            filename = self.host_job_env_filename % jobid
            outfile = open(filename, "w")
            outfile.write(lines)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" % (filename))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (lines))
            return True
        except:
            return False

    # Write out host cgroup assigned resources for this job
    def write_out_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        if not os.path.exists(self.hook_storage_dir):
            os.makedirs(self.hook_storage_dir, 0755)

        # Write out assigned_resources
        try:
            json_str = json.dumps(self.host_assigned_resources)
            outfile = open(self.hook_storage_dir+os.sep+jobid, "w")
            outfile.write(json_str)
            outfile.close()
            pbs.logmsg(pbs.EVENT_DEBUG3, "Wrote out file: %s" %
                       (self.hook_storage_dir+os.sep+jobid))
            pbs.logmsg(pbs.EVENT_DEBUG3, "Data: %s" % (json_str))
            return True
        except:
            return False

    def read_in_cgroup_host_assigned_resources(self, jobid):
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Method called" % (caller_name()))
        jobid = str(jobid)
        pbs.logmsg(pbs.EVENT_DEBUG3, "Host assigned resources: %s" %
                   (self.host_assigned_resources))
        if os.path.isfile(self.hook_storage_dir+os.sep+jobid):
            # Write out assigned_resources
            try:
                infile = open(self.hook_storage_dir+os.sep+jobid, 'r')
                json_data = json.load(infile, object_hook=decode_dict)
                self.host_assigned_resources = json_data
                pbs.logmsg(pbs.EVENT_DEBUG3,
                           "Host assigned resources: %s" %
                           (self.host_assigned_resources))
            except IOError:
                raise CgroupConfigError("I/O error reading config file")
            except json.JSONDecodeError:
                raise CgroupConfigError(
                    "JSON parsing error reading config file")
            except Exception:
                raise
            finally:
                infile.close()

        if self.host_assigned_resources is not None:
            return True
        else:
            return False

#
#
# FUNCTION main
#


def main():
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Function called" % (caller_name()))
    hostname = pbs.get_local_nodename()
    pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Host is %s" % (caller_name(), hostname))

    # Log the hook event type
    e = pbs.event()
    pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook name is %s" %
               (caller_name(), e.hook_name))

    try:
        # Instantiate the hook utility class
        hooks = HookUtils()
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Event type is %s" %
                   (caller_name(), hooks.event_name(e.type)))
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(hooks)))
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   "%s: Failed to instantiate hook utility class" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        e.accept()

    if not hooks.hashandler(e.type):
        # Bail out if there is no handler for this event
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: %s event not handled by this hook" %
                   (caller_name(), hooks.event_name(e.type)))
        e.accept()

    try:
        # If an exception occurs, jobutil must be set
        jobutil = None
        # Instantiate the job utility class first so jobutil can be accessed
        # by the exception handlers.
        if hasattr(e, 'job'):
            jobutil = JobUtils(e.job)
            pbs.logmsg(pbs.EVENT_DEBUG,
                       "%s: Job information class instantiated" %
                       (caller_name()))
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" %
                       (caller_name(), repr(jobutil)))
        else:
            pbs.logmsg(pbs.EVENT_DEBUG3, "%s: Event does not include a job" %
                       (caller_name()))
        # Instantiate the cgroup utility class
        vnode = None
        if hasattr(e, 'vnode_list'):
            if hostname in e.vnode_list.keys():
                vnode = e.vnode_list[hostname]
        cgroup = CgroupUtils(hostname, vnode)
        pbs.logmsg(pbs.EVENT_DEBUG, "%s: Cgroup utility class instantiated" %
                   (caller_name()))
        pbs.logmsg(pbs.EVENT_DEBUG3, "%s: %s" % (caller_name(), repr(cgroup)))
        # Bail out if there is nothing to do
        if len(cgroup.subsystems) < 1:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: All cgroups disabled" %
                       (caller_name()))
            e.accept()

        # Call the appropriate handler
        if hooks.invoke_handler(e, cgroup, jobutil) is True:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned success" %
                       (caller_name()))
            e.accept()
        else:
            pbs.logmsg(pbs.EVENT_DEBUG, "%s: Hook handler returned failure" %
                       (caller_name()))
            e.reject()

    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass

    except AdminError, exc:
        # Something on the system is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Admin error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except UserError, exc:
        # User must correct problem and resubmit job
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("User error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.delete()
                msg += " (deleted)"
            except:
                msg += " (delete failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except JobValueError, exc:
        # Something in PBS is misconfigured
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Job value error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

    except Exception, exc:
        # Catch all other exceptions and report them.
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
        msg = ("Unexpected error in %s handling %s event" %
               (e.hook_name, hooks.event_name(e.type)))
        if jobutil is not None:
            msg += (" for job %s" % (e.job.id))
            try:
                e.job.Hold_Types = pbs.hold_types("s")
                e.job.rerun()
                msg += " (suspended)"
            except:
                msg += " (suspend failed)"
        msg += (": %s %s" % (exc.__class__.__name__, str(exc.args)))
        pbs.logmsg(pbs.EVENT_ERROR, msg)
        e.reject(msg)

# line below required for unittesting
if __name__ == "__builtin__":
    start_time = time.time()
    try:
        main()
    except SystemExit:
        # The e.accept() and e.reject() methods generate a SystemExit
        # exception.
        pass
    except:
        pbs.logmsg(pbs.EVENT_DEBUG,
                   str(traceback.format_exc().strip().splitlines()))
    finally:
        pbs.logmsg(pbs.EVENT_DEBUG, "Elapsed time: %0.4lf" %
                   (time.time() - start_time))
---
2016-07-18 17:00:01,452 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups.json', 'content-encoding': 'default'}
2016-07-18 17:00:01,452 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups.json
2016-07-18 17:00:01,512 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups.json', 'content-encoding': 'default'}
2016-07-18 17:00:01,512 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups.json
2016-07-18 17:00:01,553 ERROR    err: ["hook 't1_l1' contents overwritten"]
2016-07-18 17:00:01,553 INFO     mom centos7: stopping MoM on host centos7.usa.org
2016-07-18 17:00:01,553 INFOCLI2 centos7: sudo -H kill -TERM 42976
2016-07-18 17:00:01,581 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 17:00:01,636 INFO     service: starting pbs_mom -d /var/spool/pbs
2016-07-18 17:00:01,636 INFOCLI  centos7: sudo -H /usr/local/sbin/pbs_mom -d /var/spool/pbs
2016-07-18 17:00:01,688 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 17:00:02,835 INFO     status on centos7: node
2016-07-18 17:00:02,835 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 17:00:02,849 INFO     status on centos7: node resources_available.vmem
2016-07-18 17:00:02,849 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 17:00:02,867 INFO     status on centos7: node resources_available.mem
2016-07-18 17:00:02,868 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 17:00:02,886 INFO     Mem-1: 1.8gb
2016-07-18 17:00:02,887 INFO     Swap-1: 3.8gb
2016-07-18 17:00:02,887 INFO     manager on centos7: import hook t1_l1 {'content-type': 'application/x-config', 'input-file': '/root/pbspro/test/tests/cgroups/pbs_cgroups2.json', 'content-encoding': 'default'}
2016-07-18 17:00:02,887 INFOCLI  centos7: sudo -H /usr/local/bin/qmgr -c import hook t1_l1 application/x-config default /root/pbspro/test/tests/cgroups/pbs_cgroups2.json
2016-07-18 17:00:02,921 ERROR    err: ["hook 't1_l1' contents overwritten"]
2016-07-18 17:00:02,921 INFO     mom centos7: stopping MoM on host centos7.usa.org
2016-07-18 17:00:02,921 INFOCLI2 centos7: sudo -H kill -TERM 60314
2016-07-18 17:00:02,959 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 17:00:03,026 INFO     service: starting pbs_mom -d /var/spool/pbs
2016-07-18 17:00:03,026 INFOCLI  centos7: sudo -H /usr/local/sbin/pbs_mom -d /var/spool/pbs
2016-07-18 17:00:03,069 INFOCLI2 centos7: sudo -H /usr/bin/cat /var/spool/pbs/mom_priv/mom.lock
2016-07-18 17:00:04,194 INFO     status on centos7: node resources_available.vmem
2016-07-18 17:00:04,194 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 17:00:04,207 INFO     status on centos7: node resources_available.mem
2016-07-18 17:00:04,207 INFOCLI  centos7: /usr/local/bin/pbsnodes -s centos7.usa.org -v -a
2016-07-18 17:00:04,224 INFO     Mem-2: 1.7gb
2016-07-18 17:00:04,224 INFO     Swap-2: 3.7gb
2016-07-18 17:00:04,224 INFO     ==============================
2016-07-18 17:00:04,224 INFO      Entered CgroupsTests tearDown
2016-07-18 17:00:04,225 INFO     ==============================
2016-07-18 17:00:04,225 INFO     ===============================
2016-07-18 17:00:04,225 INFO     Completed CgroupsTests tearDown
2016-07-18 17:00:04,225 INFO     ===============================
2016-07-18 17:00:04,225 INFO     ok

2016-07-18 17:00:04,226 INFO     
2016-07-18 17:00:04,226 INFO     ======================================================================
2016-07-18 17:00:04,226 INFO     FAILED: test_t1c (tests.cgroups_tests.CgroupsTests)

2016-07-18 17:00:04,226 INFO     ___m_oo_m___
2016-07-18 17:00:04,226 INFO     Traceback (most recent call last):
  File "/root/pbspro/test/tests/cgroups_tests.py", line 264, in test_t1c
    self.assertTrue(device in tmp_out)
AssertionError: False is not true


2016-07-18 17:00:04,226 INFO     ======================================================================
2016-07-18 17:00:04,226 INFO     FAILED: test_t4c (tests.cgroups_tests.CgroupsTests)

2016-07-18 17:00:04,226 INFO     ___m_oo_m___
2016-07-18 17:00:04,226 INFO     Traceback (most recent call last):
  File "/root/pbspro/test/tests/cgroups_tests.py", line 463, in test_t4c
    self.assertTrue(rv)
AssertionError: None is not true


2016-07-18 17:00:04,227 INFO     ================================================================================
failed: test_t1c (tests.cgroups_tests.CgroupsTests)
failed: test_t4c (tests.cgroups_tests.CgroupsTests)
Test cases with failures: CgroupsTests.test_t1c,CgroupsTests.test_t4c
Test suites with failures: CgroupsTests
run: 14, succeeded: 12, failed: 2, errors: 0, skipped: 0, timedout: 0
Tests run in 0:04:55.854761
2016-07-18 17:00:04,227 INFO     Cleaning up temporary files
